{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import csv\n",
    "\n",
    "es = Elasticsearch([{'host':'nyuvis-web.poly.edu', 'port': 80, 'url_prefix':'es'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '1001',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1001',\n",
       "     'paper_text': 'Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders Krogh\"\\nNordita\\nBlegdamsvej 17\\n2100 Copenhagen, Denmark\\n\\nJesper Vedelsby\\nElectronics Institute, Building 349\\nTechnical University of Denmark\\n2800 Lyngby, Denmark\\n\\nAbstract\\nLearning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity\\nis defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among\\nthe networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble\\ngeneralization error, and how this type of ensemble cross-validation\\ncan sometimes improve performance. It is shown how to estimate\\nthe optimal weights of the ensemble members using unlabeled data.\\nBy a generalization of query by committee, it is finally shown how\\nthe ambiguity can be used to select new training data to be labeled\\nin an active learning scheme.\\n\\n1\\n\\nINTRODUCTION\\n\\nIt is well known that a combination of many different predictors can improve predictions. In the neural networks community \"ensembles\" of neural networks has been\\ninvestigated by several authors, see for instance [1, 2, 3]. Most often the networks\\nin the ensemble are trained individually and then their predictions are combined.\\nThis combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .\\n.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk\\n\\n\\x0c232\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nAt the workshop after the last NIPS conference (December, 1993) an entire session\\nwas devoted to ensembles of neural networks ( \"Putting it all together\", chaired by\\nMichael Perrone) . Many interesting papers were given, and it showed that this area\\nis getting a lot of attention .\\nA combination of the output of several networks (or other predictors) is only useful\\nif they disagree on some inputs. Clearly, there is no more information to be gained\\nfrom a million identical networks than there is from just one of them (see also\\n[2]). By quantifying the disagreement in the ensemble it turns out to be possible\\nto state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the\\ndisagreement (called the ensemble ambiguity) and the generalization error is the\\nbasis for this paper, so we will derive it with no further delay.\\n\\n2\\n\\nTHE BIAS-VARIANCE TRADEOFF\\n\\nAssume the task is to learn a function J from RN to R for which you have a sample\\nof p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples\\nare assumed to be drawn randomly from the distribution p(x) . Anything in the\\nfollowing is easy to generalize to several output variables.\\nThe ensemble consists of N networks and the output of network a on input x is\\ncalled va (x). A weighted ensemble average is denoted by a bar , like\\n\\nV(x) =\\n\\nL Wa Va(x).\\n\\n(1)\\n\\na\\n\\nThis is the final output of the ensemble. We think of the weight Wa as our belief in\\nnetwork a and therefore constrain the weights to be positive and sum to one. The\\nconstraint on the sum is crucial for some of the following results.\\nThe ambiguity on input x of a single member of the ensemble is defined as aa (x)\\n(V a(x) - V(x))2 . The ensemble ambiguity on input x is\\n\\na(x)\\n\\n= Lwaaa(x) = LWa(va(x) a\\n\\nV(x))2 .\\n\\n=\\n\\n(2)\\n\\na\\n\\nIt is simply the variance of the weighted ensemble around the weighed mean, and\\nit measures the disagreement among the networks on input x. The quadratic error\\nof network a and of the ensemble are\\n\\n(J(x) - V a(x))2\\n(J(x) - V(X))2\\n\\n(3)\\n(4)\\n\\nrespectively. Adding and subtracting J( x) in (2) yields\\n\\na(x)\\n\\n=L\\n\\nWafa(X) - e(x)\\n\\n(5)\\n\\na\\n\\n(after a little algebra using that the weights sum to one) . Calling the weighted\\naverage of the individual errors ?( x) = La Wa fa (x) this becomes\\n\\ne(x)\\n\\n= ?(x) -\\n\\na(x).\\n\\n(6)\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\n233\\n\\nAll these formulas can be averaged over the input distribution . Averages over the\\ninput distribution will be denoted by capital letter, so\\n\\nJ dxp(xVl! (x)\\nJ dxp(x)aa(x)\\nJ dxp(x)e(x).\\n\\nE\\n\\n(7)\\n(8)\\n(9)\\n\\nThe first two of these are the generalization error and the ambiguity respectively\\nfor network n , and E is the generalization error for the ensemble. From (6) we then\\nfind for the ensemble generalization error\\n(10)\\nThe first term on the right is the weighted average of the generalization errors of\\nthe individual networks (E = La waEa), and the second is the weighted average\\nof the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.\\nThe beauty of this equation is that it separates the generalization error into a term\\nthat depends on the generalization errors of the individual networks and another\\nterm that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is\\nrequired of the real function to be approximated. The term \"unlabeled example\" is\\nborrowed from classification problems, and in this context it means an input x for\\nwhich the value of the target function f( x) is unknown.\\nEquation (10) expresses the tradeoff between bias and variance in the ensemble ,\\nbut in a different way than the the common bias-variance relation [4] in which the\\naverages are over possible training sets instead of ensemble averages. If the ensemble\\nis strongly biased the ambiguity will be small , because the networks implement very\\nsimilar functions and thus agree on inputs even outside the training set. Therefore\\nthe generalization error will be essentially equal to the weighted average of the\\ngeneralization errors of the individual networks. If, on the other hand , there is a\\nlarge variance , the ambiguity is high and in this case the generalization error will\\nbe smaller than the average generalization error . See also [5].\\nFrom this equation one can immediately see that the generalization error of the\\nensemble is always smaller than the (weighted) average of the ensemble errors,\\nE < E. In particular for uniform weights:\\n\\nE\\n\\n~ ~ \\'fEcx\\n\\n(11)\\n\\nwhich has been noted by several authors , see e.g. [3] .\\n\\n3\\n\\nTHE CROSS-VALIDATION ENSEMBLE\\n\\nFrom (10) it is obvious that increasing the ambiguity (while not increasing individual\\ngeneralization errors) will improve the overall generalization. We want the networks\\nto disagree! How can we increase the ambiguity of the ensemble? One way is to\\nuse different types of approximators like a mixture of neural networks of different\\ntopologies or a mixture of completely different types of approximators. Another\\n\\n\\x0c234\\n\\nAnders Krogh, Jesper Vedelsby\\n\\n.\\n\\n:~\\n\\n1. -\\n\\nt\\n\\n-\\n\\n,\\',\\n\\n.. ,\\n\\nE o...... -\\' \\'.- .. \\' ........ ....,.\\n\\n.\\'\\n\\n..... , ...\\n\\nv \\'. --:\\n\\n,\\n\\n.~.--c??\\n\\n__ .. -.tI\"\\n\\n.\\n\\n. -- - -\\\\\\\\\\n\\n\\'1\\n\\n-\\n\\n.~\\n\\n~.\\n\\n, . _ ? .\" ?\\n\\n.. - .....\\n\\n_._ ..... .\\'-._._.1\\n\\n,\\n\\n-\\n\\n>\\n\\n-\\n\\n-1.k!\\n~\\n\\n-4\\n\\n.t.\\n\\nf.\\n\\n1\\\\.1\\n\\n:\\\\,\\'. - ?-.l\\n\\n:--,____\\n..\\n\\n~~\\n.\\n\\n~.\\n\\n,\\n\\n,\\'\\n\\n-2\\n\\n.~\\n\\nIf\\n\\no\\n\\n2\\n\\n\\\\.\\n~\\n:\\n?\\n\\n\\' 0\\'\\n\\n~:\\n\\n4\\n\\nx\\n\\nFigure 1: An ensemble of five networks were trained to approximate the square\\nwave target function f(x). The final ensemble output (solid smooth curve) and\\nthe outputs of the individual networks (dotted curves) are shown. Also the square\\nroot of the ambiguity is shown (dash-dot line) _ For training 200 random examples\\nwere used, but each network had a cross-validation set of size 40, so they were each\\ntrained on 160 examples.\\n\\nobvious way is to train the networks on different training sets. Furthermore, to be\\nable to estimate the first term in (10) it would be desirable to have some kind of\\ncross-validation. This suggests the following strategy.\\nChose a number K :::; p. For each network in the ensemble hold out K examples for\\ntesting, where the N test sets should have minimal overlap, i. e., the N training sets\\nshould be as different as possible. If, for instance, K :::; piN it is possible to choose\\nthe K test sets with no overlap. This enables us to estimate the generalization error\\nE(X of the individual members of the ensemble, and at the same time make sure\\nthat the ambiguity increases . When holding out examples the generalization errors\\nfor the individual members of the ensemble, E(X, will increase, but the conjecture\\nis that for a good choice of the size of the ensemble (N) and the test set size\\n(K), the ambiguity will increase more and thus one will get a decrease in overall\\ngeneralization error.\\nThis conjecture has been tested experimentally on a simple square wave function\\nof one variable shown in Figure 1. Five identical feed-forward networks with one\\nhidden layer of 20 units were trained independently by back-propagation using 200\\nrandom examples. For each network a cross-validation set of K examples was held\\nout for testing as described above. The \"true\" generalization and the ambiguity were\\nestimated from a set of 1000 random inputs. The weights were uniform, w(X\\n1/5\\n(non-uniform weights are addressed later).\\n\\n=\\n\\nIn Figure 2 average results over 12 independent runs are shown for some values of\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\nFigure 2: The solid line shows the generalization error for uniform weights as\\na function of K, where K is the size\\nof the cross-validation sets. The dotted\\nline is the error estimated from equation (10) . The dashed line is for the\\noptimal weights estimated by the use of\\nthe generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.\\nThe bottom solid line is the generalization error one would obtain if the individual generalization errors were known\\nexactly (the best possible weights).\\n\\n0.08\\n\\n235\\n\\n,-----r----,--~---r-----,\\n\\no\\n\\nt=\\nw\\n0.06\\n\\nc\\n\\no\\n~\\n\\n.!::!\\n\\nco...\\n\\n~ 0.04\\n\\nQ)\\n\\n(!)\\n\\n0 .02 \\'---_---1_ _---\\'-_ _--\\'-_ _-----\\'\\no\\n20\\n40\\n60\\n80\\nSize of CV set\\n\\nK (top solid line) . First, one should note that the generalization error is the same\\nfor a cross-validation set of size 40 as for size 0, although not lower, so it supports\\nthe conjecture in a weaker form. However, we have done many experiments, and\\ndepending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,\\nonly ensembles with at least four converging networks out of five were used . If all\\nthe ensembles were kept, the error would have been significantly higher at ]{ = a\\nthan for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set\\nwas used. Thus it is still unclear under which circumstances one can expect a drop\\nin generalization error when using cross-validation in this fashion.\\n\\nThe dotted line in Figure 2 is the error estimated from equation (10) using the\\ncross-validation sets for each of the networks to estimate Ea, and one notices a\\ngood agreement.\\n\\n4\\n\\nOPTIMAL WEIGHTS\\n\\nThe weights Wa can be estimated as described in e.g. [3]. We suggest instead\\nto use unlabeled data and estimate them in such a way that they minimize the\\ngeneralization error given in (10) .\\nThere is no analytical solution for the weights , but something can be said about\\nthe minimum point of the generalization error. Calculating the derivative of E as\\ngiven in (10) subject to the constraints on the weights and setting it equal to zero\\nshows that\\nEa - Aa\\nE or Wa = O.\\n(12)\\n\\n=\\n\\n(The calculation is not shown because of space limitations, but it is easy to do.)\\nThat is, Ea - Aa has to be the same for all the networks. Notice that Aa depends\\non the weights through the ensemble average of the outputs. It shows that the\\noptimal weights have to be chosen such that each network contributes exactly waE\\n\\n\\x0c236\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nto the generalization error. Note, however, that a member of the ensemble can have\\nsuch a poor generalization or be so correlated with the rest of the ensemble that it\\nis optimal to set its weight to zero.\\nThe weights can be \"learned\" from unlabeled examples, e.g. by gradient descent\\nminimization of the estimate of the generalization error (10). A more efficient\\napproach to finding the optimal weights is to turn it into a quadratic optimization\\nproblem. That problem is non-trivial only because of the constraints on the weights\\n(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,\\nC af3\\n\\n=\\n\\nf\\n\\ndxp(x)V a (x)V f3 (x) .\\n\\n(13)\\n\\nThen, using that the weights sum to one, equation (10) can be rewritten as\\nE\\n\\n=\\n\\nL\\na\\n\\nwa Ea\\n\\n+ L w a C af3 w f3 - L\\naf3\\n\\nwaCaa .\\n\\n(14)\\n\\na\\n\\nHaving estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation\\nmatrix can be estimated from unlabeled data to any accuracy needed (provided that\\nthe input distribution p is known).\\nIn Figure 2 the results from an experiment with weight optimization are shown.\\nThe dashed curve shows the generalization error when the weights are optimized as\\ndescribed above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the\\nerrors Ea are known exactly, so it shows the lowest possible error. The performance\\nimprovement is quite convincing when the cross-validation estimates are used.\\nIt is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual\\nnetworks do not overfit, one might even use the training errors as estimates for\\nEa (see [3]). It is also possible to use some kind of regularization in (14), if the\\ncross-validation sets are small.\\n\\n5\\n\\nACTIVE LEARNING\\n\\nIn some neural network applications it is very time consuming and/or expensive\\nto acquire training data, e.g., if a complicated measurement is required to find the\\nvalue of the target function for a certain input. Therefore it is desirable to only use\\nexamples with maximal information about the function. Methods where the learner\\npoints out good examples are often called active learning.\\nWe propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by\\ncommittee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those\\npoints we would benefit the most from including in the training set.\\nSince the generalization error is always non-negative, we see from (6) that the\\nweighted average of the individual network errors is always larger than or equal to\\nthe ensemble ambiguity,\\nf(X) 2:: a(x),\\n(15)\\n\\n\\x0cNeural Network Ensembles. Cross Validation. and Active Learning\\n\\n237\\n\\n2.5 r\"\\':\":\\'T---r--\"T\"\"--.-----r---,\\n\\n.\\n\\n.\\n\\n.\\n\\n:\\n\\n0.5\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\nTraining set size\\n\\n40\\n\\n50\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\nTraining set size\\n\\nFigure 3: In both plots the full line shows the average generalization for active\\nlearning, and the dashed line for passive learning as a function of the number of\\ntraining examples. The dots in the left plot show the results of the individual\\nexperiments contributing to the mean for the active learning. The dots in right plot\\nshow the same for passive learning.\\n\\nwhich tells us that the ambiguity is a lower bound for the weighted average of the\\nsquared error. An input pattern that yields a large ambiguity will always have a\\nlarge average error. On the other hand, a low ambiguity does not necessarily imply\\na low error. If the individual networks are trained to a low training error on the\\nsame set of examples then both the error and the ambiguity are low on the training\\npoints. This ensures that a pattern yielding a large ambiguity cannot be in the close\\nneighborhood of a training example. The ambiguity will to some extent follow the\\nfluctuations in the error. Since the ambiguity is calculated from unlabeled examples\\nthe input-space can be scanned for these areas to any detail. These ideas are well\\nillustrated in Figure 1, where the correlation between error and ambiguity is quite\\nstrong, although not perfect.\\nThe results of an experiment with the active learning scheme is shown in Figure 3.\\nAn ensemble of 5 networks was trained to approximate the square-wave function\\nshown in Figure 1, but in this experiments the function was restricted to the interval\\nfrom - 2 to 2. The curves show the final generalization error of the ensemble in a\\npassive (dashed line) and an active learning test (solid line). For each training set\\nsize 2x40 independent tests were made, all starting with the same initial training\\nset of a single example. Examples were generated and added one at a time. In the\\npassive test examples were generated at random, and in the active one each example\\nwas selected as the input that gave the largest ambiguity out of 800 random ones.\\nFigure 3 also shows the distribution of the individual results of the active and\\npassive learning tests. Not only do we obtain significantly better generalization by\\nactive learning, there is also less scatter in the results. It seems to be easier for the\\nensemble to learn from the actively generated set.\\n\\n\\x0c238\\n\\n6\\n\\nAnders Krogh. Jesper Vedelsby\\n\\nCONCLUSION\\n\\nThe central idea in this paper was to show that there is a lot to be gained from\\nusing unlabeled data when training in ensembles. Although we dealt with neural\\nnetworks, all the theory holds for any other type of method used as the individual\\nmembers of the ensemble.\\nIt was shown that apart from getting the individual members of the ensemble to\\ngeneralize well, it is important for generalization that the individuals disagrees as\\nmuch as possible, and we discussed one method to make even identical networks\\ndisagree. This was done by training the individuals on different training sets by\\nholding out some examples for each individual during training. This had the added\\nadvantage that these examples could be used for testing, and thereby one could\\nobtain good estimates of the generalization error.\\nIt was discussed how to find the optimal weights for the individuals of the ensemble.\\nFor our simple test problem the weights found improved the performance of the\\nensemble significantly.\\n\\nFinally a method for active learning was described, which was based on the method\\nof query by committee developed for classification problems. The idea is that if the\\nensemble disagrees strongly on an input, it would be good to find the label for that\\ninput and include it in the training set for the ensemble. It was shown how active\\nlearning improves the learning curve a lot for a simple test problem.\\nAcknowledgements\\n\\nWe would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank\\nLars Kai Hansen for many discussions and great insights, and David Wolpert for\\nvaluable comments.\\n\\nReferences\\n[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.\\n[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.\\n[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method\\nfor neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image\\nprocessing. Chapman-Hall, 1993.\\n[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance\\ndilemma. Neural Computation, 4(1):1-58, Jan. 1992.\\n[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least\\nsquares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.\\n[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of\\nthe Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,\\nCA, 1992. Morgan Kaufmann.\\n[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query\\nby committee. In Advances in Neural Information Processing Systems, volume 5, San\\nMateo, California, 1993. Morgan Kaufmann.\\n\\n\\x0c',\n",
       "     'pdf_name': '1001-neural-network-ensembles-cross-validation-and-active-learning.pdf',\n",
       "     'title': 'Neural Network Ensembles, Cross Validation, and Active Learning',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1004',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1004',\n",
       "     'paper_text': 'ICEG Morphology Classification using an\\nAnalogue VLSI Neural Network\\n\\nRichard Coggins, Marwan Jabri, Barry Flower and Stephen Pickard\\nSystems Engineering and Design Automation Laboratory\\nDepartment of Electrical Engineering J03,\\nUniversity of Sydney, 2006, Australia.\\nEmail: richardc@sedal.su.oz.au\\n\\nAbstract\\nAn analogue VLSI neural network has been designed and tested\\nto perform cardiac morphology classification tasks. Analogue techniques were chosen to meet the strict power and area requirements\\nof an Implantable Cardioverter Defibrillator (ICD) system. The robustness of the neural network architecture reduces the impact of\\nnoise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer percept ron with on chip digital weight\\nstorage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit\\nat the output. The network was trained in loop and included a\\ncommercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better\\nthan 90% true positive and true negative detections for dangerous\\nrhythms which cannot be detected by present ICDs. The chip was\\nimplemented in 1.2um CMOS and consumes less than 200nW maximum average power in an area of 2.2 x 2.2mm2.\\n\\n1\\n\\nINTRODUCTION\\n\\nTo the present time, most ICDs have used timing information from ventricular\\nleads only to classify rhythms which has meant some dangerous rhythms can not\\nbe distinguished from safe ones, limiting the use of the device. Even two lead\\n\\n\\x0c732\\n\\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\n4.00\\n\\nHO\\n3.00\\n\\n2.00\\n\\nI.SO\\n\\n_ _ _:::::::!\\n\\nQ\\n1.00\\n\\nO.SO\\n\\nFigure 1: The Morphology of ST and VT retrograde 1:1.\\n\\natrial/ventricular systems fail to distinguish some rhythms when timing information alone is used [Leong and Jabri, 1992]. A case in point is the separation of Sinus Tachycardia (ST) from Ventricular Tachycardia with 1:1 retrograde conduction.\\nST is a safe arrhythmia which may occur during vigorous exercise and is characterised by a heart rate of approximately 120 beats/minute. VT retrograde 1:1 also\\noccurs at the same low rate but can be a potentially fatal condition. False negative\\ndetections can cause serious heart muscle injury while false positive detections deplete the batteries, cause patient suffering and may lead to costly transplantation\\nof the device. Figure 1 shows however, the way in which the morphology changes\\non the ventricular lead for these rhythms. Note, that the morphology change is\\npredominantly in the \"QRS complex\" where the letters QRS are the conventional\\nlabels for the different points in the conduction cycle during which the heart is\\nactually pumping blood.\\nFor a number of years, researchers have studied template matching schemes in order\\nto try and detect such morphology changes. However, techniques such as correlation\\nwaveform analysis [Lin et. al., 1988], though quite successful are too computationally intensive to meet power requirements. In this paper, we demonstrate that\\nan analogue VLSI neural network can detect such morphology changes while still\\nmeeting the strict power and area requirements of an implantable system. The\\nadvantages of an analogue approach are born out when one considers that an energy efficient analogue to digital converter such as [Kusumoto et. al., 1993] uses\\n1.5nJ per conversion implying 375nW power consumption for analogue to digital\\nconversion of the ICEG alone. Hence, the integration of a bucket brigade device and\\nanalogue neural network provides a very efficient way of interfacing to the analogue\\ndomain. Further, since the network is trained in loop with the ICD in real time,\\nthe effects of device offsets, noise, QRS detection jitter and signal distortion in the\\nanalogue circuits are largely alleviated.\\nThe next section discusses the chip circuit designs. Section 3 describes the method\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\n733\\n\\nAowAcId. . .\\n\\n1axl Syna.... AIRy\\n\\n\"-\\n\\nColumn\\nAoIcIr.-\\n\\nI\\n\\no.ta Reglsl...\\n\\nIClkcMmux\\n\\nI\\n\\nBu1I...\\n\\nI WTAI\\n\\n10 DOD DO\\n\\nFigure 2: Floor Plan and Photomicrograph of the chip\\nused to train the network for the morphology classification task. Section 4 describes\\nthe classifier performance on seven patients with arrhythmia which can not be\\ndistinguished using the heart rate only. Section 5 summarises the results, remaining\\nproblems and future directions for the work .\\n\\n2\\n\\nARCHITECTURE\\n\\nThe neural network chip consists of a 10:6:3 multilayer perceptron, an input bucket\\nbrigade device (BBD) and a winner take all (WTA) circuit at the output. A floor\\nplan and photomicrograph of the chip appears in figure 2. The BBD samples the\\nincoming ICEG at a rate of 250Hz. For three class problems, the winner take all\\ncircuit converts the winning class to a digital signal. For the two class problem\\nconsidered in this paper , a simple thresholding function suffices. The following\\nsubsections briefly describe the functional elements of the chip . The circuit diagrams\\nfor the chip building blocks appear in figure 3.\\n\\n2.1\\n\\nBUCKET BRIGADE DEVICE\\n\\nOne stage of the bucket brigade circuit is shown in figure 3. The BBD uses a\\ntwo phase clock to shift charge from cell to cell and is based on a design by\\nLeong [Leong, 1992] . The BBD operates by transferring charge deficits from S\\nto D in each of the cells. PHIl and PHI2 are two phase non-overlapping clocks.\\nThe cell is buffered from the synapse array to maintain high charge transfer efficiency. A sample and hold facility is provided to store the input on the gates of the\\nsynapses. The BBD clocks are generated off chip and are controlled by the QRS\\ncomplex detector in the lCD.\\n\\n2.2\\n\\nSYNAPSE\\n\\nThis synapse has been used on a number of neural network chips previously.\\ne.g . [Coggins et. al., 1994] . The synapse has five bits plus sign weight storage which\\n\\n\\x0c734\\n\\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\nNEURON\\n\\n.-----------------------------------------------------------,,,\\n,,\\n~ !\\nBUJIOIII\\'\\n\\n00\\n\\nBUCKET BRIGADE ClLL\\n\\n\"\\n\\nFigure 3: Neuron, Bucket Brigade and Synapse Circuit Diagrams.\\nsets the bias to a differential pair which performs the multiplication. The bias references for the weights are derived from a weighted current source in the corner of\\nthe chip. A four quadrant multiplication is achieved by the four switches at the top\\nof the differential pair.\\n\\n2.3\\n\\nNEURON\\n\\nDue to the low power requirements, the bias currents of the synapse arrays are of\\nthe order of hundreds of nano amps, hence the neurons must provide an effective\\nresistance of many mega ohms to feed the next synapse layer while also providing\\ngain control. Without special high resistance polysilicon, simple resistive neurons\\nuse prohibitive area, However, for larger networks with fan-in much greater than\\nten, an additional problem of common mode cancellation is encountered, That is,\\nas the fan-in increases, a larger common mode range is required or a cancellation\\nscheme using common mode feedback is needed.\\nThe neuron of figure 3 implements such a cancellation scheme, The mirrors MO/M2\\nand Ml/M3 divide the input current and facilitate the sum at the drain of M7.\\nM7/M8 mirrors the sum so that it may be split into two equal currents by the\\nmirrors formed by M4, M5 and M6 which are then subtracted from the input\\ncurrents. Thus, the differential voltage vp - Vm is a function of the transistor\\ntransconductances, the common mode input current and the feedback factor , The\\ngain of the neuron can be controlled by varying the width to length ratio of the\\nmirror transistors MO and Ml. The implementation in this case allows seven gain\\ncombinations, using a three bit RAM cell to store the gain,\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\n735\\n\\nImplantable\\nC.cio?erlor\\n\\nDefibrillalOr\\n\\nRunnngMUME\\n\\nNe .....1\\nNelwa\\'1<\\nChip\\n\\nFigure 4: Block Diagram of the Training and Testing System.\\nThe importance of a common mode cancellation scheme for large networks can\\nbe seen when compared to the straight forward approach of resistive or switched\\ncapacitor neurons. This may be illustrated by considering the energy usage of\\nthe two approaches. Firstly, we need to define the required gain of the neuron\\nas a function of its fan-in . If we assume that useful inputs to the network are\\nmostly sparse, i.e. with a small fraction of non-zero values, then the gain is largely\\nindependent of the fan-in, yet the common mode signal increases linearly with fanin. For the case of a neuron which does not cancel the common mode, the power\\nsupply voltage must be increased to accommodate the common mode signal, thus\\nleading to a quadratic increase in energy use with fan-in. A common mode cancelling\\nneuron on the other hand , suffers only a linear increase in energy use with fan-in\\nsince extra voltage range is not required and the increased energy use arises only\\ndue to the linear increase in common mode current.\\n\\n3\\n\\nTRAINING SYSTEM\\n\\nThe system used to train and test the neural network is shown in figure 4. Control\\nof training and testing takes place on the PC. The PC uses a PC-LAB card to\\nprovide analogue and digital I/O . The PC plays the ICEG signal to the input of\\nthe commercial ICD in real time. Note, that the PC is only required for initially\\ntraining the network and in this case as a source of the heart signal. The commercial\\nICD performs the function of QRS complex detection using analogue circuits. The\\nQRS complex detection signal is then used to freeze the BBD clocks of the chip, so\\nthat a classification can take place.\\nWhen training, a number of examples of the arrhythmia to be classified are selected\\nfrom a single patient data base recorded during an electrophysiological study and\\npreviously classified by a cardiologist. Since most of the morphological information\\nis in the QRS complex, only these segments of the data are repeatedly presented to\\n\\n\\x0c736\\n\\nRichard Coggins. Marwan Jabri. Barry Flower. Stephen Pickard\\n\\nPatient\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n% Training Attempts Converged\\nRun ~\\nRun 1\\n\\nH=3\\n80\\n80\\n0\\n60\\n100\\n100\\n80\\n\\nH= 6\\n10\\n100\\n0\\n10\\n80\\n40\\n100\\n\\nH=3\\n60\\n0\\n0\\n40\\n0\\n60\\n40\\n\\nH=6\\n60\\n10\\n10\\n40\\n60\\n60\\n100\\n\\nAverage\\nIterations\\n62\\n86\\n101\\n77\\n44\\n46\\n17\\n\\nTable 1: Training Performance of the system on seven patients.\\nthe network. The weights are adjusted according to the training algorithm running\\non the PC using the analogue outputs of the network to reduce the output error .\\nThe PC writes weights to the chip via the digital I/Os of the PC-LAB card and the\\nserial weight bus of network. The software package implementing the training and\\ntesting, called MUME [Jabri et. al ., 1992], provides a suite of training algorithms\\nand control options. Online training was used due to its success in training small\\nnetworks and because the presentation of the QRS complexes to the network was\\nthe slowest part of the training procedure. The algorithm used for weight updates\\nin this paper was summed weight node perturbation [Flower and Jabri, 1993].\\nThe system was trained on seven different patients separately all of whom had\\nVT with 1: 1 retrograde conduction. Note, that patient independent training has\\nbeen tried but with mixed results [Tinker, 1992] . Table 1 summarises the training\\nstatistics for the seven patients. For each patient and each architecture, five training\\nruns were performed starting from a different random initial weight set. Each\\nof the patients was trained with eight of each class of arrhythmia. The network\\narchitecture used was 10:H:1, where H is the number of hidden layer neurons and\\nthe unused neurons being disabled by setting their input weights to zero. Two sets\\nof data were collected denoted Run 1 and Run 2. Run 1 corresponded to output\\ntarget values of ?0.6V within margin 0.45V and Run 2 to output target values of\\n?0.2V within margin 0.05V. A training attempt was considered to have converged\\nwhen the training set was correctly classified within two hundred training iterations.\\nOnce the morphologies to be distinguished have been learned for a given patient,\\nthe remainder of the patient data base is played back in a continuous stream and\\nthe outputs of the classifier at each QRS complex are logged and may be compared\\nto the classifications of a cardiologist. The resulting generalisation performance is\\ndiscussed in the next section.\\n\\n4\\n\\nMORPHOLOGY CLASSIFIER GENERALISATION\\nPERFORMANCE\\n\\nTable 2 summarises the generalisation performance of the system on the seven\\npatients for the training attempts which converged. Most of the patients show a\\ncorrect classification rate better than 90% for at least one architecture on one of the\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\nPatient\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nNo. of\\nComplexes\\nST\\nVT\\n440\\n61\\n57\\n94\\n67\\n146\\n166\\n65\\n61\\n96\\n61\\n99\\n28\\n80\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n440\\n94\\n67\\n166\\n61\\n61\\n28\\n\\n61\\n57\\n146\\n65\\n96\\n99\\n80\\n\\n737\\n\\n% Correct Classifications Run 1\\nH = 6\\nH - i3\\nVT\\nST\\nST\\nVT\\n89?10 89?3\\n58?0\\n99?0\\n99?1\\n99?1\\n100?0 99?1\\n66?44 76?37\\n99?1\\n50?3\\n82?1 75?13\\n89?9\\n94?6\\n84?8\\n97?1\\n90?5\\n99?1\\n97?3\\n98?5\\n99?1\\n99?1\\n% Correct Classifications Run 2\\n86?14 99?1\\n88?2\\n99?1\\n94?6\\n94?3\\n84?2\\n99?1\\n76?18 59?2\\n87?7 100?0\\n88?2\\n49?5\\n84?1\\n82?5\\n92?6 90?10\\n99?1\\n99?1\\n94?3\\n99?0\\n94?3\\n92?3\\n\\nTable 2: Generalisation Performance of the system on seven patients.\\nruns, whereas, a timing based classifier can not separate these arrhythmia at all.\\nFor each convergent weight set the network classified the test set five times. Thus,\\nthe \"% Correct\" columns denote the mean and standard deviation of the classifier\\nperformance with respect to both training and testing variations. By duty cycling\\nthe bias to the network and buffers, the chip dissipates less than 200n W power for\\na nominal heart rate of 120 beats/minute during generalisation.\\n\\n5\\n\\nDISCUSSION\\n\\nReferring to table 1 we see that the patient 3 data was relatively difficult to train.\\nHowever, for the one occasion when training converged generalisation performance\\nwas quite acceptable. Inspection of this patients data showed that typically, the\\nmorphologies of the two rhythms were very similar. The choice of output targets,\\nmargins and architecture appear to be patient dependent and possibly interacting\\nfactors. Although larger margins make training easier for some patients they appear\\nto also introduce more variability in generalisation performance. This may be due\\nto the non-linearity of the neuron circuit. Further experiments are required to\\noptimise the architecture for a given patient and to clarify the effect of varying\\ntargets, margins and neuron gain. Penalty terms could also be added to the error\\nfunction to minimise the possibility of missed detections of the dangerous rhythm.\\nThe relatively slow rate of the heart results in the best power consumption being\\nobtained by duty cycling the bias currents to the synapses and the buffers. Hence,\\nthe bias settling time of the weighted current source is the limiting factor for reducing power consumption further for this design. By modifying the connection of the\\ncurrent source to the synapses using a bypassing technique to reduce transients in\\n\\n\\x0cRiclulrd Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\n738\\n\\nthe weighted currents, still lower power consumption could be achieved.\\n\\n6\\n\\nCONCLUSION\\n\\nThe successful classification of a difficult cardiac arrhythmia problem has been\\ndemonstrated using. an analogue VLSI neural network approach. Furthermore, the\\nchip developed has shown very low power consumption of less than 200n W, meeting the requirements of an implantable system. The chip has performed well, with\\nover 90% classification performance for most patients studied and has proved to be\\nrobust when the real world influence of analogue QRS detection jitter is introduced\\nby a commercial implantable cardioverter defibrillator placed in the signal path to\\nthe classifier.\\nAcknowledgements\\n\\nThe authors acknowledge the funding for the work in this paper provided under\\nAustralian Generic Technology Grant Agreement No. 16029 and thank Dr. Phillip\\nLeong of the University of Sydney and Dr. Peter Nickolls of Telectronics Pacing\\nSystems Ltd., Australia for their helpful suggestions and advice.\\nReferences\\n\\n[Castro et. al., 1993] H.A. Castro, S.M. Tam, M.A. Holler, \"Implementation and\\nPerformance of an analogue Nonvolatile Neural Network,\" Analogue Integrated\\nCircuits and Signal Processing, vol. 4(2), pp. 97-113, September 1993.\\n[Lin et. al., 1988] D. Lin, L.A. Dicarlo, and J .M. Jenkins, \"Identification of Ventricular Tachycardia using Intracavitary Electrograms: analysis of time and frequency domain patterns,\" Pacing (3 Clinical Electrophysiology, pp. 1592-1606,\\nNovember 1988.\\n[Leong, 1992] P.H.W. Leong, Arrhythmia Classification Using Low Power VLSI,\\nPhD Thesis, University of Sydney, Appendix B, 1992.\\n[ Kusumoto et. al., 1993] K. Kusumoto et. al., \"A lObit 20Mhz 30mW Pipelined\\nInterpolating ADC,\" ISSCC, Digest of Technical Papers, pp. 62-63, 1993.\\n[Leong and Jabri, 1992] P.H.W. Leong and M. Jabri, \"MATIC - An Intracardiac Tachycardia Classification System\", Pacing (3 Clinical Electrophysiology,\\nSeptember 1992.\\n[Coggins et. al., 1994] R.J. Coggins and M.A. Jabri, \"WATTLE: A Trainable Gain\\nAnalogue VLSI Neural Network\", NIPS6, Morgan Kauffmann Publishers, 1994.\\n[Jabri et. al., 1992] M.A. Jabri, E.A. Tinker and L. Leerink, \"MUME- A MultiNet-Multi-Architecture Neural Simulation Environment\", Neural Network Simulation Environments, Kluwer Academic Publications, January, 1994.\\n[Flower and Jabri, 1993] B. Flower and M. Jabri, \"Summed Weight Neuron Perturbation: an O(N) improvement over Weight Perturbation,\" NIPS5, Morgan\\nKauffmann Publishers, pp. 212-219, 1993.\\n[Tinker, 1992] E.A. Tinker, \"The SPASM Algorithm for Ventricular Lead Timing and Morphology Classification,\" SEDAL ICEG-RPT-016-92, Department of\\nElectrical Engineering, University of Sydney, 1992.\\n\\n\\x0c',\n",
       "     'pdf_name': '1004-iceg-morphology-classification-using-an-analogue-vlsi-neural-network.pdf',\n",
       "     'title': 'ICEG Morphology Classification using an Analogue VLSI Neural Network',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1006',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1006',\n",
       "     'paper_text': 'Real-Time Control of a Tokamak Plasma\\nUsing Neural Networks\\n\\nChris M Bishop\\nNeural Computing Research Group\\nDepartment of Computer Science\\nAston University\\nBirmingham, B4 7ET, U.K.\\nc.m .bishop@aston .ac .uk\\n\\nPaul S Haynes, Mike E U Smith, Tom N Todd,\\nDavid L Trotman and Colin G Windsor\\nAEA Technology, Culham Laboratory,\\nOxfordshire OX14 3DB\\n(Euratom/UKAEA Fusion Association)\\n\\nAbstract\\nThis paper presents results from the first use of neural networks\\nfor the real-time feedback control of high temperature plasmas in\\na tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen\\nplasmas, at temperatures of up to 100 Million K, are confined\\nby strong magnetic fields. Accurate control of the position and\\nshape of the plasma boundary requires real-time feedback control\\nof the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural\\nnetwork approach can give significantly better performance than\\nthe linear technique currently used on most tokamak experiments.\\nThe practical application of the neural network approach requires\\nhigh-speed hardware, for which a fully parallel implementation of\\nthe multilayer perceptron, using a hybrid of digital and analogue\\ntechnology, has been developed.\\n\\n\\x0c1008\\n\\n1\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nINTRODUCTION\\n\\nFusion of the nuclei of hydrogen provides the energy source which powers the sun.\\n\\nIt also offers the possibility of a practically limitless terrestrial source of energy.\\nHowever, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a\\nhigh temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the\\nRussian for \\'toroidal magnetic chamber\\') as illustrated schematically in Figure 1.\\nAt these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks\\nhad plasmas with circular cross-sections, for which feedback control of the plasma\\nposition and shape is relatively straightforward. However, recent tokamaks, such as\\nthe COMPASS experiment at Culham Laboratory, as well as most next-generation\\ntokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy\\nconfinement properties and thereby significantly enhance the performance of the\\ntokamak.\\n\\nz\\n\\nR\\n\\nFigure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma\\n(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded\\nas axisymmetric about the Z-axis, and so the plasma boundary can\\nbe described by its cross-sectional shape at one particular toroidal\\nlocation.\\nUnlike circular cross-section plasmas, highly non-circular shapes are more difficult to\\nproduce and to control accurately, since currents through several control coils must\\nbe adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape\\nmust evolve, usually from some initial near-circular shape. Due to uncertainties\\nin the current and pressure distributions within the plasma, the desired accuracy\\nfor plasma control can only be achieved by making real-time measurements of the\\nposition and shape of the boundary, and using error feedback to adjust the currents\\nin the control coils.\\nThe physics of the plasma equilibrium is determined by force balance between the\\n\\n\\x0c1009\\n\\nReal-Time Control of Tokamak Plasma Using Neural Networks\\n\\ncircle\\n\\nellipse\\n\\nO-shape\\n\\nbean\\n\\nFigure 2: Cross-sections of the COMPASS vacuum vessel showing\\nsome examples of potential plasma shapes. The solid curve is the\\nboundary of the vacuum vessel, and the plasma is shown by the\\nshaded regions.\\n\\nthermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms\\nof solutions of a non-linear partial differential equation called the Grad-Shafranov\\n(GS) equation. Due to the non-linear nature of this equation, a general analytic\\nsolution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the\\nexternal control coils which surround the vacuum vessel. On the tokamak itself it\\nis changes in these currents which are used to alter the position and cross-sectional\\nshape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used\\nto generate the training dataset for the neural network, as described in the next\\nsection. However , this approach is computationally very intensive and is therefore\\nunsuitable for feedback control purposes.\\nFor real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a\\nvariety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.\\nMost tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these\\nmagnetic signals collectively as a vector m .\\nFor a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,\\ngiven by\\n\\nR(B)\\nZ(B)\\n\\nRo + a cos(B + 8 sinB)\\nZo + a/\\\\,sinB\\n\\nwhere we have defined the following parameters\\n\\n(1)\\n\\n\\x0c1010\\n\\nRo\\nZo\\na\\nK\\n\\n6\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nradial distance of the plasma center from the major axis of the torus,\\nvertical distance of the plasma center from the torus midplane,\\nminor radius measured in the plane Z = Zo,\\nelongation,\\ntriangularity.\\n\\nWe denote these parameters collectively by Yk. The basic problem which has to be\\naddressed, therefore, is to find a representation for the (non-linear) mapping from\\nthe magnetic signals m to the values of the geometrical parameters Yk, which can\\nbe implemented in suitable hardware for real-time control.\\nThe conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical\\nparameters by a single linear transformation. However, the intrinsic non-linearity\\nof the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;\\nBishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the\\ncontrol loop for the neural network approach to tokamak equilibrium control.\\nNeural\\n\\nNetwork\\n\\nFigure 3: Block diagram of the control loop used for real-time\\nfeedback control of plasma position and shape.\\n\\n2\\n\\nSOFTWARE SIMULATION RESULTS\\n\\nThe dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base\\ncurrently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes\\nseveral minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled\\nwith the appropriate values of the shape parameters. Of the 120 magnetic signals\\navailable on COMPASS which could be used to provide inputs to the network, a\\n\\n\\x0c1011\\n\\nReal-Time Control o/Tokamak PLasma Using Neural Networks\\n\\nsubset of 16 has been chosen using sequential forward selection based on a linear\\nrepresentation for the mapping (discussed below) .\\nIt is important to note that the transformation from magnetic signals to flux surface\\nparameters involves an exact linear invariance. This follows from the fact that, if all\\nof the currents are scaled by a constant factor, then the magnetic fields will be scaled\\nby this factor, and the geometry of the plasma boundary will be unchanged . It is\\nimportant to take advantage of this prior knowledge and to build it into the network\\nstructure, rather than force the network to learn it by example. We therefore\\nnormalize the vector m of input signals to the network by dividing by a quantity\\nproportional to the total plasma current. Note that this normalization has to be\\nincorporated into the hardware implementation of the network, as will be discussed\\nin Section 3.\\n1.2\\n\\n4\\n01\\n\\n2\\n\\n2\\n\\n01\\n\\nc\\n\\nc\\n.5.\\n\\n0-\\n\\n~\\n:E\\n\\n.5.\\nCIS\\n\\n:E\\n\\n1iI\\n\\n~\\n::J\\n\\n?\\n\\n1iI\\nCD\\n\\n-2\\n\\ngo.8\\n\\n.5.\\n0-\\n\\n?\\n\\nCIS\\n\\n:E\\n\\n1iI 0 .4\\nCD\\n\\nc\\n::J\\n\\nc\\n\\n::J\\n\\n-2\\n\\n-4\\nDatabase\\n\\n?\\nDatabase\\n\\n1.2\\n\\n4\\n~\\n\\n~CD\\n\\nZ\\n\\n~\\n:::I\\n\\nCD\\n\\nz\\n\\n.2\\nDatabase\\n\\n2\\n\\n~O.8\\n~\\n\\n?\\n\\nCD\\n\\nz\\n\\n~O.4\\n\\n-2\\n\\n:::I\\n\\nCD\\n\\nZ\\n\\n?\\n\\n-4\\nDatabase\\n\\nDatabase\\n\\n.2\\nDatabase\\n\\nFigure 4: Plots of the values from the test set versus the values\\npredicted by the linear mapping for the 3 equilibrium parameters,\\ntogether with the corresponding plots for a neural network with 4\\nhidden units.\\n\\nThe results presented in this paper are based on a multilayer perceptron architecture\\nhaving a single layer of hidden units with \\'tanh\\' activation functions , and linear\\noutput units. Networks are trained by minimization of a sum-of-squares error using\\na standard conjugate gradients optimization algorithm, and the number of hidden\\n\\n\\x0cJ012\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nunits is optimized by measuring performance with respect to an independent test\\nset. Results from the neural network mapping are compared with those from the\\noptimal linear mapping, that is the single linear transformation which minimizes\\nthe same sum-of-squares error as is used in the neural network training algorithm,\\nas this represents the method currently used on a number of present day tokamaks .\\nInitial results were obtained on networks having 3 output units, corresponding to\\nthe values of vertical position ZQ, major radius RQ, and elongation K; these being\\nparameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.\\nBy comparison, the optimal linear mapping gave a normalized test set error of 18.3.\\nThis represents a reduction in error of about 30% in going from the linear mapping\\nto the neural network. Such an improvement, in the context of this application , is\\nvery significant.\\nFor the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we\\nconsider the results from this network in more detail. Figure 4 shows plots of the\\nnetwork predictions for various parameters versus the corresponding values from\\nthe test set portion of the database. Analogous plots for the optimal linear map\\npredictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,\\neven for this sub-optimal network topology.\\n\\n3\\n\\nHARDWARE IMPLEMENTATION\\n\\nThe hardware implementation of the neural network must have a bandwidth of 2:\\n20 kHz in order to cope with the fast timescales of the plasma evolution. It must\\nalso have an output precision of at least (the the analogue equivalent of) 8 bits in\\norder to ensure that the final accuracy which is attainable will not be limited by the\\nhardware system. We have chosen to develop a fully parallel custom implementation\\nof the multilayer perceptron, based on analogue signal paths with digitally stored\\nsynaptic weights (Bishop et al., 1993). A VME-based modular construction has\\nbeen chosen as this allows flexibility in changing the network architecture, ease of\\nloading network weights, and simplicity of data acquisition. Three separate types\\nof card have been developed as follows:\\n? Combined 16-input buffer and signal normalizer.\\nThis provides an analogue hardware implementation of the input normalization described earlier.\\n? 16 x 4 matrix multiplier\\nThe synaptic weights are produced using 12 bit frequency-compensated\\nmultiplying DACs (digital to analogue converters) which can be configured\\nto allow 4-quadrant multiplication of analogue signals by a digitally stored\\nnumber.\\n? 4-channel sigmoid module\\nThere are many ways to produce a sigmoidal non-linearity, and we have\\nopted for a solution using two transistors configured as along-tailed-pair,\\n\\n\\x0cReal-Time Control of Tokamak Plasma Using Neural Networks\\n\\n1013\\n\\nto generate a \\'tanh \\' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the\\nappearance of temperature in the denominator of the exponential transistor\\ntransfer characteristic. An elegant solution to this problem has been found\\nby exploiting a chip containing 5 transistors in close thermal contact. Two\\nof the transistors form the long-tailed pair, one of the transistors is used\\nas a heat source, and the remaining two transistors are used to measure\\ntemperature. External circuitry provides active thermal feedback control,\\nand stability to changes in ambient temperature over the range O?C to 50?C\\nis found to be well within the acceptable range.\\nThe complete network is constructed by mounting the appropriate combination\\nof cards in a VME rack and configuring the network topology using front panel\\ninterconnections. The system includes extensive diagnostics, allowing voltages at\\nall key points within the network to be monitored as a function of time via a series\\nof multiplexed output channels.\\n\\n4\\n\\nRESULTS FROM REAL-TIME FEEDBACK CONTROL\\n\\nFigure 5 shows the first results obtained from real-time control of the plasma in\\nthe COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time\\nduring a plasma pulse. Here the desired elongation has been preprogrammed to\\nfollow a series of steps as a function of time. The remaining 2 network outputs\\n(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,\\nbut were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the\\npost-shot reconstruction of the elongation obtained from a simple \\'filament\\' code,\\nwhich gives relatively rapid post-shot plasma shape reconstruction but with limited\\naccuracy. The circles denote the elongation values given by the much more accurate\\nreconstructions obtained from the full equilibrium code. The graph clearly shows\\nthe network generating the required elongation signal in close agreement with the\\nreconstructed values. The typical residual error is of order 0.07 on elongation values\\nup to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is\\ncurrently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units\\navailable with the initial hardware configuration. While these results represent the\\nfirst obtained using closed loop control, it is clear from earlier software modelling of\\nlarger network architectures (such as 32- 16-4) that residual errors of order a few %\\nshould be attainable. The implementation of such larger networks is being persued,\\nfollowing the successes with the smaller system.\\nAcknowledgements\\nWe would like to thank Peter Cox, Jo Lister and Colin Roach for many useful\\ndiscussions and technical contributions. This work was partially supported by the\\nUK Department of Trade and Industry.\\n\\n\\x0cC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\n1014\\n\\n1.8\\nshot 9576\\n\\nc:\\n\\no\\n\\n~\\n14\\nC)\\n?\\nc:\\n\\no\\n\\nas\\n1.0\\n0.0\\n\\n0.1\\n\\n0.2\\n\\ntime (sec.)\\nFigure 5: Plot of the plasma elongation K. as a function of time\\nduring shot no. 9576 on the COMPASS tokamak, during which the\\nelongation was being controlled in real-time by the neural network.\\n\\nReferences\\n\\nBishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman\\nD L, 1992. A neural network approach to tokamak equilibrium control. In Neural\\nNetwork Applications, Ed. J G Taylor, Springer Verlag, 114-128.\\nBishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.\\n1993. Hardware implementation of a neural network for plasma position control in\\nCOMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,\\nItaly. 2 997-1001.\\nLagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi\\nM, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time\\ncalculations of plasma equilibrium parameters for PBX-M, In Proceedings of the\\n17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.\\nLister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma\\nparameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.\\n\\n\\x0cPulsestream Synapses with Non-Volatile\\nAnalogue Amorphous-Silicon Memories.\\n\\nA.J. Holmes, A.F. Murray, S. Churcher and J. Hajto\\nDepartment of Electrical Engineering\\nUniversity of Edinburgh\\nEdinburgh, EH9 3JL\\nM. J. Rose\\nDept. of Applied Physics and Electronics,\\nDundee University\\nDundee DD14HN\\n\\nAbstract\\nA novel two-terminal device, consisting of a thin lOooA layer of p+\\na-Si:H sandwiched between Vanadium and Chromium electrodes,\\nexhibits a non-volatile, analogue memory action. This device stores\\nsynaptic weights in an ANN chip, replacing the capacitor previously\\nused for dynamic weight storage. Two different synapse designs are\\ndiscussed and results are presented.\\n\\n1\\n\\nINTRODUCTION\\n\\nAnalogue hardware implementations of neural networks have hitherto been hampered by the lack of a straightforward (local) analogue memory capability. The\\nideal storage mechanism would be compact, non-volatile, easily reprogrammable,\\nand would not interfere with the normal silicon chip fabrication process.\\nTechniques which have been used to date include resistors (these are not generally\\nreprogrammable, and suffer from being large and difficult to fabricate with any accuracy), dynamic capacitive storage [4] (this is compact, reprogrammable and simple,\\nbut implies an increase in system complexity, arising from off-chip refresh circuitry),\\n\\n\\x0c764\\n\\nA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\nEEPROM (\"floating gate\") memory [5] (which is compact, reprogrammable, and\\nnon-volatile, but is slow, and cannot be reprogrammed in situ), and local digital\\nstorage (which is non-volatile, easily programmable and simple, but consumes area\\nhorribly).\\nAmorphous silicon has been used for synaptic weight storage [1, 2], but only as\\neither a high-resistance fixed weight medium or a binary memory.\\nIn this paper, we demonstrate that novel amorphous silicon memory devices can be\\nincorporated into standard CMOS synapse circuits, to provide an analogue weight\\nstorage mechanism which is compact, non-volatile, easily reprogrammable, and simple to implement.\\n\\n2\\n\\na-Si:H MEMORY DEVICES\\n\\nThe a-Si:H analogue memory device [3] comprises a lOooA thick layer of amorphous\\nsilicon (p+ a-Si:H) sandwiched between Vanadium and Chromium electrodes.\\nThe a-Si device takes the form of a two-terminal, programmable resistor. It is an\\n\"add-on\" to a conventional CMOS process, and does not demand that the normal\\nCMOS fabrication cycle be disrupted. The a-Si device sits on top of the completed\\nchip circuitry, making contact with the CMOS arithmetic elements via holes cut in\\nthe protective passivation layer, as shown in Figure 1.\\n\\nCMOS Passivation\\nFigure 1: The construction of a-Si:H Devices on a CMOS chip\\nAfter fabrication a number of electronic procedures must be performed in order to\\nprogram the device to a given resistance state.\\nProgramming, and Pre-Programming Procedures\\n\\nBefore the a-Si device is usable, the following steps must be carried out:\\n? Forming: This is a once-only process, applied to the a-Si device in its\\n\"virgin\" state, where it has a resistance of several MO. A series of 300ns\\npulses, increasing in amplitude from 5v to 14v, is applied to the device\\nelectrodes. This creates a vertical conducting channel or filament whose\\napproximate resistance is 1KO. This filament can then be programmed to\\na value in the range lKO to 1 MO . The details of the physical mechanisms\\nare not yet fully established, but it is clear that conduction occurs through\\na narrow (sub-micron) conducting channel.\\n\\n\\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\\n\\n765\\n\\n? Write: To decrease the device\\'s resistance, negative \"Write\", pulses are\\napplied.\\n? Erase: To increase the device\\'s resistance, positive\" Erase\" , pulses are applied.\\n? Usage: Pulses below O.5v do not change the device resistance. The resistance can therefore be utilised as a weight storage medium using a voltage\\nof less than O.5v without causing reprogramming.\\nProgramming pulses, which range between 2v and 5v, are typically 120ns in duration. Programming is therefore much faster than for other EEPROM (floating\\ngate) devices used in the same context, which use a series of 100jls pulses to set the\\nthreshold voltage [5].\\nThe following sections describe synapse circuits using the a-Si:H devices. These\\nsynapses use the reprogrammable a-Si:H resistor in the place of a storage capacitor\\nor EEPROM cell. These new synapses were implemented on a chip referred to as\\nASiTEST2, consisting of five main test blocks, each comprising of four synapses\\nconnected to a single neuron.\\n\\n3\\n\\nThe EPSILON based synapse\\n\\nThe first synapse to be designed used the a-Si:H resistor as a direct replacement for\\nthe storage capacitor used in the EPSILON [4] synapse.\\n\\n+Sv\\n\\nNeuron\\n\\n1\\nI\\n\\nV\\n\\n..\\n\\nt.\\n\\n~:l:\\n\\n><!:\\n\\n~\\n\\nMirror Set\\n\\nE\\n\\n30\\n\\na-Si => Vw\\n\\nCircuitry\\n\\nOriginal\\nStorage\\nCapacitor\\n\\nO.5v\\n\\n<0-----------.,...\\n__\\n\\n...\\n\\nEPSILON Synapse\\n\\nFigure 2: The EPSILON Synapse with a-Si:H weight storage\\n\\nIn the original EPSILON chip the weight voltage was stored as a voltage on a\\ncapacitor. In this new synapse design, shown in Figure 2, the a-Si:H resistance is\\nset such that the voltage drop produced by Iset is equivalent to the original weight\\nvoltage, Vw, that was stored dynamically on the capacitor.\\nA new, simpler, synapse, which can be operated from a single +5v supply, was also\\nbe included on the ASiTEST2 chip.\\n\\n\\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\n766\\n\\n4\\n\\nThe MkII synapse\\n\\nThe circuit is shown in Figure 3. The a-Si:H memory is used to store a current,\\nIasi. This current is subtracted from a zero current, Isy...:z\" to give a weight current\\n, +/-Iw, which adds or subtracts charge from the activity capacitor, Cact, thus\\nimplementing excitation or inhibition respectively.\\nFor the circuit to function correctly we must limit the voltage on the activity capacitor to the range [1.5v,3.5v], to ensure that the transistors mirroring Isy_z and\\nIasi remain in saturation. As Figure 3 shows, there are few reference signals and\\nthe circuit operates from a single +5v power supply rail, in sharp contrast to many\\nearlier analogue neural circuits, including our own.\\n\\n1v\"\"\\n\\n+5vPWm\\n\\nII .\\n\\n~\\n\\n881\\n\\nVsel\\n\\n.r--\\\\.\\n\\n-.L\\n\\n....L\\n\\n*\\n\\nComparator\\nPWout\\n..rL\\n\\nCact\\n\\nVramp\\n\\n~\\n\\nOv\\nE\\n;.\\nMirror Set\\n\\n\"\\'E~-----------:~~\\n\\nE\\n\\nSynapse\\n\\nPower Supplies\\nV5_0=5.Ov\\n\\nReferences\\nVrstv?2.5v\\nIsy_z=5uA\\n\\n;.\\n\\nNeuron\\n\\nTail Currents\\n\\nIneu=4uA\\n\\nFigure 3: The MkII synapse\\n\\nOn first inspection the main drawback of this design would appear to be a reliance\\non the accuracy with which the zero current Isy...:z, is mirrored across an entire chip.\\nThe variation in this current means that two cells with the same synapse resistance\\ncould produce widely differing values of Iw. However, during programming we\\ndo not use the resistance of the a-Si:H device as a target value. We monitor the\\nvoltage on Cact for a given PWin signal, increasing or decreasing the resistance\\nof the a-Si:H device until the desired voltage level is achieved.\\nExample: To set a weight to be the maximum positive value, we adjust the a-Si\\nresistance until a PWin signal of 5us, the maximum input signal, gives a voltage of\\n3.5v on the integration capacitor.\\nWe are able to set the synapse weight using the whole integration range of [1.5v,3.5v]\\nby only closing Vsel for the desired synapse during programming. In normal operating mode all four Vsel switches will be closed so that the integration charge is\\nsummed over all four local capacitors.\\n\\n\\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\\n\\n4.1\\n\\n767\\n\\nExample - Stability Test\\n\\nAs an example of the use of integration voltage as means of monitoring the resistance\\nof a particular synapse we have included a stability test. This was carried out on\\none of the test chips which contained the MkII synapse.\\nThe four synapses on the test chip were programmed to give different levels of\\nactivation. The chip was then powered up for 30mins each day during a 7-day\\nperiod, and the activation levels for each synapse were measured three times.\\n3.5\\n\\nStability Test - PWin = 3us\\n\\n,----~-_._--;--__..___r...:....,.-_._,.__-,.--.,..,.-__..__:___.,\\n\\ntestl\\n\\ntest2\\n\\ntest4\\n\\ntest3\\n\\ntestS\\n\\ntest7\\n\\ntest6\\n\\n3\\n\\n?\\n,.\\nI\\n\\nt:\\'\"\\n-~\\n\\n1:1\\n?\\n\\n\\'t\\n\\n,\\n\\n?\\n\\n~\\n\\n?\\n\\n- ~ -:- - ~ -:- -\\n\\n25\\n?\\n\\n2\\n\\n?\\n\\n~.\\n\\n?\\n\\n-:- - - . of - ~-:- -\\n\\n..\\n\\n.\\n\\nI\\n\\nI\\n\\n?\\n\\n?\\n\\n{,o-:- - .\\n- ~-s4\\n.\\n\\n,\\n.\\n.\\n\\'.\\n.?\\n.?\\n.?\\n.?\\n- - ~ - - ~ - - --:- - - ~ - - -~- - - w. -- --r s2\\n- - - .;. \\'\" - -: -011>- - :.. ~ - ~ -oGii - -:- - - - i-- ~ - ..; -sl\\n?\\n.\\n.\\n.\\n.\\n.\\n?\\n\\n?\\n\\n~ - - ~ - - -~- - - ~ - ?\\n\\nI\\n\\n?\\n\\nI\\nI\\n\\n,\\nI\\n\\n?\\n?\\n\\n?\\n\\n,\\n\\n?\\n?\\nI\\n\\nI\\n?\\n?\\n\\n10\\n\\n20\\n\\n30\\n\\n-.\\nL---L- -- -~\\n\\n~ -s3\\n\\n,\\n\\n?\\n\\n,\\n,\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n,\\n\\nI\\n\\n,\\n\\n?\\n\\n?\\nI\\n:\\n\\nI\\n?\\n:\\n\\n?\\nI\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n\\n40\\n\\n50\\n\\n60\\n\\n,\\n\\n70\\n\\n80\\n\\n?\\n\\n90\\n\\nMeasurement Index\\n\\nFigure 4: ASiTEST2- Stability Test\\nAs figure 4 shows, the memories remain in the same resistance state (i.e retain their\\nprogrammed weight value) over the whole 7-day period. Separate experiments on\\nisolated devices indicate much longer hold times - of the order of months at least.\\n\\n5\\n\\nASiTEST3\\n\\nRecently we have received our latest, overtly neural, a-Si:H based test chip. This\\ncontains an 8x8 array of the MkII synapses.\\nThe circuit board for this device has been constructed and partially tested while\\nthe ASiTEST3 chips are awaiting the deposition of the a-Si:H layers. We have been\\nable to use an ASiTEST2 chip containing two of the MkII synapse test blocks i.e.\\n8 synapses and 2 neurons to exercise much of the board\\'s functionality.\\nThe test board contains a simple state machine which has four different states:\\n? State 0: Load Input Pulsewidths into SRAM from PC.\\n? State 1: Apply Input Pulsewidth signals to chipl.\\n? State 2: Use Vramp to generate threshold function for chipl. The resulting\\nPulsewidth outputs are used as the inputs to chip2, as well as being stored\\n\\n\\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\n768\\n\\nin SRAM .\\n? State 3: Use Vramp to generate threshold function for chip2. Read resulting\\nPulsewidth Outputs into SRAM .\\n? State 0: Read Output Pulsewidths from SRAM into PC.\\nThe results obtained during a typical test cycle are shown in Figure 5.\\nIE-- Statel --;,,;,,*1E E - - State2\\n\\n-----;>~i~\\n\\nState3 ---;!>~I\\n\\n~r-IF~~~--r-------~---------l\\n4v\\n3v\\n\\nPWin_O\\n\\n2v\\nIv\\n\\nOv~ . . . . . . . . .\\n\\n3.5v r;;;;;\"\"-~,\"\",,,or;:::::;;:;::;;;;:;;;::;;t---~----t--------,\\n\\n~:~\\n\\n....... ~;a~;\"\"\"\\'\"\\n\\n2.Ov\\n\\n~\\'Sig~;,id\"\"\"\"\"\\n... ~\"Li~\"\"\"\"\\n......\\n\\n. . . . . . . . . . . . . . . . . . . . . . . ,.\\n\\n....... .\\n\\n.... .....\\n\\n..........\\n\\n.. ..\\n\\nl.~\\ne?_e\\n\\n_____ ...... ___\\n\\n?\\n\\n_____\\n\\n?\\n\\n____________ . . . . . . . .\\n\\n_____\\n\\n????\\n\\n____\\n\\n..........\\n\\n.\\n\\n5v\\n4v\\n3v\\n\\n2v\\n\\n:;; ~,...,................-..--t;?.~...:...~__--! ..........~~.n-.-~~~~~~......J\\nIS.\\n\\n10.\\n\\nFigure 5: ASiTEST3 Board Scope Waveforms\\nAs this figure shows different ramp signals, corresponding to different threshold\\nfunctions, can be applied to chipl and chip2 neurons.\\n10.0\\n\\nSingle Buffer PulscWidth Sweeps\\n\\n.----.,..----r------.----r----.------,\\n\\n9.0\\n8.0\\n\\n!\\n\\n7.0 -\\n\\n~\\n\\n5\\'O~~~~~~~-~~-+++~~~~N~~~\\n\\n~\\n\\ni6.o~\\n\\n~----\\n\\nJ::\\n2D\\n\\nNeal-Syal\\nN~~JIII\\n\\n1.0\\n\\nN~~ya2\\n\\no.oL---~--~- --~--?\\n\\no\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n2.5\\n\\n3.0\\n\\nPulscwldlh Input [WI)\\n\\nFigure 6: ASiTEST3 Board - MkII Synapse Characteristic\\nWhile the signals shown in Figure 5 appear noisy the multiplier characteristic that\\nthe chip produces is still admirably linear, as shown in Figure 6. In this experiment\\nall eight synapses on a test chip were programmed into different resistance states\\nand PWin was swept from 0 to 3us.\\n\\n\\x0cPulsestream Synapses with Non- Volatile Analogue Amorphous-Silicon Memories\\n\\n6\\n\\n769\\n\\nConclusions\\n\\nWe have demonstrated the use of novel a-Si:H analogue memory devices as a means\\nof storing synaptic weights in a Pulsewidth ANN. We have also demonstrated the\\noperation of an interface board which allows two 8x8 ANN chips, operating as a\\ntwo layer network, to be controlled by a simple PC interface card.\\nThis technology is most suitable for small networks in, for example, remote control and other embedded-system applications where cost and power considerations\\nfavour a single all-inclusive ANN chip with non-volatile, but programmable weights.\\nAnother possible application of this technology is in large networks constructed\\nusing Thin Film Technology(TFT). If TFT\\'s were used in place of the CMOS transistors then the area constraint imposed by crystalline silicon would be removed,\\nallowing truly massively parallel networks to be integrated.\\nIn summary - the a-Si:H analogue memory devices described in this paper provide a\\nroute to an analogue, non-volatile and fast synaptic weight storage medium. At the\\npresent time neither the programming nor storage mechanisms are fully understood\\nmaking it difficult to compare this new device with more established technologies\\nsuch as the ubiquitous Floating-Gate EEPROM technique. Current research is\\nfocused on firstly, improving the yield on the a-Si:H device which is unacceptably\\nlow at present, a demerit that we attribute to imperfections in the a-Si fabrication\\nprocess and secondly, improving understanding of the device physics and hence the\\nprogramming and storage mechanisms.\\nAcknowledgements\\nThis research has been jointly funded by BT, and EPSRC (formerly SERC), the\\nEngineering and Physical Sciences Research Council.\\n\\nReferences\\n[1] W. Hubbard et al.(1986) Electronic Neural Networks AlP Conference Proceedings - Snowbird 1986 :227-234\\n[2] H.P. Graf (1986) VLSI Implementation of a NN memory with several hundreds\\nof neurons AlP Conference Proceedings - Snowbird 1986 :182-187.\\n[3] M.J. Rose et al (1989) Amorphous Silicon Analogue Memory Devices Journal\\nof Non-Crystalline Solids 1(115):168-170\\n[4] A.Hamilton et al. (1992) Integrated Pulse-Stream Neural Networks - Results,\\nIssues and Pointers IEEE Transactions on N.N.s 3(3):385-393\\n[5] M.Holler, S.Tam, H.Castro and R.Benson (1989) An Electrically Trainable ANN\\nwith 10240 Floating Gate Synapses. Int Conf on N.N.s Proc :191-196\\n[6] A.F.Murray and A.V.W.Smith.(1987) Asynchronous Arithmetic for VLSI Neural Systems. Electronics Letters 23(12):642-643\\n[7] A.J. Holmes et al. (1993) Use of a-Si:H Memory Devices for Non-volatile Weight\\nStorage in ANNs. Proc lCAS 15 :817-820\\n\\n\\x0c\\x0c',\n",
       "     'pdf_name': '1006-pulsestream-synapses-with-non-volatile-analogue-amorphous-silicon-memories.pdf',\n",
       "     'title': 'Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1007',\n",
       "     'paper_text': 'Learning To Play the Game of Chess\\n\\nSebastian Thrun\\nUniversity of Bonn\\nDepartment of Computer Science III\\nRomerstr. 164, 0-53117 Bonn, Germany\\nE-mail: thrun@carbon.informatik.uni-bonn.de\\n\\nAbstract\\nThis paper presents NeuroChess, a program which learns to play chess from the final\\noutcome of games. NeuroChess learns chess board evaluation functions, represented\\nby artificial neural networks. It integrates inductive neural network learning, temporal\\ndifferencing, and a variant of explanation-based learning. Performance results illustrate\\nsome of the strengths and weaknesses of this approach.\\n\\n1 Introduction\\nThroughout the last decades, the game of chess has been a major testbed for research on\\nartificial intelligence and computer science. Most oftoday\\'s chess programs rely on intensive\\nsearch to generate moves. To evaluate boards, fast evaluation functions are employed which\\nare usually carefully designed by hand, sometimes augmented by automatic parameter tuning\\nmethods [1]. Building a chess machine that learns to play solely from the final outcome of\\ngames (win/loss/draw) is a challenging open problem in AI.\\nIn this paper, we are interested in learning to play chess from the final outcome of games.\\nOne of the earliest approaches, which learned solely by playing itself, is Samuel\\'s famous\\nchecker player program [10]. His approach employed temporal difference learning (in short:\\nTO) [14], which is a technique for recursively learning an evaluation function . Recently,\\nTesauro reported the successful application of TO to the game of Backgammon, using\\nartificial neural network representations [16]. While his TO-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go\\n[12] and chess have been less successful. For example, Schafer [11] reports a system just\\nlike Tesauro\\'s TO-Gammon, applied to learning to play certain chess endgames. Gherrity [6]\\npresented a similar system which he applied to entire chess games. Both approaches learn\\npurely inductively from the final outcome of games. Tadepalli [15] applied a lazy version\\nof explanation-based learning [5, 7] to endgames in chess. His approach learns from the\\nfinal outcome, too, but unlike the inductive neural network approaches listed above it learns\\nanalytically, by analyzing and generalizing experiences in terms of chess-specific knowledge.\\n\\n\\x0c1070\\n\\nSebastian Thrun\\n\\nThe level of play reported for all these approaches is still below the level of GNU-Chess, a\\npublicly available chess tool which has frequently been used as a benchmark. This illustrates\\nthe hardness of the problem of learning to play chess from the final outcome of games.\\nThis paper presents NeuroChess, a program that learns to play chess from the final outcome\\nof games. The central learning mechanisms is the explanation-based neural network (EBNN)\\nalgorithm [9, 8]. Like Tesauro\\'s TD-Gammon approach, NeuroChess constructs a neural\\nnetwork evaluation function for chess boards using TO. In addition, a neural network version\\nof explanation-based learning is employed, which analyzes games in terms of a previously\\nlearned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate\\nsome of its strengths and weaknesses.\\n\\n2\\n\\nTemporal Difference Learning in the Domain of Chess\\n\\nTemporal difference learning (TO) [14] comprises a family of approaches to prediction in\\ncases where the event to be predicted may be delayed by an unknown number of time steps.\\nIn the context of game playing, TD methods have frequently been applied to learn functions\\nwhich predict the final outcome of games. Such functions are used as board evaluation\\nfunctions.\\nThe goal of TO(O), a basic variant of TO which is currently employed in the NeuroChess\\napproach, is to find an evaluation function, V, which ranks chess boards according to their\\ngoodness: If the board S is more likely to be a winning board than the board Sf, then\\nV(s) > V(Sf). To learn such a function, TO transforms entire chess games, denoted by\\na sequence of chess boards So, SI, s2, . . . , StunaJ\\' into training patterns for V. The TO(O)\\nlearning rule works in the following way. Assume without loss of generality we are learning\\nwhite\\'s evaluation function. Then the target values for the final board is given by\\n{\\n\\nI,\\n0,\\n-1,\\n\\nif Stu.?tI is a win for white\\nif StUnaJ is a draw\\nif StonaJ is a loss for white\\n\\nand the targets for the intermediate chess boards So, SI , S2, . .. , Stu.?tI-2 are given by\\nVt.1fget( St)\\nI? V (St+2)\\n\\n=\\n\\n(1)\\n\\n(2)\\n\\nThis update rule constructs V recursively. At the end of the game, V evaluates the final\\noutcome of the game (Eq. (l In between, when the assignment of V -values is less obvious,\\nV is trained based on the evaluation two half-moves later (Eq. (2?. The constant I (with\\no ~ I ~ 1) is a so-called discount factor. It decays V exponentially in time and hence\\nfavors early over late success. Notice that in NeuroChess V is represented by an artificial\\nneural network, which is trained to fit the target values vtarget obtained via Eqs. (l) and (2)\\n(cj [6, 11, 12, 16]).\\n\\n?.\\n\\n3\\n\\nExplanation-Based Neural Network Learning\\n\\nIn a domain as complex as chess, pure inductive learning techniques. such as neural network Back-Propagation, suffer from enormous training times. To illustrate why, consider\\nthe situation of a knight fork. in which the opponent\\'s knight attacks our queen and king\\nsimultaneously. Suppose in order to save our king we have to move it, and hence sacrifice\\nour queen. To learn the badness of a knight fork, NeuroChess has to discover that certain\\nboard features (like the position of the queen relative to the knight) are important, whereas\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n1071\\n\\nFigure 1: Fitting values and slopes in EBNN: Let V be the target function for which three\\nexamples (s\\\\, V(S\\\\)), (S2\\' V(S2)), and (S3, V(S3)) are known. Based on these points the\\nS2)OS2, and a~;:3) are\\nlearner might generate the hypothesis V\\'. If the slopes a~;:I),\\nalso known, the learner can do much better: V\".\\n\\nar\\n\\nothers (like the number of weak pawns) are not. Purely inductive learning algorithms such\\nas Back-propagation figure out the relevance of individual features by observing statistical\\ncorrelations in the training data. Hence, quite a few versions of a knight fork have to be\\nexperienced in order to generalize accurately. In a domain as complex as chess, such an\\napproach might require unreasonably large amounts of training data.\\nExplanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training\\ndata. They rely instead on the availability of domain knowledge, which they use for explaining\\nand generalizing training examples. For example, in the explanation of a knight fork, EBL\\nmethods employ knowledge about the game of chess to figure out that the position of the\\nqueen is relevant, whereas the number of weak pawns is not. Most current approaches to\\nEBL require that the domain knowledge be represented by a set of symbolic rules. Since\\nNeuroChess relies on neural network representations, it employs a neural network version\\nof EBL, called explanation-based neural network learning (EBNN) [9]. In the context of\\nchess, EBNN works in the following way: The domain-specific knowledge is represented\\nby a separate neural network, called the chess model M. M maps arbitrary chess boards St\\nto the corresponding expected board St+2 two half-moves later. It is trained prior to learning\\nV, using a large database of grand-master chess games. Once trained, M captures important\\nknowledge about temporal dependencies of chess board features in high-quality chess play.\\nEBNN exploits M to bias the board evaluation function V. It does this by extracting slope\\nconstraints for the evaluation function V at all non-final boards, i.e., all boards for which V\\nis updated by Eq. (2). Let\\nwith\\n\\nt E\\n\\n{a, 1,2, ... , tlioa\\\\ - 2}\\n\\ndenote the target slope of V at St, which, because\\nEq. (2), can be rewritten as\\n\\noV target ( St)\\n\\n=\\n\\n\\'Y.\\n\\noV( St+2) OSt+2\\n._OSt+2\\nOSt\\n\\nvtarget ( St)\\n\\n(3)\\n\\nis set to \\'Y V (St+2) according\\n(4)\\n\\nusing the chain rule of differentiation. The rightmost term in Eq. (4) measures how infinitesimal small changes of the chess board St influence the chess board St+2. It can be\\napproximated by the chess model M:\\n\\novtarget(St)\\nOSt\\n\\n~\\n\\n\\'Y.\\n\\nOV(St+2) oM(st)\\n.\\nOSt+2\\nOSt\\n\\n(5)\\n\\nThe right expression is only an approximation to the left side, because M is a trained neural\\n\\n\\x0cSebastian Thrun\\n\\n1072\\n\\n~\\n\\nbmrd at time\\n\\nf\\n\\n(W\"T\"~)\\n\\n~\\n\\nboard attime 1+ I\\n(black to move)\\n\\n~\\n\\nboard at time 1+2\\n\\n(w\"\\'?ro~)\\n\\npredictive model network M\\n\\n165 hidden unit,\\n\\nV(1+2)\\n\\nFigure 2: Learning an evaluation function in NeuroChess. Boards are mapped into a\\nhigh-dimensionalJeature vector, which forms the input for both the evaluation network V\\nand the chess model M. The evaluation network is trained by Back-propagation and the\\nTD(O) procedure. Both networks are employed for analyzing training example in order to\\nderive target slopes for V.\\nnetwork and thus its first derivative might be erroneous. Notice that both expressions on\\nthe right hand side of Eq. (5) are derivatives of neural network functions, which are easy to\\ncompute since neural networks are differentiable.\\nThe result of Eq . (5) is an estimate of the slope of the target function V at 8t . This slope\\nadds important shape information to the target values constructed via Eq. (2). As depicted in\\nFig. 1, functions can be fit more accurately if in addition to target values the slopes of these\\nvalues are known. Hence, instead of just fitting the target values vtarget ( 8t), NeuroChess also\\nfits these target slopes. This is done using the Tangent-Prop algorithm [13].\\nThe complete NeuroChess learning architecture is depicted in Fig. 2. The target slopes\\nprovide a first-order approximation to the relevance of each chess board feature in the\\ngoodness of a board position. They can be interpreted as biasing the network V based on\\nchess-specific domain knowledge, embodied in M . For the relation ofEBNN and EBL and\\nthe accommodation of inaccurate slopes in EBNN see [8].\\n\\n4\\n\\nTraining Issues\\n\\nIn this section we will briefly discuss some training issues that are essential for learning good\\nevaluation functions in the domain of chess. This list of points has mainly been produced\\nthrough practical experience with the NeuroChess and related TD approaches. It illustrates\\nthe importance of a careful design of the input representation, the sampling rule and the\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n1073\\n\\nparameter setting in a domain as complex as chess.\\nSampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for\\nexample, the opponent leads by more than a queen and a rook, one is most likely to loose.\\nWithout an appropriate sampling method there is the danger that the learner spends most\\nof its time learning from uninteresting examples. Therefore, NeuroChess interleaves selfplay and expert play for guiding the sampling process. More specifically, after presenting\\na random number of expert moves generated from a large database of grand-master games,\\nNeuroChess completes the game by playing itself. This sampling mechanism has been found\\nto be of major importance to learn a good evaluation function in a reasonable amount of time.\\nQuiescence. In the domain of chess certain boards are harder to evaluate than others. For\\nexample, in the middle of an ongoing material exchange, evaluation functions often fail to\\nproduce a good assessment. Thus, most chess programs search selectively. A common\\ncriterion for determining the depth of search is called quiescence. This criterion basically\\ndetects material threats and deepens the search correspondingly. NeuroChess\\' search engine\\ndoes the same. Consequently, the evaluation function V is only trained using quiescent\\nboards.\\nSmoothness. Obviously, using the raw, canonical board description as input representation is\\na poor choice. This is because small changes on the board can cause a huge difference in value,\\ncontrasting the smooth nature of neural network representations. Therefore, NeuroChess\\nmaps chess board descriptions into a set of board features . These features were carefully\\ndesigned by hand.\\nDiscounting. The variable \\'Y in Eq. (2) allows to discount values in time. Discounting has\\nfrequently been used to bound otherwise infinite sums of pay-off. One might be inclined to\\nthink that in the game of chess no discounting is needed, as values are bounded by definition.\\nIndeed, without discounting the evaluation function predicts the probability for winning-in\\nthe ideal case. In practice, however, random disturbations of the evaluation function can\\nseriously hurt learning, for reasons given in [4, 17]. Empirically we found that learning\\nfailed completely when no discount factor was used. Currently, NeuroChess uses \\'Y = 0.98.\\nLearning rate. TO approaches minimize a Bellman equation [2]. In the NeuroChess\\ndomain, a close-to-optimal approximation of the Bellman equation is the constant function\\nV(s) == O. This function violates the Bellman equation only at the end of games (Eq. (1?,\\nwhich is rare if complete games are considered. To prevent this, we amplified the learning\\nrate for final values by a factor of20, which was experimentally found to produce sufficiently\\nnon-constant evaluation functions.\\nSoftware architecture. Training is performed completely asynchronously on up to 20\\nworkstations simultaneously. One of the workstations acts as a weight server, keeping track\\nof the most recent weights and biases of the evaluation network. The other workstations\\ncan dynamically establish links to the weight server and contribute to the process of weight\\nrefinement. The main process also monitors the state of all other workstations and restarts\\nprocesses when necessary. Training examples are stored in local ring buffers (1000 items\\nper workstation).\\n\\n5\\n\\nResults\\n\\nIn this section we will present results obtained with the NeuroChess architecture. Prior to\\nlearning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)\\nis trained using a database of 120,000 expert games. NeuroChess then learns an evaluation\\n\\n\\x0c1074\\n\\nI. e2e3 b8c6\\n2. dlf3 c6e5\\n3. f3d5 d7d6\\n4. flb5 c7c6\\n5. b5a4 g8f6\\n6. d5d4 c8f5\\n7. f2f4 e5d7\\n8. ele2d8a5\\n9. a4b3 d7c5\\n10. b I a3 c5b3\\n11 . a2b3 e7e5\\n12. f4e5 f6e4\\n13. e5d6 e8c8\\n14. b3b4 a5a6\\n15. b4b5 a6a5\\n\\nSebastian Thrun\\n\\n16. b2b4 a5a4\\n17. b5c6 a4c6\\n18. gl f3 d8d6\\n19. d4a7 f5g4\\n20. c2c4 c8d7\\n21. b4b5 c6c7\\n22. d2d3 d6d3\\n23. b5b6 c7c6\\n24. e2d3 e4f2\\n25. d3c3 g4f3\\n26. g2f3 f2h 1\\n27. clb2 c6f3\\n28. a7a4 d7e7\\n29. a3c2 hi f2\\n30. b2a3 e7f6\\n\\n31 . a3f8 f2e4\\n32. c3b2 h8f8\\n33. a4d7 f3f5\\n34. d7b7 f5e5\\n35. b2cl f8e8\\n36. b7d5 e5h2\\n37. ala7 e8e6\\n38. d5d8 f6g6\\n39. b6b7 e6d6\\n40. d8a5 d6c6\\n41 . a5b4 h2b8\\n42. a7a8 e4c3\\n43. c2d4 c6f6\\n44. b4e7 c3a2\\n45. cldl a2c3\\n\\n46. d I c2 b8h2\\n47. c2c3 f6b6\\n48. e7e4 g6h6\\n49. d4f5 h6g5\\n50. e4e7 g5g4\\n51. f5h6 g7h6\\n52. e7d7 g4h5\\n53. d7d I h5h4\\n54. d I d4 h4h3\\n55. d4b6 h2e5\\n56. b6d4 e5e6\\n57. c3d2 e6f5\\n58. e3e4 f5 g5\\n59. d4e3 g5e3\\n60. d2e3 f7f5\\n\\n61 . e4f5 h3g4 65. a8e8 e6d7\\n62. f5f6 h6h5\\n66. e8e7 d7d8\\n63. b7b8q g4f5 67. f4c7\\n64. b8f4 f5e6\\nfinal board\\n\\nFigure 3: NeuroChess against GNU-Chess. NeuroChess plays white. Parameters: Both\\nplayers searched to depth 3, which could be extended by quiescence search to at most 11.\\nThe evaluation network had no hidden units. Approximately 90% of the training boards\\nwere sampled from expert play.\\n\\nnetwork V (175 input units, 0 to 80 hidden units, and one output units). To evaluate the level\\nof play, NeuroChess plays against GNU-Chess in regular time intervals. Both players employ\\nthe same search mechanism which is adopted from GNU-Chess. Thus far, experiments lasted\\nfor 2 days to 2 weeks on I to 20 SUN Sparc Stations.\\nA typical game is depicted in Fig. 3. This game has been chosen because it illustrates both\\nthe strengths and the shortcomings of the NeuroChess approach. The opening of NeuroChess\\nis rather weak. In the first three moves NeuroChess moves its queen to the center of the\\nboard.\\' NeuroChess then escapes an attack on its queen in move 4, gets an early pawn\\nadvantage in move 12, attacks black\\'s queen pertinaciously through moves 15 to 23, and\\nsuccessfully exchanges a rook. In move 33, it captures a strategically important pawn, which,\\nafter chasing black\\'s king for a while and sacrificing a knight for no apparent reason, finally\\nleads to a new queen (move 63). Four moves later black is mate. This game is prototypical.\\nAs can be seen from this and various other games, NeuroChess has learned successfully to\\nprotect its material, to trade material, and to protect its king. It has not learned, however, to\\nopen a game in a coordinated way, and it also frequently fails to play short.endgames even\\nif it has a material advantage (this is due to the short planning horizon). Most importantly, it\\nstill plays incredibly poor openings, which are often responsible for a draw or a loss. Poor\\nopenings do not surprise, however, as TD propagates values from the end of a game to the\\nbeginning.\\nTable I shows a performance comparison of NeuroChess versus GNU-Chess, with and\\nwithout the explanation-based learning strategy. This table illustrates that NeuroChess wins\\napproximately 13% of all games against GNU-Chess, if both use the same search engine. It\\n\\'This is because in the current version NeuroChess still heavily uses expert games for sampling.\\nWhenever a grand-master moves its queen to the center of the board, the queen is usually safe, and there\\nis indeed a positive correlation between having the queen in the center and winning in the database.\\nNeuroChess falsely deduces that having the queen in the center is good. This effect disappears when\\nthe level of self-play is increased, but this comes at the expense of drastically increased training time,\\nsince self-play requires search.\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n# of games\\n100\\n200\\n500\\n1000\\n1500\\n2000\\n2400\\n\\nGNU depth 2, NeuroChess depth 2\\nBack-propagation\\nEBNN\\n1\\n0\\n6\\n2\\n35\\n13\\n73\\n85\\n130\\n135\\n190\\n215\\n239\\n316\\n\\n1075\\n\\nGNU depth 4, NeuroChess depth 2\\nBack-propagation\\nEBNN\\n0\\n0\\n0\\n0\\nI\\n0\\n2\\n1\\n3\\n3\\n3\\n8\\nII\\n3\\n\\nTable 1: Performance ofNeuroChess vs. GNU-Chess during training. The numbers show the\\ntotal number of games won against GNU-Chess using the same number of games for testing\\nas for training. This table also shows the importance of the explanation-based learning\\nstrategy in EBNN. Parameters: both learners used the original GNU-Chess features, the\\nevaluation network had 80 hidden units and search was cut at depth 2, or 4, respectively (no\\nquiescence extensions).\\nalso illustrates the utility of explanation-based learning in chess.\\n\\n6 Discussion\\nThis paper presents NeuroChess, an approach for learning to play chess from the final\\noutcomes of games. NeuroChess integrates TD, inductive neural network learning and\\na neural network version of explanation-based learning. The latter component analyzes\\ngames using knowledge that was previously learned from expert play. Particular care has\\nbeen taken in the design of an appropriate feature representation, sampling methods, and\\nparameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in\\nseveral hundreds of games. However, the level of play still compares poorly to GNU-Chess\\nand human chess players.\\nDespite the initial success, NeuroChess faces two fundamental problems which both might\\nweB be in the way of excellent chess play. Firstly, training time is limited, and it is to\\nbe expected that excellent chess skills develop only with excessive training time. This is\\nparticularly the case if only the final outcomes are considered. Secondly, with each step of\\nTO-learning NeuroChess loses information. This is partially because the features used for\\ndescribing chess boards are incomplete, i.e., knowledge about the feature values alone does\\nnot suffice to determine the actual board exactly. But, more importantly, neural networks have\\nnot the discriminative power to assign arbitrary values to all possible feature combinations.\\nIt is therefore unclear that a TD-like approach will ever, for example, develop good chess\\nopenmgs.\\nAnother problem of the present implementation is related to the trade-off between knowledge\\nand search. It has been well recognized that the ul timate cost in chess is determi ned by the ti me\\nit takes to generate a move. Chess programs can generally invest their time in search, or in the\\nevaluation of chess boards (search-knowledge trade-off) [3] . Currently, NeuroChess does a\\npoor job, because it spends most of its time computing board evaluations. Computing a large\\nneural network function takes two orders of magnitude longer than evaluating an optimized\\nlinear evaluation function (like that of GNU-Chess). VLSI neural network technology offers\\na promising perspective to overcome this critical shortcoming of sequential neural network\\nsimulations.\\n\\n\\x0c1076\\n\\nSebastian Thrun\\n\\nAcknowledgment\\nThe author gratefully acknowledges the guidance and advise by Hans Berliner, who provided\\nthe features for representing chess boards, and without whom the current level of play would\\nbe much worse. He also thanks Tom Mitchell for his suggestion on the learning methods,\\nand Horst Aurisch for his help with GNU-Chess and the database.\\n\\nReferences\\n[I] Thomas S. Anantharaman. A Statistical Study of Selective Min-Max Search in Computer Chess.\\nPhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA, 1990.\\nTechnical Report CMU-CS-90-173.\\n[2] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.\\n[3] Hans J. Berliner, Gordon Goetsch, Murray S. Campbell, and Carl Ebeling. Measuring the\\nperformance potential of chess programs. Artificial Intelligence, 43:7-20, 1990.\\n[4] Justin A. Boyan. Generalization in reinforcement learning: Safely approximating the value\\nfunction. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information\\nProcessing Systems 7, San Mateo, CA, 1995. Morgan Kaufmann. (to appear).\\n[5] Gerald Dejong and Raymond Mooney. Explanation-based learning: An alternative view. Machine Learning, 1(2): 145-176, 1986.\\n[6] Michael Gherrity. A Game-Learning Machine. PhD thesis, University of California, San Diego,\\n1993.\\n[7] Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. Explanation-based generalization: A\\nunifying view. Machine Learning, 1(1 ):47-80, 1986.\\n[8] Tom M. Mitchell and Sebastian Thrun. Explanation based learning: A comparison of symbolic\\nand neural network approaches. In Paul E. Utgoff, editor, Proceedings of the Tenth International\\nConference on Machine Learning, pages 197-204, San Mateo, CA, 1993. Morgan Kaufmann.\\n[9] Tom M. Mitchell and Sebastian Thrun. Explanation-based neural network learning for robot\\ncontrol. In S. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information\\nProcessing Systems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.\\n[10] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal on\\nresearch and development, 3:210-229, 1959.\\n[11] Johannes Schafer. Erfolgsorientiertes Lemen mit Tiefensuche in Bauemendspielen. Technical\\nreport, UniversiUit Karlsruhe, 1993. (in German).\\n[12] Nikolaus Schraudolph, Pater Dayan, and Terrence J. Sejnowski. Using the TD(lambda) algorithm\\nto learn an evaluation function for the game of go. In Advances in Neural Information Processing\\nSystems 6, San Mateo, CA, 1994. Morgan Kaufmann.\\n[13] Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. Tangent prop -a formalism for\\nspecifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P.\\nLippmann, editors, Advances in Neural Information Processing Systems 4, pages 895-903, San\\nMateo, CA, 1992. Morgan Kaufmann.\\n[14] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\\n3,1988.\\n[15] Prasad Tadepalli. Planning in games using approximately learned macros. In Proceedings of the\\nSixth International Workshop on Machine Learning, pages 221-223, Ithaca, NY, 1989. Morgan\\nKaufmann.\\n[16] Gerald J. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8, 1992.\\n[17] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors,\\nProceedings of the 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Erlbaum\\nAssociates.\\n\\n\\x0c',\n",
       "     'pdf_name': '1007-learning-to-play-the-game-of-chess.pdf',\n",
       "     'title': 'Learning to Play the Game of Chess',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1013',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1013',\n",
       "     'paper_text': 'Ocular Dominance and Patterned Lateral\\nConnections in a Self-Organizing Model of the\\nPrimary Visual Cortex\\nJoseph Sirosh and Risto Miikkulainen\\n\\nDepartment of Computer Sciences\\nUniversity of Texas at Austin, Austin, \\'IX 78712\\nemail:\\n\\nsirosh.risto~cs.utexas.edu\\n\\nAbstract\\nA neural network model for the self-organization of ocular dominance and\\nlateral connections from binocular input is presented. The self-organizing\\nprocess results in a network where (1) afferent weights of each neuron organize into smooth hill-shaped receptive fields primarily on one of the retinas, (2) neurons with common eye preference form connected, intertwined\\npatches, and (3) lateral connections primarily link regions of the same eye\\npreference. Similar self-organization of cortical structures has been observed experimentally in strabismic kittens. The model shows how patterned lateral connections in the cortex may develop based on correlated\\nactivity and explains why lateral connection patterns follow receptive field\\nproperties such as ocular dominance.\\n\\n1 Introduction\\nLateral connections in the primary visual cortex have a patterned structure that closely\\nmatches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.1993).\\nFor example, in the normal visual cortex, long-range lateral connections link areas with similar orientation preference (Gilbert and Wiesel 1989). Like cortical response properties, the\\nconnectivity pattern is highly plastic in early development and can be altered by experience\\n(Katz and Callaway 1992). In a cat that is brought up squint-eyed from birth, the lateral connections link areas with the same ocular dominance instead of orientation (Lowel and Singer\\n1992). Such patterned lateral connections develop at the same time as the orientation selectivity and ocular dominance itself (Burkhalter et al.1993; Katz and Callaway 1992). Together,\\n\\n\\x0c110\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\nthese observations suggest that the same experience-dependent process drives the development of both cortical response properties and lateral connectivity.\\nSeveral computational models have been built to demonstrate how orientation preference,\\nocular dominance, and retinotopy can emerge from simple self-organizing processes (e.g.\\nGoodhill1993; Miller 1994; Obermayer et al.1992; von der Malsburg 1973). These models\\nassume that the neuronal response properties are primarily determined by the afferent connections, and concentrate only on the self-organization of the afferent synapses to the cortex. Lateral interactions between neurons are abstracted into simple mathematical functions\\n(e.g. Gaussians) and assumed to be uniform throughout the network; lateral connectivity is not\\nexplicitly taken into account. Such models do not explicitly replicate the activity dynamics\\nof the visual cortex, and therefore can make only limited predictions about cortical function.\\nWe have previously shown how Kohonen\\'s self-organizing feature maps (Kohonen 1982)\\ncan be generalized to include self-organizing lateral connections and recurrent activity dynamics (the Laterally Interconnected Synergetically Self-Organizing Map (LISSOM); Sirosh\\nand Miikkulainen 1993, 1994a), and how the algorithm can model the development of ocular dominance columns and patterned lateral connectivity with abstractions of visual input.\\nLISSOM is a low-dimensional abstraction of cortical self-organizing processes and models a\\nsmall region of the cortex where all neurons receive the same input vector. This paper shows\\nhow realistic, high-dimensional receptive fields develop as part of the self-organization, and\\nscales up the LISSOM approach to large areas of the cortex where different parts of the cortical network receive inputs from different parts of the receptor surface. The new model shows\\nhow (1) afferent receptive fields and ocular dominance columns develop from simple retinal images, (2) input correlations affect the wavelength of the ocular dominance columns and\\n(3) lateral connections self-organize cooperatively and simultaneously with ocular dominance\\nproperties. The model suggests new computational roles for lateral connections in the cortex,\\nand suggests that the visual cortex maybe maintained in a continuously adapting equilibrium\\nwith the visual input by co adapting lateral and afferent connections.\\n\\n2\\n\\nThe LISSOM Model of Receptive Fields and Ocular Dominance\\n\\nThe LISSOM network is a sheet of interconnected neurons (figure 1). Through afferent connections, each neuron receives input from two \"retinas\". In addition, each neuron has reciprocal excitatory and inhibitory lateral connections with other neurons. Lateral excitatory connections are short-range, connecting only close neighbors. Lateral inhibitory connections run\\nfor long distances, and may even implement full connectivity between neurons in the network.\\nNeurons receive afferent connections from broad overlapping patches on the retina called\\nanatomical receptive fields, or RFs. The N x N network is projected on to each retina of\\nR x R receptors, and each neuron is connected to receptors in a square area of side s around\\nthe projections. Thus, neurons receive afferents from corresponding regions of each retina.\\nDepending on the location of the projection, the number of afferents to a neuron from each\\nretina could vary from\\nx ~s (at the comers) to s x s (at the center).\\n\\nts\\n\\nThe external and lateral weights are organized through an unsupervised learning process. At\\neach training step, neurons start out with zero activity. The initial response TJij of neuron (i, j)\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\nLoft _ . .\\n\\n111\\n\\nfllgIIl Roll . .\\n\\nFigure 1: The Receptive-Field LISSOM architecture. The afferent and lateral connectionsof a single\\nneuron in the liSSOM network are shown. All connection weights are positive.\\n\\nis based on the scalar product\\nTJij\\n\\n=\\n\\n(T\\n\\n(L\\n\\neabJJij ,ab\\n\\n+\\n\\na,b\\n\\nL\\n\\n(1)\\n\\neCdJJij,Cd) ,\\n\\nc,d\\n\\nwhere eab and ecd are the activations of retinal receptors (a, b) and (c, d) within the receptive\\nfields of the neuron in each retina, JJij,ab and JJij,cd are the corresponding afferent weights,\\nand (T is a piecewise linear approximation of the familiar sigmoid activation function. The\\nresponse evolves over time through lateral interaction. At each time step, the neuron combines the above afferent activation I:: eJJ with lateral excitation and inhibition:\\nTJij(t)\\n\\n=\\n\\n(T\\n\\n(L eJJ + L\\n\"Ie\\n\\nEij,kITJkl(t -\\n\\n1) - L\\n\"Ii\\n\\nk,1\\n\\nIij,klTJkl(t -\\n\\n1)) ,\\n\\n(2)\\n\\nk,1\\n\\nwhere Eij,kl is the excitatory lateral connection weight on the connection from neuron (k, l)\\nto neuron (i, j), Iij,kl is the inhibitory connection weight, and TJkl (t - 1) is the activity of\\nneuron (k, I) during the previous time step. The constants \"Ie and \"Ii determine the relative\\nstrengths of excitatory and inhibitory lateral interactions. The activity pattern starts out diffuse and spread over a substantial part of the map, and converges iteratively into stable focused\\npatches of activity, or activity bubbles. After the-activity has settled, typically in a few iterations of equation 2, the connection weights of each neuron are modified. Both afferent and\\nlateral weights adapt according to the same mechanism: the Hebb rule, normalized so that the\\nsum of the weights is constant:\\n(\\n\\nWij,mn t\\n\\nr ) _\\n\\n+ vt\\n\\n-\\n\\n+\\n\\nWij,mn(t)\\nCtTJijXmn\\n\\'\"\"\\n( )\\nwmn [Wij ,mn t\\nCtTJijXmn\\n\\n+\\n\\n1\\'\\n\\n(3)\\n\\nwhere TJij stands for the activity of neuron (i, j) in the final activity bubble, Wij,mn is the afferent or lateral connection weight (JJ, E or I), Ct is the learning rate for each type of connection\\n(Ct a for afferent weights, Ct E for excitatory, and Ct I for inhibitory) and X mn is the presynaptic\\nactivity for afferent, TJ for lateral).\\n\\n(e\\n\\n\\x0cJoseph Sirosh, Risto Miikkulainen\\n\\n112\\n\\n\"\\n(a) Random Initial Weights\\n\\n(b) Monocular RF\\n\\n(c) Binocular RF\\n\\nFigure 2: Self-organization of the afferent input weights into receptive fields. The afferent weights\\nof a neuron at position (42,39) in a 60 x 60 network are shown before (a) and after self-organization\\n(b). This particular neuron becomes monocular with strong connections to the right eye, and weak connections to the left. A neuron at position (38, 23) becomes binocular with appoximately equal weights\\nto both eyes (c).\\nBoth excitatory and inhibitory lateral connections follow the same Hebbian learning process and strengthen by correlated activity. The short-range excitation keeps the activity of\\nneighboring neurons correlated, and as self-organization progresses, excitation and inhibition strengthen in the vicinity of each neuron. At longer distances, very few neurons have\\ncorrelated activity and therefore most long-range connections become weak. Such weak connections are eliminated, and through weight normalization, inhibition concentrates in a closer\\nneighborhood of each neuron. As a result, activity bubbles become more focused and local,\\nweights change in smaller neighborhoods, and receptive fields become better tuned to local\\nareas of each retina.\\nThe input to the model consists of gaussian spots of \"light\" on each retina:\\nt\\n_\\n((x\\n<\"x,y - exp -\\n\\n- xd 2 + (y - Yi)2)\\nu2\\n\\n(4)\\n\\nwhere ex,y is the activation of receptor (x, V), u 2 is a constant determining the width of the\\nspot, and (Xi,Yi): 0 ~ xi, Yi < R its center. At each input presentation, one spot is randomly\\nplaced at (Xi ,Yi) in the left retina, and a second spot within a radius of p x RN of (Xi, yd\\nin the right retina. The parameter p E [0, 1] specifies the spatial correlations between spots\\nin the two retinas, and can be adjusted to simulate different degrees of correlations between\\nimages in the two eyes.\\n\\n3\\n\\nSimulation results\\n\\nTo see how correlation between the input from the two eyes affects the columnar structures\\nthat develop, several simulations were run with different values of p. The afferent weights of\\nall neurons were initially random (as shown in figure 2a), with the total strength to both eyes\\nbeing equal.\\nFigures 2b,c show the final afferent receptive fields of two typical neurons in a simulation\\nwith p = 1. In this case, the inputs were uncorrelated, simulating perfect strabismus. In\\nthe early stages of such simulation, some of the neurons randomly develop a preference for\\none eye or the other. Nearby neurons will tend to share the same preference because lateral\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\n(a) Connections of a Monocular Neuron\\n\\n113\\n\\n(b) Connections of a Binocular Neuron\\n\\nFigure 3: Ocular dominance and lateral connection patterns. The ocular dominance of a neuron is\\nmeasured as the difference in total afferent synaptic weight from each eye to the neuron. Each neuron\\nis labeled with a grey-scale value (black ~ white) that represents continuously changing eye preference from exclusive left through binocular to exclusive right. Small white dots indicate the lateral input\\nconnections to the neuron marked with a big white dot. (a) The surviving lateral connections of a left\\nmonocular neuron predominantly link areas of the same ocular dominance. (b) The lateral connections\\nof a binocular neuron come from both eye regions.\\n\\nexcitation keeps neural activity partially correlated over short distances. As self-organization\\nprogresses, such preferences are amplified, and groups of neurons develop strong weights to\\none eye. Figure 2b shows the afferent weights of a typical monocular neuron.\\nThe extent of activity correlations on the network detennines the size of the monocular neuronal groups. Farther on the map, where the activations are anticorrelated due to lateral inhibition, neurons will develop eye preferences to the opposite eye. As a result, alternating\\nocular dominance patches develop over the map, as shown in figure 3. 1 In areas between ocular dominance patches, neurons will develop approximately equal strengths to both eyes and\\nbecome binocular, like the one shown in figure 2e.\\nThe width and number of ocular dominance columns in the network (and therefore, the wavelength of ocular dominance) depends on the input correlations (figure 4). When inputs in the\\ntwo eyes become more correlated (p < 1), the activations produced by the two inputs in the\\nnetwork overlap closely and activity correlations become shorter range. By Hebbian adaptation, lateral inhibition concentrates in the neighborhood of each neuron, and the distance at\\nwhich activations becomes anticorrelated decreases. Therefore, smaller monocular patches\\ndevelop, and the ocular dominance wavelength decreases. Similar dependence was very recently observed in the cat primary visual cortex (LoweI1994). The LISSOM model demonstrates that the adapting lateral interactions and recurrent activity dynamics regulate the wavelength, and suggests how these processes help the cortex develop feature detectors at a scale\\n1 For a thorough treatment of the mathematical principles underlying the development of ocular dominance columns, see (GoodhillI993; Miller et al.1989; von der Malsburg and Singer 1988).\\n\\n\\x0c114\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\n-0\\n-0\\n\\n(a) Strabismic case\\n\\n(b ) Normal case\\n\\nFigure 4: Ocular dominance wavelength in strabismic and normal models. In the strabismic case,\\nthere are no between-eye correlations (p = 1), and broad ocular dominance columns are produced (a) .\\nWith normal, partial between-eye correlations (p = 0.45 in this example), narrower stripes are formed\\n(b). As a result, there are more ocular dominance columns in the normal case and the ocular dominance\\nwavelength is smaller.\\n\\nthat matches the input correlations.\\nAs eye preferences develop, left or right eye input tends to cause activity only in the left or\\nright ocular dominance patches. Activity patterns in areas of the network with the same ocular dominance tend to be highly correlated because they are caused by the same input spot.\\nTherefore, the long-range lateral connections between similar eye preference areas become\\nstronger, and those between opposite areas weaker. After the weak lateral connections are\\neliminated, the initially wide-ranging connections are pruned, and eventually only connect\\nareas of similar ocular dominance as shown in figure 3. Binocular neurons between ocular\\ndominance patches will see some correlated activity in both the neigbboring areas, and maintain connections to both ocular dominance columns (figure 3b).\\nThe lateral connection patterns shown above closely match observations in the primary visual cortex. Lowel and Singer (1992) observed that when between-eye correlations are abolished in kittens by surgically induced strabismus, long-range lateral connections primarily\\nlink areas of the same ocular dominance. However, binocular neurons, located between ocular dominance columns, retained connections to both eye regions. The receptive field model\\nconfinns that such patterned lateral connections develop based on correlated neuronal activity,\\nand demonstrates that they can self-organize simultaneously with ocular dominance columns.\\nThe model also predicts that the long-range connections have an inhibitory function.\\n\\n4 Discussion\\nIn LISSOM, evolving lateral interactions and dynamic activity patterns are explicitly modeled. Therefore, LISSOM has several novel properties that set it apart from other selforganizing models of the cortex.\\nPrevious models (e.g. Goodhill1993; Milleret al.1989; Obermayer et al.1992; von der Malsburg 1973) have concentrated only on forming ordered topographic maps where clusters of\\nadjacent neurons assume similar response properties such as ocular dominance or orientation\\npreference. The lateral connections in LISSOM, in addition, adapt to encode correlations be-\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\n115\\n\\ntween the responses. 2 This property can be potentially very useful in models of cortical function. While afferent connections learn to detect the significant features in the input space (such\\nas ocularity or orientation), the lateral connections can learn correlations between these features (such as Gestalt principles), and thereby form a basis for feature grouping.\\nAs an illustration, consider a single spot of light presented to the left eye. The spot causes disjoint activity patterns in the left-eye-dominant patches. How can these multiple activity patterns be recognized as representing the same spatially coherent entity? As proposed by Singer\\net al. (1990), the long-range lateral connections between similar ocular dominance columns\\ncould synchronize cortical activity, and form a coherently firing assembly of neurons. The\\nspatial coherence of the spot will then be represented by temporal coherence of neural activity. LISSOM can be potentially extended to model such feature binding.\\nEven after the network has self-organized, the lateral and afferent connections remain plastic\\nand in a continuously-adapting dynamic equilibrium with the input. Therefore, the receptive\\nfield properties of neurons can dynamically readapt when the activity correlations in the network are forced to change. For example, when a small area of the cortex is set inactive (or\\nlesioned), the sharply-tuned afferent weight profiles of the neurons surrounding that region\\nexpand in size, and neurons begin to respond to the stimuli that previously activated only the\\nlesioned area (Sirosh and Miikkulainen 1994b, 1994c). This expansion of receptive fields is\\nreversible, and when the lesion is repaired, neurons return to their original tuning. Similar\\nchanges occur in response to retinal lesions as well. Such dynamic expansions of receptive\\nfields have been observed in the visual cortex (Pettet and Gilbert 1992). The LISSOM model\\ndemonstrates that such plasticity is a consequence of the same self-organizing mechanisms\\nthat drive the development of cortical maps.\\n\\n5\\n\\nConclusion\\n\\nThe LISSOM model shows how a single local and unsupervised self-organizing process can\\nbe responsible for the development of both afferent and lateral connection structures in the primary visual cortex. It suggests that this same developmental mechanism also encodes higherorder visual information such as feature correlations into the lateral connections. The model\\nforms a framework for future computational study of cortical reorganization and plasticity, as\\nwell as dynamic perceptual processes such as feature grouping and binding.\\nAcknowledgments\\n\\nThis research was supported in part by National Science Foundation under grant #IRI9309273. Computer time for the simulations was provided by the Pittsburgh Supercomputing\\nCenter under grants IRI930005P and TRA940029P.\\n\\nReferences\\nBurkhalter, A., Bernardo, K. L., and Charles, V. (1993). Development of local circuits in\\nhuman visual cortex. Journalo/Neuroscience, 13:1916-1931.\\nGilbert, C. D., and Wiesel, T. N. (1989). Columnar specificity of intrinsic horizontal and\\ncorticocortical connections in cat visual cortex. Journal 0/ Neuroscience, 9:2432-2442.\\n2Tbe idea was conceived by von der Malsburg and Singer (1988), but not modeled.\\n\\n\\x0c116\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\nGoodhill, G. (1993). Topography and ocular dominance: a model exploring positive correlations. Biological Cybernetics, 69:109-118.\\nKatz, L. C., and Callaway, E. M. (1992). Development of local circuits in mammalian visual\\ncortex. Annual Review o/Neuroscience, 15:31-56.\\nKohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biolog-\\n\\nical Cybernetics, 43:59-69.\\nLowel, S. (1994). Ocular dominance column development: Strabismus changes the spacing\\nof adjacent columns in cat visual cortex. Journal 0/ Neuroscience, 14(12):7451-7468.\\nLowel, S., and Singer, W. (1992). Selection of intrinsic horizontal connections in the visual\\ncortex by correlated neuronal activity. Science, 255:209-212.\\nMalach, R., Amir, Y., Harel, M., and Grinvald, A (1993). Relationship between intrinsic\\nconnections and functional architecture revealed by optical imaging and in vivo targeted\\nbiocytin injections in the primate striate cortex. Proceedings o/the National Academy\\n\\no/Sciences, USA,90:10469-10473.\\nMiller, K. D. (1994). A model for the development of simple cell receptive fields and the\\nordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs. Journalo/Neuroscience, 14:409-441.\\nMiller, K. D., Keller, 1. B., and Stryker, M. P. (1989). Ocular dominance column development:\\nAnalysis and simulation. Science, 245:605-615.\\nObermayer, K., Blasdel, G. G., and Schulten, K. J. (1992). Statistical-mechanical analysis of\\nself-organization and pattern formation during the development of visual maps. Physical\\n\\nReview A, 45:7568-7589.\\nPettet, M. W., and Gilbert, C. D. (1992). Dynamic changes in receptive-field size in cat primary visual cortex. Proceedings o/the NationalAcademy 0/ Sciences, USA,89:83668370.\\nSinger, W., Gray, C., Engel, A, Konig, P., Artola, A, and Bracher, S. (1990). Formation of\\ncortical cell assemblies. In Cold Spring Harbor Symposia on Quantitative Biology, Vol.\\nLV, 939-952. Cold Spring Harbor, NY: Cold Spring Harbor Laboratory.\\nSirosh, J., and Miikkulainen, R. (1993). How lateral interaction develops in a self-organizing\\nfeature map. In Proceedings o/the IEEE International Conference on Neural Networks\\n(San Francisco, CA), 1360--1365. Piscataway, NJ: IEEE.\\nSirosh, J., and Miikkulainen, R. (1994a). Cooperative self-organization of afferent and lateral\\nconnections in cortical maps. Biological Cybernetics, 71(1):66--78.\\nSirosh, 1., and Miikkulainen, R. (1994b). Modeling cortical plasticity based on adapting lateral interaction. In The Neurobiologyo/Computation: Proceedings o/the Annual ComputationalNeuroscience Meeting. Dordrecht; Boston: Kluwer. In Press.\\nSirosh, J., and Miikkulainen, R. (1994c). A neural network model oftopographic reorganization following cortical lesions. In Proceedings o/the World Congress on Computational\\nMediCine, Public Health and BioteChnology (Austin, TX). World Scientific. In Press.\\nvon der Malsburg, C. (1973). Self-organization of orientation-sensitive cells in the striate\\ncortex. Kybernetik, 15:85-100.\\nvon der Malsburg, C., and Singer, W. (1988). Principles of cortical network organization. In\\nRakic, P., and Singer, W., editors, Neurobiology 0/Neocortex, 69-99. New York: Wiley.\\n\\n\\x0c',\n",
       "     'pdf_name': '1013-ocular-dominance-and-patterned-lateral-connections-in-a-self-organizing-model-of-the-primary-visual-cortex.pdf',\n",
       "     'title': 'Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1032',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1032',\n",
       "     'paper_text': 'VLSI Model of Primate Visual Smooth Pursuit\\n\\nRalph Etienne-Cummings\\n\\nJan Van der Spiegel\\n\\nDepartment of Electrical Engineering,\\nSouthern Illinois University, Carbondale,\\nIL 62901\\n\\nMoore School of Electrical Engineering,\\nUniversity of Pennsylvania, Philadelphia,\\nPA 19104\\n\\nPaul Mueller\\nCorticon, Incorporated,\\n3624 Market Str, Philadelphia,\\nPA 19104\\n\\nAbstract\\nA one dimensional model of primate smooth pursuit mechanism has\\nbeen implemented in 2 11m CMOS VLSI. The model consolidates\\nRobinson\\'s negative feedback model with Wyatt and Pola\\'s positive\\nfeedback scheme, to produce a smooth pursuit system which zero\\'s the\\nvelocity of a target on the retina. Furthermore, the system uses the\\ncurrent eye motion as a predictor for future target motion. Analysis,\\nstability and biological correspondence of the system are discussed. For\\nimplementation at the focal plane, a local correlation based visual\\nmotion detection technique is used. Velocity measurements, ranging\\nover 4 orders of magnitude with < 15% variation, provides the input to\\nthe smooth pursuit system. The system performed successful velocity\\ntracking for high contrast scenes. Circuit design and performance of the\\ncomplete smooth pursuit system is presented.\\n\\n1 INTRODUCTION\\nThe smooth pursuit mechanism of primate visual systems is vital for stabilizing a region\\nof the visual field on the retina. The ability to stabilize the image of the world on the\\nretina has profound architectural and computational consequences on the retina and visual\\ncortex, such as reducing the required size, computational speed and communication\\nhardware and bandwidth of the visual system (Bandera, 1990; Eckert and Buchsbaum,\\n1993). To obtain similar benefits in active machine vision, primate smooth pursuit can\\nbe a powerful model for gaze control. The mechanism for smooth pursuit in primates\\nwas initially believed to be composed of a simple negative feedback system which\\nattempts to zero the motion of targets on the fovea, figure I (a) (Robinson, 1965).\\nHowever, this scheme does not account for many psychophysical properties of smooth\\n\\n\\x0c707\\n\\nVLSI Model of Primate Visual Smooth Pursuit\\n\\npursuit, which led Wyatt and Pola (1979) to proposed figure l(b), where the eye\\nmovement signal is added to the target motion in a positive feed back loop. This\\nmechanism results from their observation that eye motion or apparent target motion\\nincreases the magnitude of pursuit motion even when retinal motion is zero or constant.\\nTheir scheme also exhibited predictive qualities, as reported by Steinbach (1976). The\\nsmooth pursuit model presented in this paper attempts the consolidate the two models\\ninto a single system which explains the findings of both approaches.\\nTarget\\nMoticn\\n\\nEye\\nMotion\\n\\nRetinal\\nMotion\\n\\ne~\\n\\nlee\\n\\nG\\n\\nee = e t G+l\\n~;\\n\\n>\\n\\nI\\nG ~ co G r\\n\\nTarget\\nMotion\\n\\nEye\\nMotion\\n\\ne~~\\n\\n>\\n\\n=0\\n(b)\\n\\n(a)\\n\\nFigure I: System Diagrams of Primate Smooth Pursuit Mechanism.\\n(a) Negative feedback model by Robinson (1965). (b) Positive\\nfeedback model by Wyatt and Pola (1979).\\nThe velocity based smooth pursuit implemented here attempts to zero the relative velocity\\nof the retina and target. The measured retinal velocity, is zeroed by using positive\\nfeedback to accumulate relative velocity error between the target and the retina, where the\\naccumulated value is the current eye velocity. Hence, this model uses the Robinson\\napproach to match target motion, and the Wyatt and Pola positive feed back loop to\\nachieve matching and to predict the future velocity of the target. Figure 2 shows the\\nsystem diagram of the velocity based smooth pursuit system. This system is analyzed\\nand the stability criterion is derived. Possible computational blocks for the elements in\\nfigure I (b) are also discussed. Furthermore, since this entire scheme is implemented on a\\nsingle 2 /lm CMOS chip, the method for motion detection, the complete tracking circuits\\nand the measured results are presented.\\nRetinal\\nMotion\\n\\nEye\\nMotion\\n\\ner\\n\\nFigure 2: System Diagram of VLSI Smooth Pursuit Mechanism.\\nis target velocity in space, Bt is projected target velocity, Be is the eye\\nvelocity and Br is the measured retinal velocity.\\n\\n2 VELOCITY BASED SMOOTH PURSUIT\\nAlthough figure I (b) does not indicate how retinal motion is used in smooth pursuit, it\\nprovides the only measurement of the projected target motion. The very process of\\ncalculating retinal motion realizes negative feed back between the eye movement and the\\ntarget motion, since retinal motion is the difference between project target and eye\\nmotion. If Robinson\\'s model is followed, then the eye movement is simply the\\namplified version of the retinal motion. If the target disappears from the retina, the eye\\nmotion would be zero. However, Steinbach showed that eye movement does not cea~\\nwhen the target fades off and on, indicating that memory is used to predict target motion.\\nWyatt and Palo showed a direct additive influence of eye movement on pursuit. However,\\nthe computational blocks G\\' and a of their model are left unfilled.\\n\\n\\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\n708\\n\\nIn figure 2, the gain G models the internal gain of the motion detection system , and the\\ninternal representation of retinal velocity is then Vr. Under zero-slip tracking, the retinal\\nvelocity is zero. This is obtained by using positive feed back to correct the velocity error\\nand eye,\\nThe delay element represents a memory of the last eye\\nbetween target,\\nvelocity while the current retinal motion is measured. If the target disappears, the eye\\nmotion continues with the last value, as recorded by Steinbach, thus anticipating the\\nposition of the target in space. The memory also stores the current eye velocity during\\nperfect pursuit. The internal representation of eye velocity, Ve , is subsequently amplified\\nby H and used to drive the eye muscles. The impulse response of the system is given in\\nequations (I). Hence, the relationship between eye velocity and target velocity is recursive\\nand given by equations (2). To prove the stability of this system, the retinal velocity can\\nbe expressed in terms of the target motion as given in equations (3a). The ideal condition\\nfor accurate performance is for GH = 1. However, in practice, gains of different amplifiers\\n\\ner,\\n\\n()\\n\\nz-)\\n\\n=GH--_-)\\n\\n-.f..(z)\\n\\n1- Z\\n\\n(}r\\n\\nee.\\n\\n()\\n\\n(a); ~(I1)\\n(}r\\n\\n=GH[-8(11) + u(n)]\\n\\n(I)\\n\\n(b)\\nn-)\\n\\n(}e(n)\\n\\n= (},(n) -\\n\\n(}r(n)\\n\\n=GH[-8(n) + u(n)] * (}r(n) = GHL(},.(k)\\n\\n(2)\\n\\nk=O\\n() r ( 11)\\n\\n() r (n)\\n\\n= (),( n ) (1\\n11\\n\\n~\\n\\n00\\n\\n)\\n\\n11\\n\\n- GH)\\n0\\n\\nif 11 -\\n\\n=> () r( 1l )\\n\\nI\\n\\nGH < 1\\n\\n= 0 if\\n\\nGH\\n\\n= 1 =>\\n\\n() in)\\n\\n= (),( 11 )\\n\\n=> 0 < GH < 2 for stability\\n\\n(\\n\\na)\\n\\n(3)\\n\\n( b)\\n\\nare rarely perfectly matched. Equations (3b) shows that stability is assured for O<GH< 2.\\nFigure 3 shows a plot of eye motion versus updates for various choices of GH. At each\\nupdate, the retinal motion is computed. Figure 3(a) shows the eye\\'s motion at the on-set\\nof smooth pursuit. For GH = 1, the eye movement tracks the target\\'s motion exactly,\\nand lags slightly only when the target accelerates. On the other hand, if GH? I, the\\neye\\'s motion always lags the target\\'s. If GH -> 2, the system becomes increasing\\nunstable, but converges for GH < 2. The three cases presented correspond to the smooth\\npursuit system being critically, over and under damped, respectively.\\n\\n3 HARDWARE IMPLEMENTATION\\nUsing the smooth pursuit mechanism described, a single chip one dimensional tracking\\nsystem has been implemented. The chip has a multi-layered computational architecture,\\nsimilar to the primate\\'s visual system. Phototransduction, logarithmic compression,\\nedge detection, motion detection and smooth pursuit control has been integrated at the\\nfocal-plane. The computational layers can be partitioned into three blocks, where each\\nblock is based on a segment of biological oculomotor systems.\\n\\n3.1\\n\\nIMAGING AND PREPROCESSING\\n\\nThe first three layers of the system mimics the photoreceptors, horizontal cells arx:l\\nbipolar cells of biological retinas. Similar to previous implementations of silicon\\nretinas, the chip uses parasitic bipolar transistors as the photoreceptors. The dynamic\\nrange of photoreceptor current is compressed with a logarithmic response in low light arx:l\\nsquare root response in bright light. The range compress circuit represents 5-6 orders of\\nmagnitude of light intensity with 3 orders of magnitude of output current dynamic range.\\nSubsequently, a passive resistive network is used to realize a discrete implementation of a\\nLaplacian edge detector. Similar to the rods and cones system in primate retinas, the\\nresponse time, hence the maximum detectable target speed, is ambient intensity dependent\\n(160 (12.5) Ils in 2.5 (250) IlW/cm2). However, this does prevent the system from\\nhandling fast targets even in dim ambient lighting.\\n\\n\\x0cVLSI Model of Primate Visual Smooth Pursuit\\n\\n~\\n\\ng\\n\\nu\\n>\\n\\n709\\n\\n20\\n\\n20\\n\\n15\\n\\n15\\n\\n10\\n\\n10\\n\\n5\\n\\n~\\n\\n5\\n\\n0\\n\\n]\\n\\n-5\\n\\n>\" -5\\n\\n- 10\\n\\n?\\n\\n? 10\\n\\nTarget\\n\\n- -Eye: GH=I 99\\n- E ye GH=IOO\\n__ . Eye: GH=O_IO\\n\\n-15\\n\\n0\\n\\n? 15\\n-20\\n\\n-20\\n100\\n\\n50\\n\\n0\\n\\n150\\n\\n500\\n\\n600\\n\\nUpdates\\n\\n(a)\\n\\n700\\n800\\nUpdates\\n\\n900\\n\\n1000\\n\\n(b)\\n\\nFigure 3: (a) The On-Set of Smooth Pursuit for Various GH Values.\\n(b) Steady-State Smooth Pursuit.\\n\\n3.2\\n\\nMOTION MEASUREMENT\\n\\nThis computational layer measures retinal motion. The motion detection technique\\nimplemented here differs from those believed to exist in areas V 1 and MT of the primate\\nvisual cortex. Alternatively, it resembles the fly\\'s and rabbit\\'s retinal motion detection\\nsystem (Reichardt, 1961; Barlow and Levick, 1965; Delbruck, 1993). This is not\\ncoincidental, since efficient motion detection at the focal plane must be performed in a\\nsmall areas and using simple computational elements in both systems.\\nThe motion detection scheme is a combination of local correlation for direction\\ndetermination, and pixel transfer time measurement for speed. In this framework, motion\\nis defined as the disappearance of an object, represented as the zero-crossings of its edges,\\nat a pixel , followed by its re-appearance at a neighboring pixel. The (dis)appearance of\\nthe zero-crossing is determined using the (negative) positive temporal derivative at the\\npixel. Hence, motion is detected by AND gating the positive derivative of the zerocrossing of the edge at one pixel with the negative derivative at a neighboring pixel. The\\ndirection of motion is given by the neighboring pixel from which the edge disappeared.\\nProvided that motion has been detected at a pixel, the transfer time of the edge over the\\npixel\\'s finite geometry is inversely proportional to its speed.\\nEquation (4) gives the mathematical representation of the motion detection process for an\\nobject moving in +x direction. In the equation. f,(l.\\'k ,y.t) is the temporal response of\\npixel k as the zero crossing of an edge of an object passes over its 2a aperture. Equation\\n(4) gives the direction of motion, while equation (5) gives the speed. The schematic of\\n\\nmotion _ x = [\\n\\nf f,( l: k, y, t) > 0] [ f f t(l.\\' k + J, y, t) < 0] =0\\n\\nmotion+x=[~f,(l.\\'k-J,y,t)<O][~f/l.\\'k , y,t?O]\\n\\n= 8[t\\nMotion.\\' t m =\\n\\nSpeed + x\\n\\n=\\n\\nt\\n\\n-\\n\\n(b)\\n\\n(4)\\n\\n2a(k-n)-a\\nv\\n]8[x - 2ak]\\nx\\n\\n2a(k -n) -a\\nvx\\n\\nJ\\n- t\\n\\n( a)\\n\\nvx\\n\\n2a\\n\\nDisappear .\\' t d\\n\\n2a(k -n) +a\\n\\n= --~--?\\nvx\\n\\n(5)\\n\\nd\\nm\\nthe VLSI circuit of the motion detection model is shown in figure 4(a). Figure 4(b)\\nshows reciprocal of the measured motion pulse-width for 1 D motion. The on-chip speed,\\net, is the projected target speed. The measured pulse-widths span 3-4 orders magnitude,\\n\\n\\x0c710\\n\\nR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\nOne-Over Pulse-Width vs On-Chip Speed\\n\\n?\\n\\nO.R\\n\\n~\\n\\n0.4\\n\\n\"\\n\\n~ -0.0 +--------::II~-----__+\\nM\\n~ -0.4\\n\\n---e-- \\\\IPW_Lefi\\n\\n-0 .8\\n\\n- - . - - IIPW_ Rlght\\n\\n- 1.2 +-\\'----\\'--\\'\\'-+--\\'--\\'--\\'--t---\\'--\\'\\'--\\'-+-\\'--\\'--\\'-t-\\'--\\'--\\'-t---\\'--\\'--\\'-+\\n-40\\n00\\n4.0\\n8.0\\n12.0\\n-12.0\\n-R.O\\nOn-Chip Speed rcml~J\\n\\nRight\\n\\nLeft\\n\\n(b)\\n\\n(a)\\n\\nFigure 4: (a) Schematic of the Motion Detection Circuit.\\nMeasured Output of the Motion Detection Circuit.\\n\\n(b)\\n\\ndepending on the ambient lighting, and show less than 15% variation between chips,\\npixels, and directions (Etienne-Cummings, 1993).\\n\\n3.3\\n\\nTHE SMOOTH PURSUIT CONTROL SYSTEM\\n\\nThe one dimensional smooth pursuit system is implemented using a 9 x I array of\\nmotion detectors. Figure 5 shows the organization of the smooth pursuit chip. In this\\nsystem, only diverging motion is computed to reduce the size of each pixel. The outputs\\nof the motion detectors are grouped into one global motion signal per direction. This\\ngrouping is performed with a simple, but delayed, OR, which prevents pulses from\\nneighboring motion cells from overlapping. The motion pulse trains for each direction\\nare XOR gated, which allows a single integrator to be used for both directions, thus\\nlimiting mis-match_ The final value of the integrator is inversely proportional to the\\ntarget\\'s speed. The OR gates conserve the direction of motion. The reciprocal of the\\nintegrator voltage is next computed using the linear mode operation of a MOS transistor\\n(Etienne-Cummings, 1993). The unipolar integrated pulse allows a single inversion\\ncircuit to be used for both directions of motion, again limiting mis-match. The output of\\nthe \"one-over\" circuit is amplified, and the polarity of the measured speed is restored.\\nThis analog voltage is proportional to retinal speed.\\nThe measured retinal speed is subsequently ailed to the stored velocity. Figure 6 shows\\nthe schematic for the retinal velocity accumulation (positive feedback) and storage (analog\\nWave Forms\\n\\nMotion Pulse Integration\\nand \"One-Over\"\\nV = GIRetinal Velocityl\\n\\nPolarity\\nRestoration\\n\\nRetinal Velocity\\nAccumulation\\nand Sample/Hold\\n\\nFigure 5: Architecture of the VLSI Smooth Pursuit System. Sketches\\nof the wave forms for a fast leftward followed by a slow rightward\\nretinal motion are shown.\\n\\n\\x0c711\\n\\nVLSI Model of Primate Visual Smooth Pursuit\\n\\nmemory). The output of the XOR gate in figure 5 is used by the sample-and-hold circuit\\nto control sampling switches S I and S2. During accumulation, the old stored velocity\\nvalue, which is the current eye velocity, is isolated from the summed value. At the\\nfalling edge of the XOR output, the stored value on C2 is replaced by the new value on\\nCl. This stored value is amplified using an off chip motor driver circuit, and used to\\nmove the chip. The gain of the motor driver can be finely controlled for optimal\\noperation.\\n\\nMotor\\n\\nRetinal\\nVelocity\\n\\nSystem\\n\\nAccumulatiun\\n\\nTarget\\nVelocity\\n\\nTwo Phase Sample/Hold\\n\\nFigure 6: Schematic Retinal Velocity Error Accumulation, Storage and\\nMotor Driver Systems.\\nFigure 7(a) shows a plot of one-over the measured integrated voltage as a function of on\\nchip target speed. Due to noise in the integrator circuit, the dynamic range of the motion\\ndetection system is reduced to 2 orders of magnitude. However, the matching between left\\nand right motion is unaffected by the integrator. The MaS \"one-over\" circuit, used to\\ncompute the analog reciprocal of the integrated voltage, exhibits only 0.06% deviation\\nfrom a fitted line (Etienne-Cummings, 1993b). Figure 7(b) shows the measured\\nincrements in stored target velocity as a function of retinal (on-chip) speed. This is a test\\nof all the circuit components of the tracking system. Linearity between retinal velocity\\nincrements and target velocity is observed, however matching between opposite motion\\nhas degraded. This is caused by the polarity restoration circuit since it is the only\\nlocation where different circuits are used for opposite motion. On average, positive\\nincrements are a factor of 1.2 times larger than negative increments. The error bars shows\\nthe variation in velocity increments for different motion cells and different Chips. The\\ndeviation is less than 15 %. The analog memory has a leakage of 10 mV/min and an\\nasymmetric swing of 2 to -1 V, caused by the buffers. The dynamic range of the\\ncomplete smooth pursuit system is measured to be 1.5 orders magnitude. The maximum\\nspeed of the system is adjustable by varying the integrator charging time. The maximum\\nspeed is ambient intensity dependent and ranges from 93 cmls to 7 cm/s on-chip speed in\\nVelocity Error Increment vs On-Chip Speed\\n\\nIntegrated Pulse vs On-Chip Speed\\n1.4\\n24\\n\\n~\\n\\n16\\n\\n~\\n\\n8\\n\\n~\\n\\n0\\n\\nil\\n?\\noS\\n\\n.\\'\\n._\\n\\n1.2\\n\\n~\\n\\nl\\'! 1.0\\n\\n\"e~\\nu\\n\\n-t--------\",/II!...------+\\n\\n-8\\n\\n.s\\n\\nO.R\\n\\ng 0 .6\\n\\nLLl\\n\\n.::;.\\n\\ng 04\\n\\n:: -16\\n-e--lnlPuI~_l..xft\\n\\n-24\\n\\n_ _? _\\n\\nJntPlllo;e_Rl~hl\\n\\n-32 -t-\\'---\\'---\\'-\\'--+-\\'--~~-t--\\'\"-\\'-~_t_--\"--\\'\\'---\\'---\"-t\\n10.0\\n-100\\n-5.0\\n0.0\\n5.0\\nOn-Chip Speed lemlsl\\n\\n(a)\\n\\nOJ\\n\\n>\\n\\n- - - - . Nc~_ Jn c rt~nl\\n\\n02\\n\\n__ ? _ _Po,,_Incremclll\\n\\n0.0\\n0\\n\\n4\\n6\\nOn-Chip Speed lem/s)\\n\\n(b)\\n\\nFigure 7. (a) Measured integrated motion pulse voltage. (b) Measured\\noutput for the complete smooth pursuit system.\\n\\n10\\n\\n\\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\n712\\n\\nbright (250 JlW/cm 2) and dim (2.5 JlW/cm 2) lighting, respectively. However, for any\\nmaximum speed chosen, the minimum speed is a factor of 0.03 slower. The minimum\\nspeed is limited by the discharge time of the temporal differentiators in the motion\\ndetection circuit to 0.004 cmls on chip. The contrast sensitivity of this system proved to\\nbe the stumbling block, and it can not track objects in normal indoor lighting. However,\\nall circuits components tested successfully when a light source is used as the target.\\nAdditional measured data can be found in (Etienne-Cummings, 1995). Further work will\\nimprove the contrast sensitivity, combat noise and also consider two dimensional\\nimplementations with target acquisition (saccades) capabilities.\\n\\n4\\n\\nCONCLUSION\\n\\nA model for biological and silicon smooth pursuit has been presented. It combines the\\nnegative feed back and positive feedback models of Robinson and Wyatt and Pola. The\\nsmooth pursuit system is stable if the gain product of the retinal velocity detection\\nsystem and the eye movement system is less than 2. VLSI implementation of this\\nsystem has been performed and tested. The performance of the system suggests that wide\\nrange (92.9 - 0.004 cmls retinal speed) target tracking is possible with a single chip focal\\nplane system. To improve this chip\\'s performance, care must be taken to limit noise,\\nimprove matching and increase contrast sensitivity. Future design should also include a\\nsaccadic component to re-capture escaped targets, similar to biological systems.\\n\\nReferences\\nC. Bandera, \"Foveal Machine Vision Systems\", Ph.D. Thesis, SUNY Buffalo, New\\nYork, ]990\\nH. Barlow and W. Levick, \\'The Mechanism for Directional Selective Units in Rabbit\\' s\\nRetina\", Journal of Physiology, Vol. 178, pp. 477-504, ]965\\nT. Delbruck, \"Silicon Retina with Correlation-Based, Velocity-Tuned Pixels \", IEEE\\nTransactions on Neural Networks, Vol. 4:3, pp. 529-41, 1993\\n\\nM. Eckert and G. Buchsbaum, \"Effect of Tracking Strategies on the Velocity Structure of\\nTwo-Dimensional Image Sequences\", J. Opt. Soc. Am., Vol. AIO:7, pp. 1582-85, 1993\\nR. Etienne-Cummings et at., \"A New Temporal Domain Optical Flow Measurement\\nTechnique for Focal Plane VLSI Implementation\", Proceedings of CAMP 93, M.\\nBayoumi, L. Davis and K. Valavanis (Eds.), pp. 24]-25] , 1993\\nR. Etienne-Cummings, R. Hathaway and J. Van der Spiegel, \"An Accurate and Simple\\nCMOS \\'One-Over\\' Circuit\", Electronic Letters, Vol. 29-18, pp. ]618-]620, 1993b\\nR. Etienne-Cummings et aI., \"Real-Time Visual Target Tracking: Two Implementations\\nof Velocity Based Smooth Pursuit\", Visual Information Processing IV, SPIE Vol. 2488,\\nOrlando, 17-18 April 1995\\n\\nW. Reichardt, \"Autocorrelation, A Principle for the Evaluation of Sensory Information by\\nthe Central Nervous System\", Sensory Communication, Wiley, New York, 1961\\nD. Robinson, \"The Mechanism of Human Smooth Pursuit Eye Movement\", Journal of\\nPhysiology ( London) Vol. 180, pp. 569-591 , 1965\\nM. Steinbach, \"Pursuing the Perceptual Rather than the Retinal Stimuli\", Vision\\nResearch, Vol. 16, pp. 1371-1376,1976\\nH. Wyatt and J. Pola, \"The Role of Perceived Motion in Smooth Pursuit Eye\\nMovements\", Vision Research, Vol. 19, pp. 613-618, 1979\\n\\n\\x0c',\n",
       "     'pdf_name': '1032-vlsi-model-of-primate-visual-smooth-pursuit.pdf',\n",
       "     'title': 'VLSI Model of Primate Visual Smooth Pursuit',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1033',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1033',\n",
       "     'paper_text': 'Gradient and Hamiltonian Dynamics\\nApplied to Learning in Neural Networks\\nJames W. Howse\\n\\nChaouki T. Abdallah\\n\\nGregory L. Heileman\\n\\nDepartment of Electrical and Computer Engineering\\nUniversity of New Mexico\\nAlbuquerque, NM 87131\\n\\nAbstract\\nThe process of machine learning can be considered in two stages: model\\nselection and parameter estimation. In this paper a technique is presented\\nfor constructing dynamical systems with desired qualitative properties. The\\napproach is based on the fact that an n-dimensional nonlinear dynamical\\nsystem can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and\\nHamiltonian portions appropriately so that a certain behavior is obtainable.\\nTo estimate the parameters, a stably convergent learning rule is presented.\\nThis algorithm has been proven to converge to the desired system trajectory\\nfor all initial conditions and system inputs. This technique can be used to\\ndesign neural network models which are guaranteed to solve the trajectory\\nlearning problem.\\n\\n1\\n\\nIntroduction\\n\\nA fundamental problem in mathematical systems theory is the identification of dynamical systems. System identification is a dynamic analogue of the functional approximation problem. A set of input-output pairs {u(t), y(t)} is given over some time\\ninterval t E [7i, 1j]. The problem is to find a model which for the given input sequence\\nreturns an approximation of the given output sequence. Broadly speaking, solving an\\nidentification problem involves two steps. The first is choosing a class of identification models which are capable of emulating the behavior of the actual system. The\\nsecond is selecting a method to determine which member of this class of models best\\nemulates the actual system. In this paper we present a class of nonlinear models and\\na learning algorithm for these models which are guaranteed to learn the trajectories\\nof an example system. Algorithms to learn given trajectories of a continuous time\\nsystem have been proposed in [6], [8], and [7] to name only a few. To our knowledge,\\nno one has ever proven that the error between the learned and desired trajectories\\nvanishes for any of these algorithms. In our trajectory learning system this error is\\nguaranteed to vanish. Our models extend the work in [1] by showing that Cohen\\'s\\nsystems are one instance of the class of models generated by decomposing the dynamics into a component normal to some surface and a set of components tangent to the\\nsame surface. Conceptually this formalism can be used to design dynamical systems\\nwith a variety of desired qualitative properties. Furthermore, we propose a provably\\nconvergent learning algorithm which allows the parameters of Cohen\\'s models to be\\nlearned from examples rather than being programmed in advance. The algorithm is\\n\\n\\x0c275\\n\\nGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\nconvergent in the sense that the error between the model trajectories and the desired trajectories is guaranteed to vanish. This learning procedure is related to one\\ndiscussed in [5] for use in linear system identification.\\n\\n2\\n\\nConstructing the Model\\n\\nFirst some terminology will be defined. For a system of n first order ordinary differential equations, the phase space of the system is the n-dimensional space of all state\\ncomponents. A solution trajectory is a curve in phase space described by the differential equations for one specific starting point. At every point on a trajectory there\\nexists a tangent vector. The space of all such tangent vectors for all possible solution\\ntrajectories constitutes the vector field for this system of differential equations.\\nThe trajectory learning models in this paper are systems of first order ordinary differential equations. The form of these equations will be obtained by considering the\\nsystem dynamics as motion relative to some surface. At each point in the state space\\nan arbitrary system trajectory will be decomposed into a component normal to this\\nsurface and a set of components tangent to this surface. This approach was suggested\\nto us by the results in [4], where it is shown that an arbitrary n-dimensional vector\\nfield can be decomposed locally into the sum of one gradient vector field and (n - 1)\\nHamiltonian vector fields. The concept of a potential function will be used to define these surfaces. A potential function V(:z:) is any scalar valued function of the\\nsystem states :z: = [Xl, X2, ??? , Xn.] t which is at least twice continuously differentiable\\n(Le. V(:z:) E or : r ~ 2). The operation [.]t denotes the transpose of the vector. If\\nthere are n components in the system state, the function V{:z:), when plotted with\\nrespect all of the state components, defines a surface in an (n + 1)-dimensional space.\\nThere are two curves passing through every point on this potential surface which are\\nof interest in this discussion, they are illustrated in Figure 1(a). The dashed curve is\\n(z - zo)t \\\\7 ... v (z)l ...o = 0\\n\\n(a)\\n\\n(b)\\n\\nV(z) = K-\\n\\nFigure 1: (a) The potential function V(z) = X~ (Xl _1)2 +x~ plotted versus its two dependent variables Xl and X2. The dashed curve is called a level surface and is given\\nby V(z) = 0.5. The solid curve follows the path of steepest descent through Zo.\\n(b) The partitioning of a 3-dimensional vector field at the point Zo into a 1dimensional portion which is normal to the surface V(z) = K- and a 2-dimensional\\nportion which is tangent to V(z) = K-. The vector -\\\\7 ... V(z) 1\"\\'0 is the normal vector to the surface V(z) = K- at the point Zo. The plane (z - zo)t \\\\7 ... V (z) 1\"\\'0 = 0\\ncontains all of the vectors which are tangent to V(z) = K- at Zo. Two linearly\\nindependent vectors are needed to form a basis for this tangent space, the pair\\nQ2(z) \\\\7 ... V (z)l ... o and Q3(Z) \\\\7 ... V (z)l ... o that are shown are just one possibility.\\nreferred to as a level surface, it is a surface along which V(:z:) = K for some constant\\nK. Note that in general this level surface is an n-dimensional object. The solid curve\\n\\n\\x0c276\\n\\nJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\nmoves downhill along V (X) following the path of steepest descent through the point\\nXo. The vector which is tangent to this curve at Xo is normal to the level surface\\nat Xo. The system dynamics will be designed as motion relative to the level surfaces\\nof V(x). The results in [4] require n different local potential functions to achieve\\narbitrary dynamics. However, the results in [1] suggest that a considerable number\\nof dynamical systems can be achieved using only a single global potential function.\\nA system which is capable of traversing any downhill path along a given potential\\nsurface V(x), can be constructed by decomposing each element of the vector field\\ninto a vector normal to the level surface of V(x) which passes through each point\\nand a set of vectors tangent to the level surface of V(x) which passes through the\\nsame point. So the potential function V(x) is used to partition the n-dimensional\\nphase space into two subspaces. The first contains a vector field normal to some\\nlevel surface V(x) = }( for }( E IR, while the second subspace holds a vector field\\ntangent to V(x) = IC. The subspace containing all possible normal vectors to the\\nn-dimensional level surface at a given point, has dimension one. This is equivalent\\nto the statement that every point on a smooth surface has a unique normal vector.\\nSimilarly, the subspace containing all possible tangent vectors to the level surface at\\na given point has dimension (n - 1). An example of this partition in the case of a\\n3-dimensional system is shown in Figure 1(b). Since the space of all tangent vectors\\nat each point on a level surface is (n - I)-dimensional, (n - 1) linearly independent\\nvectors are required to form a basis for this space.\\nMathematically, there is a straightforward way to construct dynamical systems which\\neither move downhill along V(x) or remain at a constant height on V(x). In this\\npaper, dynamical systems which always move downhill along some potential surface\\nare called gradient-like systems. These systems are defined by differential equations\\nof the form\\nx = -P(x) VII:V(x),\\n(1)\\nwhere P(x) is a matrix function which is symmetric (Le. pt = P) and positive\\n:z~]f. These systems\\ndefinite at every point x, and where VIII V(x) =\\nare similar to the gradient flows discussed in [2]. The trajectories of the system\\nformed by Equation (1) always move downhill along the potential surface defined by\\nV(x). This can be shown by taking the time derivative of V(x) which is V(x) =\\n-[VII: V (x)]t P(x) [VII: V(x)] :5 O. Because P(x) is positive definite, V(x) can only be\\nzero where V II: V (x) = 0, elsewhere V(x) is negative. This means that the trajectories\\nof Equation (1) always move toward a level surface of V(x) formed by \"slicing\" V(x)\\nat a lower height, as pointed out in [2]. It is also easy to design systems which remain\\nat a constant height on V(x). Such systems will be denoted Hamiltonian-like systems.\\nThey are specified by the equation\\nx = Q(x) VII: V(x),\\n(2)\\nwhere Q(x) is a matrix function which is skew-symmetric (Le. Qt = -Q) at every\\npoint x. These systems are similar to the Hamiltonian systems defined in [2]. The\\nelements of the vector field defined by Equation (2) are always tangent to some level\\nsurface of V (x). Hence the trajectories ofthis system remain at a constant height on\\nthe potential surface given by V(x). Again this is indicated by the time derivative\\nof V(x), which in this case is V(x) = [VII: V(x)]f Q(x)[VII: V(x)] = o. This indicates\\nthat the trajectories of Equation (2) always remain on the level surface on which the\\nsystem starts. So a model which can follow an arbitrary downhill path along the\\npotential surface V(x) can be designed by combining the dynamics of Equations (1)\\nand (2) . The dynamics in the subspace normal to the level surfaces of V(x) can be\\n\\n[g;: , g;: ,... ,\\n\\n\\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\n277\\n\\ndefined using one equation of the form in Equation (1). Similarly the dynamics in the\\nsubspace tangent to the level surfaces of Vex) can be defined using (n - 1) equations\\nof the form in Equation (2). Hence the total dynamics for the model are\\nn\\n\\nz= -P(x)VIDV(x) + LQi(X)VIDV(x).\\n\\n(3)\\n\\ni=2\\n\\nFor this model the number and location of equilibria is determined by the function\\nVex), while the manner in which the equilibria are approached is determined by the\\nmatrices P(x) and Qi(x).\\nIf the potential function Vex) is bounded below (i.e. Vex) > Bl V x E IRn , where\\nBl is a constant), eventually increasing (i.e. limlllDlI-+oo Vex) ~ 00) , and has only\\na finite number of isolated local maxima and minima (i.e. in some neighborhood\\nof every point where V III V (x) = 0 there are no other points where the gradient\\nvanishes), then the system in Equation (3) satisfies the conditions of Theorem 10\\nin [1]. Therefore the system will converge to one of the points where V ID Vex) = 0,\\ncalled the critical points of Vex), for all initial conditions. Note that this system\\nis capable of all downhill trajectories along the potential surface only if the (n - 1)\\nvectors Qi(X) V ID Vex) V i = 2, ... , n are linearly independent at every point x. It\\nis shown in [1] that the potential function\\n\\nV(z) = C (\\n\\n1:., (-y) d-y +\\n\\nt, [~\\n\\n(XI - I:.,(xd)\\'\\n\\n+~\\n\\nJ:\\'\\n\\n1:., h )II:.: (-y)]\\' d-y\\n\\n1\\n\\n(4)\\n\\nsatisfies these three criteria. In this equation ?.i(Xt} Vi = 1, ... , n are interpolation\\npolynomials, C is a real positive constant, Xi Vi = 1, ... , n are real constants chosen\\nso that the integrals are positive valued, and ?.Hxt} ==\\n\\nf:-.\\n\\n3\\n\\nThe Learning Rule\\n\\nIn Equation (3) the number and location of equilibria can be controlled using the\\npotential function Vex), while the manner in which the equilibria are approached can\\nbe controlled with the matrices P(x) and Qi(X). If it is assumed that the locations\\nof the equilibria are known, then a potential function which has local minima and\\nmaxima at these points can be constructed using Equation (4). The problem of\\ntrajectory learning is thereby reduced to the problem of parameterizing the matrices\\nP(x) and Qi(x) and finding the parameter values which cause this model to best\\nemulate the actual system. If the elements P(x) and Qi(x) are correctly chosen,\\nthen a learning rule can be designed which makes the model dynamics converge to\\nthat of the actual system. Assume that the dynamics given by Equation (3) are a\\nparameterized model of the actual dynamics. Using this model and samples of the\\nactual system states, an estimator for states of the actual system can be designed. The\\nbehavior of the model is altered by changing its parameters, so a parameter estimator\\nmust also be constructed. The following theorem provides a form for both the state\\nand parameter estimators which guarantees convergence to a set of parameters for\\nwhich the error between the estimated and target trajectories vanishes.\\nTheorem 3.1. Given the model system\\nk\\n\\nZ = LAili(x) +Bg(u)\\n\\n(5)\\n\\ni=l\\n\\nwhere Ai E IRnxn and BE IRnxm are unknown, and li(\\') and g(.) are known smooth\\nfunctions such that the system has bounded solutions for bounded inputs u(t). Choose\\n\\n\\x0cJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\n278\\n\\na state estimator of the form\\nk\\n\\n~ = \\'R. B (x - x) +\\n\\nL Ai fi(x) + iJ g(u)\\n\\n(6)\\n\\ni=1\\n\\nwhere\\'R. B is an (n x n) matrix of real constants whose eigenvalues must all be in the\\nleft half plane, and Ai and iJ are the estimates of the actual parameters. Choose\\nparameter estimators of the form\\n~\\nt\\nAi = -\\'R.p (x - x) [fi(x)] V i = 1, ... , k\\n(7)\\n= -\\'R.p (x - x) [g(u)]t\\n\\nB\\n\\nwhere \\'R. p is an (n x n) matrix of real constants which is symmetric and positive\\ndefinite, and (x - x) [.]t denotes an outer product. For these choices of state and\\nparameter estimators limt~oo(x(t) -x(t? = 0 for all initial conditions. Furthermore,\\nthis remains true if any of the elements of Ai or iJ are set to 0, or if any of these\\nmatrices are restricted to being symmetric or skew-symmetric.\\nThe proof of this theorem appears in [3]. Note that convergence of the parameter\\nestimates to the actual parameter values is not guaranteed by this theorem. The\\nmodel dynamics in Equation (3) can be cast in the form of Equation (5) by choosing\\neach element of P(x) and Qi(X) to have the form\\nI-I\\n\\nn\\n\\nn\\n\\nI-I\\n\\n= LL~rBjkt?k(Xj)\\n\\nand\\nQrB = LLArBjk ek(Xj),\\n(8)\\nj=1 k=O\\nj=1 k=O\\nwhere {t?o(Xj), t?1 (Xj), ... ,t?I-1 (Xj)} and {eo(Xj), el (Xj), ... ,el-l (Xj)} are a set of 1\\northogonal polynomials which depend on the state Xj\\' There is a set of such polynomials for every state Xj, j = 1,2, ... , n. The constants ~rBjk and ArBjk determine\\nthe contribution of the kth polynomial which depends on the jth state to the value\\nof Prs and Qrs respectively. In this case the dynamics in Equation (3) become\\nPrB\\n\\n:i:\\n\\n=\\n\\nt. ~ {\\n\\nS;. [11.(x;) V. V (z)j\\n\\n+\\n\\nt,\\n\\nA;;. [e;.(x;)\\n\\nv. V(z)j } + T g(u(t))\\n\\n(9)\\n\\nwhere 8 jk is the (n x n) matrix of all values ~rsjk which have the same value of j and\\nk. Likewise A ijk is the (n x n) matrix of all values Arsjk, having the same value of\\nj and k, which are associated with the ith matrix Qi(X). This system has m inputs,\\nwhich may explicitly depend on time, that are represented by the m-element vector\\nfunction u(t). The m-element vector function g(.) is a smooth, possibly nonlinear,\\ntransformation of the input function. The matrix Y is an (n x m) parameter matrix\\nwhich determines how much of input S E {I, ... , m} effects state r E {I, ... , n}.\\nAppropriate state and parameter estimators can be designed based on Equations (6)\\nand (7) respectively.\\n\\n4\\n\\nSimulation Results\\n\\nNow an example is presented in which the parameters of the model in Equation (9)\\nare trained, using the learning rule in Equations (6) and (7), on one input signal and\\nthen are tested on a different input signal. The actual system has three equilibrium\\npoints, two stable points located at (1,3) and (3,5), and a saddle point located at\\n(2 - ~,4 + ~). In this example the dynamics of both the actual system and the\\nmodel are given by\\n\\n(~1) =\\nZ2\\n\\nZ~\\n\\nZ~\\n\\nO\\n\\n(1\\'1 + 1\\'2\\n+:3\\n2)\\n0 1\\'4 + 1\\'5 Z1 + 1\\'6 Z2\\n\\n(:~)\\n+ (0 - {1\\'7 + 1\\'8 Z1 + 1\\'9 Z2}) (:~ ) + (1\\'10) u(t)\\n8Y\\n\\'P7 + \\'P8 ZI + 1\\'9 Z2\\n8Y\\n0\\n\\n8Z2\\n\\n0\\n\\n8Z2\\n\\n(10)\\n\\n\\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\n279\\n\\nwhere V(x) is defined in Equation (4) and u(t) is a time varying input. For the actual\\nsystem the parameter values were \\'PI = \\'P4 = -4, \\'P2 = \\'Ps = -2, \\'P3 = \\'P6 = -1,\\n\\'P7 = 1, \\'Ps = 3, \\'P9 = 5, and \\'PIO = 1. In the model the 10 elements \\'Pi are\\ntreated as the unknown parameters which must be learned. Note that the first matrix\\nfunction is positive definite if the parameters \\'PI-\\'P6 are all negative valued. The\\nsecond matrix function is skew-symmetric for all values of \\'P7-\\'P9. The two input\\nsignals used for training and testing were Ul = 10000 (sin! 1000t + sin ~ 1000t) and\\nU2 = 5000 sin 1000 t. The phase space responses of the actual system to the inputs UI\\nand U2 are shown by the solid curves in Figures 3(b) and 3(a) respectively. Notice that\\nboth of these inputs produce a periodic attractor in the phase space of Equation (10).\\nIn order to evaluate the effectiveness of the learning algorithm the Euclidean distance\\nbetween the actual and learned state and parameter values was computed and plotted\\nversus time. The results are shown in Figure 2. Figure 2(a) shows these statistics when\\n{1I~zll, II~\\'PII}\\n\\n{1I~zll, II~\\'PII}\\n\\n17.5\\n15\\n15\\n12.5\\n12.5\\n10\\n\\n7.5\\n\\ni\\n\\n----\\n\\n,., ~--.----... ... .......\\n\\n- --\\n\\n2.5\\n\\n150\\n200\\n250\\n300 t\\n50\\n100\\n150\\n200\\n250\\n300 t\\n(a)\\n(b)\\nFigure 2: (a) The state and parameter errors for training using input signal Ut. The solid\\ncurve is the Euclidean distance between the state estimates and the actual states\\nas a function of time. The dashed curve shows the distance between the estimated\\nand actual parameter values versus time.\\n(b) The state and parameter errors for training using input signal U2.\\n50\\n\\n100\\n\\ntraining with input UI, while Figure 2(b) shows the same statistics for input U2. The\\nsolid curves are the Euclidean distance between the learned and actual system states,\\nand the dashed curves are the distance between the learned and actual parameter\\nvalues. These statistics have two noteworthy features. First, the error between the\\nlearned and desired states quickly converges to very small values, regardless of how\\nwell the actual parameters are learned. This result was guaranteed by Theorem 3.1.\\nSecond, the final error between the learned and desired parameters is much lower when\\nthe system is trained with input UI. Intuitively this is because input Ul excites more\\nfrequency modes of the system than input U2. Recall that in a nonlinear system the\\nfrequency modes excited by a given input do not depend solely on the input because\\nthe system can generate frequencies not present in the input. The quality of the\\nlearned parameters can be qualitatively judged by comparing the phase plots using\\nthe learned and actual parameters for each input, as shown in Figure 3. In Figure 3(a)\\nthe system was trained using input Ul and tested with input U2, while in Figure 3(b)\\nthe situation was reversed. The solid curves are the system response using the actual\\nparameter values, and the dashed curves are the response for the learned parameters.\\nThe Euclidean distance between the target and test trajectories in Figure 3(a) is in\\nthe range (0,0.64) with a mean distance of 0.21 and a standard deviation of 0.14. The\\ndistance between the the target and test trajectories in Figure 3(b) is in the range\\n(0,4.53) with a mean distance of 0.98 and a standard deviation of 1.35. Qualitatively,\\nboth sets of learned parameters give an accurate response for non-training inputs.\\n\\n\\x0c280\\n\\n1. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\n5\\nI\\n\\no\\n\\n{i\\n\\n-------r-- -- ----- --- -- I\\n\\n-5\\n\\n-10\\n\\n-15\\n\\n-l\\n\\n-1\\n\\n1\\n\\n-2\\n\\n-1\\n\\n4\\n\\nXl\\n\\n(a)\\n(b)\\nFigure 3: (a) A phase plot of the system response when trained with input UI and tested\\nwith input U2. The solid line is the response to the test input using the actual\\nparameters. The dotted line is the system response using the learned parameters.\\n(b) A phase plot of the system response when trained with input U2 and tested\\nwith input UI.\\n\\nNote that even when the error between the learned and actual parameters is large,\\nthe periodic attractor resulting from the learned parameters appears to have the same\\n\"shape\" as that for the actual parameters.\\n\\n5\\n\\nConclusion\\n\\nWe have presented a conceptual framework for designing dynamical systems with\\nspecific qualitative properties by decomposing the dynamics into a component normal\\nto some surface and a set of components tangent to the same surface. We have\\npresented a specific instance of this class of systems which converges to one of a finite\\nnumber of equilibrium points. By parameterizing these systems, the manner in which\\nthese equilibrium points are approached can be fitted to an arbitrary data set. We\\npresent a learning algorithm to estimate these parameters which is guaranteed to\\nconverge to a set of parameter values for which the error between the learned and\\ndesired trajectories vanishes.\\n\\nAcknowledgments\\nThis research was supported by a grant from Boeing Computer Services under Contract\\nW-300445. The authors would like to thank Vangelis Coutsias, Tom Caudell, and Bill\\nHome for stimulating discussions and insightful suggestions.\\n\\nReferences\\n[1] M.A. Cohen. The construction of arbitrary stable dynamics in nonlinear neural networks.\\nNeural Networks, 5(1):83-103, 1992.\\n[2] M.W. Hirsch and S. Smale. Differential equations, dynamical systems, and linear algebra,\\nvolume 60 of Pure and Applied Mathematics. Academic Press, Inc., San Diego, CA, 1974.\\n[3] J.W. Howse, C.T. Abdallah, and G.L. Heileman. A gradient-hamiltonian decomposition\\nfor designing and learning dynamical systems. Submitted to Neural Computation, 1995.\\n[4] R.V. Mendes and J .T. Duarte. Decomposition of vector fields and mixed dynamics.\\nJournal of Mathematical Physics, 22(7):1420-1422, 1981.\\n[5] K.S. Narendra and A.M. Annaswamy. Stable adaptitJe systems. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1989.\\n[6] B.A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural\\nComputation, 1(2):263-269, 1989.\\n[7] D. Saad. Training recurrent neural networks via trajectory modification. Complex Systems, 6(2) :213-236, 1992.\\n[8] M.-A. Sato. A real time learning algorithm for recurrent analog neural networks. Biological Cybernetics, 62(2):237-241, 1990.\\n\\n\\x0c',\n",
       "     'pdf_name': '1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf',\n",
       "     'title': 'Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1035',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1035',\n",
       "     'paper_text': 'A Dynamical Model of Context Dependencies for the\\nVestibulo-Ocular Reflex\\nTerrence J. Sejnowskit\\n\\nOlivier J.M.D. Coenen*\\n\\nComputational Neurobiology Laboratory\\nHoward Hughes Medical Institute\\nThe Salk Institute for Biological Studies\\n10010 North Torrey Pines Road\\nLa Jolla, CA 92037, U.S.A.\\nDepartments oftBiology and *tPhysics\\nUniversity of California, San Diego\\nLa Jolla, CA 92093, U.S.A\\n\\n{olivier,terry}@salk.edu\\n\\nAbstract\\nThe vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid\\nhead motions. The gain of the VOR (the ratio of eye to head rotation velocity)\\nis typically around -1 when the eyes are focused on a distant target. However, to\\nstabilize images accurately, the VOR gain must vary with context (eye position,\\neye vergence and head translation). We first describe a kinematic model of the\\nVOR which relies solely on sensory information available from the semicircular\\ncanals (head rotation), the otoliths (head translation), and neural correlates of eye\\nposition and vergence angle. We then propose a dynamical model and compare it\\nto the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and\\nsuggests one way to combine the required neural signals within the cerebellum and\\nthe brain stem. It also makes predictions for the responses of neurons to multiple\\ninputs (head rotation and translation, eye position, etc.) in the oculomotor system.\\n\\n1 Introduction\\nThe VOR stabilizes images on the retina during rapid head motions: Rotations and translations of\\nthe head in three dimensions must be compensated by appropriate rotations of the eye. Because the\\nhead\\'s rotation axis is not the same as the eye\\'s rotation axis, the calculations for proper image stabilization of an object must take into account diverse variables such as object distance from each eye,\\n\\n\\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\\n\\n90\\n\\ngaze direction, and head translation (Viire et al., 1986). The stabilization is achieved by integrating\\ninfonnation from different sources: head rotations from the semicircular canals of the inner ear, head\\ntranslations from the otolith organs, eye positions, viewing distance, as well as other context infonnation, such as posture (head tilts) or activity (walking, running) (Snyder and King, 1992; Shelhamer\\net al.,1992; Grossman et al., 1989). In this paper we concentrate on the context modulation of the\\nVOR which can be described by the kinematics of the reflex, i.e. eye position, eye vergence and\\nhead translation.\\n\\n2\\n\\nThe Vestibulo-Ocular Reflex: Kinematic Model\\nDefinition of Vectors\\n\\nTarget Object\\n\\nCoordinate System\\n\\nGaze Vector\\n\\nGaze Angle\\nInterocular\\nDistance\\nEye position\\nVector\\n\\nRotation Axis\\n\\nSemicircular\\nCanals and\\nOtoliths\\nHead\\nTop View\\n\\n?\\n\\n~_--+_\\n\\nOrigin of coordinate\\nsyste,,-, (arbitrary)\\n\\nFigure 1: Diagram showing the definition of the vectors used in the equation of the kinematic model of the\\nvestibulo-ocular reflex.\\n\\nThe ideal VOR response is a compensatory eye movement which keeps the image fixed on the retina\\nfor any head rotations and translations. We therefore derived an equation for the eye rotation velocity\\nby requiring that a target remains stationary on the retina. The velocity of the resulting compensatory\\neye rotation can be written as (see fig. 1):\\n\\nw= -Oe + 1:1\\n\\nx [Dej x\\n\\nOe - To;]\\n\\n(1)\\n\\nwhere Oe is the head rotation velocity sensed by the semicircular canals, TOj is the head translation\\nvelocity sensed by the otoliths, Dej == (e - OJ), eis a constant vector specifying the location of an\\neye in the head, OJ is the position of either the left or right otolith, fJ and Igl are the unit vector and\\namplitude of the gaze vector: fJ gives the eye position (orientation of the eye relative to the head),\\nand Igl gives the distance from the eye to the object, and the symbol x indicates the cross-product\\nbetween two vectors. wand Oe are rotation vectors which describe the instantaneous angUlar velocity\\nof the eye and head, respectively. A rotation vector lies along the instantaneous axis of rotation;\\nits magnitude indicates the speed of rotation around the axis, and its direction is given by the righthand screw rule. A motion of the head combining rotation (0) and translation (T) is sensed as the\\ncombination of a rotation velocity Oe measured by the semicircular canals and a translation velocity\\nTo sensed by the otoliths. The rotation vectors are equal (0 = Oe), and the translation velocity vector\\nas measured by the otoliths is given by: TOj = OOj x 0 + T, where OOj == (a - OJ), and a is the\\nposition vector of the axis of rotation.\\n\\n\\x0c91\\n\\nA Dynarnical Model of Context Dependencies for the Vestibula-Ocular Reflex\\n\\nThe special case where the gaze is horizontal and the rotation vector is vertical (horizontal head rotation) has been studied extensively in the literature. We used this special case in the sirnulations.\\nIn that case rnay be sirnplify by writing its equation with dot products. Since 9 and\\nare then\\nperpendicular (9 . fie = 0). the first term of the following expression in brackets is zero:\\n\\nw\\n\\nslc\\n\\n(2)\\n\\nThe sernicircular canals decornpose and report acceleration and velocity of head rotation fi by its\\ncornponents along the three canals on each side of the head fie : horizontal. anterior and posterior.\\nThe two otolith organs on each side report the dynamical inertial forces generated during linear rnotion (translation) in two perpendicular plane. one vertical and the other horizontal relative to the head.\\nHere we assurne that a translation velocity signal (To) derived frorn or reported by the otolith afferents is available. The otoliths encode as well the head orientation relative to the gravity vector force.\\nbut was not included in this study.\\nTo cornplete the correspondence between the equation and a neural correlate. we need to determine\\nThe eye position 9 is assurned to be given by the output of the\\na physiological source for 9 and\\nvelocity-to-position transformation or so-called \"neural integrator\" which provides eye position information and which is necessary for the activation of the rnotoneuron to sustain the eye in a fixed\\nposition. The integrator for horizontal eye position appears to be located in the nucleus prepositus\\nhypoglossi in the pons. and the vertical integrator in the rnidbrain interstitial nucleus of Cajal. (Crawford. Cadera and Vilis. 1991; Cannon and Robinson. 1987). We assurne that the eye position is given\\nas the coordinates of the unit vector 9 along the ~ and 1; of fig. 1. The eye position depends on the\\neye velocity according to\\n= 9 x w. For the special case w(t) = w(t)z. i.e. for horizontal head\\nrotation. the eye position coordinates are given by:\\n\\nI!I.\\n\\n\\'*\\n\\n91 (t) =\\n\\n91 (0) + f~ iJ2( r )w( r) dr\\n\\n92(t) =\\n\\n92(0) - f~ 91(r)w(r)dr\\n\\n(3)\\n\\nThis is a set of two negatively coupled integrators. The \"neural integrator\" therefore does not integrate the eye velocity directly but a product of eye position and eye velocity. The distance frorn eye\\nto target\\ncan be written using the gaze angles in the horizontal plane of the head:\\n\\nI!I\\n\\n1\\n\\n(4)\\n\\n1\\n\\n(5)\\n\\nRight eye:\\n\\n19RT\\n\\nLeft eye:\\n\\n19LT\\n\\nwhere ?()R - () L) is the vergence angle. and I is the interocular distance; the angles are rneasured frorn\\na straight ahead gaze. and take on negative values when the eyes are turned towards the right. Within\\nthe oculornotor systern. the vergence angle and speed are encoded by the rnesencephalic reticular\\nformation neurons (Judge and Curnrning. 1986; Mays. 1984). The nucleus reticularis tegrnenti pontis\\nwith reciprocal connections to the flocculus. oculornotor vermis. paravermis of the cerebellurn also\\ncontains neurons which activity varies linearly with vergence angle (Gamlin and Clarke. 1995).\\nWe conclude that it is possible to perform the cornputations needed to obtain an ideal VOR with signals known to be available physiologically.\\n\\n\\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\\n\\n92\\nDynamical Model Overview\\n\\nNod_\\nPftpoIItao\\n\\nIIyposIoooI\\n\\nFigure 2: Anatomical connections considered in the dynamical model. Only the left side is shown, the right\\nside is identical and connected to the left side only for the calculation of vergence angle. The nucleus prepositus\\nhypoglossi and the nucleus reticularis tegmenti pontis are meant to be representative of a class of nuclei in the\\nbrain stem carrying eye position or vergence signal. All connections are known to exist except the connection\\nbetween the prepositus nucleus to the reticularis nucleus which has not been verified. Details of the cerebellum\\nare in fig. 3 and of the vestibular nucleus in fig. 4.\\n\\n3 Dynamical Model\\nSnyder & King (1992) studied the effect of viewing distance and location of the axis of rotation on\\nthe VOR in monkeys; their main results are reproduced in fig. 5. In an attempt to reproduce their\\ndata and to understand how the signals that we have described in section 2 may be combined in time,\\nwe constructed a dynamical model based on the kinematic model. Its basic anatomical structure is\\nshown in fig. 2. Details of the model are shown in fig. 3, and fig . 4 where all constants are written\\nusing a millisecond time scale. The results are presented in fig. 5. The dynamical variables represent\\nthe change of average firing rate from resting level of activity. The firing rate of the afferents has a\\ntonic component proportional to the velocity and a phasic component proportional to the acceleration\\nof movement. Physiologically, the afferents have a wide range of phasic and tonic amplitudes. This\\nis reflected by a wide selection of parameters in the numerators in the boxes of fig. 3 and fig. 4. The\\nLaplace transform of the integration operator in equation (3) of the eye position coordinates is ~.\\nFollowing Robinson (1981), we modeled the neural integrator with a gain and a time constant of\\n20 seconds. We therefore replaced the pure integrator ~ with 20~~~~1 in the calculations of eye\\nposition. The term 1 in fig. 3 is calculated by using equations (4) and (5), and by using the integrator\\n9\\n\\n20~o:!~1 on the eye velocity motor command to find the angles (h and (JR.\\n\\nThe dynamical model is based on the assumption that the cerebellum is required for context modulation, and that because of its architecture, the cerebellum is more likely to implement complex functions of multiple signals than other relevant nuclei. The major contributions of vergence and eye\\nposition modulation on the VOR are therefore mediated by the cerebellum. Smaller and more transient contributions from eye position are assumed to be mediated through the vestibular nucleus as\\nshown in fig. 4. The motivation for combining eye position as in fig . 4 are, first, the evidence for eye\\nresponse oscillations; second, the theoretical consideration that linear movement information (To) is\\nuseless without eye position information for proper VOR.\\nThe parameters in the dynamical model were adjusted by hand after observing the behavior of the different components of the model and noting how these combine to produce the oscillations observed\\n\\n\\x0c93\\n\\nA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\\nVestibular\\nSemicirtular\\n\\nCerebellum\\n\\nc..l\\n\\nO-\\n\\n- - - - t 401+1 r-----?--f--..j\\n\\nx\\n\\n300+1\\n\\nOIolith\\n0Igan\\n\\nVHlibabr\\nNuc1tul\\n\\nFigure 3: Contribution of the cerebellum to the dynamical model. Filtered velocity inputs from the canals and\\notoliths are combined with eye position according to equation (2). These calculations could be performed either\\noutside the cerebellum in one or multiple brain stem nuclei (as shown) or possibly inside the cerebellum. The\\nonly output is to the vestibular nucleus. The Laplace notation is used in each boxes to represent a leaky integrator\\nwith a time constant. input derivative and input gain. The term oe are the coordinates of the vector oe shown\\nin fig. 1. The x indicates a multiplication. The term! multiplies each inputs individually. The open arrows\\nindicate inhibitory (negative) connections.\\nCere... lIum\\n\\nVHlibalu\\n\\nSemicimtlu\\n\\nc.w\\n\\nO--\\'----t~l---+--?----t~~\\n\\nX\\nFigure 4: Contribution of the vestibular nucleus to the dynamical model. Three pathways in the vestibular nucleus process the canal and otolith inputs to drive the eye. The first pathway is modulated by the output of the\\ncerebellum through a FIN (Flocculus Target Neuron). The second and third pathways report transient information from the inputs which are combined with eye position in a manner identical to fig. 3. The location of these\\ncalculations is hypothetical.\\n\\nin the data. Even though the number of parameters in the model is not small. it was not possible to\\nfit any single response in fig. 5 without affecting most of the other eye responses. This puts severe\\nlimits on the set of parameters allowed in the model.\\nThe dynamical model suggests that the oscillations present in the data reflect: 1) important acceleration components in the neural signals. both rotational and linear, 2) different time delays between the\\ncanal and otolith signal processing. and 3) antagonistic or synergistic action of the canal and otolith\\nsignals with different axes of rotation, as described by the two terms in the bracket of equation (2).\\n\\n4 Discussion\\nBy fitting the dynamical model to the data, we tested the hypothesis that the VOR has a response\\nclose to ideal taking into account the time constraints imposed by the sensory inputs and the neural\\nnetworks performing the computations. The vector computations that we used in the model may not\\n\\n\\x0c94\\n\\nO. J. M. D. COENEN, T. J. SEJNOWSKI\\nDynamical Model Responses vs Experimental Data\\n80\\n\\n80\\nLOMtIOftof\\n.... 01 rotMIon\\n\\n-,a.-om\\n\\n.-\\n\\nT..........~\\n\\n60\\n\\n40\\n20\\n\\n~\\nw\\n\\n-20\\n\\n-20\\n\\n-400~----~5~0------~\\n10\\n=0\\n~\\nTime (m.)\\n\\n-40oL-----~\\n5~\\n0 ----~1~\\n0~\\n0-?\\n\\nTime (m.)\\n\\nFigure 5: Comparison between the dynamical model and monkey data. The dotted lines show the effect of\\nviewing distance and location of the axis of rotation on the VOR as recorded by Snyder & King (1992) from\\nmonkeys in the dark. The average eye velocity response (of left and right eye) to a sudden change in head velocity is shown for different target distances (left) and rotational axes (right). On the left, the location of the axis\\nof rotation was in the midsagittal plane 12.5 cm behind the eyes (-12.5 cm), and the target distance was varied\\nbetween 220 cm and 9 cm. On the right, the target di stance was kept constant at 9 cm in front of the eye, and the\\nlocation of the axis of rotation was varied from 14 cm behind t04cm in front of the eyes (-14cm to 4cm) in the\\nmidsagittal plane. The solid lines show the model responses. The model replicates many characteristics of the\\ndata. On the left the model captures the eye velocity fluctuations between 20-50 ms, followed by a decrease and\\nan increase which are both modulated with target distance (50-80 ms). The later phase of the response (80-100\\nms) is almost exact for 220 cm, and one peak is seen at the appropriate location for the other distances. On the\\nright the closest fits were obtained for the 4 cm and 0 cm locations. The mean values are in good agreement and\\nthe waveforms are close, but could be shifted in time for the other locations of the axis of rotations. Finally, the\\nlatest peak (..... lOOms) in the data appears in the model for -14 cm and 9 cm location.\\n\\nbe the representation used in the oculomotor system. Mathematically, the vector representation is\\nonly one way to describe the computations involved. Other representations exist such as the quaternion representation which has been studied in the context of the saccadic system (Tweed and Vilis,\\n1987; see also Handzel and Flash, 1996 for a very general representation). Detailed comparisons\\nbetween the model and recordings from neurons will be require to settle this issue.\\nDirect comparison between Purkinje cell recordings (L.H. Snyder & W.M. King, unpublished data)\\nand predictions of the model could be used to determine more precisely the different inputs to some\\nPurkinje cells. The model can therefore be an important tool to gain insights difficult to obtain directly with experiments.\\nThe question of how the central nervous system learns the transformations that we described still\\nremains. The cerebellum may be one site of learning for these transformations, and its output may\\nmodulate the VOR in real time depending on the context. This view is compatible with the results\\nof Angelaki and Hess (1995) which indicate that the cerebellum is required to correctly perform an\\notolith transformation. It is also consistent with adaptation results in the VOR. To test this hypothesis,\\nwe have been working on a model of the cerebellum which learns to anticipate sensory inputs and\\nfeedbacks, and use these signals to modulate the VOR. The learning in the cerebellum and vestibular\\nnuclei is mediated by the climbing fibers which report a reinforcement signal of the prediction error\\n(Coenen and Sejnowski. in preparation).\\n\\n\\x0cA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\\n\\n95\\n\\n5 Conclusion\\nMost research on the VOR has assumed forward gaze focussed at infinity. The kinematics of offcenter gaze and fixation at finite distance necessitates nonlinear corrections that require the integration of a variety of sensory inputs. The dynamical model studied here is a working hypothesis for\\nhow these corrections could be computed and is generally consistent with what is known about the\\ncerebellum and brain stem nuclei. We are, however, far from knowing the mechanisms underlying\\nthese computations, or how they are learned through experience.\\n\\n6 Acknowledgments\\nThe first author was supported by a McDonnell-Pew Graduate Fellowship during this research. We\\nwould like to thank Paul Viola for helpful discussions.\\nReferences\\nAngelaki, D. E. and Hess, B. J. (1995). Inertial representation of angular motion in the vestibular system of rhesus monkeyus. II. Otolith-controlled transformation that depends on an intact cerebellar nodulus. Journal\\nof Neurophysiology, 73(5): 1729-1751.\\nCannon, S. C. and Robinson, D. A. (1987). Loss of the neural integrator of the oculomotor system from brain\\nstem lesions in monkey. Journal of Neurophysiology, 57(5):1383-1409.\\nCrawford, J. D., Cadera, W., and Vilis, T. (1991). Generation of torsional and vertical eye position signals by\\nthe interstitial nucleus of Cajal. Science, 252:1551-1553.\\nGamlin, P. D. R. and Clarke, R. J. (1995). Single-unit activity in the primate nucleus reticularis tegmenti pontis\\nrelated to vergence and ocular accomodation. Journal of Neurophysiology, 73(5):2115-2119.\\nGrossman, G. E., Leigh, R. J., Bruce, E. N., Huebner, W. P.,and Lanska, D.J. (1989). Performanceofthe human\\nvestibu1oocu1ar reflex during locomotion. Journal of Neurophysiology, 62(1 ):264-272.\\nHandzel, A. A. and Flash, T. (1996). The geometry of eye rotations and listing\\'s law. In Touretzky, D., Mozer,\\nM., and Hasselmo, M., editors, Advances in Neural Information Processing Systems 8, Cambridge, MA.\\nMIT Press.\\nJudge, S. J. and Cumming, B. G. (1986). Neurons in the monkey midbrain with activity related to vergence eye\\nmovement and accomodation. Journal of Neurophysiology, 55:915-930.\\nMays, L. E. (1984). Neural control of vergence eye movements: Convergence and divergence neurons in midbrain. Journal of Neurophysiology, 51:1091-1108.\\nRobinson, D. A. (1981). The use of control systems analysis in the neurophysiology of eye movements. Ann.\\nRev. Neurosci., 4:463-503.\\nShelhamer, M., Robinson, D. A., and Tan, H. S. (1992). Context-specific adaptation of the gain of the vestibuloocular reflex in humans. Journal of Vestibular Research, 2:89-96.\\nSnyder, L. H. and King, W. M. (1992). Effect of viewing distance and location ofthe axis of head rotation on the\\nmonkey\\'s vestibuloocular reflex I. eye movement response. Journal of Neurophysiology, 67(4):861-874.\\nTweed, D. and Vilis, T. (1987). Implications of rotational kinematics for the oculomotor system in three dimensions. Journal of Neurophysiology, 58(4):832-849.\\nViire, E., Tweed, D., Milner, K., and Vilis, T. (1986). A reexamination of the gain ofthe vestibuloocular reflex.\\nJournal of Neurophysiology, 56(2):439-450.\\n\\n\\x0c',\n",
       "     'pdf_name': '1035-a-dynamical-model-of-context-dependencies-for-the-vestibulo-ocular-reflex.pdf',\n",
       "     'title': 'A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1036',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1036',\n",
       "     'paper_text': 'Improved Gaussian Mixture Density\\nEstimates Using Bayesian Penalty Terms\\nand Network Averaging\\n\\nDirk Ormoneit\\nInstitut fur Informatik (H2)\\nTechnische Universitat Munchen\\n80290 Munchen, Germany\\normoneit@inJormatik.tu-muenchen.de\\n\\nVolker Tresp\\nSiemens AG\\nCentral Research\\n81730 Munchen, Germany\\nVolker. Tresp@zJe.siemens.de\\n\\nAbstract\\n\\nWe compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density\\nestimates. The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules\\nwhich maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation.\\nThis includes Breiman\\'s \"bagging\" , which recently has been found\\nto produce impressive results for classification networks.\\n\\n1\\n\\nIntroduction\\n\\nGaussian mixture models have recently attracted wide attention in the neural network community. Important examples of their application include the training of\\nradial basis function classifiers, learning from patterns with missing features, and\\nactive learning. The appeal of Gaussian mixtures is based to a high degree on the\\napplicability of the EM (Expectation Maximization) learning algorithm, which may\\nbe implemented as a fast neural network learning rule ([Now91], [Orm93]). Severe\\nproblems arise, however, due to singularities and local maxima in the log-likelihood\\nfunction. Particularly in high-dimensional spaces these problems frequently cause\\nthe computed density estimates to possess only relatively limited generalization capabilities in terms of predicting the densities of new data points. As shown in this\\npaper, considerably better generalization can be achieved using regularization.\\n\\n\\x0c543\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nWe will compare two regularization methods. The first one uses a Bayesian prior\\non the parameters. By using conjugate priors we can derive EM learning rules\\nfor finding the MAP (maximum a posteriori probability) parameter estimate. The\\nsecond approach consists of averaging the outputs of ensembles of Gaussian mixture\\ndensity estimators trained on identical or resampled data sets. The latter is a form\\nof \"bagging\" which was introduced by Breiman ([Bre94]) and which has recently\\nbeen found to produce impressive results for classification networks. By using the\\nregularized density estimators in a Bayes classifier ([THA93], [HT94], [KL95]) , we\\ndemonstrate that both methods lead to density estimates which are superior to the\\nunregularized Gaussian mixture estimate.\\n\\n2\\n\\nGaussian Mixtures and the EM Algorithm\\n\\nConsider the lroblem of estimating the probability density of a continuous random\\nvector x E \\'R based on a set x* = {x k 11 S k S m} of iid. realizations of x. As a density model we choose the class of Gaussian mixtures p(xle) = L:7=1 Kip(xli, pi, E i ),\\nwhere the restrictions Ki ~ 0 and L:7=1 Kj = 1 apply. e denotes the parameter\\nvector (Ki\\' Iti, E i )i=1. The p(xli, Pi, E i ) are multivariate normal densities:\\np( xli , Pi , Ei) = (271\")- 41Ei 1- 1 / 2 exp [-1/2(x - Pi)tEi 1 (x - Iti)] .\\nThe Gaussian mixture model is well suited to approximate a wide class of continuous\\nprobability densities. Based on the model and given the data x*, we may formulate\\nthe log-likelihood as\\n\\nlee)\\n\\n= log [rr mk=l p(xkle)] = \",m\\nlog \"\\'~ Kip(xkli, Pi, Ei) .\\n.L...\".k=1\\n.L...\".J=l\\n\\ne\\n\\nMaximum likelihood parameter estimates may efficiently be computed with the\\nEM (Expectation Maximization) algorithm ([DLR77]) . It consists of the iterative\\napplication of the following two steps:\\n1. In the E-step, based on the current parameter estimates, the posterior\\nprobability that unit i is responsible for the generation of pattern xk is\\n\\nestimated as\\n(1)\\n\\n2. In the M-step, we obtain new parameter estimates (denoted by the prime):\\n,\\n\\nK ?\\nJ\\n\\n= -m1 L mk=1 h?k\\nJ\\n\\n~m\\n\\n,\\n\\n(2)\\n\\n=\\n\\nPi\\n\\nwk-l\\n\\n~m\\n\\nhki X k\\nhi\\n\\nwl=l\\n\\n~.\\' _ L:~1 hf(x k - pD(x k - pDt\\nL.J J\\n\\n-\\n\\nm\\n\\nI\\n\\n(3)\\n\\ni\\n\\n(4)\\n\\nL:l=l hi\\nNote that K~ is a scalar , whereas p~ denotes a d-dimensional vector and E/\\nis a d x d matrix.\\nIt is well known that training neural networks as predictors using the maximum\\nlikelihood parameter estimate leads to overfitting. The problem of overfitting is\\neven more severe in density estimation due to singularities in the log-likelihood\\nfunction. Obviously, the model likelihood becomes infinite in a trivial way if we\\nconcentrate all the probability mass on one or several samples of the training set.\\n\\n\\x0c544\\n\\nD. ORMONEIT, V. TRESP\\n\\nIn a Gaussian mixture this is just the case if the center of a unit coincides with\\none of the data points and E approaches the zero matrix. Figure 1 compares the\\ntrue and the estimated probability density in a toy problem. As may be seen,\\nthe contraction of the Gaussians results in (possibly infinitely) high peaks in the\\nGaussian mixture density estimate. A simple way to achieve numerical stability\\nis to artificially enforce a lower bound on the diagonal elements of E. This is a\\nvery rude way of regularization, however, and usually results in low generalization\\ncapabilities. The problem becomes even more severe in high-dimensional spaces.\\nTo yield reasonable approximations, we will apply two methods of regularization,\\nwhich will be discussed in the following two sections.\\n\\nFigure 1: True density (left) and unregularized density estimation (right).\\n\\n3\\n\\nBayesian Regularization\\n\\nIn this section we propose a Bayesian prior distribution on the Gaussian mixture\\nparameters, which leads to a numerically stable version of the EM algorithm. We\\nfirst select a family of prior distributions on the parameters which is conjugate*.\\nSelecting a conjugate prior has a number of advantages. In particular, we obtain\\nanalytic solutions for the posterior density and the predictive density. In our case,\\nthe posterior density is a complex mixture of densities t . It is possible, however, to\\nderive EM-update rules to obtain the MAP parameter estimates.\\nA conjugate prior of a single multivariate normal density is a product of a normal\\ndensity N(JLilft,1]-lE i ) and a Wishart density Wi(E;lla,,8) ([Bun94]). A proper\\nconjugate prior for the the mixture weightings \\'\" = (\"\\'1, ... , \"\\'n) is a Dirichlet density\\nD(\"\\'hV. Consequently, the prior of the overall Gaussian mixture is the product\\nD(\",lr)\\nN(JLilil, 71- 1Ei)Wi(E;1I a , ,8). Our goal is to find the MAP parameter\\nestimate, that is parameters which assume the maximum of the log-posterior\\n\\nil7=1\\n\\nIp(S)\\n\\n2:=~=1 log 2:=;=1 \"\\'iP(X k Ii, JLi, Ei ) + log D(\"\\'lr)\\n\\n+ 2:=;=1 [logN(JLilft, 71- 1Ei) + log Wi(E;lla, ,8)].\\nAs in the unregularized case, we may use the EM-algorithm to find a local maximum\\n? A family F of probability distributions on 0 is said to be conjugate if, for every 1r E F,\\nthe posterior 1r(0Ix) also belongs to F ([Rob94]).\\ntThe posterior distribution can be written as a sum of nm simple terms.\\ntThose densities are defined as follows (b and c are normalizing constants):\\n\\nbII n\\n\\nD(1I:17)\\n\\n.=1\\n\\n~.=l\\n\\n(21r)-i 11,-IE;I-l/2 exp [-~(Il\\' -\\n\\nN(Il.lp,1,-IE.)\\nW i(Ei l la,,8)\\n\\n11:7,-1, with 11:, ~ 0 and \",n\\n\\n=\\n\\ncIEillo-Cd+l)/2 exp [-tr(,8Ei 1 )]\\n\\n11:.\\n\\n=1\\n\\nMt Ei 1 (1l\\' - M]\\n?\\n\\n\\x0c545\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nof Ip(8). The E-step is identical to (1). The M-step becomes\\n,\\nL..\"k-l hki + ri - 1\\n(5)\\n,L..\"k=l hki x k + \\'1J1.\\n\"\\'i\\nJ1.i\\nhi\\nm + L..\"i=l ri - n\\nL..,,1=1 i + 11\\n\\n=\\n\\n\"m\\n\\nE~ =\\n\\n\"m\\n\\n\"n\\n\\n2:;-1 hf(x k -\\n\\nA\\n\\n= \"m\\n\\n+ 11(J1.i 2:~1 h~ + 20: - d\\n\\nJ1.D(xk - J1.D t\\n\\nI\\n\\njJ.)(J1.i - jJ.)t\\n\\n(6)\\n\\n+ 2f3\\n\\n(7)\\n\\nAs typical for conjugate priors, prior knowledge corresponds to a set of artificial\\ntraining data which is also reflected in the EM-update equations. In our experiments, we focus on a prior on the variances which is implemented by f3 =F 0, where\\no denotes the d x d zero matrix. All other parameters we set to \"neutral\" values:\\n\\nri=l\\'v\\'i : l::;i::;n,\\n\\n0:= (d+I)/2,\\n\\n11=0,\\n\\nf3=iJl d\\n\\nld is the d x d unity matrix. The choice of 0: introdu~es a bias which favors large\\nvariances?. The effect of various values of the scalar f3 on the density estimate is\\nillustrated in figure 2. Note that if iJ is chosen too small, overfitting still occurs. If\\nit is chosen to large , on the other hand, the model is too constraint to recognize the\\nunderlying structure.\\n\\nFigure 2: Regularized density estimates (left:\\n\\niJ =\\n\\n0.05, right: \\'iJ = 0.1).\\n\\nTypically, the optimal value for iJ is not known a priori. The simplest procedure\\nconsists of using that iJ which leads to the best performance on a validation set,\\nanalogous to the determination of the optimal weight decay parameter in neural\\nnetwork training. Alternatively, iJ might be determined according to appropriate\\nBayesian methods ([Mac9I]). Either way, only few additional computations are\\nrequired for this method if compared with standard EM.\\n\\n4\\n\\nAveraging Gaussian Mixtures\\n\\nIn this section we discuss the averaging of several Gaussian mixtures to yield improved probability density estimation. The averaging over neural network ensembles\\nhas been applied previously to regression and classification tasks ([PC93]) .\\nThere are several different variants on the simple averaging idea. First, one may\\ntrain all networks on the complete set of training data. The only source of disagreement between the individual predictions consists in different local solutions\\nfound by the likelihood maximization procedure due to different starting points.\\nDisagreement is essential to yield an improvement by averaging, however, so that\\nthis proceeding only seems advantageous in cases where the relation between training data and weights is extremely non-deterministic in the sense that in training,\\n?If A is distributed according to Wi(AIO\\', (3), then E[A- 1 ] = (0\\' - (d + 1)/2)-1 {3. In our\\ncase A is B;-I, so that E[Bi] -+ 00 ? {3 for 0\\' -+ (d + 1)/2.\\n\\n\\x0c546\\n\\nD. ORMONEIT, V. TRESP\\n\\ndifferent solutions are found from different random starting points. A straightforward way to increase the disagreement is to train each network on a resampled\\nversion of the original data set. If we resample the data without replacement, the\\nsize of each training set is reduced, in our experiments to 70% of the original. The\\naveraging of neural network predictions based on resampling with replacement has\\nrecently been proposed under the notation \"bagging\" by Breiman ([Bre94]), who\\nhas achieved dramatic.ally improved results in several classification tasks. He also\\nnotes, however, that an actual improvement of the prediction can only result if the\\nestimation procedure is relatively unstable. As discussed, this is particularly the\\ncase for Gaussian mixture training. We therefore expect bagging to be well suited\\nfor our task.\\n\\n5\\n\\nExperiments and Results\\n\\nTo assess the practical advantage resulting from regularization, we used the density\\nestimates to construct classifiers and compared the resulting prediction accuracies\\nusing a toy problem and a real-world problem. The reason is that the generalization error of density estimates in terms of the likelihood based on the test data\\nis rather unintuitive whereas performance on a classification problem provides a\\ngood impression of the degree of improvement. Assume we have a set of N labeled\\ndata z* = {(xk, lk)lk = 1, ... , N}, where lk E Y = {I, ... , C} denotes the class label\\nof each input xk . A classifier of new inputs x is yielded by choosing the class I\\nwith the maximum posterior class-probability p(llx). The posterior probabilities\\nmay be derived from the class-conditional data likelihood p(xll) via Bayes theorem:\\np(llx) = p(xll)p(l)/p(x) ex p(xll)p(l) . The resulting partitions ofthe input space are\\noptimal for the true p(llx). A viable way to approximate the posterior p(llx) is to\\nestimate p(xll) and p(l) from the sample data.\\n5.1\\n\\nToy Problem\\n\\nIn the toy classification problem the task is to discriminate the two classes of circulatory arranged data shown in figure 3. We generated 200 data points for each class\\nand subdivided them into two sets of 100 data points. The first was used for training, the second to test the generalization performance. As a network architecture\\nwe chose a Gaussian mixture with 20 units. Table 1 summarizes the results, beginning with the unregularized Gaussian mixture which is followed by the averaging\\nand the Bayesian penalty approaches. The three rows for averaging correspond to\\nthe results yielded without applying resampling (local max.), with resampling with-\\n\\nFigure 3: Toy Classification Task.\\n\\n\\x0c547\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nout replacement (70% subsets), and with resampling with replacement (bagging).\\nThe performances on training and test set are measured in terms of the model loglikelihood. Larger values indicate a better performance. We report separate results\\nfor dass A and B, since the densities of both were estimated separately. The final\\ncolumn shows the prediction accuracy in terms of the percentage of correctly classified data in the test set. We report the average results from 20 experiments. The\\nnumbers in brackets denote the standard deviations u of the results. Multiplying u\\nwith T19;95%/v\\'20 = 0.4680 yields 95% confidence intervals. The best result in each\\ncategory is underlined.\\nAlgorithm\\n\\nLog- Likelihood\\n\\nunreg.\\nAveraging:\\nlocal max.\\n70% subset\\nbagging\\nPenalty:\\n[3 = 0.01\\n[3 = 0.02\\n[3 = 0.05\\n[3 = 0.1\\n\\nAccuracy\\n\\nA\\n-120.8 (13.3)\\n\\n-120.4 (10.8)\\n\\nTest\\nA\\nB\\n-224.9 (32.6) -241.9 (34.1)\\n\\n-115.6 (6.0)\\n-106.8 (5.8)\\n-83.8 (4.9)\\n\\n-112.6 (6.6)\\n-105.1 (6.7)\\n-83.1 (7.1)\\n\\n-200.9 (13.9)\\n-188.8 (9.5)\\n-194.2 (7.3)\\n\\n-209.1 (16.3)\\n-196.4 (11.3)\\n-200.1 (11.3)\\n\\n81.8% (3.1)\\n83.2% (2.9)\\n82.6% (3.4)\\n\\n-149.3\\n-156.0\\n-173.9\\n-183.0\\n\\n-146.5 (5.9)\\n-153.0 (4.8)\\n-167.0 (15.8)\\n-181.9 (21.1)\\n\\n-186.2\\n-177.1\\n-182.0\\n-184.6\\n\\n-182.9 (11.6)\\n-174.9 (7.0)\\n-173.9 (14.3)\\n-182.5 (21.1)\\n\\n83.1%\\n84.4%\\n81.5%\\n78.5%\\n\\nTraining\\n\\n(18.5)\\n(16.5)\\n(24.3)\\n(21.9)\\n\\nB\\n\\n(13.9)\\n(11.8)\\n(20.1)\\n(21.0)\\n\\nI\\n80.6\\'70 (2.8)\\n\\n(2.9)\\n(6.3)\\n(5.9)\\n(5.1)\\n\\nTable 1: Performances in the toy classification problem .\\nAs expected, all regularization methods outperform the maximum likelihood approach in terms of correct classification. The performance of the Bayesian regularization is hereby very sensitive to the appropriate choice of the regularization\\nparameter (3. Optimality of (3 with respect to the density prediction and oytimality\\nwith respect to prediction accuracy on the test set roughly coincide (for (3 = 0.02).\\nA veraging is inferior to the Bayesian approach if an optimal {3 is chosen.\\n5.2\\n\\nBUPA Liver Disorder Classification\\n\\nAs a second task we applied our methods to a real-world decision problem from\\nthe medical environment. The problem is to detect liver disorders which might\\narise from excessive alcohol consumption. Available information consists of five\\nblood tests as well as a measure of the patients\\' daily alcohol consumption. We\\nsubdivided the 345 available samples into a training set of 200 and a test set of 145\\nsamples. Due to the relatively few data we did not try to determine the optimal\\nregularization parameter using a validation process and will report results on the\\ntest set for different parameter values.\\nAlgorithm\\nunregularized\\nBayesian penalty ({3 = 0.05)\\nBayesian penalty ?(3 = 0.10)\\nBayesian penal ty (3 = 0.20\\naveraging local maxima\\naveraging (70 % subset)\\naveraging (bagging)\\n\\nAccuracy\\n64.8 %\\n65.5 %\\n66.9 %\\n61.4 %\\n65 .5 0\\n72.4 %\\n71.0 %\\n\\nTable 2: Performances in the liver disorder classification problem.\\n\\n\\x0c548\\n\\nD. ORMONEIT. V. TRESP\\n\\nThe results of our experiments are shown in table 2. Again, both regularization\\nmethods led to an improvement in prediction accuracy. In contrast to the toy problem, the averaged predictor was superior to the Bayesian approach here. Note that\\nthe resampling led to an improvement of more than five percent points compared\\nto unresampled averaging.\\n\\n6\\n\\nConclusion\\n\\nWe proposed a Bayesian and an averaging approach to regularize Gaussian mixture\\ndensity estimates. In comparison with the maximum likelihood solution both approaches led to considerably improved results as demonstrated using a toy problem\\nand a real-world classification task. Interestingly, none of the methods outperformed\\nthe other in both tasks. This might be explained with the fact that Gaussian mixture density estimates are particularly unstable in high-dimensional spaces with\\nrelatively few data. The benefit of averaging might thus be greater in this case.\\nA veraging proved to be particularly effective if applied in connection with resampIing of the training data, which agrees with results in regression and classification\\ntasks. If compared to Bayesian regularization, averaging is computationally expensive. On the other hand, Baysian approaches typically require the determination of\\nhyper parameters (in our case 13), which is not the case for averaging approaches.\\n\\nReferences\\n[Bre94]\\n\\nL. Breiman. Bagging predictors. Technical report , UC Berkeley, 1994.\\n\\n[Bun94]\\n\\nW . Buntine. Operations for learning with graphical models. Journal of Artificial\\nIntelligence Research, 2:159-225, 1994.\\n\\n[DLR77] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from\\nincomplete data via the EM algorithm. J. Royal Statistical Society B, 1977.\\n[HT94]\\n\\nT. Hastie and R. Tibshirani. Discriminant analysis by gaussian mixtures. Technical report , AT&T Bell Labs and University of Toronto, 1994.\\n\\n[KL95]\\n\\nN. Kambhatla and T. K. Leen. Classifying with gaussian mixtures and clusters.\\nIn Advances in Neural Information Processing Systems 7. Morgan Kaufman,\\n1995.\\n\\n[Mac91]\\n\\nD. MacKay. Bayesian Modelling and Neural Networks. PhD thesis, California\\nInstitute of Technology, Pasadena, 1991.\\n\\n[Now91] S. J. Nowlan. Soft Competitive Adaption: Neural Network Learning Algorithms\\nbased on Fitting Statistical Mixtures. PhD thesis, School of Computer Science,\\nCarnegie Mellon University, Pittsburgh, 1991.\\n[Orm93] D. Ormoneit. Estimation of probability densities using neural networks. Master\\'s\\nthesis, Technische Universitiit Munchen, 1993.\\n[PC93]\\n\\nM. P. Perrone and L. N. Cooper. When networks disagree: Ensemble methods for\\nhybrid Neural networks. In Neural Networks for Speech and Image Processing.\\nChapman Hall, 1993.\\n\\n[Rob94]\\n\\nC. P. Robert. The Bayesian Choice. Springer-Verlag, 1994.\\n\\n[THA93] V. Tresp, J. Hollatz, and S. Ahmad. Network structuring and training using\\nrule-based knowledge. In Advances in Neural Information Processing Systems 5.\\nMorgan Kaufman, 1993.\\n\\n\\x0c',\n",
       "     'pdf_name': '1036-improved-gaussian-mixture-density-estimates-using-bayesian-penalty-terms-and-network-averaging.pdf',\n",
       "     'title': 'Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1044',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1044',\n",
       "     'paper_text': 'Learning with ensembles: How\\nover-fitting can be useful\\n\\nPeter Sollich\\nDepartment of Physics\\nUniversity of Edinburgh, U.K.\\nP.SollichGed.ac.uk\\n\\nAnders Krogh\\'\"\\nNORDITA, Blegdamsvej 17\\n2100 Copenhagen, Denmark\\nkroghGsanger.ac.uk\\n\\nAbstract\\nWe study the characteristics of learning with ensembles. Solving\\nexactly the simple model of an ensemble of linear students, we\\nfind surprisingly rich behaviour. For learning in large ensembles,\\nit is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can\\nbe obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble\\nweights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide\\nrange of regularization parameters makes this improvement robust\\nagainst changes in the unknown level of noise in the training data.\\n\\n1\\n\\nINTRODUCTION\\n\\nAn ensemble is a collection of a (finite) number of neural networks or other types\\nof predictors that are trained for the same task. A combination of many different predictors can often improve predictions, and in statistics this idea has been\\ninvestigated extensively, see e.g. [1, 2, 3]. In the neural networks community, ensembles of neural networks have been investigated by several groups, see for instance\\n[4, 5, 6, 7]. Usually the networks in the ensemble are trained independently and\\nthen their predictions are combined.\\nIn this paper we study an ensemble of linear networks trained on different but\\noverlapping training sets. The limit in which all the networks are trained on the\\nfull data set and the one where all the data sets are different has been treated in\\n[8] . In this paper we treat the case of intermediate training set sizes and overlaps\\n?Present address: The Sanger Centre, Hinxton, Cambs CBIO IRQ, UK.\\n\\n\\x0cLearning with Ensembles: How Overfitting Can Be Useful\\n\\n191\\n\\nexactly, yielding novel insights into ensemble learning. Our analysis also allows us to\\nstudy the effect of regularization and of having different predictors in an ensemble.\\n\\n2\\n\\nGENERAL FEATURES OF ENSEMBLE LEARNING\\n\\nWe consider the task of approximating a target function fo from RN to R. It\\nwill be assumed that we can only obtain noisy samples of the function, and the\\n(now stochastic) target function will be denoted y(x) . The inputs x are taken\\nto be drawn from some distribution P(x). Assume now that an ensemble of K\\nindependent predictors fk(X) of y(x) is available. A weighted ensemble average is\\ndenoted by a bar, like\\n(1)\\nlex) = L,wkfk(X),\\nk\\n\\nwhich is the final output of the ensemble. One can think of the weight Wk as the\\nbelief in predictor k and we therefore constrain the weights to be positive and sum\\nto one. For an input x we define the error of the ensemble c(x), the error of the\\nkth predictor ck(X), and its ambiguity ak(x)\\nc(x)\\nck(X)\\n\\n(y(x) -lex)?\\n(y(x) - fk(X)?\\n(fk(X) -1(x?2.\\n\\n=\\n\\n(2)\\n(3)\\n(4)\\n\\n=\\n\\nThe ensemble error can be written as c(x)\\nlex) - a(x) [7], where lex)\\nL,k Wkck(X) is the average error over the individual predictors and a(x) =\\nL,k Wkak(X) is the average of their ambiguities, which is the variance of the output\\nover the ensemble. By averaging over the input distribution P(x) (and implicitly\\nover the target outputs y(x?, one obtains the ensemble generalization error\\n(5)\\nwhere c(x) averaged over P(x) is simply denoted c, and similarly for land a. The\\nfirst term on the right is the weighted average of the generalization errors of the individual predictors, and the second is the weighted average of the ambiguities, which\\nwe refer to as the ensemble ambiguity. An important feature of equation (5) is that\\nit separates the generalization error into a term that depends on the generalization\\nerrors of the individual students and another term that contains all correlations between the students. The latter can be estimated entirely from unlabeled data, i. e.,\\nwithout any knowledge of the target function to be approximated. The relation (5)\\nalso shows that the more the predictors differ, the lower the error will be, provided\\nthe individual errors remain constant.\\n\\nIn this paper we assume that the predictors are trained on a sample of p examples\\nof the target function, (xt\\',yt\\'), where yt\\' = fo(xt\\') + TJt\\' and TJt\\' is some additive\\nnoise (Jl. = 1, ... ,p). The predictors, to which we refer as students in this context\\nbecause they learn the target function from the training examples, need not be\\ntrained on all the available data. In fact, since training on different data sets will\\ngenerally increase the ambiguity, it is possible that training on subsets of the data\\nwill improve generalization. An additional advantage is that, by holding out for\\neach student a different part of the total data set for the purpose of testing, one\\ncan use the whole data set for training the ensemble while still getting an unbiased\\nestimate of the ensemble generalization error. Denoting this estimate by f, one has\\n(6)\\nwhere Ctest = L,k WkCtest,k is the average of the students\\' test errors. As already\\npointed out, the estimate ft of the ensemble ambiguity can be found from unlabeled\\ndata.\\n\\n\\x0cP. SOLLICH, A. KROGH\\n\\n192\\n\\nSo far, we have not mentioned how to find the weights Wk. Often uniform weights\\nare used, but optimization of the weights in some way is tempting. In [5, 6] the\\ntraining set was used to perform the optimization, i.e., the weights were chosen to\\nminimize the ensemble training error. This can easily lead to over-fitting, and in [7]\\nit was suggested to minimize the estimated generalization error (6) instead. If this\\nis done, the estimate (6) acquires a bias; intuitively, however, we expect this effect\\nto be small for large ensembles.\\n\\n3\\n\\nENSEMBLES OF LINEAR STUDENTS\\n\\nIn preparation for our analysis of learning with ensembles of linear students we now\\nbriefly review the case of a single linear student, sometimes referred to as \\'linear\\nperceptron learning\\'. A linear student implements the input-output mapping\\n1 T\\nJ(x) = ..JNw x\\nparameterized in terms of an N-dimensional parameter vector w with real components; the scaling factor 1/..JN is introduced here for convenience, and . ..T denotes\\nthe transpose of a vector. The student parameter vector w should not be confused with the ensemble weights Wk. The most common method for training such\\na linear student (or parametric inference models in general) is minimization of the\\nsum-of-squares training error\\nE = L:(y/J - J(x/J))2 + Aw2\\n/J\\nwhere J.L = 1, ... ,p numbers the training examples. To prevent the student from\\nfitting noise in the training data, a weight decay term Aw2 has been added. The size\\nof the weight decay parameter A determines how strongly large parameter vectors\\nare penalized; large A corresponds to a stronger regularization of the student.\\nFor a linear student, the global minimum of E can easily be found. However,\\nin practical applications using non-linear networks, this is generally not true, and\\ntraining can be thought of as a stochastic process yielding a different solution each\\ntime. We crudely model this by considering white noise added to gradient descent\\nupdates of the parameter vector w. This yields a limiting distribution of parameter\\nvectors P(w) ex: exp(-E/2T), where the \\'temperature\\' T measures the amount of\\nnoise in the training process.\\nWe focus our analysis on the \\'thermodynamic limit\\' N - t 00 at constant normalized\\nnumber of training examples, ex = p/N. In this limit, quantities such as the training\\nor generalization error become self-averaging, i.e., their averages over all training\\nsets become identical to their typical values for a particular training set. Assume\\nnow that the training inputs x/J are chosen randomly and independently from a\\nGaussian distribution P(x) ex: exp( - ~x2), and that training outputs are generated\\nby a linear target function corrupted by additive noise, i.e., y/J = w\\'f x/J /..IN + 1]/J,\\nwhere the 1]/J are zero mean noise variables with variance u 2 ? Fixing the length of the\\nparameter vector of the target function to w~ = N for simplicity, the generalization\\nerror of a linear student with weight decay A and learning noise T becomes [9]\\n(; = (u 2 + T)G + A(U 2\\n\\n-\\n\\n8G\\n\\nA) 8A .\\n\\n(7)\\n\\nOn the r.h.s. of this equation we have dropped the term arising from the noise on\\nthe target function alone, which is simply u 2 , and we shall follow this convention\\nthroughout . The \\'response function\\' Gis [10, 11]\\n\\nG = G(ex, A) = (1 - ex - A+ )(1 - ex - A)2 + 4A)/2A.\\n\\n(8)\\n\\n\\x0c193\\n\\nLearning with Ensembles: How Overfitting Can Be Useful\\n\\nFor zero training noise, T = 0, and for any a, the generalization error (7} is minimized when the weight decay is set to A = (T2j its value is then (T2G(a, (T2), which\\nis the minimum achievable generalization error [9].\\n\\n3.1\\n\\nENSEMBLE GENERALIZATION ERROR\\n\\nWe now consider an ensemble of K linear students with weight decays Ak and\\nlearning noises Tk (k = 1 . . . K). Each ,student has an ensemble weight Wk and\\nis trained on N ak training examples, with students k and I sharing N akl training\\nexamples (of course, akk = ak). As above, we consider noisy training data generated\\nby a linear target function. The resulting ensemble generalization error can be\\ncalculated by diagrammatic [10] or response function [11] methods. We refer the\\nreader to a forthcoming publication for details and only state the result:\\n\\n(9)\\nwhere\\n(10)\\nHere Pk is defined as Pk = AkG(ak, Ak). The Kronecker delta in the last term\\nof (10) arises because the training noises of different students are uncorrelated. The\\ngeneralization errors and ambiguities of the individual students are\\n\\nak = ckk - 2 LWlckl\\nI\\n\\n+ LWIWmclm;\\n1m\\n\\nthe result for the Ck can be shown to agree with the single student result (7). In\\nthe following sections, we shall explore the consequences of the general result (9) .\\nWe will concentrate on the case where the training set of each student is sampled\\nrandomly from the total available data set of size NO\\', For the overlap of the training\\nsets of students k and I (k \\'II) one then has akl/a = (ak/a)(al/a) and hence\\n\\nak/ = akal/a\\n\\n(11)\\nup to fluctuations which vanish in the thermodynamic limit. For finite ensembles\\none can construct training sets for which akl < akal/a. This is an advantage,\\nbecause it results in a smaller generalization error, but for simplicity we use (11).\\n\\n4\\n\\nLARGE ENSEMBLE LIMIT\\n\\nWe now use our main result (9) to analyse the generalization performance of an ensemble with a large number K of students, in particular when the size of the training\\nsets for the individual students are chosen optimally. If the ensemble weights Wk\\nare approximately uniform (Wk ~ 1/ K) the off-diagonal elements of the matrix\\n(ckl) dominate the generalization error for large K, and the contributions from the\\ntraining noises\\nare suppressed. For the special case where all students are identical and are trained on training sets of identical size, ak = (1 - c)a, the ensemble\\ngeneralization error is shown in Figure 1(left). The minimum at a nonzero value\\nof c, which is the fraction of the total data set held out for testing each student,\\ncan clearly be seen. This confirms our intuition: when the students are trained\\non smaller, less overlapping training sets, the increase in error of the individual\\nstudents can be more than offset by the corresponding increase in ambiguity.\\n\\nn\\n\\nThe optimal training set sizes ak can be calculated analytically:\\n_\\n\\nCk\\n\\n=1-\\n\\nak/ a\\n\\n1 - Ak/(T2\\n\\n= 1 + G(a, (T2) \\'\\n\\n(12)\\n\\n\\x0cP. SOLLICH, A. KROGH\\n\\n194\\n1.0 r - - - , - - - - - , r - - - . , - - - - , . - - - - : .\\n\\n1.0 r - - - , - - - - - , - - - . - - - - r - - - - \"\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n\\n0.6\\n\\nw\\n\\n.\\'\\n\\nw\\n0.4\\n\\n,...-------\\n\\n0.2\\n/\\n\\n/\\n\\n0.0 /\\n\\n0.0\\n\\n/\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n,,\\n,\\n0.8\\n\\n0.2\\n\\n------,\\n\\n1.0\\n\\n0.0\\n\\n0.0\\n\\nC\\n\\n...\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nC\\n\\nFigure 1: Generalization error and ambiguity for an infinite ensemble of identical\\nstudents. Solid line: ensemble generalization error, fj dotted line: average generalization error of the individual students, l; dashed line: ensemble ambiguity, a.\\nFor both plots a = 1 and (72 = 0.2 . The left plot corresponds to under-regularized\\nstudents with A = 0.05 < (72. Here the generalization error of the ensemble has\\na minimum at a nonzero value of c. This minimum exists whenever>\\' < (72. The\\nright plot shows the case of over-regularized students (A = 0.3 > (72), where the\\ngeneralization error is minimal at c = O.\\nThe resulting generalization error is f = (72G(a, (72) + 0(1/ K), which is the globally\\nminimal generalization error that can be obtained using all available training data,\\nas explained in Section 3. Thus, a large ensemble with optimally chosen training\\nset sizes can achieve globally optimal generalization performance. However, we see\\nfrom (12) that a valid solution Ck > 0 exists only for Ak < (72, i.e., if the ensemble\\nis under-regularized. This is exemplified, again for an ensemble of identical students, in Figure 1(right) , which shows that for an over-regularized ensemble the\\ngeneralization error is a: monotonic function of c and thus minimal at c = o.\\nWe conclude this section by discussing how the adaptation of the training set sizes\\ncould be performed in practice, for simplicity confining ourselves to an ensemble of\\nidentical students, where only one parameter c = Ck = 1- ak/a has to be adapted.\\nIf the ensemble is under-regularized one expects a minimum of the generalization\\nerror for some nonzero c as in Figure 1. One could, therefore, start by training\\nall students on a large fraction of the total data set (corresponding to c ~ 0), and\\nthen gradually and randomly remove training examples from the students\\' training\\nsets. Using (6), the generalization error of each student could be estimated by\\ntheir performance on the examples on which they were not trained, and one would\\nstop removing training examples when the estimate stops decreasing. The resulting\\nestimate of the generalization error will be slightly biased; however, for a large\\nenough ensemble the risk of a strongly biased estimate from systematically testing\\nall students on too \\'easy\\' training examples seems small, due to the random selection\\nof examples.\\n\\n5\\n\\nREALISTIC ENSEMBLE SIZES\\n\\nWe now discuss some effects that occur in learning with ensembles of \\'realistic\\' sizes.\\nIn an over-regularized ensemble nothing can be gained by making the students more\\ndiverse by training them on smaller, less overlapping training sets. One would also\\n\\n\\x0c195\\n\\nLearning with Ensembles: How Overfitting Can Be Useful\\n\\nFigure 2: The generalization error of\\nan ensemble with 10 identical students as a function of the test set\\nfraction c. From bottom to top the\\ncurves correspond to training noise\\nT = 0,0.1,0.2, ... ,1.0. The star on\\neach curve shows the error of the optimal single perceptron (i. e., with optimal weight decay for the given T)\\ntrained on all examples, which is independent of c. The parameters for\\nthis example are: a = 1, A = 0.05,\\n0\\'2 = 0.2.\\n\\n0.2\\n0.0 L - _ - - \\' - _ - - - \\'_ _-\\'--_--\\'-_~\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nC\\n\\nexpect this kind of \\'diversification\\' to be unnecessary or even counterproductive\\nwhen the training noise is high enough to provide sufficient \\'inherent\\' diversity of\\nstudents. In the large ensemble limit, we saw that this effect is suppressed, but\\nit does indeed occur in finite ensembles. Figure 2 shows the dependence of the\\ngeneralization error on c for an ensemble of 10 identical, under-regularized students\\nwith identical training noises Tk = T. For small T, the minimum of f. at nonzero c\\npersists. For larger T, f. is monotonically increasing with c, implying that further\\ndiversification of students beyond that caused by the learning noise is wasteful. The\\nplot also shows the performance of the optimal single student (with A chosen to\\nminimize the generalization error at the given T), demonstrating that the ensemble\\ncan perform significantly better by effectively averaging out learning noise.\\nFor realistic ensemble sizes the presence of learning noise generally reduces the\\npotential for performance improvement by choosing optimal training set sizes. In\\nsuch cases one can still adapt the ensemble weights to optimize performance, again\\non the basis of the estimate of the ensemble generalization error (6). An example is\\n1.0\\n\\n1.0\\n\\n,,\\n\\n0.8\\n\\nI\\nI\\n\\n0.8\\n,-\\n\\n,-\\n\\n/\\n\\nI\\n\\n0.6\\n\\n0.6\\n\\nI\\n\\ntV\\n\\ntV\\n\\n0.4 ..... -_ .................\\n\\n0.4\\n0.2\\n... ....\\n0.0\\n0.001\\n\\n---0.010\\n\\n0.2\\n\\n0.100\\n\\n02\\n\\n1.000\\n\\n0.0\\n0.001\\n\\n0.010\\n\\n02\\n\\n0.100\\n\\n1.000\\n\\nFigure 3: The generalization error of an ensemble of 10 students with different\\nweight decays (marked by stars on the 0\\'2-axis) as a function of the noise level\\n0\\'2. Left: training noise T = 0; right: T = 0.1. The dashed lines are for the\\nensemble with uniform weights, and the solid line is for optimized ensemble weights.\\nThe dotted lines are for the optimal single perceptron trained on all data. The\\nparameters for this example are: a = 1, c = 0.2.\\n\\n\\x0c196\\n\\nP. SOu...ICH, A. KROGH\\n\\nshown in Figure 3 for an ensemble of size 1< = 10 with the weight decays >\\'k equally\\nspaced on a logarithmic axis between 10- 3 and 1. For both of the temperatures T\\nshown, the ensemble with uniform weights performs worse than the optimal single\\nstudent. With weight optimization, the generalization performance approaches that\\nof the optimal single student for T = 0, and is actually better at T = 0.1 over\\nthe whole range of noise levels rr2 shown. Even the best single student from the\\nensemble can never perform better than the optimal single student, so combining the\\nstudent outputs in a weighted ensemble average is superior to simply choosing the\\nbest member of the ensemble by cross-validation, i.e., on the basis of its estimated\\ngeneralization error. The reason is that the ensemble average suppresses the learning\\nnoise on the individual students.\\n\\n6\\n\\nCONCLUSIONS\\n\\nWe have studied ensemble learning in the simple, analytically solvable scenario of\\nan ensemble of linear students. Our main findings are: In large ensembles, one\\nshould use under-regularized students in order to maximize the benefits of the\\nvariance-reducing effects of ensemble learning. In this way, the globally optimal\\ngeneralization error on the basis of all the available data can be reached by optimizing the training set sizes of the individual students. At the same time an estimate\\nof the generalization error can be obtained. For ensembles of more realistic size, we\\nfound that for students subjected to a large amount of noise in the training process\\nit is unnecessary to increase the diversity of students by training them on smaller,\\nless overlapping training sets. In this case, optimizing the ensemble weights can\\nstill yield substantially better generalization performance than an optimally chosen\\nsingle student trained on all data with the same amount of training noise. This\\nimprovement is most insensitive to changes in the unknown noise levels rr2 if the\\nweight decays of the individual students cover a wide range. We expect most of these\\nconclusions to carryover, at least qualitatively, to ensemble learning with nonlinear\\nmodels, and this correlates well with experimental results presented in [7].\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n\\nC. Granger, Journal of Forecasting 8, 231 (1989).\\nD. Wolpert, Neural Networks 5, 241 (1992) .\\nL. Breimann, Tutorial at NIPS 7 and personal communication.\\nL. Hansen and P. Salamon, IEEE Trans. Pattern Anal. and Mach. Intell. 12,\\n993 (1990).\\nM. P. Perrone and L. N. Cooper, in Neural Networks for Speech and Image\\nprocessing, ed. R. J. Mammone (Chapman-Hall, 1993).\\nS. Hashem: Optimal Linear Combinations of Neural Networks. Tech. Rep .\\nPNL-SA-25166, submitted to Neural Networks (1995) .\\nA. Krogh and J. Vedelsby, in NIPS 7, ed. G. Tesauro et al., p. 231 (MIT Press,\\n1995).\\nR. Meir, in NIPS 7, ed. G. Tesauro et al., p. 295 (MIT Press, 1995).\\nA. Krogh and J. A. Hertz, J. Phys. A 25,1135 (1992).\\nJ. A. Hertz, A. Krogh, and G. I. Thorbergsson, J. Phys. A 22, 2133 (1989).\\nP. Sollich, J. Phys. A 27, 7771 (1994).\\n\\n\\x0c',\n",
       "     'pdf_name': '1044-learning-with-ensembles-how-overfitting-can-be-useful.pdf',\n",
       "     'title': 'Learning with ensembles: How overfitting can be useful',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1045',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1045',\n",
       "     'paper_text': 'SEEMORE: A View-Based Approach to\\n3-D Object Recognition Using Multiple\\nVisual Cues\\nBartlett W. Mel\\nDepartment of Biomedical Engineering\\nUniversity of Southern California\\nLos Angeles, CA 90089\\nmel@quake.usc.edu\\n\\nAbstract\\nA neurally-inspired visual object recognition system is described\\ncalled SEEMORE, whose goal is to identify common objects from\\na large known set-independent of 3-D viewiag angle, distance,\\nand non-rigid distortion. SEEMORE\\'s database consists of 100 objects that are rigid (shovel), non-rigid (telephone cord), articulated (book), statistical (shrubbery), and complex (photographs of\\nscenes). Recognition results were obtained using a set of 102 color\\nand shape feature channels within a simple feedforward network architecture. In response to a test set of 600 novel test views (6 of\\neach object) presented individually in color video images, SEEMORE\\nidentified the object correctly 97% of the time (chance is 1%) using\\na nearest neighbor classifier. Similar levels of performance were\\nobtained for the subset of 15 non-rigid objects. Generalization behavior reveals emergence of striking natural category structure not\\nexplicit in the input feature dimensions.\\n\\n1\\n\\nINTRODUCTION\\n\\nIn natural contexts, visual object recognition in humans is remarkably fast, reliable,\\nand viewpoint invariant. The present approach to object recognition is \"view-based\"\\n(e.g. see [Edelman and Bulthoff, 1992]), and has been guided by three main dogmas.\\nFirst, the \"natural\" object recognition problem faced by visual animals involves a\\nlarge number of objects and scenes, extensive visual experience, and no artificial\\n\\n\\x0c866\\n\\nB. W.MEL\\n\\ndistinctions among object classes, such as rigid, non-rigid, articulated, etc.\\nSecond, when an object is recognized in the brain, the \"heavy lifting\" is done by\\nthe first wave of action potentials coursing from the retina to the inferotemporal\\ncortex (IT) over a period of 100 ms [Oram and Perrett, 1992]. The computations\\ncarried out during this time can be modeled as a shallow but very wide feedforward\\nnetwork of simple image filtering operations. Shallow means few processing levels,\\nwide means a sparse, high-dimensional representation combining cues from multiple\\nvisual submodalities, such as color, texture, and contour [Tanaka et al., 1991].\\nThird, more complicated processing mechanisms, such as those involving focal attention, segmentation, binding, normalization, mental rotation, dynamic links, parts\\nrecognition, etc., may exist and may enhance recognition performance but are not\\nnecessary to explain rapid, robust recognition with objects in normal visual situations.\\n\\nIn this vein, the main goal of this project has been to explore the limits of performance of a shallow-but very wide-feedforward network of simple filtering operations\\nfor viewpoint-invariant 3-D object recognition, where the filter \"channels\" themselves have been loosely modeled after the shape- and color-sensitive visual response\\nproperties seen in the higher levels of the primate visual system [Tanaka et al., 1991].\\nArchitecturally similar approaches to vision have been most often applied in the domain of optical character recognition [Fukushima et al., 1983, Le Cun et al., 1990].\\nSEEMORE\\'S architecture is also similar in spirit to the color histogramming approach\\nof [Swain and Ballard, 1991], but includes spatially-structured features that provide\\nalso for shape-based generalization.\\n\\nFigure 1: The database includes 100 objects of many different types, including rigid\\n(soup can), non-rigid (necktie), statistical (bunch of grapes), and photographs of\\ncomplex indoor and outdoor scenes.\\n\\n\\x0cSEEMORE: A View-based Approach to 3-D Object Recognition\\n\\n2\\n\\n867\\n\\nSEEMORE\\'S VISUAL WORLD\\n\\nSEEMORE\\'s database contains 100 common 3-D objects and photogaphs of scenes,\\neach represented by a set of pre-segmented color video images (fig. 1). The training\\nset consisted of 12-36 views of each object as follows. For rigid objects, 12 training\\nviews were chosen at roughly 60? intervals in depth around the viewing sphere, and\\neach view was then scaled to yield a total of three images at 67%, 100%, and 150%.\\nImage plane orientation was allowed to vary arbitrarily. For non-rigid objects, 12\\ntraining views were chosen in random poses.\\nDuring a recognition trial, SEEMORE was required to identify novel test images of\\nthe database objects. For rigid objects, test images were drawn from the viewpoint\\ninterstices of the training set, excluding highly foreshortened views (e.g. bottom of\\ncan). Each test view could therefore be presumed to be correctly recognizable, but\\nnever closer than roughly 30-> in orientation in depth or 22% in scale to the nearest\\ntraining view of the object, while position and orientation in the image plane could\\nvary arbitrarily. For non-rigid objects, test images consisted of novel random poses.\\nEach test view depicted the isolated object on a smooth background.\\n\\n2.1\\n\\nFEATURE CHANNELS\\n\\nSEEMORE\\'s internal representation of a view of an object is encoded by a set\\nof feature channels. The ith channel is based on an elemental nonlinear filter\\nfi(z, y, (h, (J2, .? .), parameterized by position in the visual field and zero or more\\ninternal degrees of freedom. Each channel is by design relatively sensitive to changes\\nin the image that are strongly related to object identity, such as the object\\'s shape,\\ncolor, or texture, while remaining relatively insensitive to changes in the image that\\nare unrelated to object identity, such as are caused by changes in the object\\'s pose.\\nIn practice, this invariance is achieved in a straightfOl\\'ward way for each channel by\\nsubsampling and summing the output of the elemental channel filter over the entire\\nvisual field and one or more of its internal degrees of freedom, giving a channel\\noutput Fi = Lx,y,(h , .. . fiO. For example, a particular shape-sensitive channel might\\n\"look\" for the image-plane projections of right-angle corners, over the entire visual\\nfield, 360? of rotation in the image plane, 30? of rotation in depth, one octave in\\nscale, and tolerating partial occlusion and/or slight misorientation of the elemental\\ncontours that define the right angle. In general, then, Fi may be viewed as a \"cell\"\\nwith a large receptive field whose output is an estimate of the number of occurences\\nof distal feature i in the workspace over a large range of viewing parameters.\\nSEEMORE\\'S architecture consists of 102 feature channels, whose outputs form an\\ninput vector to a nearest-neighbor classifer. Following the design of the individual\\nchannels, the channel vector F = {FI, ... F 102 } is (1) insensitive to changes in image\\nplane position and orientation of the object, (2) modestly sensitive to changes in\\nobject scale, orientation in depth, or non-rigid deformation, but (3) highly sensitive\\nto object \"quality\" as pertains to object identity. Within this representation, total\\nmemory storage for all views of an object ranged from 1,224 to 3,672 integers.\\nAs shown in fig . 2, SEEMORE\\'s channels fall into in five groups: (1) 23 color channels, each of which responds to a small blob of color parameterized by \"best\" hue\\nand saturation, (2) 11 coarse-scale intensity corner channels parameterized by open\\nangle, (3) 12 \"blob\" features, parameterized by the shape (round and elongated) and\\n\\n\\x0c868\\n\\nB.\\n\\nW.MEL\\n\\nsize (small, medium, and large) of bright and dark intensity blobs, (4) 24 contour\\nshape features, including straight angles, curve segments of varying radius, and parallel and oblique line combinations, and (5) 16 shape/texture-related features based\\non the outputs of Gabor functions at 5 scales and 8 orientations. The implementations of the channel groups were crude, in the interests of achieving a working,\\nmultiple-cue system with minimal development time. Images were grabbed using an\\noff-the-shelf Sony S-Video Camcorder and SunVideo digitizing board.\\nColors\\n\\nBlobs\\n\\nAngles\\n\\no.\\n\\nContours\\n\\n0.1\\n\\ne.\\noe\\noe\\n\\n.e\\n\\no\\n\\n00\\n\\n??\\n??\\n??\\n\\n0\\n\\nc\\n\\n??\\n\\noe\\no\\n\\n=:>\\n\\nGabor-Based Features\\n\\n./\" 1: -\\n\\nsin2 +cos2 .......... 0 _\\n2\\n\\nenergy @ scale i\\nenergy variance\\n@scalei\\n\\n0\\n\\n6\\n45 90\\n\\n<30\\n>30\\n\\nFigure 2: SEEMORE\\'s 102 channels fall into 5 groups, sensitive to (1) colors, (2) intensity corners, (3) circular and elongated intensity blobs, (4) contour shape features,\\nand (5) 16 oriented-energy and relative-orientation features based on the outputs of\\nGabor functions at several scales and orientations.\\n\\n3\\n\\nRECOGNITION\\n\\nSEEMORE\\'s recognition performance was assesed quantitatively as follows. A test\\nset consisting of 600 novel views (100 objects x 6 views) was culled from the database, and presented to SEEMORE for identification. It was noted empirically that\\na compressive transform on the feature dimensions (histogram values) led to improved classification performance; prior to all learning and recognition operations,\\n\\n\\x0cSEEMORE: A View-based Approach to 3-D Object Recognition\\n\\n869\\n\\nFigure 3: Generalization using only shape-related channels. In each row, a novel\\ntest view is shown at far left. The sequence of best matching training views (one\\nper object) is shown to right, in order of decreasing similarity.\\n\\ntherefore, each feature value was replaced by its natural logarithm (0 values were\\nfirst replaced with a small positive constant to prevent the logarithm from blowing\\nup). For each test view, the city-block distance was computed to every training view\\nin the database and the nearest neighbor was chosen as the best match. The log\\ntransform of the feature dimension:.; thus tied this distance to the ratios of individual\\nfeature values in two images rather than their differences.\\n\\n4\\n\\nRESULTS\\n\\nRecognition time on a Sparc-20 was 1-2 minutes per view; the bulk of the time was\\ndevoted to shape processing, with under 2 seconds required for matching.\\nRecognition results are reported as the proportion of test views that were correctly\\nclassified. Performance using all 102 channels for the 600 novel object views in the\\nintact test set was 96.7%; the chance rate of correct classification was 1%. Across\\nrecognition conditions, second-best matches usually accounted for approximately\\nhalf the errors. Results were broken down in terms of the separate contributions\\nto recognition performance of color-related vs. shape-related feature channels. Performance using only the 23 color-related channels was 87.3%, and using only the\\n79 shape-related channels was 79.7%. Remarkably, very similar performance figures\\nwere obtained for the subset of 90 test views of the non-rigid objects, which included\\nseveral scarves, a bike chain, necklace, belt, sock, necktie, maple-leaf cluster, bunch\\nof grapes, knit bag, and telephone cord. Thus, a novel random configuration of a\\ntelephone cord was as easily recognized as a novel view of a shovel.\\n\\n\\x0c870\\n\\n5\\n\\nB. W.MEL\\n\\nGENERALIZATION BEHAVIOR\\n\\nNumerical indices of recognition performance are useful, but do not explicitly convey\\nthe similarity structure of the underlying feature space. A more qualitative but\\nextremely informative representation of system performance lies in the sequence of\\nimages in order of increasing distance from a test view. Records of this kind are\\nshown in fig. 3 for trials in which only shape-related channels were used. In each, a\\ntest view is shown at the far left, and the ordered set of nearest neighbors is shown\\nto the right. When a test view\\'s nearest neighbor (second image from left) was not\\nthe correct match, the trial was classified as an error.\\nAs shown in row (1), a view of a book is judged most similar to a series of other books\\n(or the bottom of a rectangular cardboard box)---each a view of a rectangular object\\nwith high-frequency surface markings. A similar sequence can be seen in subsequent\\nrows for (2) a series of cans, each a right cylinder with detailed surface markings, (3)\\na series of smooth, not-quite-round objects, (4) a series of photographs of complex\\nscenes, and (5) a series of dinosaurs (followed by a teddy bear). In certain cases,\\nSEEMORE\\'S shape-related similarity metric was more difficult to visually interpret\\nor verbalize (last two rows), or was different from that of a human observer.\\n\\n6\\n\\nDISCUSSION\\n\\nThe ecology of natural object vision gives rise to an apparent contradiction: (i)\\ngeneralization in shape-space must in some cases permit an object whose global\\nshape has been grossly perturbed to be matched to itself, such as the various tangled\\nforms of a telephone cord, but (ii) quasi-rigid basic-level shape categories (e.g. chair,\\nshoe, tree) must be preserved as well, and distinguished from each other.\\nA partial It wi uti on to this conundrum lies in the observation tbat locally-cumputed\\nshape statistics are in large part preserved under the global shape deformations that\\nnon-rigid common objects (e.g. scarf, bike-chain) typically undergo. A feature-space\\nrepresentation with an emphasis on locally-derived shape channels will therefore\\nexhibit a significant degree of invariance to global nonrigid shape deformations. The\\ndefinition of shape similarity embodied in the present approach is that two objects\\nare similar if they contain similar profiles (histograms) of their shape measures,\\nwhich emphasize locality. One way of understanding the emergence of global shape\\ncategories, then, such as \"book\", \"can\", \"dinosaur\", etc., is to view each as a set of\\ninstances of a single canonical object whose local shape statistics remain quasi-stable\\nas it is warped into various global forms. In many cases, particularly within rigid\\nobject categories, exemplars may share longer-range shape statistics as well.\\nIt is useful to consider one further aspect of SEEMORE\\'S shape representation, pertaining to an apparent mismatch between the simplicity of the shape-related feature channels and the complexity of the shape categories that can emerge from\\nthem. Specifically, the order of binding of spatial relations within SEEMORE\\'s shape\\nchannels is relatively low, i.e. consisting of single simple open or closed curves,\\nor conjunctions of two oriented contours or Gabor patches. The fact that shape\\ncategories, such as \"photographs of rooms\", or \"smooth lumpy objects\", cluster\\ntogether in a feature space of such low binding order would therefore at first seem\\nsurprising. This phenomenon relates closely to the notion of \"wickelfeatures\" (see\\n[Rumelhart and McClelland, 1986], ch. 18), in which features (relating to phonemes)\\n\\n\\x0cSEEMORE: A View-based Approach to 3-D Object Recognition\\n\\n871\\n\\nthat bind spatial information only locally are nonetheless used to represent global\\npatterns (words) with little or no residual ambiguity.\\nThe pre segmentation of objects is a simplifying assumption that is clearly invalid in\\nthe real world. The advantage of the assumption from a methodological perspective\\nis that the object similarity structure induced by the feature dimensions can be\\nstudied independently from the problem of segmenting or indexing objects imbedded\\nin complex scenes. In continuing work, we are pursuing a leap to sparse very-highdimensional space (e.g. 10,000 dimensions), whose advantages for classification in\\nthe presence of noise (or clutter) have been discussed elsewhere [Kanerva, 1988,\\nCalifano and Mohan, 1994].\\nAcknowledgements\\nThanks to J6zsef Fiser for useful discusf!ions and for development of the Gabor-based\\nchannel set, to Dan Lipofsky and Scott Dewinter for helping in the construction of\\nthe image database, and to Christof Koch for providing support at Caltech where\\nthis work was initiated. This work was funded by the Office of Naval Research, and\\nthe McDonnell-Pew Foundation.\\n\\nReferences\\n[Califano and Mohan, 1994] Califano, A. and Mohan, R. (1994). Multidimensional\\nindexing for recognizing visual shapes. IEEE Trans. on PAMI, 16:373-392.\\n[Edelman and Bulthoff, 1992] Edelman, S. and Bulthoff, H. (1992). Orientation dependence in the recognition of familiar and novel views of three-dimensional objects. Vision Res., 32:2385-2400.\\n[Fukushima et al., 1983] Fukushima, K., Miyake, S., and Ito, T. (1983). Neocognitron: A neural network model for a mechanism of visual pattern recognition.\\nIEEE Trans. Sys. Man & Cybernetics, SMC-13:826-834.\\n[Kanerva, 1988] Kanerva, P. (1988). Sparse distributed memory. MIT Press, Cambridge, MA.\\n[Le Cun et al., 1990] Le Cun, Y., Matan, 0., Boser, B., Denker, J., Henderson, D.,\\nHoward, R., Hubbard, W., Jackel, L., and Baird, H. (1990). Handwritten zip\\ncode recognition with multilayer networks. In Proc. of the 10th Int. Conf. on\\nPatt. Rec. IEEE Computer Science Press.\\n[Oram and Perrett, 1992] Oram, M. and Perrett, D. (1992). Time course of neural\\nresponses discriminating different views of the face and head. J. Neurophysiol.,\\n68(1) :70-84.\\n[Rumelhart and McClelland, 1986] Rumelhart, D. and McClelland, J. (1986). Parallel distributed processing. MIT Press, Cambridge, Massachusetts.\\n[Swain and Ballard, 1991] Swain, M. and Ballard, D. (1991). Color indexing. Int.\\nJ. Computer Vision, 7:11-32.\\n[Tanaka et al., 1991] Tanaka, K., Saito, H., Fukada, Y., and Moriya, M. (1991).\\nCoding visual images of objects in the inferotemporal cortex of the macaque\\nmonkey. J. Neurophysiol., 66:170-189.\\n\\n\\x0c\\x0cPART VIII\\nAPPLICATIONS\\n\\n\\x0c\\x0c',\n",
       "     'pdf_name': '1045-seemore-a-view-based-approach-to-3-d-object-recognition-using-multiple-visual-cues.pdf',\n",
       "     'title': 'SEEMORE: A View-Based Approach to 3-D Object Recognition Using Multiple Visual Cues',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '105',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '105',\n",
       "     'paper_text': '340\\n\\nBACKPROPAGATION AND ITS\\nAPPLICATION TO HANDWRITTEN\\nSIGNATURE VERIFICATION\\nDorothy A. Mighell\\nElectrical Eng. Dept.\\nInfo. Systems Lab\\nStanford University\\nStanford, CA 94305\\n\\nTimothy S. Wilkinson\\nElectrical Eng. Dept.\\nInfo. Systems Lab\\nStanford University\\nStanford, CA 94305\\n\\nJoseph W. Goodman\\nElectrical Eng. Dept.\\nInfo. Systems Lab\\nStanford University\\nStanford, CA 94305\\n\\nABSTRACT\\nA pool of handwritten signatures is used to train a neural network for the task of deciding whether or not a given signature is a\\nforgery. The network is a feedforward net, with a binary image as\\ninput. There is a hidden layer, with a single unit output layer. The\\nweights are adjusted according to the backpropagation algorithm.\\nThe signatures are entered into a C software program through the\\nuse of a Datacopy Electronic Digitizing Camera. The binary signatures are normalized and centered. The performance is examined\\nas a function of the training set and network structure. The best\\nscores are on the order of 2% true signature rejection with 2-4%\\nfalse signature acceptance.\\n\\nINTRODUCTION\\nSignatures are used everyday to authorize the transfer of funds for millions of people.\\nWe use our signature as a form of identity, consent, and authorization. Bank checks,\\ncredit cards, legal documents and waivers all require the everchanging personalized\\nsignature. Forgeries on such transactions amount to millions of dollars lost each\\nyear. A trained eye can spot most forgeries, but it is not cost effective to handcheck\\nall signatures due to the massive number of daily transactions. Consequently, only\\ndisputed claims and checks written for large amounts are verified. The consumer\\nwould certainly benefit from the added protection of automated verification. Neural\\nnetworks lend themselves very well to signature verification. Previously, they have\\nproven applicable to other signal processing tasks, such as character recognition\\n{Fukishima, 1986} {Jackel, 1988}, sonar target classification {Gorman, 1986}, and\\ncontrol- as in the broom balancer {Tolat, 1988}.\\n\\nHANDWRITING ANALYSIS\\nSignature verification is only one aspect of the study of handwriting analysis.\\nRecognition is the objective, whether it be of the writer or the characters. Writer\\nrecognition can be further broken down into identification and verification. Identi-\\n\\n\\x0cBackpropagation and Handwritten Signature Verification\\n\\nfication selects the author of a sample from among a group of writers. Verification\\nconfirms or rejects a written sample for a single author. In both cases, it is the\\nstyle of writing that is important.\\nDeciphering written text is the basis of character recognition. In this task, linguistic\\ninformation such as the individual characters or words are extracted from the text.\\nStyle must be eliminated to get at the content. A very important application of\\ncharacter recognition is automated reading of zip-codes in the post office {Jackel,\\n1988}.\\nData for handwriting analysis may be either dynamic or static. Dynamic data\\nrequires special devices for capturing the temporal characteristics of the sample.\\nFeatures such as pressure, velocity, and position are examined in the dynamic\\nframework. Such analysis is usually performed on-line in real time.\\nStatic analysis uses the final trace of the writing, as it appears on paper. Static\\nanalysis does not require any special processing devices while the signature is being\\nproduced. Centralized verification becomes possible, and the processing may be\\ndone off-line.\\nWork has been done in both static and dynamic analysis {Sato, 1982} {Nemcek,\\n1974}. Generally, signature verification efforts have been more successful using\\nthe dynamic information. It would be extremely useful though, to perform the\\nverification using only the written signature. This would eliminate the need for\\ncostly machinery at every place of business. Personal checks may also be verified\\nthrough a static signature analysis.\\n\\nTASK\\nThe handwriting analysis task with which this paper is concerned is that of signature verification using an off-line method to detect casual forgeries. Casual forgeries\\nare non-professional forgeries, in which the writer does not practice reproducing\\nthe signature. The writer may not even have a copy of the true signature. Casual\\nforgeries are very important to detect. They are far more abundant, and involve\\ngreater monetary losses than professional forgeries. This signature verification task\\nfalls into the writer recognition category, in which the style of writing is the important variable. The off-line analysis allows centralized verification at a lower cost\\nand broader use.\\n\\nHANDWRITTEN SIGNATURES\\nThe signatures for this project were gathered from individuals to produce a pool\\nof 80 true signatures and 66 forgeries. These are signatures, true and false, for one\\nperson. There is a further collection of signatures, both true and false, for other\\npersons, but the majority of the results presented will be for the one individual. It\\nwill be clear when other individuals are included in the demonstration.\\nThe signatures are collected on 3x5 index cards which have a small blue box as\\n\\n341\\n\\n\\x0c342\\n\\nWilkinson, Mighell and Goodman\\n\\na guideline. The cards are scanned with a CCD array camera from Datacopy,\\nand thresholded to produce binary images. These binary images are centered and\\nnormalized to fit into a 128x64 matrix. Either the entire 128x64 image is presented\\nas input, or a 90x64 image of the three initials alone is presented. It is also possible\\nto present preprocessed inputs to the network.\\n\\nSOFTWARE SIMULATION\\nThe type of learning algorithm employed is that of backpropagation. Both dwell\\nand momentum are included. Dwell is the type of scheduling employed, in which\\nan image is presented to the network, and the network is allowed to \"dwell\" on that\\ninput for a few iterations while updating its weights. C. Rosenberg and T. Sejnowski\\nhave done a few studies on the effects of scheduling on learning {Rosenberg, 1986}.\\nMomentum is a term included in the change of weights equation to speed up learning\\n{Rumelhart, 1986}.\\nThe software is written in Microsoft C, and run on an IBM PC/AT with an 80287\\nmath co-processor chip.\\nIncluded in the simulation is a piece-wise linear approximation to the sigmoid transfer function as shown in Figure 1. This greatly improves the speed of calculation,\\nbecause an exponential is not calculated. The non-linearity is kept to allow for\\nlayering of the network. Most of the details of initialization and update are the\\nsame as that reported in NetTalk {Sejnowski, 1986}.\\n\\nOUT\\n\\n~-111111::::+~----\\'.\\n\\nIN\\n\\nFigure 1. Piece-wise linear transfer function.\\nMany different nets were trained in this signature verification project, all of which\\nwere feed-forward. The output layer most often consisted of a single output neuron,\\nbut 5 output neurons have been used as well. If a hidden layer was used, then\\nthe number of hidden units ranged from 2 to 53. The networks were both fullyconnected and partially-connected.\\n\\nSAMPLE RUN\\nThe simplest network is that of a single neuron taking all 128x64 pixels as input,\\nplus one bias. Each pixel has a weight associated with it, so that the total number\\nof weights is 128x64 + 1 = 8193. Each white pixel is assigned an input value of + 1,\\neach black pixel has a value of -1. The training set consists of 10 true signatures\\n\\n\\x0cBackpropagation and Handwritten Signature Verification\\n\\nwith 10 forgeries. Figure 2a depicts the network structure of this sample run.\\n\\nOUT\\n\\nc::\\n\\n1\\n\\n0\\n\\n-\"\\nu\\n\\nCD\\nCD\\n\\n0.5\\n\\n-\\n\\nf- ..\\n\\nCD\\n\\n-:J\\n\"-\\n\\nQ.\\n\\n0\\n\\n0.5\\n\\n0\\nP(false\\n\\n~1~111J\\n\\n111.\\n\\n1\\n\\nacceptance)\\n\\n(b)\\n\\n\"~~~mlla\\n1\\n\\n1/\\n\\n(a)\\n\\n~\\n\\nen\\n\\nLL.\\n\\nC\\n\\n0\\n\\n0.5\\n\\nf\\n\\n0\\n0\\n(e)\\n\\n0.5\\n\\n1\\n\\nOutput Values\\n\\n(d)\\nFigure 2. Sample run.\\na) Network = one output neuron, one weight per pixel, fully connected. Training set = 10 true signatures + 10 forgeries.\\nb) ROC plot for the sample run. (Probability of fa1se acceptance\\nvs probability of true detection). Test set = 70 true signatures\\n+ 56 forgeries.\\nc) Clipped picture of the weights for the sample run. White\\npositive weight, black = negative weight.\\n\\n=\\n\\nd) Cumulative distribution function for the true signatures (+) and\\nfor the forgeries (0) of the sample run.\\nThe network is trained on these 20 signatures until all signatures are classified\\n\\n343\\n\\n\\x0c344\\n\\nWilkinson, Mighell and Goodman\\n\\ncorrectly. The trained network is then tested on the remaining 70 true signatures\\nand 56 forgeries.\\nThe results are depicted in Figures 2b and 2d. Figure 2b is a radar operating\\ncharacteristic curve, or roc plot for short. In this presentation of data, the probability of detecting a true signature is plotted against the probability of accepting a\\nforgery. Roc plots have been used for some time in the radar sciences as a means\\nfor visualizing performance {Marcum, 1960}. A perfect roc plot has a right angle\\nin the upper left-hand corner which would show perfect separation of true signatures from forgeries. The curve is plotted by varying the threshold for classification.\\nEverything above the threshold is labeled a true signature, everything below the\\nthreshold is labeled a forgery. The roc plot in Figure 2b is close to perfect, but\\nthere is some overlap in the output values of the true signatures and forgeries. The\\noverlap can be seen in the cumulative distribution functions (cdfs) for the true and\\nfalse signatures as shown in Figure 2d. As seen in the cdfs, there is fairly good\\nseparation of the output values. For a given threshold of 0.5, the network produces\\n1% rejection of true signatures as false, with 4% acceptance of forgeries as being\\ntrue. IT one lowers the threshold for classification down to 0.43, the true rejection\\nbecomes nil, with a false acceptance of 7% . A simplified picture of the weights is\\nshown in Figure 2c, with white pixels designating positive weights, and black pixels\\nnegative weights.\\n\\nOTHER NETWORKS\\nThe sample run above was expanded to include 2 and 3 hidden neurons with the\\nsingle output neuron. The results were similar to the single unit network, implying\\nthat the separation is linear.\\nThe 128x64 input image was also divided into regions, with each region feeding into\\na single neuron. In one network structure, the input was sectioned into 32 equally\\nsized regions of 16x16 pixels. The hidden layer thus has 32 neurons, each neuron\\nreceiving 16x16 + 1 inputs. The output neuron had 33 inputs. Likewise, the input\\nimage was divided into 53 regions of 16x16 pixels, this time overlapping.\\nFinally, only the initials were presented to the network. (Handwriting experts\\nhave noted that leading strokes and separate capital letters are very significant in\\nclassification {Osborn, 1929}.) In this case, two types of networks were devised.\\nThe first had a single output neuron, the second had three hidden neurons plus one\\noutput neuron. Each of the hidden neurons received inputs from only one initial,\\nrather than from all three. The network with the single output neuron produced\\nthe best results of all, with 2% true rejection and 2% false acceptance.\\n\\nIMPORTANCE OF FORGERIES IN THE TRAINING SET\\nIn all cases, the networks performed much better when forgeries were included in the\\ntraining set. When an all-white image is presented as the only forgery, performance\\ndeteriorates significantly. When no forgeries are present, the network decides that\\n\\n\\x0cBackpropagation and Handwritten Signature Verification\\n\\nall signatures are true signatures. It is therefore desirable to include actual forgeries\\nin the training set, yet they may be impractical to obtain. One possibility for\\navoiding ?the collection of forgeries is to use computer generated forgeries. Another\\nis to distort the true signatures. A third is to use true signatures of other people as\\nforgeries for the person in question. The attraction of this last option is that the\\nmasquerading forgeries are already available for use.\\n\\nNETWORK WITHOUT FORGERIES\\nTo test the use of true signatures of other people for forgeries, the following network\\nis devised. Once again, the input is the 128x64 pixel image. The output layer is\\ncomprised of five output neurons fully connected to the input image. The function\\nof each output neuron is to be active when presented with a particular persons\\'\\nsignature. When a forgery is present, the output is to be low. Figure 3a depicts this\\nnetwork. The training set has 50 true signatures, ten for each of five people. Each\\nsignature has a desired output of true for one neuron, and false for the remaining\\nfour neurons. Once the network is trained, it is tested on 210 true signatures and\\n150 forgeries. Figures 3b and 3c record the results. At a threshold of 0.5, the true\\nrejection is 3% and the false acceptance is 14%. Decreasing the threshold down to\\n0.41 gives 0% true rejection and 28% false acceptance. These results are similar\\nto the sample run, though not as good. This is a simple demonstration of the use\\nof other true signatures as forgeries. More sophisticated techniques could improve\\nthe discrimination. For instance, selecting names with similar lengths or spelling\\nshould improve the classification.\\n\\nCONCLUSION\\nAutomated signature verification systems would be extremely important in the\\nbusiness world for verifying monetary transactions. Countless dollars are lost each\\nday to instances of casual forgeries. An artificial neural network employing the\\nbackpropagation learning algorithm has been trained on both true and false signatures for classification. The results have been very good: 2% rejection of genuine\\nsignatures with 2% acceptance of forgeries. The analysis requires only the static\\npicture of the signature, there by offering widespread use through centralized verification. True signatures of other people may substitute for the forgeries in the\\ntraining set - eliminating the need for collecting non-genuine signatures.\\n\\n345\\n\\n\\x0c346\\n\\nWilkinson, Mighell and Goodman\\n\\nJWG JTH TSW LDK ABH\\n\\n- r--::iif1l---------..\\noC\\n\\n(a)\\nlr-----------~~~--_=~\\n\\n1\\n\\n( ,)\\n\\nCD\\n\\nQ)\\n\\n.-\\n\\n(f.\\n\\nI\\n\\n~\\n\\n00.5\\n\\n\"t:S U.5\\n\\no\\n\\nQ)\\n\\n::J\\n\\n~\\n\\no~----------~--------~\\n\\no\\n\\n0.5\\n\\nP(false\\n\\nacceptance)\\n\\n1\\n\\no~~----~~~--------~\\n\\no\\n\\n0.5\\n\\n1\\n\\nOutput Values\\n\\n(b)\\n\\n(c)\\n\\nFigure 3. Network without forgeries for 5 individuals.\\na) Network\\n5 output neurons, one for each individua~ as indicated by the initials. Training set = 10 true signatures for each\\nindividual.\\n\\n=\\n\\nb) ROC plot for the network without forgeries.\\n210 true signatures + 150 forgeries.\\nTest set\\n\\n=\\n\\nc) Cumulative distribution function for the true signatures (+) and\\nfor the forgeries (0) of the network without forgeries.\\n\\nReferenees\\nK. Fukishima and S. Miyake, \"Neocognitron: A biocybernetic approach to visual\\npattern recognition Jt , in NHK Laboratorie~ Note, Vol. 336, Sep 1986 (NHK\\nScience and Technical Research Laboratories, Tokyo).\\n\\n\\x0cBackpropagation and Handwritten Signature Verification\\n\\nP. Gorman and T. J. Sejnowski, \"Learned classification of sonar targets using a\\nmassively parallel network\", in the proceedings of the IEEE ASSP Oct 21,\\n1986 DSP Workshop, Chatham, MA.\\nL. D. Jackel, H. P. Graf, W. Hubbard, J. S. Denker, and D. Henderson, \"An\\napplication of neural net chips: handwritten digit recognition\", in IEEE International Oonference on Neural Networks 1988, II 107-115.\\nJ. T. Marcum, \"A statistical theory of target detection by pulsed radar\", in IRE\\nTransactions in Information Theory, Vol. IT-6 (Apr.), pp 145-267, 1960.\\nW. F. Nemcek and W. C. Lin, \"Experimental investigation of automatic signature\\nverification\" in IEEE Transactions on Systems, Man, and Oybernetics, Jan.\\n1974, pp 121-126.\\nA. S. Osborn, Questioned Documents, 2nd edition (Boyd Printing Co, Albany NY)\\n1929.\\nC. R. Rosenberg and T. J. Sejnowski, \"The spacing effect on NETtalk, a massively parallel network\", in Proceedings of the Eighth Annual Oonference of\\nthe Oognitive Science Society, (Hillsdale, New Jersey: Lawrence Erlbaum\\nAssociates, 1986) 72-89.\\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning internal representations by error propagation\", in Parallel Distributed Processing: Explorations\\nin the Microstructures of Oognition. Vol. 1: Foundations, edited by D. E.\\nRumelhart & J. L. McClelland, (MIT Press, 1986).\\nY. Sato and K. Kogure, \"Online signature verification based on shape, motion,\\nand writing pressure\", in Proceedings of the 6th International Oonference on\\nPattern Recognition, Vol. 2, pp 823-826 (IEEE NY) 1982.\\nT. J. Sejnowski and C. R. Rosenberg, \"NETtalk: A Parallel Network that Learns\\nto Read Aloud\", Johns Hopkins University Department of Electrical Engineering and Computer Science Technical Report JHU /EECS-86/01, (1986).\\nV. V. Tolat and B. Widrow, \"An adaptive \\'broom balancer\\' with visual inputs\" , in\\nIEEE International Oonference on Neural Networks 1988, II 641-647.\\n\\n347\\n\\n\\x0c',\n",
       "     'pdf_name': '105-backpropagation-and-its-application-to-handwritten-signature-verification.pdf',\n",
       "     'title': 'Backpropagation and Its Application to Handwritten Signature Verification',\n",
       "     'year': '1988'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1052',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1052',\n",
       "     'paper_text': 'Learning the structure of similarity\\n\\nJoshua B. Tenenbaum\\nDepartment of Brain and Cognitive Sciences\\nMassachusetts Institute of Technology\\nCambridge, MA 02139\\njbt~psyche.mit.edu\\n\\nAbstract\\nThe additive clustering (ADCL US) model (Shepard & Arabie, 1979)\\ntreats the similarity of two stimuli as a weighted additive measure\\nof their common features. Inspired by recent work in unsupervised\\nlearning with multiple cause models, we propose anew, statistically\\nwell-motivated algorithm for discovering the structure of natural\\nstimulus classes using the ADCLUS model, which promises substantial gains in conceptual simplicity, practical efficiency, and solution\\nquality over earlier efforts. We also present preliminary results with\\nartificial data and two classic similarity data sets.\\n\\n1\\n\\nINTRODUCTION\\n\\nThe capacity to judge one stimulus, object, or concept as similado another is thought\\nto play a pivotal role in many cognitive processes, including generalization , recognition, categorization, and inference. Consequently, modeling subjective similarity\\njudgments in order to discover the underlying structure of stimulus representations\\nin the brain/mind holds a central place in contemporary cognitive science. Mathematical models of similarity can be divided roughly into two families: spatial models,\\nin which stimuli correspond to points in a metric (typically Euclidean) space and\\nsimilarity is treated as a decreasing function of distance; and set-theoretic models, in\\nwhich stimuli are represented as members of salient subsets (presumably corresponding to natural classes or features in the world) and similarity is treated as a weighted\\nsum of common and distinctive subsets.\\nSpatial models, fit to similarity judgment data with familiar multidimensional scaling (MDS) techniques, have yielded concise descriptions of homogeneous, perceptual\\ndomains (e.g. three-dimensional color space), often revealing the salient dimensions\\nof stimulus variation (Shepard, 1980). Set-theoretic models are more general , in\\nprinciple able to accomodate discrete conceptual structures typical of higher-level\\ncognitive domains, as well as dimensional stimulus structures more common in per-\\n\\n\\x0c4\\n\\n1. B. TENENBAUM\\n\\nception (Tversky, 1977). In practice, however, the utility of set-theoretic models is\\nlimited by the hierarchical clustering techniques that underlie conventional methods\\nfor discovering the discrete features or classes of stimuli. Specifically, hierarchical\\nclustering requires that any two classes of stimuli correspond to disjoint or properly\\ninclusive subsets, while psychologically natural classes may correspond in general to\\narbitrarily overlapping subsets of stimuli. For example, the subjective similarity of\\ntwo countries results from the interaction of multiple geographic and cultural factors, and there is no reason a priori to expect the subsets of communist, African, or\\nFrench-speaking nations to be either disjoint or properly inclusive.\\nIn this paper we consider the additive clustering (ADCL US) model (Shepard & Arabie, 1979), the simplest instantiation of Tversky \\'s (1977) general contrast model that\\naccommodates the arbitrarily overlapping class structures associated with multiple\\ncauses of similarity. Here, the similarity of two stimuli is modeled as a weighted\\nadditive measure of their common clusters:\\nK\\n\\nSij\\n\\n=\\n\\nI:\\n\\nwkfikfJk\\n\\n+ C,\\n\\n(1)\\n\\nk=l\\n\\nwhere Sij is the reconstructed similarity of stimuli i and j, the weight Wk captures\\nthe salience of cluster k, and the binary indicator variable fik equals 1 if stimulus i\\nbelongs to cluster k and 0 otherwise. The additive constant c is necessary because the\\nsimilarity data are assumed to be on an interval scale. 1 As with conventional clustering models, ADCLUS recovers a system of discrete subsets of stimuli, weighted by\\nsalience, and the similarity of two stimuli increases with the number (and weight)\\nof their common subsets. ADCLUS, however, makes none of the structural assumptions (e.g. that any two clusters are disjoint or properly inclusive) which limit the\\napplicability of conventional set-theoretic models. Unfortunately this flexibility also\\nmakes the problem of fitting the ADCL US model to an observed similarity matrix\\nexceedingly difficult.\\nPrevious attempts to fit the model have followed a heuristic strategy to minimize a\\nsquared-error energy function ,\\n\\nE\\n\\n= I:(Sij - Sij)2 = I:(Sij itj\\n\\nitj\\n\\nI:\\n\\nwklikfJk)2,\\n\\n(2)\\n\\nk\\n\\nby alternately solving for the best cluster configurations fik given the current weights\\nWk and solving for the best weights given the current clusters (Shepard & Arabie,\\n1979; Arabie & Carroll, 1980). This strategy is appealing because given the cluster configuration, finding the optimal weights becomes a simple linear least-squares\\nproblem.2 However, finding good cluster configurations is a difficult problem in combinatorial optimization, and this step has always been the weak point in previous\\nwork . The original ADCLUS (Shepard & Arabie, 1979) and later MAPCLUS (Arabie & Carroll, 1980) algorithms employ ad hoc techniques of combinatorial optimization that sometimes yield unexpected or uninterpretable final results. Certainly, no\\nrigorous theory exists that would explain why these approaches fail to discover the\\nunderlying structure of a stimulus set when they do.\\nEssentially, the ADCL US model is so challenging to fit because it generates similarities from the interaction of many independent underlying causes . Viewed this way,\\nmodeling the structure of similarity looks very similar to the multiple-cause learning\\nIn the remainder of this paper, we absorb c into the sum over k, taking the sum over\\n== c, and fixing !iO = 1, (Vi) .\\n2Strictly speaking, because the weights are typically constrained to be nonnegative, more\\nelaborate techniques than standard linear least-squares procedures may be required.\\n1\\n\\nk\\n\\n= 0, ... , K , defining Wo\\n\\n\\x0c5\\n\\nLearning the Structure of Similarity\\n\\nproblems that are currently a major focus of study in the neural computation literature (Ghahramani, 1995; Hinton, Dayan, et al., 1995; Saund, 1995; Neal, 1992). Here\\nwe propose a novel approach to additive clustering, inspired by the progress and\\npromise of work on multiple-cause learning within the Expectation-Maximization\\n(EM) framework (Ghahramani, 1995; Neal, 1992). Our BM approach still makes\\nuse of the basic insight behind earlier approaches, that finding {wd given {lid is\\neasy, but obtains better performance from treating the unknown cluster memberships\\nprobabilistically as hidden variables (rather than parameters of the model), and perhaps more importantly, provides a rigorous and well-understood theory. Indeed, it\\nis natural to consider {/ik} as \"unobserved\" features of the stimuli, complementing the observed data {Sij} in the similarity matrix. Moreover, in some experimental\\nparadigms, one or more of these features may be considered observed data, if subjects\\nreport using (or are requested to use) certain criteria in their similarity judgments.\\n\\n2\\n\\nALGORITHM\\n\\n2.1\\n\\nMaximum likelihood formulation\\n\\nWe begin by formulating the additive clustering problem in terms of maximum likelihood estimation with unobserved data. Treating the cluster weights w\\n{Wk}\\nas model parameters and the unobserved cluster memberships I = {lik} as hidden\\ncauses for the observed similarities S {Sij}, it is natural to consider a hierarchical\\ngenerative model for the \"complete data\" (including observed and unobserved components) of the form p(s, Ilw) = p(sl/, w)p(flw). In the spirit of earlier approaches\\nto ADCLUS that seek to minimize a squared-error energy function, we take p(sl/, w)\\nto be gaussian with common variance u 2 :\\n\\n=\\n\\n=\\n\\np(sl/, w) ex: exp{ -~ \\'L:(Sij - Sij )2} = exp{ -~ \\'L:(Sij 2u itj\\n2u itj\\n\\n\\'L: wklik/ik)2}.\\n\\n(3)\\n\\nk\\n\\nNote that logp(sl/, w) is equivalent to -E/(2u 2 ) (ignoring an additive constant),\\nwhere E is the energy defined above. In general, priors p(flw) over the cluster\\nconfigurations may be useful to favor larger or smaller clusters, induce a dependence\\nbetween cluster size and cluster weight, or bias particular kinds of class structures,\\nbut only uniform priors are considered here. In this case -E /(2u 2 ) also gives the\\n\"complete data\" loglikelihood logp(s, Ilw).\\n\\n2.2\\n\\nThe EM algorithm for additive clustering\\n\\nGiven this probabilistic model, we can now appeal to the EM algorithm as the basis\\nfor a new additive clustering technique. EM calls for iterating the following twostep procedure, in order to obtain successive estimates of the parameters w that are\\nguaranteed never to decrease in likelihood (Dempster et al., 1977). In the E-step, we\\ncalculate\\n\\nQ(wlw(n)) =\\n\\nL,: p(f\\' Is, wen)) logp(s,f/lw) =\\nl\\'\\n\\n2 \\\\ (-E}3,w(n).\\n\\n(4)\\n\\nu\\n\\nQ(wlw(n) is equivalent to the expected value of E as a function of w, averaged over\\n\\nall possible configurations I\\' of the N K binary cluster memberships, given the observed data s and the current parameter estimates wen). In the M-step, we maximize\\nQ(wlw(n) with respect to w to obtain w(n+l).\\nEach cluster configuration I\\' contributes to the mean energy in proportion to its\\nprobability under the gaussian generative model in (3). Thus the number of configurations making significant contributions depends on the model variance u 2 . For large\\n\\n\\x0c6\\n\\nJ. B. TENENBAUM\\n\\nthe probability is spread over many configurations. In the limiting case u 2 ---+ 0,\\nonly the most likely configuration contributes, making EM effectively equivalent to\\nthe original approaches presented in Section 1 that use only the single best cluster\\nconfiguration to solve for the best cluster weights at each iteration.\\nU2 ,\\n\\nIn line with the basic insight embodied less rigorously in these earlier algorithms, the\\nM-step still reduces to a simple (constrained) linear least-squares problem, because\\nthe mean energy (E} = L:i#j (srj - 2Sij L:k Wk(fik!ik} + L:kl WkWl(fik!jk!il!il}) ,\\nlike the energy E, is quadratic in the weights Wk. The E-step, which amounts to\\ncomputing the expectations mijk = (fik!ik} and mijkl = (fik !ik!il/j I} , is much\\nmore involved , because the required sums over all possible cluster configurations f\\'\\nare intractable for any realistic case. We approximate these calculations using Gibbs\\nsampling, a Monte Carlo method that has been successfully applied to learning similar\\ngenerative models with hidden variables (Ghahramani, 1995; Neal 1992).3\\nFinally, the algorithm should produce not only estimates of the cluster weights, but\\nalso a final cluster configuration that may be interpreted as the psychologically natural\\nfeatures or classes of the relevant domain. Consider the expected cluster memberships\\nPik = (fik}$ w(n) , which give the probability that stimulus i belongs to cluster k, given\\nthe observed similarity matrix and the current estimates of the weights. Only when\\nall Pik are close to 0 or 1, i.e. when u 2 is small enough that all the probability becomes\\nconcentrated on the most likely cluster configuration and its neighbors, can we fairly\\nassert which stimuli belong to which classes.\\n2.3\\n\\nSimulated annealing\\n\\nTwo major computational bottlenecks hamper the efficiency of the algorithm as described so far. First, Gibbs sampling may take a very long time to converge to the\\nequilibrium distribution, particularly when u 2 is small relative to the typical energy\\ndifference between neighboring cluster configurations. Second, the likelihood surfaces\\nfor realistic data sets are typically riddled with local maxima. We solve both problems\\nby annealing on the variance. That is, we run Gibbs sampling using an effective variance\\ninitially much greater than the assumed model variance 2 , and decrease\\ntowards u 2 according to the following two-level scheme. We anneal within the\\nnth iteration of EM to speed the convergence of the Gibbs sampling E-step (Neal,\\n1993) , by lowering u;jJ from some high starting value down to a target U~arg(n) for\\nthe nth EM iteration . We also anneal between iterations of EM to avoid local maxima\\n(Ros~ et al., 1990), by intializing U~arg(o) at a high value and taking U~arg(n) ---+ u 2\\nas n Increases.\\n\\nu;\"\\n\\n3\\n\\nu;\"\\n\\nu\\n\\nRESULTS\\n\\nIn all of the examples below, one run of the algorithm consisted of 100-200 iterations\\nof EM, annealed both within and between iterations. Within each E-step, 10-100\\ncycles of Gibbs sampling were carried out at the target temperature UTarg while the\\nstatistics for mik and mijk were recorded. These recorded cycles were preceeded\\nby 20-200 unrecorded cycles, during which the system was annealed from a higher\\ntemperature (e.g. 8u~arg) down to U~arg, to ensure that statistics were collected as\\nclose to equilibrium as possible. The precise numbers of recorded and unrecorded\\niterations were chosen as a compromise between the need for longer samples as the\\n3We generally also approximate\\nsults with much greater efficiency.\\n\\nmiJkl\\n\\n~\\n\\nmiJkmi;\"l,\\n\\nwhich usually yields satisfactory re-\\n\\n\\x0c7\\n\\nLearning the Structure of Similarity\\n\\nTable 1: Classes and weights recovered for the integers 0-9.\\nRank\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n\\nWeight\\n.444\\n.345\\n.331\\n.291\\n.255\\n.216\\n.214\\n.172\\n\\nVariance accounted for\\n\\nStimuli in class\\n2\\n4\\n\\n8\\n\\n012\\n\\n3\\n\\n9\\n6\\n6 789\\n2 345 6\\n1\\n3\\n5\\n7\\n9\\n1 2 3 4\\n4 5 6 7 8\\n\\n= 90.9% with\\n\\nInterpretation\\npowers of two\\nsmall numbers\\nmultiples of three\\nlarge numbers\\nmiddle numbers\\nodd numbers\\nsmallish numbers\\nlargish numbers\\n\\n8 clusters (additive constant\\n\\n= .148).\\n\\nnumber of hidden variables is increased and the need to keep computation times\\npractical.\\n3.1\\n\\nArtificial data\\n\\nWe first report results with artificial data, for which the true cluster memberships and\\nweights are known, to verify that the algorithm does in fact find the desired structure.\\nWe generated 10 data sets by randomly assigning each of 12 stimuli independently\\nand with probability 1/2 to each of 8 classes, and choosing random weights for the\\nclasses uniformly from [0.1,0.6]. These numbers are grossly typical of the real data\\nsets we examine later in this section. We then calculated the observed similarities\\nfrom (1), added a small amount of random noise (with standard deviation equal to\\n5% of the mean noise-free similarity), and symmeterized the similarity matrix.\\nThe crucial free parameter is K, the assumed number of stimulus classes. When the\\nalgorithm was configured with the correct number of clusters (K = 8), the original\\nclasses and weights were recovered during the first run of the algorithm on all 10 data\\nsets, after an average of 58 EM iterations (low 30, high 92). When the algorithm\\nwas configured with K = 7 clusters, one less than the correct number, the seven\\nclasses with highest weight were recovered on 9/10 first runs. On these runs, the\\nrecovered weights and true weights had a mean correlation of 0.948 (p < .05 on each\\nrun). When configured with K = 5, the first run recovered either four of the top\\nfive classes (6/10 trials) or three of the top five (4/10 trials). When configured with\\ntoo many clusters (K = 12), the algorithm typically recovered only 8 clusters with\\nsignificantly non-zero weights, corresponding to the 8 correct classes. Comparable\\nresults are not available for ADCLUS or MAPCLUS, but at least we can be satisfied\\nthat our algorithm achieves a basic level of competence and robustness.\\n3.2\\n\\nJudged similarities of the integers 0-9\\n\\nShepard et al. (1975) had subjects judge the similarities of the integers 0 through\\n9, in terms of the \"abstract concepts\" of the numbers. We analyzed the similarity\\nmatrix (Shepard, personal communication) obtained by pooling data across subjects\\nand across three conditions of stimulus presentation (verbal, written-numeral, and\\nwritten-dots). We chose this data set because it illustrates the power of additive\\nclustering to capture a complex, overlapping system of classes, and also because\\nit serves to compare the performance of our algorithm with the original ADCL US\\nalgorithm. Observe first that two kinds of classes emerge in the solution. Classes\\n1, 3, and 6 represent familiar arithmetic concepts (e.g. \"multiples of three\", \"odd\\nnumbers\"), while the remaining classes correspond to subsets of consecutive integers\\n\\n\\x0c8\\n\\n1. B. TENENBAUM\\n\\nTable 2: Classes and weights recovered for the 16 consonant phonemes.\\nRank\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n\\nWeight\\n.800\\n.572\\n.463\\n.424\\n.357\\n.292\\n.169\\n.132\\n\\nStimuli in class\\nf 0\\ndg\\n\\np k\\nb\\n\\nv {t\\n\\npt k\\nmn\\ndgvCTz2\\n\\nptkfOs\\n\\nInterpretation\\nfront unvoiced fricatives\\nback voiced stops\\nunvoiced stops (omitting t)\\nfront voiced\\nunvoiced stops\\nnasals\\nvoiced (omitting b)\\nunvoiced (omittings)\\n\\nVariance accounted for = 90.2% with 8 clusters (additive constant = .047).\\n\\nand thus together represent the dimension of numerical magnitude. In general, both\\narithmetic properties and numerical magnitude contribute to judged similarity, as\\nevery number has features of both types (e.g. 9 is a \"large\" \"odd\" \"multiple of three\"),\\nexcept for 0, whose only property is \"small.\" Clearly an overlapping clustering model\\nis necessary here to accomodate the multiple causes of similarity.\\nThe best solution reported for these data using the original ADCLUS algorithm\\nconsisted of 10 classes, accounting for 83.1% of the variance of the data (Shepard &\\nArabie, 1979).4 Several of the clusters in this solution differed by only one or two\\nmembers (e.g. three of the clusters were {0,1}, {0,1,2}, and {0,1,2,3,4}), which led\\nus to suspect that a better fit might be obtained with fewer than 10 classes. Table 2\\nshows the best solution found in five runs of our algorithm, accounting for 90.9% of\\nthe variance with eight classes. Compared with our solution, the original ADCLUS\\nsolution leaves almost twice as much residual variance unaccounted for, and with 10\\nclasses, is also less parsimonious.\\n\\n3.3\\n\\nConfusions between 16 consonant phonemes\\n\\nFinally, we examine Miller & Nicely\\'s (1955) classic data on the confusability of 16\\nconsonant phonemes, collected under varying signal/noise conditions with the original intent of identifying the features of English phonology (compiled and reprinted\\nin Carroll & Wish, 1974). Note that the recovered classes have reasonably natural\\ninterpretations in terms of the basic features of phonological theory, and a very different overall structure from the classes recovered in the previous example. Quite\\nsignificantly, the classes respect a hierarchical structure almost perfectly, with class\\n3 included in class 5, classes 1 and 5 included in class 8, and so on. Only the absence\\nof /b / in class 7 violates the strict hierarchy.\\nThese data also provide the only convenient oppportunity to compare our algorithm\\nwith the MAPCLUS approach to additive clustering (Arabie & Carroll, 1980). The\\npublished MAPCLUS solution accounts for 88.3% of the variance in this data, using\\neight clusters. Arabie & Carroll (1980) report being \"substantively pe...turbed\" (p.\\n232) that their algorithm does not recover a distinct cluster for the nasals /m n/,\\nwhich have been considered a very salient subset in both traditional phonology (Miller\\n& Nicely, 1955) and other clustering models (Shepard, 1980). Table 3 presents our\\neight-cluster solution, accounting for 90.2% of the variance. While this represents\\nonly a marginal improvement, our solution does contain a cluster for the nasals, as\\nexpected on theoretical grounds.\\n4Variance accounted for = 1- Ej Ei#j(SiJ - 8)2, where\\n\\ns is\\n\\nthe mea.n of the set {Sij}.\\n\\n\\x0cLearning the Structure of Similarity\\n\\n3.4\\n\\n9\\n\\nConclusion\\n\\nThese examples show that ADCLUS can discover meaningful representations of stimuli with arbitrarily overlapping class structures (arithmetic properties), as well as dimensional structure (numerical magnitude) or hierarchical structure (phoneme families) when appropriate. We have argued that modeling similarity should be a natural\\napplication of learning generative models with multiple hidden causes, and in that\\nspirit, presented a new probabilistic formulation of the ADCLUS model and an algorithm based on EM that promises better results than previous approaches. We\\nare currently pursuing several extensions: enriching the generative model, e.g. by\\nincorporating significant prior structure, and improving the fitting process, e.g. by\\ndeveloping efficient and accurate mean field approximations . More generally, we hope\\nthis work illustrates how sophisticated techniques of computational learning can be\\nbrought to bear on foundational problems of structure discovery in cognitive science.\\nAcknowledgements\\nI thank P. Dayan, W. Richards, S. Gilbert, Y. Weiss, A. Hershowitz, and M. Bernstein\\nfor many helpful discussions, and Roger Shepard for generously supplying inspiration and\\nunpublished data. The author is a Howard Hughes Medical Institute Predoctoral Fellow.\\n\\nReferences\\nArabie, P. & Carroll, J. D. (1980). MAPCLUS: A mathematical programming approach to\\nfitting the ADCLUS model. Psychometrika 45, 211-235.\\nCarroll, J. D. & Wish, M. (1974) Multidimensional perceptual models and measurement\\nmethods. In Handbook of Perception, Vol. 2. New York: Academic Press, 391-447.\\nDempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood estimation from\\nincomplete data via the EM Algorithm (with discussion). J. Roy. Stat. Soc. B39, 1-38.\\nGhahramani, Z. (1995). Factorial learning and the EM algorithm. In G. Tesauro, D. S.\\nTouretzky, & T . K. Leen (eds.), Advances in Neural Information Processing Systems 7.\\nCambridge, MA: MIT Press, 617-624.\\nHinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995) The ((wake-sleep\" algorithm for\\nunsupervised neural networks. Science 268, 1158-1161.\\nMiller, G. A. & Nicely, P. E. (1955). An analysis of perceptual confusions among some\\nEnglish consonants. J. Ac. Soc. Am. 27, 338-352.\\nNeal, R . M. (1992). Connectionist learning of belief networks. Arti/. Intell. 56, 71-113.\\nNeal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods.\\nTechnical Report CRG-TR-93-1, Dept. of Computer Science, U. of Toronto.\\nRose, K., Gurewitz, F., & Fox, G. (1990). Statistical mechanics and phase transitions in\\nclustering. Physical Review Letters 65, 945-948.\\nSaund, E. (1995). A multiple cause mixture model for unsupervised learning. Neural Computation 7, 51-71.\\nShepard, R. N. & Arabie, P. (1979). Additive clustering: Representation of similarities as\\ncombinations of discrete overlapping properties. Psychological Review 86, 87-123.\\nShepard, R. N., Kilpatric, D. W., & Cunningham, J. P., (1975). The internal representation\\nof numbers. Cognitive Psychology 7, 82-138.\\nShepard, R. N. (1980) . Multidimensional scaling, tree-fitting, and clustering. Science 210,\\n390-398.\\nTversky, A. (1977). Features of similarity. Psychological Review 84, 327-352.\\n\\n\\x0c',\n",
       "     'pdf_name': '1052-learning-the-structure-of-similarity.pdf',\n",
       "     'title': 'Learning the Structure of Similarity',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1054',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1054',\n",
       "     'paper_text': 'Implementation Issues in the Fourier\\nTransform Algorithm\\n\\nYishay Mansour\" Sigal Sahar t\\nComputer Science Dept.\\nTel-Aviv University\\nTel-Aviv, ISRAEL\\n\\nAbstract\\nThe Fourier transform of boolean functions has come to play an\\nimportant role in proving many important learnability results. We\\naim to demonstrate that the Fourier transform techniques are also\\na useful and practical algorithm in addition to being a powerful\\ntheoretical tool. We describe the more prominent changes we have\\nintroduced to the algorithm, ones that were crucial and without\\nwhich the performance of the algorithm would severely deteriorate. One of the benefits we present is the confidence level for each\\nprediction which measures the likelihood the prediction is correct.\\n\\n1\\n\\nINTRODUCTION\\n\\nOver the last few years the Fourier Transform (FT) representation of boolean functions has been an instrumental tool in the computational learning theory community. It has been used mainly to demonstrate the learnability of various classes of\\nfunctions with respect to the uniform distribution . The first connection between the\\nFourier representation and learnability of boolean functions was established in [6]\\nwhere the class ACo was learned (using its FT representation) in O(nPoly-log(n))\\ntime. The work of [5] developed a very powerful algorithmic procedure: given a\\nfunction and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was\\nused to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial size\\nDNF. The FT technique applies naturally to the uniform distribution, though some\\nof the learnability results were extended to product distribution [1, 3] .\\n.. e-mail: manSQur@cs.tau.ac.il\\nt e-mail: gales@cs.tau .ac.il\\n\\n\\x0cImplementation Issues in the Fourier Transform Algorithm\\n\\n261\\n\\nA great advantage of the FT algorithm is that it does not make any assumptions\\non the function it is learning. We can apply it to any function and hope to obtain\\n\"large\" Fourier coefficients. The prediction function simply computes the sum of\\nthe coefficients with the corresponding basis functions and compares the sum to\\nsome threshold. The procedure is also immune to some noise and will be able to\\noperate even if a fraction of the examples are maliciously misclassified. Its drawback\\nis that it requires to query the target function on randomly selected inputs.\\nWe aim to demonstrate that the FT technique is not only a powerful theoretical\\ntool, but also a practical one. In the process of implementing the Fourier algorithm\\nwe enhanced it in order to improve the accuracy of the hypothesis we generate while\\nmaintaining a desirable run time. We have added such feartures as the detection\\nof inaccurate approximations \"on the fly\" and immediate correction of the errors\\nincurred at a minimal cost. The methods we devised to choose the \"right\" parameters proved to be essential in order to achieve our goals. Furthermore, when making\\npredictions, it is extremely beneficial to have the prediction algorithm supply an\\nindicator that provides the confidence level we have in the prediction we made. Our\\nalgorithm provides us naturally with such an indicator as detailed in Section 4.1.\\nThe paper is organized as follows: section 2 briefly defines the FT and describes\\nthe algorithm. In Section 3 we describe the experiments and their outcome and in\\nSection 4 the enhancements made. We end with our conclusions in Section 5.\\n\\n2\\n\\nFOURIER TRANSFORM (FT) THEORY\\n\\nIn this section we briefly introduce the FT theory and algorithm. its connection to\\nlearning and the algorithm that finds the large coefficients. A comprehensive survey\\nof the theoretical results and proofs can be found in [7].\\nWe consider boolean functions of n variables: f : {O, l}n - t {-I, I}. We define the\\ninner product: < g, f >= 2- n L::XE{O,l}R f(x)g(x) = E[g . f], where E is the expected value with respect to the uniform distribution . The basis is defined as follows:\\nfor each z E {O,l}n, we define the basis function :\\\\:z(Xl,???,X n ) = (_1)L::~=lx;z ?.\\nAny function of n boolean inputs can be uniquely expressed as a linear combination\\nof the basis functions . For a function f, the zth Fourier coefficient of f is denoted\\nby j(z) , i.e. , f(x) = L::zE{O,l}R j(z)XAx) . The Fourier coefficients are computed\\nby j(z) =< f, Xz > and we call z the coefficient-name of j(z). We define at-sparse\\nfunction to be a function that has at most t non-zero Fourier coefficients.\\n2.1\\n\\nPREDICTION\\n\\nOur aim is to approximate the target function f by a t-sparse function h. In many\\ncases h will simply include the \"large\" coefficients of f. That is, if A = {Zl\\' ... , zm}\\nis the set of z\\'s for which j(Zi) is \"large\", we set hex) = L::z;EA aiXz;(x), where\\nat is our approximation of j(Zi). The hypothesis we generate using this process,\\nhex), does not have a boolean output. In order to obtain a boolean prediction\\nwe use Sign(h(x)), i.e., output +1 if hex) 2 0 and -1 if hex) < o. We want to\\nbound the error we get from approximating f by h using the expected error squared,\\nE[(J - h )2]. It can be shown that bounding it bounds the boolean prediction error\\nprobability, i.e., Pr[f(x) f. sign(h(x))] ~ E[(J - h)2] . For a given t, the t-sparse\\n\\n\\x0cY. MANSOUR, S. SAHAR\\n\\n262\\n\\nhypothesis h that minimizes E[(J - h)2] simply includes the t largest coefficients of\\nf. Note that the more coefficients we include in our approximation and the better\\nwe approximate their values, the smaller E[(J - h)2] is going to be. This provides\\nus with the motivation to find the \"large\" coefficients.\\n2.2\\n\\nFINDING THE LARGE COEFFICIENTS\\n\\nThe algorithm that finds the \"large\" coefficients receives as inputs a function 1 (a\\nblack-box it can query) and an interest threshold parameter (J > 0. It outputs a list\\nof coefficient-names that (1) includes all the coefficients-names whose corresponding coefficients are \"large\", i.e., at least (J , and (2) does not include \"too many\"\\ncoefficient-names. The algorithm runs in polynomial time in both 1/() and n .\\nSUBROUTINE search( a)\\nIF TEST[J, a, II] THEN IF\\n\\nlal\\n\\n=n\\n\\nTHEN OUTPUT a\\nELSE search(aO); search(al);\\n\\nFigure 1: Subroutine search\\nThe basic idea of the algorithm is to perform a search in the space of the coefficientnames of I. Throughout the search algorithm (see Figure (1)) we maintain a prefix\\nof a coefficient-name and try to estimate whether any of its extensions can be\\na coefficient-name whose value is \"large\". The algorithm commences by calling\\nsearch(A) where A is the empty string. On each invocation it computes the predicate TEST[/, a, (J]. If the predicate is true, it recursively calls search(aO) and\\nsearch(al). Note that if TEST is very permissive we may reach all the coefficients, in which case our running time will not be polynomial; its implementation\\nis therefore of utmost interest. Formally, T EST[J, a, (J] computes whether\\n\\nE xe {O,l}n-\"E;e{O,lP.[J(YX)Xa(Y)] 2: (J2,\\n\\nwhere k = Iiali .\\n\\n(1)\\n\\nDefine la(x) = L:,ae{O,l}n-\" j(aj3)x.,a(x). It can be shown that the expected value\\nin (1) is exactly the sum of the squares of the coefficients whose prefix is a , i.e.,\\nE xe {o,l}n-\"E;e{o,l}d/(yx)x.a(Y)] = Ex[/~(x)] = L:,ae{o,l}n-\" p(aj3), implying\\nthat if there exists a coefficient Ii( a,8)1 2: (), then E[/;] 2: (J2 . This condition\\nguarantees the correctness of our algorithm, namely that we reach all the \"large\"\\ncoefficients. We would like also to bound the number of recursive calls that search\\nperforms. We can show that for at most 1/(J2 of the prefixes of size k, TEST[!, a , (J]\\nis true. This bounds the number of recursive calls in our procedure by O(n/(J2).\\nIn TEST we would like to compute the expected value, but in order to do so\\nefficiently we settle for an approximation of its value. This can be done as follows:\\n(1) choose ml random Xi E {a, l}n-k, (2) choose m2 random Yi,j E {a, l}k , (3)\\nquery 1 on Yi,jXi (which is why we need the query model-to query f on many\\npoints with the same prefix Xi) and receive I(Yi,j xd, and (4) compute the estimate\\nas, Ba =\\n\\n3\\n\\n\\';1 L:~\\\\ (~~ L:~l I(Yi,iXdXa(Yi,j)f\\n\\n. Again , for more details see [7].\\n\\nEXPERIMENTS\\n\\nWe implemented the FT algorithm (Section 2.2) and went forth to run a series of\\nexperiments. The parameters of each experiment include the target function , (J , ml\\n\\n\\x0cImplementation Issues in the Fourier Transform Algorithm\\n\\n263\\n\\nand m2. We briefly introduce the parameters here and defer the detailed discussion.\\nThe parameter () determines the threshold between \"small\" and \"large\" coefficients,\\nthus controlling the number of coefficients we will output. The parameters wI and\\nw2 determine how accurately we approximate the TEST predicate. Failure to approximate it accurately may yield faulty, even random, results (e.g., for a ludicrous\\nchoice of m1 = 1 and m2 = 1) that may cause the algorithm to fail (as detailed in\\nSection 4.3). An intelligent choice of m1 and m2 is therefore indispensable. This\\nissue is discussed in greater detail in Sections 4.3 and 4.4.\\n\\nFigure 2:\\n\\nTypical frequency plots and typical errors . Errors occur in two cases: (1) the algorithm\\npredicts a +1 response when the actual response is -1 (the lightly shaded area), and (2) the algorithm\\npredicts a -1 response , while the true response is +1 (the darker shaded area) .\\n\\nFigures (3)-(5) present representative results of our experiments in the form of\\ngraphs that evaluate the output hypothesis of the algorithm on randomly chosen\\ntest points. The target function, I, returns a boolean response, ?1, while the FT\\nhypothesis returns a real response. We therefore present, for each experiment, a\\ngraph constituting of two curves: the frequency of the values of the hypothesis,\\nh( x), when I( x) = +1, and the second curve for I( x) = -1. If the two curves\\nintersect, their intersection represents the inherent error the algorithm makes.\\n\\nFigure 3: Decision trees of depth 5 and 3 with 41 variables . The 5-deep (3-deep) decision tree\\nreturns -1 about 50% (62.5%) of the time . The results shown above are for values (J\\n0.03, ml\\n100\\nand m2 = 5600 ?(J = 0.06, ml = 100 and m2 = 1300). Both graphs are disjoint, signifying 0% error.\\n\\n=\\n\\n4\\n4.1\\n\\n=\\n\\nRESULTS AND ALGORITHM ENHANCEMENTS\\nCONFIDENCE LEVELS\\n\\nOne of our most consistent and interesting empirical findings was the distribution\\nof the error versus the value of the algorithm\\'s hypothesis: its shape is always that\\nof a bell shaped curve. Knowing the error distribution permits us to determine with\\na high (often 100%) confidence level the result for most of the instances, yielding\\nthe much sought after confidence level indicator. Though this simple logic thus far\\nhas not been supported by any theoretical result, our experimental results provide\\noverwhelming evidence that this is indeed the case.\\nLet us demonstrate the strength of this technique: consider the results of the 16-term\\nDNF portrayed in Figure (4) . If the algorithm\\'s hypothesis outputs 0.3 (translated\\n\\n\\x0c264\\n\\nY. MANSOUR, S. SAHAR\\n\\nFigure 4: 16 terlD DNF. This (randomly generated) DNF of 40 variables returns -1 about 61 % of\\nthe time. The results shown above are for the values of 9\\n0 .02 , m2\\n12500 and ml\\n100. The\\nhypothesis uses 186 non-zero coefficients . A total of 9 .628% error was detected.\\n\\n=\\n\\n=\\n\\n=\\n\\ninto 1 in boolean terms by the Sign function), we know with an 83% confidence\\nlevel that the prediction is correct. If the algorithm outputs -0.9 as its prediction,\\nwe can virtually guarantee that the response is correct. Thus, although the total\\nerror level is over 9% we can supply a confidence level for each prediction. This is\\nan indispensable tool for practical usage of the hypothesis .\\n\\n4.2\\n\\nDETERMINING THE THRESHOLD\\n\\nOnce the list of large coefficients is built and we compute the hypothesis h( x), we\\nstill need to determine the threshold, a, to which we compare hex) (i.e., predict +1\\niff hex) > a). In the theoretical work it is assumed that a = 0, since a priori one\\ncannot make a better guess . We observed that fixing a\\'s value according to our\\nhypothesis, improves the hypothesis. a is chosen to minimize the error with respect\\nto a number of random examples.\\n\\nFigure 5:\\n\\n8 terlD DNF . This (randomly generated) DNF of 40 variables returns -1 about 43% of the\\ntime. The results shown above are for the values of 9\\n0 .03, m2\\n5600 and ml\\n100. The hypothesis\\nconsists of 112 non-zero coefficients.\\n\\n=\\n\\n=\\n\\n=\\n\\nFor example, when trying to learn an 8-term DNF with the zero threshold we will\\nreceive a total of 1.22% overall error as depicted in Figure (5). However, if we\\nchoose the threshold to be 0.32, we will get a diminished error of 0.068%.\\n\\n4.3\\n\\nERROR DETECTION ON THE FLY - RETRY\\n\\nDuring our experimentations we have noticed that at times the estimate Ba for\\nE[J~] may be inaccurate. A faulty approximation may result in the abortion of the\\ntraversal of \"interesting\" subtreees, thus decreasing the hypothesis\\' accuracy, or in\\ntraversal of \"uninteresting\" subtrees, thereby needlessly increasing the algorithm\\'s\\nruntime. Since the properties of the FT guarantee that E[J~] = E[f~o] + E[J~d,\\nwe expect Ba :::::: Bao + Bal . Whenever this is not true, we conclude that at least\\none of our approximations is somewhat lacking. We can remedy the situation by\\n\\n\\x0c265\\n\\nImplementation Issues in the Fourier Transform Algorithm\\n\\nrunning the search procedure again on the children, i.e., retry node a. This solution increases the probability of finding all the \"large\" coefficients. A brute force\\nimplementation may cost us an inordinate amount of time since we may retraverse\\nsubtrees that we have previously visited. However, since any discrepancies between\\nthe parent and its children are discovered-and corrected-as soon as they appear,\\nwe can circumvent any retraversal. Thus, we correct the errors without any superfluous additions to the run time.\\n\\n-J:\\n,-\\n\\nFigure 6:\\nand\\n\\n(J\\n\\ni\\\\\"\\no\\n\" .......\\n\\nMajority function of 41 variables. The result portrayed are for values m1 = 100 , m2 = 800\\n\\n=0 .08 . Note the majority-function characteristic distribution of the results 1 .\\n\\nWe demonstrate the usefulness of this approach with an example of learning the\\nmajority function of 41 boolean variables . Without the retry mechanism, 8 (of a\\ntotal of 42) large coefficients were missed, giving rise to 13.724% error represented by\\nthe shaded area in Figure (6). With the retries all the correct coefficients were found,\\nyielding perfect (flawless) results represented in the dotted curve in Figure (6).\\n4.4\\n\\nDETERMINING THE PARAMETERS\\n\\nOne of our aims was to determine the values of the different parameters, m1, m2 and\\n(}. Recall that in our algorithm we calculate B a , the approximation of Ex[f~(x)]\\nwhere m1 is the number of times we sample x in order to make this approximation.\\nWe sample Y randomly m2 times to approximate fa(Xi) = Ey[f(YXih:a(Y)), for each\\nXi ? This approximation of fa(Xi) has a standard deviation of approximately\\nAssume that the true value is 13i, i.e. f3i = fa(Xi), then we expect the contribution\\nof the ith element to Ba to be (13i ? )n;? = 131 ?\\n+ rr!~. The algorithm tests\\nBa =\\nL 131 ? (}2, therefore, to ensure a low error, based on the above argument,\\nwe choose m2 = (J52 ?\\n\\nA.\\n\\nJ&;\\n\\nrr!1\\n\\nChoosing the right value for m2 is of great importance. We have noticed on more\\nthan one occasion that increasing the value of m2 actually decreases the overall run\\ntime. This is not obvious at first : seemingly, any increase in the number of times we\\nloop in the algorithm only increases the run time. However, a more accurate value\\nfor m2 means a more accurate approximation of the TEST predicate, and therefore\\nless chance of redundant recursive calls (the run time is linear in the number of\\nrecursive calls) . We can see this exemplified in Figure (7) where the number of\\nrecursive calls increase drastically as m2 decreases. In order to present Figure (7) ,\\n1The \"peaked\" distribution of the results is not coincidental. The FT of the majority function has 42 large\\nequal coefficients, labeled cmaj\\' one for each singleton (a vector of the form 0 .. 010 .. 0) and one for parity (the\\nall-ones vector). The zeros of an input vector with z zeros we will contribute ?1(2z - 41). cmajl to the result\\nand the parity will contribute ?cma ) (depending on whether z is odd or even), so that the total contribution is\\nan even factor of c ma )\\' Since c ma ) =\\naround the peaks is due to the\\n\\nf~ct\\n\\n(~g);tcr\\n\\n- 0 .12, we have peaks around factors of 0.24 . The distribution\\n\\nwe only approximate each coefficient and get a value close to c ma )\\'\\n\\n\\x0cY. MANSOUR, S. SAHAR\\n\\n266\\n\\nwe learned the same 3 term DNF always using e = 0.05 and mr\\nThe trials differ in the specific values chosen in each trial for m2.\\n\\n* m2\\n\\n100000.\\n\\nFigure 7: Deter01ining 012\\' Note that the number of recursive calls grows dramatically as m2 \\'s\\nvalue decreases. For example, for m2\\n400, the number of recursive calls is 14,433 compared with only\\n1,329 recursive calls for m2\\n500 .\\n\\n=\\n\\n=\\n\\nSPECIAL CASES: When k = 110\\'11 is either very small or very large, the values we\\nchoose for ml and m2 can be self-defeating: when k ,..... n we still loop ml (~ 2n - k )\\ntimes, though often without gaining additional information. The same holds for very\\nsmall values of k, and the corresponding m2 (~ 2k) values. We therefore add the\\nfollowing feature: for small and large values of k we calculate exactly the expected\\nvalue thereby decreasing the run time and increasing accuracy.\\n\\n5\\n\\nCONCLUSIONS\\n\\nIn this work we implemented the FT algorithm and showed it to be a useful practical\\ntool as well as a powerful theoretical technique. We reviewed major enhancements\\nthe algorithm underwent during the process. The algorithm successfully recovers\\nfunctions in a reasonable amount of time. Furthermore, we have shown that the\\nalgorithm naturally derives a confidence parameter. This parameter enables the user\\nin many cases to conclude that the prediction received is accurate with extremely\\nhigh probability, even if the overall error probability is not negligible.\\nAcknowledgements\\nThis research was supported in part by The Israel Science Foundation administered by The Israel\\nAcademy of Science and Humanities and by a grant of the Israeli Ministry of Science and Technology.\\n\\nReferences\\n[1) Mihir Bellare. A technique for upper bounding the spectral norm with applications to learning.\\nAnnual Work&hop on Computational Learning Theory, pages 62-70, July 1992.\\n\\nIn 5 th\\n\\n(2) Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly\\nlearning DNF and characterizing statistical query learning using fourier analysis. In The 26 th Annual AC M\\nSympo&ium on Theory of Computing, pages 253 - 262, 1994 .\\n(3) Merrick L . Furst , Jeffrey C. Jackson, and Sean W. Smith. Improved learning of AC O functions .\\nAnnual Work&hop on Computational Learning Theory, pages 317-325, August 1991.\\n\\nIn 4th\\n\\n(4) J. Jackson . An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. In Annual Sympo&ium on Switching and Automata Theory, pages 42 - 53, 1994.\\n(5) E. Kushilevitz and Y . Mansour. Learning decision trees using the fourier spectrum. SIAM Journal on\\nComputing 22(6): 1331-1348, 1993.\\n(6) N. Linial, Y. Mansour, and N . Nisan. Constant depth circuits, fourier transform and learnability.\\n\\nJACM\\n\\n40(3):607-620, 1993.\\n\\n(7) Y. Mansour . Learning Boolean Functions via the Fourier Transform. Advance& in Neural Computation,\\nedited by V.P. Roychodhury and K-Y. Siu and A. Orlitsky, Kluwer Academic Pub. 1994. Can be accessed\\nvia Up :/ /ftp .math.tau.ac.iJ/pub/mansour/PAPERS/LEARNING/fourier-survey.ps.Z.\\n(8) Yishay Mansour. An o(nlog log n) learning algorihm for DNF under the uniform distribution . J. of Computer\\nand Sy&tem Science, 50(3):543-550, 1995.\\n\\n\\x0c',\n",
       "     'pdf_name': '1054-implementation-issues-in-the-fourier-transform-algorithm.pdf',\n",
       "     'title': 'Implementation Issues in the Fourier Transform Algorithm',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1055',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1055',\n",
       "     'paper_text': 'Adaptive Retina with Center-Surround\\nReceptive Field\\n\\nShih-Chii Lin and Kwabena Boahen\\nComputation and Neural Systems\\n139-74 California Institute of Technology\\nPasadena, CA 91125\\nshih@pcmp.caltech.edu, buster@pcmp.caltech.edu\\n\\nAbstract\\nBoth vertebrate and invertebrate retinas are highly efficient in extracting contrast independent of the background intensity over five\\nor more decades. This efficiency has been rendered possible by\\nthe adaptation of the DC operating point to the background intensity while maintaining high gain transient responses. The centersurround properties of the retina allows the system to extract information at the edges in the image. This silicon retina models the\\nadaptation properties of the receptors and the antagonistic centersurround properties of the laminar cells of the invertebrate retina\\nand the outer-plexiform layer of the vertebrate retina. We also illustrate the spatio-temporal responses of the silicon retina on moving\\nbars. The chip has 59x64 pixels on a 6.9x6.8mm2 die and it is\\nfabricated in 2 J-tm n-well technology.\\n\\n1\\n\\nIntroduction\\n\\nIt has been observed previously that the initial layers of the vertebrate and invertebrate retina systems perform very similar processing functions on the incoming\\ninput signal[1]. The response versus log intensity curves of the receptors in invertebrate and vertebrate retinas look similar. The curves show that the receptors\\nhave a larger gain for changes in illumination than to steady illumination, i.e, the\\nreceptors adapt. This adaptation property allows the receptor to respond over a\\nlarge input range without saturating.\\nAnatomically, the eyes of invertebrates differ greatly from that of vertebrates. Ver-\\n\\n\\x0cAdaptive Retina with Center-Surround Receptive Field\\n\\n679\\n\\ntebrates normally have two simple eyes while insects have compound eyes. Each\\ncompound eye in the fly consists of 3000-4000 ommatidia and each ommatidium\\nconsists of 8 photoreceptors. Six of these receptors (which are also called RI-R6)\\nare in a single spectral class. The other two receptors, R7 and R8 provide channels\\nfor wavelength discrimination and polarization.\\nThe vertebrate eye is divided into the outer-plexiform layer and the inner-plexiform\\nlayer. The outer-plexiform layer consists of the rods and cones, horizontal cells\\nand bipolar cells. Invertebrate receptors depolarise in response to an increase in\\nlight, in contrast to vertebrate receptors, which hyperpolarise to an increase in light\\nintensity. Both vertebrate and invertebrate receptors show light adaptation over at\\nleast five decades of background illumination. This adaptation property allows the\\nretina to maintain a high transient gain to contrast over a wide range of background\\nintensities.\\nThe invertebrate receptors project to the next layer which is called the lamina layer.\\nThis layer consists primarily of monopolar cells which show a similar response versus log intensity curve to that of vertebrate bipolar cells in the outer-plexiform\\nlayer. Both cells respond with graded potentials to changes in illumination. These\\ncells also show a high transient gain to changes in illumination while ignoring the\\nbackground intensity and they possess center-surround receptive fields. In vertebrates, the cones which are excited by the incoming light, activate the horizontal\\ncells which in tum inhibit the cones. The horizontal cells thus mediate the lateral\\ninhibition which produces the center-surround properties. In insects, a possible\\nprocess of this lateral inhibition is done by current flow from the photoreceptors\\nthrough the epithelial glial cells surrounding an ommatidium or the modulation\\nof the local field potential in the lamina to influence the transmembrane potential\\nof the photoreceptor[2]. The center-surround receptive fields allow contrasts to be\\naccentuated since the surround computes a local mean and subtracts that from the\\ncenter signal.\\nMahowald[3] previously described a silicon retina with adaptive photoreceptors and\\nBoahen et al.[4] recently described a compact current-mode analog model of the\\nouter-plexiform layer of the vertebrate retina and analysed the spatio-temporal\\nprocessing properties of this retina[5]. A recent array of photoreceptors from\\nDelbriick[6] uses an adaptive photoreceptor circuit that adapts its operating point\\nto the background intensity so that the pixel shows a high transient gain over 5\\ndecades of background illumination. However this retina does not have spatial\\ncoupling between pixels.\\nThe pixels in the silicon retina described here has a compact circuit that incorporates both spatial and temporal filtering with light adaptation over 5 decades\\nof background intensity. The network exhibits center-surround behavior. Boahen\\net al.[4] in their current-mode diffusor retina, draw an analogy between parts of\\nthe diffusor circuit and the different cells in the outer-plexiform layer. While the\\nsame analogy cannot be drawn from this silicon retina to the invertebrate retina\\nsince the function of the cells are not completely understood, the output responses\\nof the retina circuit are similar to the output responses of the photoreceptor and\\nmonopolar cells in invertebrates.\\nThe circuit details are described in Section 2 and the spatio-temporal processing\\nperformed by the retina on stimulus moving at different speeds is shown in Section\\n\\n\\x0cS.-C. LIU, K. BOAHEN\\n\\n680\\n\\n3.\\n\\n2\\n\\nCircuit\\n-----VI\\n\\nVb\\n\\nVI\\n\\n1\\n\\np1\\n\\n1\\n\\nVI\\n\\nM4\\nVI?I\\n\\nVh\\n\\nVI+I\\n\\nVh\\n\\n.1.\\n\\n.bel\\n\\n.1.\\n\\nVr\\n\\n---------\\n\\nMI\\n\\n(a)\\n\\nim.l\\n\\niia\\n\\nrrr\\n\\niI...I\\n\\nrrr\\n\\nrrr\\n\\n\\'II\\n\\n\\'II\\n\\n(b)\\n\\nFigure 1: (a) One-dimensional version of the retina. (b) Small-signal equivalent of\\ncircuit in (a).\\nA one-dimensional version of the retina is shown in Figure l(a). The retina consists\\nof an adaptive photoreceptor circuit at each pixel coupled together with diffusors,\\ncontrolled by voltages, Vg and Vh. The output of this network can either be obtained\\nat the voltage output, V, or at the current output, 10 but the outputs have different\\nproperties. Phototransduction is obtained by using a reverse-biased photodiode\\nwhich produces current that is proportional to the incident light. The logarithmic\\nproperties are obtained by operating the feedback transistor shown in Figure l(a)\\nin the subthreshold region. The voltage change at the output photoreceptor, V r , is\\nproportional to a small contrast since\\n\\nUT\\n\\nVr\\n\\nUTdI\\n\\nU\\n\\ni\\n\\nT\\n= -d(logl)\\n==K,\\nK,\\n1\\nK, h\\ng\\n\\nCO:rCd \\'\\n\\nwhere UT is the thermal voltage, K, =\\nCoz is the oxide capacitance and\\nCd is the depletion capacitance of a transistor. The circuit works as follows: If\\nthe photocurrent through the photodiode increases, Vr will be pulled low and the\\noutput voltage at V, increases by VI = AVr where A is the amplifier gain of the\\noutput stage. This output change in V, is coupled into Vel through a capacitor\\n\\n\\x0cAdaptive Retina with Center-Surround Receptive Field\\n\\n681\\n\\ndivider ratio, Cl~2C2. The feedback transistor, M4, operates in the subthreshold\\nregion and supplies the current necessary to offset the photocurrent. The increase\\nin Vel (i.e. the gate voltage of M4) causes the current supplied by M3 to increase\\nwhich pulls the node voltage, Vr , back to the voltage level needed by Ml to sink\\nthe bias current from transistor, M2.\\n\\n3.5\\n\\n3.45\\n\\n...-=\\n\\n3.4\\n\\n??c\\n?\\na:?\\n\\n3.35\\n\\n0\\n\\n~\\n\\n-2\\n\\n0\\n\\nQ.\\n\\n3.3\\n-1\\n\\n3.25\\n0\\n3.2\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\nTime (Sec)\\n\\nFigure 2: This figure shows the output response of the receptor to a variation of\\nabout 40% p-p in the intensity of a flickering LED light incident on the chip. The\\nresponse shows that the high sensitivity of the receptor to the LED is maintained\\nover 5 decades of differing background intensities. The numbers on the section of\\nthe curve indicate the log intensity of the mean value. 0 log is the absolute intensity\\nfrom the LED.\\nThe adaptive element, M3, has an I-V curve which looks like a hyperbolic sine.\\nThe small slope of the I-V curve in the middle means that for small changes of\\nvoltages across M3, the element looks like an open-circuit. With large changes of\\nvoltage across M3, the current through M3 becomes exponential and Vel is charged\\nor discharged almost instantaneously.\\nFigure 2 shows the output response of the photoreceptor to a square-wave variation\\nof about 40% p-p in the intensity of a red LED (635 nm). The results show that\\nthe circuit is able to discern the small contrast over five decades of background intensity while the steady-state voltage of the photoreceptor output varies only about\\n15mV. Further details of the photoreceptor circuit and its adaptation properties\\nare described in Delbriick[6].\\n\\n3\\n\\nSpatio-Temporal Response\\n\\nThe spatio-temporal response of the network to different moving stimuli is explored\\nin this section. The circuit shown in Figure l(a) can be transferred to an equivalent\\nnetwork of resistors and capacitors as shown in Figure l(b) to obtain the transfer\\nfunction of the circuit. The capacitors at each node are necessary to model the\\n\\n\\x0cS.-C. LIU, K. BOAHEN\\n\\n682\\n\\n8.5\\n\\ni\\n\\n1lJ;\\n\\n~ 7.5\\n\\n:;\\n:;\\n\\n...\\n\\no\\n\\ni\\n\\nI\\n\\n~\\n\\n...\\nr.\\n\\n0.4\\n\\n(a)\\n\\n0.6\\n\\n0.8\\n\\n1.2\\n\\n1.4\\n\\nTime (Sec)\\n\\n3.8 ~_---:-\":--_--::\\'=_ _\\':\"\\':-_--::\\':-_--::,\\'::-_--.J\\n0.3\\n0.4\\n0.5\\n0 .6\\n0.7\\n0.8\\n\\n(b)\\n\\nTime (Sec)\\n\\nFigure 3: (a) Response of a pixel to a grey strip 2 pixels wide of gray-level \"0.4\"\\non a dark background of level \"0\" moving past the pixel at different speeds. (b)\\nResponse of a pixel to a dark strip of gray-level \"0.6\" on a white background of level\\n\"1\" moving past the pixel at different speeds. The voltage shown on these curves is\\nnot the direct measurement of the voltage at V, but rather V, drives a current-sensing\\ntransistor and this current is then sensed by an offchip current sense-amplifier.\\n\\n\\x0cAdaptive Retina with Center-Surround Receptive Field\\n\\n683\\n\\ntemporal responses of the circuit.\\nThe chip results from the experiments below illustrate the center-surround properties of the network and the difference in time-constants between the surround and\\ncenter.\\n3.1\\n\\nChip Results\\n\\nData from the 2D chip is shown in the next few figures. In these experiments, we\\nare only looking at one pixel of the 2D array. A rotating circular fly-wheel stimulus\\nwith strips of alternating contrasts is mounted above the chip. The stimulus was\\ncreated using Mathematica. Figure 3a shows the spati~temporal impulse response\\nof one pixel measured at V, with a small strip at level \"0.4\" on a dark background of\\nlevel \"0\" moving past the pixels on the row. At slow speeds, the impulse response\\nshows a center-surround behavior where the pixel first receives inhibition from the\\npreceding pixels which are excited by the stimulus. When the stimulus moves by\\nthe pixel of interest, it is excited and then it is inhibited by the subsequent pixels\\nseeing the stimulus.\\n\\nI\\no\\n\\nf\\n\\nI\\ni\\n\\nTim. (Sec)\\n\\nFigure 4: Response of a pixel to a strip of varying contrasts on a dark background\\nmoving past the pixel at a constant speed.\\nAt faster speeds, the initial inhibition in the response grows smaller until at some\\neven faster speed, the initial inhibition is no longer observed. This response comes\\nabout because the inhibition from the surround has a longer-time constant than the\\ncenter. When the stimulus moves past the pixel of interest, the inhibition from the\\npreceding pixels excited by the stimulus does not have time to inhibit the pixel of\\ninterest. Hence the excitation is seen first and then the inhibition comes into place\\nwhen the stimulus passes by. Note that in these figures (Figures 3-4), the curves\\nhave been displaced to show the pixel response at different speeds of the moving\\nstimulus. The voltage shown on these curves is not the direct measurement of the\\nvoltage at V, but rather V, drives a current-sensing transistor and this current is\\nthen sensed by an off-chip current sense-amplifier.\\nFigure 3b shows the\\n\\nspati~temporal\\n\\nimpulse response of one pixel with a similar\\n\\n\\x0cs.-c. LlU, K. BOAHEN\\n\\n684\\n\\nsize strip of level \"0.6\" on a light background of level \"1\" moving past the row of\\npixels. The same inhibition behavior is seen for increasing stimulus speeds. Figure 4 shows the output response at V, for the same stimulus of gray-levels varying\\nfrom \"0.2\" to \"0.8\" on a dark background of level \"0\" moving at one speed. The\\npeak excitation response is plotted against the contrast in Figure 5. A level of \"0.2\"\\ncorresponds to a irradiance of 15mW/m2 while a level of \"0.8\" corresponds to a irradiance of 37.4mW/m2. These measurements are done with a photometer mounted\\nabout 1.5in above a piece of paper with the contrast which is being measured. The\\nirradiance varies exponentially with increasing level.\\n\\n4\\n\\nConclusion\\n\\nIn this paper, we described an adaptive retina with a center-surround receptive\\nfield. The system properties of this retina allows it to model functionally either the\\nresponses of the laminar cells in the invertebrate retina or the outer-plexiform layer\\nof vertebrate retina. We show that the circuit shows adaptation to changes over\\n5 decades of background intensities. The center-surround property of the network\\ncan be seen from its spatio-temporal response to different stimulus speeds. This\\nproperty serves to remove redundancy in space and time of the input signal.\\nAcknowledgements\\n\\nWe thank Carver Mead for his support and encouragement. SC Liu is supported by\\nan NIMH fellowship and K Boahen is supported by a Sloan fellowship. We thank\\nTobias Delbriick for the inspiration and help in testing the design. We also thank\\nRahul Sarpeshkar and Bradley Minch for comments. Fabrication was provided by\\nMOSIS.\\n\\nReferences\\n[1] S. B. Laughlin, \"Coding efficiency and design in retinal processing\", In: Facets\\nof Vision (D. G. Stavenga and R. C. Hardie, eds) pp. 213-234. Springer, Berlin,\\n1989.\\n[2] S. R. Shaw, \"Retinal resistance barriers and electrica1lateral inhibition\", Nature, Lond.255,: 480-483, 1975.\\n[3] M. A. Mahowald, \"Silicon Retina with Adaptive Photoreceptors\" in\\nSPIE/SPSE Symposium on Electronic Science and Technology: From Neurons\\nto Chips. Orlando, FL, April 1991.\\n[4] K. A. Boahen and A. G. Andreou, \"A Contrast Sensitive Silicon Retina with\\nReciprocal Synapses\", In D. S. Touretzky (ed.), Advances in Neural Information Processing Systems 4, 764-772. San Mateo, CA: Morgan Kaufmann, 1992.\\n[5] K. A. Boahen, \"Spatiotemporal sensitivity of the retina: A physical model\",\\nCNS Memo CNS-TR-91-06, California Institute of Technology, Pasadena, CA\\n91125, June 1991.\\n[6] T. Delbriick, \"Analog VLSI Phototransduction by continous-time, adaptive,\\nlogarithmic photoreceptor circuits\", CNS Memo No.30, California Institute of\\nTechnology, Pasadena, CA 91125, 1994.\\n\\n\\x0c',\n",
       "     'pdf_name': '1055-adaptive-retina-with-center-surround-receptive-field.pdf',\n",
       "     'title': 'Adaptive Retina with Center-Surround Receptive Field',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1061',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1061',\n",
       "     'paper_text': 'Stable Dynamic Parameter Adaptation\\n\\nStefan M. Riiger\\nFachbereich Informatik, Technische Universitat Berlin\\nSekr. FR 5-9, Franklinstr. 28/29\\n10587 Berlin, Germany\\nasync~cs. tu-berlin.de\\n\\nAbstract\\nA stability criterion for dynamic parameter adaptation is given. In\\nthe case of the learning rate of backpropagation, a class of stable\\nalgorithms is presented and studied, including a convergence proof.\\n\\n1\\n\\nINTRODUCTION\\n\\nAll but a few learning algorithms employ one or more parameters that control the\\nquality of learning. Backpropagation has its learning rate and momentum parameter; Boltzmann learning uses a simulated annealing schedule; Kohonen learning\\na learning rate and a decay parameter; genetic algorithms probabilities, etc. The\\ninvestigator always has to set the parameters to specific values when trying to solve\\na certain problem. Traditionally, the metaproblem of adjusting the parameters is\\nsolved by relying on a set of well-tested values of other problems or an intensive\\nsearch for good parameter regions by restarting the experiment with different values. In this situation, a great deal of expertise and/or time for experiment design\\nis required (as well as a huge amount of computing time).\\n\\n1.1\\n\\nDYNAMIC PARAMETER ADAPTATION\\n\\nIn order to achieve dynamic parameter adaptation, it is necessary to modify the\\nlearning algorithm under consideration: evaluate the performance of the parameters\\nin use from time to time, compare them with the performance of nearby values, and\\n(if necessary) change the parameter setting on the fly. This requires that there\\nexist a measure of the quality of a parameter setting, called performance, with the\\nfollowing properties: the performance depends continuously on the parameter set\\nunder consideration, and it is possible to evaluate the performance locally, i. e., at\\na certain point within an inner loop of the algorithm (as opposed to once only at\\nthe end of the algorithm). This is what dynamic parameter adaptation is all about.\\n\\n\\x0c226\\n\\nS.M.RUOER\\n\\nDynamic parameter adaptation has several virtues. It is automatic; and there is no\\nneed for an extra schedule to find what parameters suit the problem best. When\\nthe notion of what the good values of a parameter set are changes during learning,\\ndynamic parameter adaptation keeps track of these changes.\\n\\n1.2\\n\\nEXAMPLE: LEARNING RATE OF BACKPROPAGATION\\n\\nBackpropagation is an algorithm that implements gradient descent in an error\\nfunction E: IRn ~ llt Given WO E IRn and a fixed \\'\" > 0, the iteration rule is\\nW H1 = w t - \",V E(wt). The learning rate\", is a local parameter in the sense that\\nat different stages of the algorithm different learning rates would be optimal. This\\nproperty and the following theorem make\", especially interesting.\\nTrade-off theorem for backpropagation. Let E: JR1l ~ IR be the error function of\\na neural net with a regular minimum at w? E IRn , i. e., E is expansible into a\\nTaylor series about w? with vanishing gradient V E( w?) and positive definite Hessian\\nmatrix H(w?) . Let A denote the largest eigenvalue of H(w?). Then, in general,\\nbackpropagation with a fixed learning rate\", > 2/ A cannot converge to w? .\\nProof. Let U be an orthogonal matrix that diagonalizes H(w?), i. e., D :=\\nUT H (w?) U is diagonal. Using the coordinate transformation x = UT (w - w?)\\n\\nand Taylor expansion, E(w) - E(w?) can be approximated by F(x) := x T Dx/2.\\nSince gradient descent does not refer to the coordinate system, the asymptotic behavior of backpropagation for E near w? is the same as for F near O. In the latter\\ncase, backpropagation calculates the weight components x~ = x~(I- Dii\",)t at time\\nstep t. The diagonal elements Dii are the eigenvalues of H(w?); convergence for all\\ngeometric sequences t 1-7 x~ thus requires\", < 2/ A.\\nI\\nThe trade-off theorem states that, given \"\\', a large class of minima cannot be found,\\nnamely, those whose largest eigenvalue of the corresponding Hessian matrix is larger\\nthan 2/\",. Fewer minima might be overlooked by using a smaller \"\\', but then the\\nalgorithm becomes intolerably slow. Dynamic learning-rate adaptation is urgently\\nneeded for backpropagation!\\n\\n2\\n\\nSTABLE DYNAMIC PARAMETER ADAPTATION\\n\\nTransforming the equation for gradient descent, wt+l = w t - \",VE(wt), into a\\ndifferential equation, one arrives at awt fat = -\",V E(wt). Gradient descent with\\nconstant step size\", can then be viewed as Euler\\'s method for solving the differential\\nequation. One serious drawback of Euler\\'s method is that it is unstable: each finite\\nstep leaves the trajectory of a solution without trying to get back to it. Virtually\\nany other differential-equation solver surpasses Euler\\'s method, and there are even\\nsome featuring dynamic parameter adaptation [5].\\nHowever, in the context of function minimization, this notion of stability (\"do not\\ndrift away too far from a trajectory\") would appear to be too strong. Indeed,\\ndifferential-equation solvers put much effort into a good estimation of points that\\nare as close as possible to the trajectory under consideration. What is really needed\\nfor minimization is asymptotic stability: ensuring that the performance of the parameter set does not decrease at the end of learning. This weaker stability criterion\\nallows for greedy steps in the initial phase of learning.\\nThere are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive \",-tensor; individual learning rates for the weights [3, 8]; conjugate gradient as a one-dimensional\\n\",-estimation [4]; or straightforward \",-adaptation [1, 7].\\n\\n\\x0cStable Dynamic Parameter Adaptation\\n\\n227\\n\\nA particularly good example of dynamic parameter adaptation was proposed by\\nSalomon [6, 7]: let ( > 1; at every step t of the backpropagation algorithm test two\\nvalues for 17, a somewhat smaller one, 17d(, and a somewhat larger one, 17t(; use as\\n17HI the value with the better performance, i. e., the smaller error:\\n\\nThe setting of the new parameter (proves to be uncritical (all values work, especially\\nsensible ones being those between 1.2 and 2.1). This method outperforms many\\nother gradient-based algorithms, but it is nonetheless unstable.\\n\\nb)\\nFigure 1: Unstable Parameter Adaptation\\nThe problem arises from a rapidly changing length and direction of the gradient,\\nwhich can result in a huge leap away from a minimum, although the latter may have\\nbeen almost reached. Figure 1a shows the niveau lines of a simple quadratic error\\nfunction E: 1R2 -+ IR along with the weight vectors wo, WI , . .. (bold dots) resulting\\nfrom the above algorithm. This effect was probably the reason why Salomon suggested using the normalized gradient instead of the gradient, thus getting rid of the\\nchanges in the length of the gradient. Although this works much better, Figure 1b\\nshows the instability of this algorithm due to the change in the gradient\\'s direction.\\nThere is enough evidence that these algorithms converge for a purely quadratic\\nerror function [6, 7]. Why bother with stability? One would like to prove that an\\nalgorithm asymptotically finds the minimum, rather than occasionally leaping far\\naway from it and thus leaving the region where the quadratic Hessian term of a\\nglobally nonquadratic error function dominates.\\n\\n3\\n\\nA CLASS OF STABLE ALGORITHMS\\n\\nIn this section, a class of algorithms is derived from the above ones by adding\\nstability. This class provides not only a proof of asymptotic convergence, but also\\na significant improvement in speed.\\nLet E: IRn -+ IR be an error function of a neural net with random weight vector\\nW O E IRn. Let ( > 1, 170 > 0, 0 < c ~ 1, and 0 < a ~ 1 ~ b. At step t of the algorithm, choose a vector gt restricted only by the conditions gtV E(wt)/Igtllv Ew t I ~ c\\nand that it either holds for all t that 1/1gtl E [a, b) or that it holds for all t that\\nIVE(wt)I/lgtl E [a, b), i. e., the vectors g have a minimal positive projection onto\\nthe gradient and either have a uniformly bounded length or are uniformly bounded\\nby the length of the gradient. Note that this is always possible by choosing gt as the\\ngradient or the normalized gradient.\\nLet e: 17 t-t E (wt - 17gt) denote a one-dimensional error function given by E, w t and\\ngt. Repeat (until the gradient vanishes or an upper limit of t or a lower limit Emin\\n\\n\\x0cS.M.ROOER\\n\\n228\\n\\nof E is reached) the iteration\\n\\'T/* ..\\'T/Hl\\n\\n=\\n\\'T/d(\\n\\'T/t(\\n\\nW H1\\n\\n= w t - \\'T/tHg t with\\n\\n\\'T/t(/2\\n1 + e(\\'T/t() - e(O)\\n\\'T/t(gt\\\\1 E(wt)\\n\\nif e(O) < e(\\'T/t()\\n(1)\\n\\nif e(\\'T/d() ::; e(\\'T/t() ::; e(O)\\notherwise.\\n\\nThe first case for \\'T/Hl is a stabilizing term \\'T/*, which definitely decreases the error\\nwhen the error surface is quadratic, i. e., near a minimum. \\'T/* is put into effect\\nwhen the errOr e(T}t() , which would occur in the next step if\\'T/t+l = \\'T/t( was chosen,\\nexceeds the error e(O) produced by the present weight vector w t . By construction,\\n\\'T/* results in a value less than \\'T/t(/2 if e(\\'T/t() > e(O); hence, given ( < 2, the learning\\nrate is decreased as expected, no matter what E looks like. Typically, (if the values\\nfor ( are not extremely high) the other two cases apply, where \\'T/t( and \\'T/d ( compete\\nfor a lower error.\\nNote that, instead of gradient descent, this class of algorithms proposes a \"gt descent,\" and the vectors gt may differ from the gradient. A particular algorithm is\\ngiven by a specification of how to choose gt.\\n\\n4\\n\\nPROOF OF ASYMPTOTIC CONVERGENCE\\n\\nAsymptotic convergence. Let E: w f-t 2:~=1 AiW; /2 with Ai > O. For all ( > 1,\\n1, 0 < a ::; 1 ::; b, \\'T/o > 0, and WO E IRn , every algorithm from Section :1\\nproduces a sequence t f-t wt that converges to the minimum 0 of E with an at least\\nexponential decay of t f-t E(wt).\\n\\no < c ::;\\n\\nProof. This statement follows if a constant q < 1 exists with E(W H1 ) ::; qE(wt) for\\nall t. Then, limt~oo w t = 0, since w f-t ..jE(w) is a norm in IRn.\\nFix a w t , \\'T/t, and a gt according to the premise. Since E is a positive definite\\nquadratic form, e: \\'T/ f-t E( wt - \\'T/g t ) is a one-dimensional quadratic function with\\na minimum at, say, \\'T/*. Note that e(O) = E(wt) and e(\\'T/tH) = E(wt+l). e is\\ncompletely determined by e(O), e\\'(O) = -gt\\\\1 E(wt), \\'T/te and e(\\'T/t(). Omitting the\\nalgebra, it follows that \\'T/* can be identified with the stabilizing term of (1).\\n\\ne(O)\\n.A\\'-~--I\\n-...-...J\\'----+I\\n\\ne\"----r-++--+j\\n\\nqe( 0)\\n(1 - q11)e(0) + q11e(\\'T/*)\\nqee(O)\\n\\n__~<-+--+I 11t+~:11? e(O)\\n\\ne(\\'T/*)\\n\\n1--_ _ _ _---\"\"\\'......-- --A~-_+_--+t\\n\\n+ (1 -\\n\\n11t?~:11? )e(\\'T/*)\\n\\ne(\\'T/tH)\\n\\no\\nFigure 2: Steps in Estimating a Bound q for the Improvement of E.\\n\\n\\x0c229\\n\\nStable Dynamic Parameter Adaptation\\n\\nIf e(17t() > e(O), by (1) 17t+l will be set to 17?; hence, Wt+l has the smallest possible\\nerror e(17?) along the line given by l. Otherwise, the three values 0, 17t!(, and 17t(\\ncannot have the same error e, as e is quadratic; e(17t() or e(17t!() must be less than\\ne(O), and the argument with the better performance is used as 17tH\\' The sequence\\nt I-t E(wt) is strictly decreasing; hence, a q ~ 1 exists. The rest of the proof shows\\nthe existence of a q < 1.\\n\\nAssume there are two constants 0\\n\\n< qe, qT/ < 1 with\\nE\\n~\\n\\nLet 17tH\\n\\n~\\n\\n(2)\\n(3)\\n\\n[qT/,2 - qT/]\\nqee(O).\\n\\n17?; using first the convexity of e, then (2), and (3), one obtains\\ne(17tH -17? 2 ?\\n17.\\n17\\n\\n+ (1- 17t+l -17?)\\n17.\\n\\n17\\n\\n.)\\n\\n< 17t+l -17? e(O) + (1- 17tH -17? )e(17.)\\n<\\n<\\n\\n17?\\n(1 - qT/)e(O) + qf/e(17?)\\n(1- qT/(1 - qe))e(O).\\n\\n17?\\n\\nFigure 2 shows how the estimations work. The symmetric case 0\\nthe same result E(wt+l) ~ qE(wt) with q := 1 - qT/(1 - qe) < 1.\\n\\n< 17tH\\n\\n~\\n\\n17? has\\n\\nLet ,X < := minPi} and ,X> := max{\\'xi}. A straightforward estimation for qe yields\\n,X<\\n\\nqe := 1 - c2 ,X> < 1.\\nNote that 17? depends on w t and gt. A careful analysis of the recursive dependence\\nof 17t+l /17? (w t , gt) on 17t /17?( wt - 1 ,l-l) uncovers an estimation\\n\\n( <)\\n\\n._ min _2_ ~ ca ~\\nqT/ .{(2 + l\\' (2 + 1 b\\'x>\\n\\n5\\n\\n3/2\\n\\n<\\n\\n17o (,X\\n, bmax{1, J2\\'x> E(WO)}}\\n\\n>0\\n\\n.\\n\\n?\\n\\nNON-GRADIENT DIRECTIONS CAN IMPROVE\\nCONVERGENCE\\n\\nIt is well known that the sign-changed gradient of a function is not necessarily the\\nbest direction to look for a minimum. The momentum term of a modified backpropagation version uses old gradient directions; Newton or quasi-Newton methods\\nexplicitly or implicitly exploit second-order derivatives for a change of direction;\\nanother choice of direction is given by conjugate gradient methods [5].\\nThe algorithms from Section 3 allow almost any direction, as long as it is not nearly\\nperpendicular to the gradient. Since they estimate a good step size, these algorithms\\ncan be regarded as a sort of \"trial-and-error\" line search without bothering to find\\nan exact minimum in the given direction, but utilizing any progress made so far.\\nOne could incorporate the Polak-Ribiere rule, ctt H\\ngate directions with dO = \\\\1 E (WO), a = 1, and\\n(3\\n\\n=\\n\\n= \\\\1 E(Wt+l) + a(3ctt, for\\n\\n(\\\\1E(Wt+l) - \\\\1E(wt))\\\\1E(wt+l)\\n(\\\\1 E(Wt))2\\n\\nconju-\\n\\n\\x0cS.M. RUOER\\n\\n230\\n\\nto propose vectors gt := ett /Iettl for an explicit algorithm from Section 3. As in\\nthe conjugate gradient method, one should reset the direction ett after each n (the\\nnumber of weights) updates to the gradient direction. Another reason for resetting\\nthe direction arises when gt does not have the minimal positive projection c onto\\nthe normalized gradient.\\n\\na = 0 sets the descent direction gt to the normalized gradient \"V E(wt)/I\"V E(wt)lj\\nthis algorithm proves to exhibit a behavior very similar to Salomon\\'s algorithm with\\nnormalized gradients. The difference lies in the occurrence of some stabilization\\nsteps from time to time, which, in general, improve the convergence.\\nSince comparisons of Salomon\\'s algorithm to many other methods have been published [7], this paper confines itself to show that significant improvements are\\nbrought about by non-gradient directions, e. g., by Polak-Ribiere directions (a = 1).\\nTable 1: Average Learning Time for Some Problems\\n\\nPROBLEM\\n\\nEmin\\n\\na = 0\\n\\na = 1\\n\\n(a) 3-2-4 regression\\n(b) 3-2-4 approximation\\n(c) Pure square (n = 76)\\n(d) Power 1.8 (n = 76)\\n(e) Power 3.8 (n = 76)\\n(f) 8-3-8 encoder\\n\\n10?\\n10- 4\\n10- 16\\n10- 4\\n10- 16\\n10- 4\\n\\n195? 95%\\n1070 ? 140%\\n464? 17%\\n486? 29%\\n28 ? 10%\\n1380? 60%\\n\\n58 ? 70%\\n189? 115%\\n118? 9%\\n84? 23%\\n37? 14%\\n300? 60%\\n\\nTable 1 shows the average number of epochs of two algorithms for some problems.\\nThe average was taken over many initial random weight vectors and over values of\\n( E [1.7,2.1]j the root mean square error of the averaging process is shown as a\\npercentage. Note that, owing to the two test steps for \",t/( and \"\\'t(, one epoch has\\nan overhead of around 50% compared to a corresponding epoch of backpropagation.\\na f:. 0 helps: it could be chosen by dynamic parameter adaptation.\\nProblems (a) and (b) represent the approximation of a function known only from\\nsome example data. A neural net with 3 input, 2 hidden, and 4 output nodes was\\nused to generate the example dataj artificial noise was added for problem (a). The\\nsame net with random initial weights was then used to learn an approximation.\\nThese problems for feedforward nets are expected to have regular minima.\\nProblem (c) uses a pure square error function E: w rt L:~1 ilwil P /2 with p = 2\\nand n = 76. Note that conjugate gradient needs exactly n epochs to arrive at the\\nminimum [5]. However, the few additional epochs that are needed by the a = 1\\nalgorithm to reach a fairly small error (here 118 as opposed to 76) must be compared\\nto the overhead of conjugate gradient (one line search per epoch).\\nPowers other than 2, as used in (d) or (e), work well as long as, say, p > 1.5. A power\\n< 1 will (if n ~ 2) produce a \"trap\" for the weight vector at a location near a\\ncoordinate axis, where, owing to an infinite gradient component, no gradient-based\\nalgorithm can escape1 . Problems are expected even for p near 1: the algorithms of\\nSection 3 exploit the fact that the gradient vanishes at a minimum, which in turn\\nis numerically questionable for a power like 1.1. Typical minima, however, employ\\npowers 2,4, ... Even better convergence is expected and found for large powers.\\n\\np\\n\\nIDynamic parameter adaptation as in (1) can cope with the square-root singularity\\n(p = 1/2) in one dimension, because the adaptation rule allows a fast enough decay of\\nthe learning rate; the ability to minimize this one-dimensional square-root singularity is\\nsomewhat overemphasized in [7].\\n\\n\\x0cStable Dynamic Parameter Adaptation\\n\\n231\\n\\nThe 8-3-8 encoder (f) was studied, because the error function has global minima\\nat the boundary of the domain (one or more weights with infinite length). These\\nminima, though not covered in Section 4, are quickly found. Indeed, the ability\\nto increase the learning rate geometrically helps these algorithms to approach the\\nboundary in a few steps.\\n\\n6\\n\\nCONCLUSIONS\\n\\nIt has been shown that implementing asymptotic stability does help in the case of the\\nbackpropagation learning rate: the theoretical analysis has been simplified, and the\\nspeed of convergence has been improved. Moreover, the presented framework allows\\ndescent directions to be chosen flexibly, e. g., by the Polak-Ribiere rule. Future work\\nincludes studies of how to apply the stability criterion to other parametric learning\\nproblems.\\n\\nReferences\\n[1] R. Battiti. Accelerated backpropagation learning: Two optimization methods.\\nComplex Systems, 3:331-342, 1989.\\n[2] S. Becker and Y. Ie Cun. Improving the convergence of back-propagation learning with second order methods. In D. Touretzky, G. Hinton, and T. Sejnowski,\\neditors, Proceedings of the 1988 Connectionist Models Summer School, pages\\n29-37. Morgan Kaufmann, San Mateo, 1989.\\n[3] R. Jacobs. Increased rates of convergence through learning rate adaptation.\\nNeural Networks, 1:295-307, 1988.\\n[4] A. Kramer and A. Sangiovanni-Vincentelli. Efficient parallel learning algorithms\\nfor neural networks. In D. Touretzky, editor, Advances in Neural Information\\nProcessing Systems 1, pages 40-48. Morgan Kaufmann, San Mateo, 1989.\\n[5] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical\\nRecipes in C. Cambridge University Press, 1988.\\n[6] R. Salomon. Verbesserung konnektionistischer Lernverfahren, die nach der Gradientenmethode arbeiten. PhD thesis, TU Berlin, October 1991.\\n[7] R. Salomon and J. L. van Hemmen. Accelerating backpropagation through\\ndynamic self-adaptation. Neural Networks, 1996 (in press).\\n[8] F. M. Silva and L. B. Almeida. Speeding up backpropagation. In Proceedings of\\nNSMS - International Symposium on Neural Networks for Sensory and Motor\\nSystems, Amsterdam, 1990. Elsevier.\\n\\n\\x0c',\n",
       "     'pdf_name': '1061-stable-dynamic-parameter-adaption.pdf',\n",
       "     'title': 'Stable Dynamic Parameter Adaption',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1064',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1064',\n",
       "     'paper_text': 'Estimating the Bayes Risk from Sample Data\\n\\nRobert R. Snapp? and Tong Xu\\n\\nComputer Science and Electrical Engineering Department\\nUniversity of Vermont\\nBurlington, VT 05405\\n\\nAbstract\\nA new nearest-neighbor method is described for estimating the Bayes risk\\nof a multiclass pattern claSSification problem from sample data (e.g., a\\nclassified training set). Although it is assumed that the classification problem can be accurately described by sufficiently smooth class-conditional\\ndistributions, neither these distributions, nor the corresponding prior probabilities of the classes are required. Thus this method can be applied to\\npractical problems where the underlying probabilities are not known. This\\nmethod is illustrated using two different pattern recognition problems.\\n\\n1 INTRODUCTION\\nAn important application of artificial neural networks is to obtain accurate solutions to\\npattern classification problems. In this setting, each pattern, represented as an n-dimensional\\nfeature vector, is associated with a discrete pattern class, or state of nature (Duda and Hart,\\n1973). Using available information, (e.g., a statistically representative set of labeled feature\\nvectors {(Xi, fin, where Xi E Rn denotes a feature vector and fi E l:::: {Wl,W2, ... ,we},\\nits correct pattern class), one desires a function (e.g., a neural network claSSifier) that assigns\\nnew feature vectors to pattern classes with the smallest possible misclassification cost.\\nIf the classification problem is stationary, such that the patterns from each class are generated\\n\\naccording to known probability distributions, then it is possible to construct an optimal\\nclasSifier that assigns each pattern to a class with minimal expected risk. Although our\\nmethod can be generalized to problems in which different types of classification errors\\nincur different costs, we shall simplify our discussion by assuming that all errors are equal.\\nIn this case, a Bayes claSSifier assigns each feature vector to a class with maximum posterior\\nprobability. The expected risk of this classifier, or Bayes risk then reduces to the probability\\nof error\\nRB =\\n? E-mail:snapp<Demba.uvm.edu\\n\\nr [1 - SUPPCf!X)]\\nJCx)dx,\\nfEL\\n\\nJs\\n\\n(1)\\n\\n\\x0c233\\n\\nEstimating the Bayes Risk from Sample Data\\n\\n(Duda and Hart, 1973). Here, P( fix) denotes the posterior probability of class f conditioned\\non observing the feature vector x, f(x) denotes the unconditional mixture density of the\\nfeature vector x, and S C Rn denotes the probability-one support of f.\\nKnowing how to estimate the value of the Bayes risk of a given classification problem with\\na specific input representation, may facilitate the design of more accurate classifiers. For\\nexample, since the value of RB depends upon the set of features chosen to represent each\\npattern (e.g., the significance of the input units in a neural network classifier), one might\\ncompare estimates of the Bayes risk for a number of different feature sets, and then select\\nthe representation that yields the smallest value. Unfortunately, it is necessary to know the\\nexplicit probability distributions to evaluate (1). Thus with the possible exception of trivial\\nexamples, the Bayes risk cannot be determined exactly for practical classification problems.\\nLacking the means to evaluate the Bayes risk exactly, motivates the development of statistical\\nestimators of RB. In this paper, we use a recent asymptotic analysis of the finite-sample\\nrisk of the k-nearest-neighbor classifier to obtain a new procedure for estimating the Bayes\\nrisk from sample data. Section 2 describes the k-nearest-neighbor algorithm, and briefly\\ndescribes how estimates of its finite-sample risk have been used to estimate RB. Section 3\\ndescribes how a recent asymptotic analysis of the finite-sample risk can be applied to obtain\\nnew statistical estimators of the Bayes risk. In Section 4 the k-nearest-neighbor algorithm\\nis used to estimate the Bayes risk of two example problems. Section 5 contains some\\nconcluding remarks.\\n\\n2\\n\\nTHE k-NEAREST-NEIGHBOR CLASSIFIER\\n\\nDue to its analytic tractability, and its nearly optimal performance in the large sample limit,\\nthe k-nearest-neighbor classifier has served as a useful framework for estimating the Bayes\\nrisk from classified samples. Recall, that the k-nearest-neighbor algorithm (Fix and Hodges,\\n1951) clasSifies an n-dimensional feature vector x by consulting a reference sample of m\\ncorrectly clasSified feature vectors Xm = {(Xi, f i ) : i = 1, ... m}. First, the algorithm\\nidentifies the k nearest neighbors of x, Le., the k feature vectors within Xm that lie closest\\nto x with respect to a given metric. Then, the classifier assigns x to the most frequent class\\nlabel represented by the k nearest neighbors. (A variety of procedures can be used to resolve\\nties.) In the following, C denotes the number of pattern classes.\\nThe finite-sample risk of this algorithm, R m , equals the probability that the k-nearestneighbor classifier assigns x to an incorrect class, averaged over all input vectors x, and\\nall m-samples, X m . The following properties have been shown to be true under weak\\nassumptions:\\nProperty 1 (Cover and Hart, 1967): For.fixed k,\\nRm\\n\\nwith\\nRB\\n\\n-+\\n\\nRoo(k),\\n\\nas m\\n\\n~ Roo(1) ~ RB (2 -\\n\\n-+ 00\\n\\nC ~ 1R B ).\\n\\n(2)\\n\\nProperty 2 (Devroye, 1981): If k ~ 5, and C = 2, then there exist universal constants\\na =0.3399? .. , and =0.9749 ... such that Roo(k) is bounded by\\n\\n(3\\n\\nRB ~ Roo(k) ~ (1 + ak)RB ,\\n\\nwhere\\n\\nak\\n\\n(3)\\n\\na..(f (\\n= k _ 3.25 1 + ~ .\\n\\nMore generally, ifC = 2, then\\nHB \\'\" Roo(k) \\'\"\\n\\n(1 If)\\n+\\n\\nRB-\\n\\n(3)\\n\\n\\x0cR. R. SNAPP, T. XU\\n\\n234\\n\\nBy the latter property, this algorithm is said to be Bayes consistent in that for any c > 0, it\\nis possible to construct a k-nearest-neighbor classifier such that IRm - RBI < c if m and\\nk are sufficiently large. Bayes consistency is also evident in other nonparametric pattern\\nclassifiers.\\nSeveral methods for estimating RB from sample data have previously been proposed, e.g.,\\n(Devijver, 1985), (Fukunaga, 1985), (Fukunaga and Hummels, 1987), (Garnett and Yau,\\n1977), and (Loizou and Maybank, 1987). Typically, these methods involve constructing\\nsequences of k-nearest neighbor classifiers, with increasing values of k and m. The misclassification rates are estimated using an independent test sample, from which upper and\\nlower bounds to RB are obtained. Because these experiments are necessarily performed\\nwith finite reference samples, these bounds are often imprecise. This is especially true for\\nproblems in which Rm converges to Roo(k) at a slow rate. In order to remedy this deficiency,\\nit is necessary to understand the manner in which the limit in Property 1 is achieved. In the\\nnext section we describe how this information can be used to construct new estimators for\\nthe Bayes risk of sufficiently smooth claSSification problems.\\n\\n3\\n\\nNEW ESTIMATORS OF THE BAYES RISK\\n\\nFor a subset of multiclass classification problems that can be described by probability\\ndensities with uniformly bounded partial derivatives up through order N + 1 (with N 2: 2),\\nthe finite-sample risk of a k-nearest-neighbor classifier that uses a weighted Lp metric can\\nbe represented by the truncated asymptotic expansion\\nN\\n\\nRm\\n\\n= Roo(k) + 2:::Cjm- j/n + 0\\n\\n(m- CN +1)/n) ,\\n\\n(4)\\n\\nj=2\\n\\n(Psaltis, Snapp, and Venkatesh, 1994), and (Snapp and Venkatesh, 1995). In the above,\\nn equals the dimensionality of the feature vectors, and Roo(k), C2, ... ,CN, are the expansion coefficients that depend upon the probability distributions that define the pattern\\nclassification problem.\\nThis asymptotic expansion provides a parametric description of how the finite-sample risk\\nRm converges to its infinite sample limit Roo(k). Using a large sample of classified data,\\none can obtain statistical estimates of the finite-sample risk flm for different values of\\nm. Specifically, let {md denote a sequence of M different sample sizes, and select fixed\\nvalues for k and N. For each value of mi, construct an ensemble of k-nearest-neighbor\\nclassifiers, i.e., for each classifier construct a random reference sample X mi by selecting\\nmi patterns with replacement from the original large sample. Estimate the empirical risk\\nof each classifier in the ensemble with an independently drawn set of \"test\" vectors. Let\\nflmi denote the average empirical risk of the i-th ensemble. Then, using the resulting set\\nof data points {(mi, RmJ}, find the values of the coefficients Roo(k), and C2 through CN,\\nthat minimizes the sum of the squares:\\n\\n8\\n\\nM (\\n\\nflmi - Roo(k) -\\n\\nN)2\\n~\\nCjm;j/n\\n\\n(5)\\n\\nSeveral inequalities can then be used obtain approximations of RB from the estimated value\\nof Roo(k). For example, if k = 1, then Cover and Hart\\'s inequality in Property 1 implies\\nthat\\nRoo(l) < R < R (1).\\n2\\nB_\\n00\\nTo enable an estimate of RB with preciSion c, choose k > 2/c 2 , and estimate Roo(k) by the\\nabove methOd. Then Devroye\\'s inequality (3) implies\\nRoo(k) - c ~ Roo(k)(1 - c) ~ RB ~ Roo(k).\\n\\n\\x0cEstimating the Bayes Risk from Sample Data\\n\\n4\\n\\n235\\n\\nEXPERIMENTAL RESULTS\\n\\nThe above procedure for estimating RB was applied to two pattern recognition problems.\\nFirst consider the synthetic, two-class problem with prior probabilities PI = P2 = 1/2, and\\nnormally distributed, class-conditional densities\\n\\nf( x)=\\nI\\n\\n1\\n\\n(27r)n/2\\n\\ne-H(Xl+(-1)t)2+I:~=2xn\\n\\n\\'\\n\\nfor f = 1 and 2. Pseudorandom labeled feature vectors (x, f) were numerically generated in\\naccordance with the above for dimensions n = 1 and n = 5. Twelve sample sizes between\\n10 and 3000 were examined. For each dimension and sample size the risks Rm of many\\nindependent k-nearest-neighborclassifiers with k = 1,7, and 63 were empirically estimated.\\n(Because the asymptotic expansion (4) does not accurately describe the very small sample\\nbehavior of the k-nearest-neighbor classifier, sample sizes smaller than 2k were not included\\nin the fit.)\\nEstimates of the coefficients in (5) for six different fits appear in the first equation of each cell\\nin the third and fourth columns of Table 1. For reference, the second column contains the\\nvalues of RooCk) that were obtained by numerically evaluating an exact integral expression\\n(Cover and Hart, 1967). Estimates of the Bayes risk appear in the second equation of each\\ncell in the third and fourth columns. Cover and Hart\\'s inequality (2) was used for the\\nexperiments that assumed k = 1, and Devroye\\'s inequality (3) was used if k ~ 7. For thiS\\nproblem, formula (1) evaluates to RB = (l/2)erfc(I/V2) = 0.15865.\\n\\nTable 1: Estimates of the model coefficients and Bayes error for a classification problem\\nwith two normal classes.\\nk\\n\\n1\\n\\n7\\n\\n63\\n\\nRoo(k)\\n\\n0.2248\\n\\n0.1746\\n\\n0.1606\\n\\nn=1\\n\\n(N=2)\\n\\n0.6536\\nm2\\nRB =0.172 ? 0.057\\n\\nR m =0.2287 +\\n\\n4.842\\nm\\nRB =0.152 ?0.023\\n\\nRm =0.1744 + -2-\\n\\n20.23\\nm\\nRB =0.157 ? 0.004\\n\\nRm =0.1606 + - 2 -\\n\\nn=5\\n\\n(N =6)\\n\\n0.0222\\n0.1121 0.2001\\n+\\n5\\n4\\n5\\n2\\nm6/ 5\\nm /\\nm /\\nRB =0.172 ? 0.057\\n\\nRm =0.2287 +\\n\\n0.2218\\n1.005 3.782\\n-+-m4/ 5 m 6/ 5\\nm 2/ 5\\nRB =0.148 ? 0.022\\n\\nR m =0.1700+\\n\\n0.1002 1.426 10.96\\n- - 4-5+ -6 m /\\nm /5\\nm 2/ 5\\nRB =0.156 ? 0.004\\n\\nRm =0.1595 +\\n\\nThe second pattern recognition problem uses natural data; thus the underlying probability\\ndistributions are not known. A pool of 222 classified multispectral pixels were was extracted\\nfrom a seven band satellite image. Each pixel was represented by five spectral components,\\nx = (Xl, .. . ,X5), each in the range 0 ~ X\" ~ 255. (Thus, n = 5.) The class label of\\neach pixel was determined by one of the remaining spectral components, 0 ~ y ~ 255.\\nTwo pattern classes were then defined: Wl = {y < B}, and W2 = {y ~ B}, where B was a\\npredetermined threshOld. (This particular problem was chosen to test the feasibility of this\\nmethod. In future work, we will examine more interesting pixel claSsification problems.)\\n\\n\\x0cR. R. SNAPP, T. XU\\n\\n236\\n\\nTable 2: Coefficients that minimize the squared error fit for different N. Note that\\nand Cs = 0 in (2) ifn ~ 4 (Psaltis, Snapp, and Venkatesh, 1994).\\n\\nN\\n\\nRoo(l)\\n\\n2\\n\\n0.0757133\\n\\n0.126214\\n\\n4\\n\\n0.0757846\\n\\n0.124007\\n\\n0.0132804\\n\\n6\\n\\n0.0766477\\n\\n0.0785847\\n\\n0.689242\\n\\nC3\\n\\n=0\\n\\n-2.68818\\n\\nWith k = 1, a large number of Bernoulli trials (e.g., 2~1000) were performed for each\\nvalue of mi . Each trial began by constructing a reference sample of mi classified pixels\\nchosen at random from the pool. The risk of each reference sample was then estimated by\\nclassifying t pixels with the nearest-neighbor algorithm under a Euclidean metric. Here,\\nthe t pixels, with 2000 ~ t ~ 20000, were chosen independently, with replacement, from\\nthe pool. The risk 11m. was then estimated as the average risk of each reference sample\\nof size mi . (The number of experiments performed for each value of mi, and the values\\noft, were chosen to ensure that the variance of\\nwas sufficiently small, less than 10- 4\\nin this case.) This process was repeated for M = 33 different values of mi in the range\\n100 ~ mi ~ 15000. Results of these experiments are displayed in Table 2 and Figure 1\\nfor three different values of N. Note that the robustness of the fit begins to dissolve, for this\\ndata, at N = 6, either the result of overfitting, or insuffiCient smoothness in the underlying\\nprobability distributions. However, the estimate for Roo(l) appears to be stable. For this\\nclaSSification problem, we thus obtain RB = 0.0568 ? 0.0190.\\n\\nHm.\\n\\n5\\n\\nCONCLUSION\\n\\nThe described method for estimating the Bayes risk is based on a recent asymptotic analysis\\nof the finite-sample risk of the k-nearest-neighbor classifier (Snapp and Venkatesh, 1995).\\nRepresenting the finite-sample risk as a truncated asymptotic series enables an efficient\\nestimation of the infinite-sample risk Roo(k) from the classifier\\'s finite-sample behavior.\\nThe Bayes risk can then be estimated by the Bayes consistency of the k-nearest-neighbor\\nalgorithm. Because such finite-sample analyses are difficult, and consequently rare, this\\nnew method has the potential to evolve into a useful algorithm for estimating the Bayes risk.\\nFurther improvements in efficiency may be obtained by incorporating principles of optimal\\nexperimental deSign, cf., (Elfving, 1952) and (Federov, 1972).\\nIt is important to emphasize, however, that the validity of (4) rests on several rather strong\\nsmoothness assumptions, including a high-degree of differentiability of the class-conditional\\nprobability densities. For problems that do not satisfy these conditions, other finite-sample\\ndescriptions need to be constructed before this method can be applied. Nevertheless, there\\nis much evidence that nature favors smoothness. Thus, these restrictive assumptions may\\nstill be applicable to many important problems.\\nAcknowledgments\\n\\nThe work reported here was supported in part by the National Science Foundation under\\nGrant No. NSF OSR-9350540 and by Rome Laboratory, Air Force Material Command,\\nUSAF, under grant number F30602-94-1-OOlO.\\n\\n\\x0c237\\n\\nEstimating the Bayes Risk from Sample Data\\n\\n-1.8\\n\\n-rl\\nI\\n\\n-2.0\\n\\nt:\\n\\n-0.::\\n0\\n\\n-2.2\\n\\n\\' ol)\\n\\n0\\n\\n-2.4\\n\\n100\\n\\n1000\\nm\\n\\n10000\\n\\nFigure 1: The best fourth-order (N = 4) fit of Eqn. (5) to 33 empirical estimates of Hmo\\nfor a pixel classification problem obtained from a multispectral Landsat image. Using\\nRXI = 0.0758, the fourth-order fit, Rm =0.0758 + 0.124m- 2 / 5 + 0.0133m - 4/5, is plotted\\non a log-log scale to reveal the significance of the j = 2 term.\\n\\nReferences\\nT. M. Cover and P. E. Hart, \"Nearest neighbor pattern classification,\" IEEE Trans. Inform.\\nTheory,vol.IT-13,1967,pp.21-27.\\n\\nP. A. Devijver, \"A multiclass, k - N N approach to Bayes risk estimation,\" Pattern Recognition Letters, vol. 3, 1985, pp. 1-6.\\nL. Devroye, \"On the asymptotic probability of error in nonparametric discrimination,\" Annals Of Statistics, vol. 9, 1981, pp. 1320-1327.\\n\\nR. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis. New York, New York:\\nJohn Wiley & Sons, 1973.\\n\\nG. Elfving, \"Optimum allocation in linear regression theory,\" Ann. Math. Statist., vol. 23,\\n1952,pp.255-262.\\nV. V. Federov, Theory Of Optimal Experiments, New York, New York: Academic Press,\\n1972.\\nE. Fix and J. L. Hodges, \"Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties,\" from Project 21-49-004, Repon Number 4, UASF School of Aviation\\nMedicine, Randolf Field, Texas, 1951, pp. 261-279.\\n\\n\\x0c238\\n\\nR. R. SNAPP, T. XU\\n\\nK. Fukunaga, \"The estimation of the Bayes error by the k-nearest neighbor approach,\" in L.\\nN. Kanal and A. Rosenfeld (ed.), Progress in Pattern Recognition, vol. 2, Elesvier Science\\nPublishers B.V. (North Holland), 1985, pp. 169-187.\\nK. Fukunaga and D. Hummels, \"Bayes error estimation using Parzen and k-NN procedures,\"\\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 9, 1987, pp. 634-643.\\n\\nJ. M. Garnett, III and S. S. Yau, \"Nonparametric estimation of the Bayes error of feature\\nextractors using ordered nearest neighbor sets,\" IEEE Transactions on Computers, vol. 26,\\n1977,pp.46-54.\\nG. Loizou and S. J. Maybank, \"The nearest neighbor and the Bayes error rate,\" IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, vol. 9, 1987, pp. 254-262.\\nD. Psaltis, R. R. Snapp, and S. S. Venkatesh, \"On the finite sample performance of the\\nnearest neighbor classifier,\" IEEE Trans. Inform. Theory, vol. IT-40, 1994, pp. 820--837.\\nR. R. Snapp and S. S. Venkatesh, \"k Nearest Neighbors in Search of a Metric,\" 1995,\\n(submitted).\\n\\n\\x0c',\n",
       "     'pdf_name': '1064-estimating-the-bayes-risk-from-sample-data.pdf',\n",
       "     'title': 'Estimating the Bayes Risk from Sample Data',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1069',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1069',\n",
       "     'paper_text': 'How Perception Guides Production\\nBirdsong Learning\\n\\n?\\n\\nIn\\n\\nChristopher L. Fry\\ncfry@cogsci.ucsd.edu\\nDepartment of Cognitive Science\\nUniversity of California at San Diego\\nLa Jolla, CA 92093-0515\\n\\nAbstract\\nA c.:omputational model of song learning in the song sparrow\\n(M elospiza melodia) learns to categorize the different syllables of\\na song sparrow song and uses this categorization to train itself to\\nreproduce song. The model fills a crucial gap in the computational\\nexplanation of birdsong learning by exploring the organization of\\nperception in songbirds. It shows how competitive learning may\\nlead to the organization of a specific nucleus in the bird brain,\\nreplicates the song production results of a previous model (Doya\\nand Sejnowski, 1995), and demonstrates how perceptual learning\\ncan guide production through reinforcement learning.\\n\\n1\\n\\nINTRODUCTION\\n\\nThe passeriformes or songbirds make up more than half of all bird species and\\nare divided into two groups: the os cines which learn their songs and sub-oscines\\nwhich do not. Oscines raised in isolation sing degraded species typical songs similar\\nto wild song. Deafened oscines sing completely degraded songs (Konishi, 1965) ,\\nwhile deafened sub-oscines develop normal songs (Kroodsma and Konishi, 1991)\\nindicating that auditory feedback is crucial in oscine song learning.\\nInnate structures in the bird brain regulate song learning. For example, song sparrows show innate preferences for their own species\\' songs and song structure (Marler, 1991). Innate preferences are thought to be encoded in an auditory template\\nwhich limits the sounds young birds may copy. According to the auditory template hypothesis birds go through two phases during song learning , a memorization phase and a motor phase. In the memorization phase, which lasts\\nfrom approximately 20 to 50 days after birth in the song sparrow, the bird selects\\nwhich sounds to copy based on an innate template and refines the template based\\n\\n\\x0c111\\n\\nHow Perception Guides Production in Birdsong Learning\\nPOSTERIOR\\n\\nANTERIOR\\n\\ning\\n--+ ......\\nPall..,.\\n~l\\'IodUclcn\\n\\n10 ?? chea ?\\n\\nSJrIn I\\n\\nPall ...,\\n\\nFigure 1: A simplified sketch of a saggital section of the songbird brain . Field L (Field\\nL) receives auditory input and projects to the production pathway: HVc (formerly the\\ncaudal nucleus of the hyperstriatum), RA (robust nucleus of archistriatum), nXIIts (hypoglossal nerve), the syrinx (vocal organ) and the learning pathway: X (area X), DLM\\n(medial nucleus of the dorsolateral thalamus), LMAN (lateral magnocellular nucleus of\\nthe anterior neostriatum), RA (Konishi, 1989; Vicario, 1994). V is the lateral ventricle.\\non the sounds it hears . In the motor phase (from approximately 272 to 334 days\\nafter birth) the template provides feedback during singing. Learning to sing the\\nmemorized, template song is a gradual process of refining the produced song to\\nmatch memory (Marler, 1991).\\nA song is made up of phrases, phrases of syllables and syllables of notes. Syllables,\\nusually separated by periods of silence, are the main units of analysis. Notes typically last from 10-100 msecs and are used to construct syllables (100-200 msecs)\\nwhich are reused to produce trills and other phrases.\\n\\n2\\n\\nNEUROBIOLOGY OF SONG\\n\\nThe two main neural pathways that govern song are the motor and learning pathways seen in figure 1 (Konishi , 1989). Lesions to the motor pathway interrupt\\nsinging throughout life while lesions to the learning pathway disrupt early song\\nlearning. Although these pathways seem to have segregated functions , recordings\\nof neurons during song playback have shown that cells throughout the song system\\nrespond to song (Konishi, 1989).\\nStudies of song perception have shown the best auditory stimulus that will evoke a\\nresponse in the song system is the bird\\'s own song (Margoliash , 1986) . The song\\nspecific neurons in HV c of the white-crowned sparrow often require a sequence of\\ntwo syllables to respond (Margoliash , 1986 ; Margoliash and Fortune , 1992) and are\\nmade up of two main types in HV c . One type is sensitive to temporal combinations\\nof stimuli while the other is sensitive to harmonic characteristics (Margoliash and\\nFortune, 1992) .\\n\\n3\\n\\nCOMPUTATION\\n\\nPrevious computational work on birdsong learning predicted individual neural responses using back-propagation (Margoliash and Bankes , 1993) and modelled motor\\nmappings for song production (Doya and Sejnowski, 1995). The current work de-\\n\\n\\x0cC.L.FRY\\n\\n112\\n1\\n\\n2\\n\\n00\\n\\n8\\n\\nKohonen Neuron\\n\\nInpulLayer\\n\\nSliding\\nWindo\"\",.\\n\\n~\\n\\n..\\n\\n_ _ _ _ __\\n\\n~\\n\\n1000\\n\\nFigure 2: Perceptual network input encoding. The song is converted into frequency bins\\nwhich are presented to the Kohonen layer over four time steps.\\nvelops a model of birdsong syllable perception which extends Doya and Sejnowski\\'s\\n(1995) model of birdsong learning. Birdsong syllable segmentation is accomplished\\nusing an unsupervised system and this system is used to train the network to reproduce its input using reinforcement learning.\\nThe model implements the two phases of the auditory template hypothesis, memorization and motor. In the first phase the template song is segmented into\\nsyllables by an unsupervised Kohonen network (Kohonen, 1984). In the second\\nphase the syllables are reproduced using a reinforcement learning paradigm based\\non Doya and Sejnowski (1995).\\nThe model extends previous work in three ways: 1) a self-organizing network picks\\nout syllables in the song; 2) the self-organizing network provides feedback during\\nsong production; and 3) a more biologically plausible model of the syrinx is used to\\ngenerate song.\\n3.1\\n\\nPerception\\n\\nRecognizing a syllable involves identifying a short sequence of notes. Kohonen\\nnetworks use an unsupervised learning method to categorize an input space based\\non similar neural responses. Thus a Kohonen network is a natural candidate for\\nidentifying the syllables in a song.\\nOne song from the repertoire of a song sparrow was chosen as the training song\\nfor the network . The song was encoded by passing a sliding window across the\\ntraining waveform (sampled at 22 .255 kHz) of the selected song. At each time step,\\na non-overlapping 256 point (~ .011 sec) fast fourier transform (FFT) was used to\\ngenerate a power spectrum (figure 2). The power spectrum was divided into 8 bins.\\nEach bin was mapped to a real number using a gaussian summation procedure with\\nthe peak of the gaussian at the center of each frequency bin. Four time-steps were\\npassed to each Kohonen neuron.\\nThe network\\'s task was to identify similar syllables in the input song. The input\\nsong was broken down into syllables by looking for points where the power at all\\n\\n\\x0c113\\n\\nHow Perception Guides Production in Birdsong Learning\\n\\n10\\n\\n>.\\nu\\n.:\\n\\ng.\"\\n\"...\"\\nkH<\\n\\n5\\n\\nt\\n-/>,\\n\\na\\ns\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n20\\n\\ntime\\n\\n\"ii:..\\nCD\\n\\n.\\n\\nS\\n:I\\nCD\\n\\nz\\n\\nn1\\nn2\\nn3\\nn4\\n\\nn5\\nn6\\nn7\\nn8\\n\\nFigure 3: Categorization of song syllables by a Kohonen network. The power-spectrum of\\nthe training song is at the top. The responses of the Kohonen neurons are at the bottom.\\nFor each time-step the winning neuron is shown with a vertical bar. The shaded areas\\nindicate the neuron that fired the most during the presentation of the syllable.\\n\\nfrequencies dropped below a threshold . A syllable was defined as sound of duration\\ngreater than .011 seconds bounded by two low-power points . The network was not\\ntrained on the noise between syllables. The song was played for the network ten\\ntimes (1050 training vectors), long enough for a stable response pattern to emerge.\\n\\n=\\n\\nThe activation of a neuron was : N etj\\n\\'ExiWij\\' Where: N etj = output of neuron\\nj , Wij = the weight connecting inputi to n eu ronj , Xi = inputi. The Kohonen network was trained by initializing the connection weights to 1/Jnumber of neurons\\n+ small random component (r S; .01) , normalizing the inputs , and updating the\\nweights to the winning neuron by the following rule : W n ew = W old + a(x - W old)\\nwhere : a = training rat e = .20 . If the same neuron won twice in a row the training rate was decreased by 1/2. Only the winning neuron was reinforced resulting\\nin a non-localized feature map .\\n3.1.1\\n\\nPerceptual Results\\n\\nThe Kohonen network was able to assign a unique neuron to each type of syllable\\n(figure 3) . Of the eight neurons in the network. the one that fired the most frequently\\nduring the presentation of a syllable uniquely identified the type of syllable. The\\nfirst four syllables of the input song sound alike, contain similar frequencies , and\\nare coded by the first neuron (N1). The last three syllables sound alike, contain\\nsimilar frequencies , and are coded by the fourth neuron (N4). Syllable five was\\ncoded by neuron six (N6) , syllable six by neuron two (N2) and syllable seven by\\nneuron eight (N8).\\nFigure 4 shows the frequency sensitivity of each neuron (1-8, figure 3) plotted against\\neach time step (1-4). This plot shows the harmonic and temporally sensitive neurons that developed during the learning phase of the Kohonen network. Neuron 2\\nis sensitive to only one frequency at approximately 6-7 kHz , indicated by the solid\\nwhite band across the 6-7 kHz frequency range in figure 4. Neuron 4 is sensitive\\nto mid-range frequencies of short duration . Note that in figure 4 N4 responds\\n\\n\\x0cC. L. FRY\\n\\n114\\n\\no\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n01\\n\\nN6\\n\\nN5\\n\\n23\\nN7\\n\\n4\\nN8\\n\\nTime S t e p\\n\\nFigure 4: The values of the weights mapping frequency bins and time steps to Kohonen\\nneurons. White is maximum , Black is minimum .\\nmaximally to mid-range frequencies only in the first two time steps. It uses this\\ntemporal sensitivity to distinguish between the last three syllables and the fifth syllable (figure 3) by keying off the length of time mid-range frequencies are present.\\nContrast this early response sensitivity with neuron 6, which is sensitive to midrange frequencies of long duration , but responds only after one time step . It uses\\nthis temporal sensitivity to respond to the long sustained frequency of syllable four .\\nConsidered together, neurons 2,4,6 and 8 illustrate the two types of neurons (temporal and harmonic) found in HVc by Margoliash and Fortune (1993). Competitive\\nlearning may underly the formation of these neurons in HV c.\\n3.2\\n\\nProduction\\n\\nAfter competitive learning trains the perceptual part of the network to categorize\\nthe song into syllables , the perceptual network can be used to train the production\\nside of the network to sing.\\nThe first step in modelling song production is to create a model of the avian vocal apparatus , the syrinx. In the syrinx sounds arise when air flows through the\\nsyringeal passage and causes the tympanic membrane to vibrate. The frequency is\\ncontrolled by the tension of the membrane controlled by the syringeal musculature.\\nThe amplitude is dependent on the area of the syringeal orifice which is dependent\\non the tension of the labium. The interactions of this system were modelled by\\nmodulated sine waves. Four parameters governed the fundamental frequency(p) ,\\nfrequency modulation(tm) , amplitude (ex) and frequency of amplitude modulation(I). The range of the parameters was set according to calculations in Greenwalt\\n(1968). The parameters were combined in the following equation (based on Greenwalt, 1968), f(ex , l,p, tm , t) excos(21l\"t 1) cos(21l\"t p + cos(21l\"t tm)) .\\n\\n=\\n\\nUsing this equation song can be generated over time by making assumptions about\\nthe response properties of neurons in RA . Following Doya and Sejnowski (1995) it\\nwas assumed that pools of RA neurons have different temporal response profiles.\\nSyllable like temporal responses can be generated by modifying the weights from\\nthe Kohonell layer (HV c) to the production layer (RA) .\\n\\n\\x0cHow Perception Guides Production in Birdsong Learning\\nTnnni~\\n\\n115\\n\\nSong\\n\\n,\\n\\n. .. .\\n\\n.f . ?~:if>vr.\\n\\n...\\ni\\n\\ni\\n\\nI\\n\\n\\'0\\n\\n15\\n\\n20\\n\\nTim.\\n\\nJ\\'etworl< Song trained with Spectmgmm Target\\n\\nTi\\'llll!\\n\\nJ\\'etworl< Song trained with J\\'euraJ. Activation Target\\n10\\n\\nkHz\\n\\nS\\n\\naa\\n\\nas\\n\\n10\\n\\n15\\n\\n20\\n\\nFigure 5: Training song and two songs produced with different representations of the\\ntraining song.\\nThe production side of the network was trained using the reinforcement learning\\nparadigm described in Doya and Sejnowski (1995). Each syllable was presented in\\nthe order it occurred in the training song to the Kohonen layer, which turned on a\\nsingle neuron. A random vector was added to the weights from the Kohonen layer to\\nthe output layer and a syllable was produced. The produced syllable was compared\\nto the stored representation of the template song which was used to generate an\\nerror signal and an estimate of the gradient. If the evaluation of the produced\\nsyllable was better than a threshold the weights were kept, otherwise they were\\ndiscarded .\\nTwo experiments were done using different representations of the template song.\\nIn the first experiment the template song was the stored power spectrum of each\\nsyllable and the error signal was the cosine of the angle between the power spectrum\\nof the produced syllable and the template syllable. In the second experiment the\\ntemplate song was the stored neural responses to song (recorded during the memorization phase) and the error signal was the Euclidean distance between neural\\nresponses to the produced syllable and the neural responses to the template song.\\n3.2.1\\n\\nProduction Results\\n\\nFigure 5 shows the output of the production network after training with different\\nrepresentations of the training song. The network was able to replicate the major\\nfrequency components of the training song to a high degree of accuracy. The song\\ntrained with the spectrogram target was learned to a 90% average cosine between\\nthe spectrograms of the produced song and the training song on each syllable with\\nthe best syllable learned to 100% accuracy and the worst to 85% after 1000 trials. A\\ncrucial aspect to achieving performance was smoothing the template spectrogram.\\nThe third song shows that the network was able to learn the template song using the\\nneural responses of the perceptual system to generate the reinforcement signal. The\\naverage distance between the initial randomly produced syllables and the training\\n\\n\\x0cC. L.FRY\\n\\n116\\n\\nsong was reduced by 50%.\\n\\n4\\n\\nDISCUSSION\\n\\nThis work fills a crucial gap in the computational explanation of song learning left\\nby prior work . Doya and Sejnowski (1995) showed how song could be produced\\nbut left unanswered the questions of how song is perceived and how the perceptual\\nsystem provides feedback during song production. This study shows a time-delay\\nKohonen network can learn to categorize the syllables of a sample song and this\\nnetwork can train song production with no external teacher. The Kohonen network\\nexplains how neurons sensitive to temporal and harmonic structure could arise in\\nthe songbird brain through competitive learning. Taken as a whole , the model\\npresents a concrete proposal of the computational principles governing the Auditory Template Hypothesis and how a song is memorized and used to train song\\nproduction. Future work will flesh out the effects of innate structure on learning by\\nexamining how the settings of the initial weights on the network affect song learning\\nand predict experimental effects of deafening and isolation .\\nAcknowledgements\\nThanks to S. Vehrencamp for providing the song data, J . Batali, J. Elman, J. Bradbury and T. Sejnowski for helpful comments , and K. Doya for advice on replicating\\nhis model.\\nReferences\\nDoya, K . and Sejnowski, T .J. (1995). A novel reinforcement model of bird song vocalization\\nlearning. In Tesauro, G ., Touretzky, D. S. and Leen , T.K., editors, Advances in Neural\\nInformation Processing Systems 7. MIT Press, Cambridge, MA.\\nGreenwalt, C.H. (1968). Bird Song: Acoustics and Physiology. Smithsonian Institution\\nPress. Wash., D.C.\\nKohonen, T . (1984). Self-organization and Associative Memory, Vol. 8. Springer-Verlag,\\nBerlin.\\nKonishi, M. (1965). The role of auditory feedback in the control of vocalization in the\\nwhite-crowned sparrow. Zeitschrijt fur Tierpsychogie , 22,770-783.\\nKonishi, M. (1989). Birdsong for Neurobiologists. Neuron , 3, 541-549.\\nKroodsma, D.E . and Konishi , M. (1991) . A suboscine bird (eastern phoebe, Sayonoris\\nphoebe) develops normal song without auditory feedback. Animal Behavior, 42, 477-487.\\nMarler , P. (1991). The instinct to learn. In The Epigenesis of Mind: Essays on Biology\\nand Cognition, eds. S. Carey and R. Gelman. Lawrence Erlbaum Associates.\\nMargoliash , D . (1986). Preference for autogenous song by auditory neurons in a song\\nsystem nucleus of the white-crowned sparrow . Journal of Neuroscience, 6,1643-1661.\\nMargoliash, D . and Bankes, S.C. (1993) . Computations in the Ascending Auditory Pathway in Songbirds Related to Song Learning. American Zoologist, 33 , 94-103.\\nMargoliash , D. and Fortune, E . (1992). Temporal and Harmonic Combination-Sensitive\\nNeurons in the Zebra Finch\\'s HVc. Journal of Neuroscien ce, 12, 4309-4326.\\nVicario , D. (1994). Motor Mechanisms Relevant to Auditory-Vocal Interactions in Songbirds. Bra in, Behavior and Evolution,44, 265-278 .\\n\\n\\x0c',\n",
       "     'pdf_name': '1069-how-perception-guides-production-in-birdsong-learning.pdf',\n",
       "     'title': 'How Perception Guides Production in Birdsong Learning',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1073',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1073',\n",
       "     'paper_text': 'Improving Elevator Performance Using\\nReinforcement Learning\\n\\nRobert H. Crites\\nComputer Science Department\\nUniversity of Massachusetts\\nAmherst, MA 01003-4610\\ncritesGcs.umass.edu\\n\\nAndrew G. Barto\\nComputer Science Department\\nUniversity of Massachusetts\\nAmherst, MA 01003-4610\\nbartoGcs.umass.edu\\n\\nAbstract\\nThis paper describes the application of reinforcement learning (RL)\\nto the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most\\nRL research to date. Elevator systems operate in continuous state\\nspaces and in continuous time as discrete event dynamic systems.\\nTheir states are not fully observable and they are nonstationary\\ndue to changing passenger arrival rates. In addition, we use a team\\nof RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which\\nappears noisy to each agent due to the effects of the actions of the\\nother agents, the random nature of the arrivals and the incomplete\\nobservation of the state. In spite of these complications, we show\\nresults that in simulation surpass the best of the heuristic elevator\\ncontrol algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic\\noptimization problem of practical utility.\\n\\n1\\n\\nINTRODUCTION\\n\\nRecent algorithmic and theoretical advances in reinforcement learning (RL) have\\nattracted widespread interest. RL algorithms have appeared that approximate dynamic programming (DP) on an incremental basis. Unlike traditional DP algorithms, these algorithms can perform with or without models of the system, and\\nthey can be used online as well as offline, focusing computation on areas of state\\nspace that are likely to be visited during actual control. On very large problems,\\nthey can provide computationally tractable ways of approximating DP. An example of this is Tesauro\\'s TD-Gammon system (Tesauro, 1992j 1994; 1995), which\\nused RL techniques to learn to play strong masters level backgammon. Even the\\n\\n\\x0cR. H . CR~.A.G. BARTO\\n\\n1018\\n\\nbest human experts make poor teachers for this class of problems since they do not\\nalways know the best actions. Even if they did, the state space is so large that\\nit would be difficult for experts to provide sufficient training data. RL algorithms\\nare naturally suited to this class of problems, since they learn on the basis of their\\nown experience. This paper describes the application of RL to elevator dispatching,\\nanother problem where classical DP is completely intractable. The elevator domain\\nposes a number of difficulties that were not present in backgammon. In spite of\\nthese complications, we show results that surpass the best of the heuristic elevator\\ncontrol algorithms of which we are aware. The following sections describe the elevator dispatching domain, the RL algorithm and neural network architectures that\\nwere used, the results, and some conclusions.\\n\\n2\\n\\nTHE ELEVATOR SYSTEM\\n\\nThe particular elevator system we examine is a simulated 10-story building with\\n4 elevator cars (Lewis, 1991; Bao et al, 1994). Passenger arrivals at each floor are\\nassumed to be Poisson, with arrival rates that vary during the course of the day.\\nOur simulations use a traffic profile (Bao et al, 1994) which dictates arrival rates for\\nevery 5-minute interval during a typical afternoon down-peak rush hour. Table 1\\nshows the mean number of passengers arriving at each floor (2-10) during each\\n5-minute interval who are headed for the lobby. In addition, there is inter-floor\\ntraffic which varies from 0% to 10% of the traffic to the lobby.\\n\\nTable 1: The Down-Peak Traffic Profile\\nThe system dynamics are approximated by the following parameters:\\n? Floor time (the time to move one floor at the maximum speed): 1.45 secs.\\n? Stop time (the time needed to decelerate, open and close the doors, and\\naccelerate again): 7.19 secs.\\n? Turn time (the time needed for a stopped car to change direction): 1 sec.\\n? Load time (the time for one passenger to enter or exit a car): random\\nvariable from a 20th order truncated Erlang distribution with a range from\\n0.6 to 6.0 secs and a mean of 1 sec.\\n? Car capacity: 20 passengers.\\nThe state space is continuous because it includes the elapsed times since any hall\\ncalls were registered. Even if these real values are approximated as binary values,\\nthe size of the state space is still immense. Its components include 218 possible\\ncombinations of the 18 hall call buttons (up and down buttons at each landing\\nexcept the top and bottom), 240 possible combinations of the 40 car buttons, and\\n184 possible combinations of the positions and directions of the cars (rounding off\\nto the nearest floor). Other parts of the state are not fully observable, for example,\\nthe desired destinations of the passengers waiting at each floor. Ignoring everything\\nexcept the configuration of the hall and car call buttons and the approximate position and direction of the cars, we obtain an extremely conservative estimate of the\\nsize of a discrete approximation to the continuous state space:\\n\\n\\x0cImproving Elevator Performance Using Reinforcement Learning\\n\\n1019\\n\\nEach car has a small set of primitive actions. Ifit is stopped at a floor, it must either\\n\"move up\" or \"move down\". If it is in motion between floors, it must either \"stop\\nat the next floor\" or \"continue past the next floor\". Due to passenger expectations,\\nthere are two constraints on these actions: a car cannot pass a floor if a passenger\\nwants to get off there and cannot turn until it has serviced all the car buttons in its\\npresent direction. We have added three additional action constraints in an attempt\\nto build in some primitive prior knowledge: a car cannot stop at a floor unless\\nsomeone wants to get on or off there, it cannot stop to pick up passengers at a floor\\nif another car is already stopped there, and given a choice between moving up and\\ndown, it should prefer to move up (since the down-peak traffic tends to push the\\ncars toward the bottom of the building). Because of this last constraint, the only\\nreal choices left to each car are the stop and continue actions. The actions of the\\nelevator cars are executed asynchronously since they may take different amounts of\\ntime to complete.\\nThe performance objectives of an elevator system can be defined in many ways. One\\npossible objective is to minimize the average wait time, which is the time between\\nthe arrival of a passenger and his entry into a car. Another possible objective is\\nto minimize the average 6y6tem time, which is the sum of the wait time and the\\ntravel time. A third possible objective is to minimize the percentage of passengers\\nthat wait longer than some dissatisfaction threshold (usually 60 seconds). Another\\ncommon objective is to minimize the sum of 6quared wait times. We chose this\\nlatter performance objective since it tends to keep the wait times low while also\\nencouraging fair service.\\n\\n3\\n\\nTHE ALGORITHM AND NETWORK\\nARCHITECTURE\\n\\nElevator systems can be modeled as ducrete event systems, where significant events\\n(such as passenger arrivals) occur at discrete times, but the amount oftime between\\nevents is a real-valued variable. In such systems, the constant discount factor \\'Y\\nused in most discrete-time reinforcement learning algorithms is inadequate. This\\nproblem can be approached using a variable discount factor that depends on the\\namount of time between events (Bradtke & Duff, 1995). In this case, returns are\\ndefined as integrals rather than as infinite sums, as follows:\\nbecomes\\nwhere rt is the immediate cost at discrete time t, r.,. is the instantaneous cost at\\ncontinuous time T (e.g., the sum of the squared wait times of all waiting passengers),\\nand {3 controls the rate of exponential decay.\\nCalculating reinforcements here poses a problem in that it seems to require knowledge of the waiting times of all waiting passengers. There are two ways of dealing\\nwith this problem. The simulator knows how long each passenger has been waiting.\\nIt could use this information to determine what could be called omnucient reinforcements. The other possibility is to use only information that would be available\\nto a real system online. Such online reinforcements assume only that the waiting\\ntime of the first passenger in each queue is known (which is the elapsed button\\ntime). If the Poisson arrival rate A for each queue is estimated as the reciprocal of\\nthe last inter-button time for that queue, the Gamma distribution can be used to\\nestimate the arrival times of subsequent passengers. The time until the nth. subsequent arrival follows the Gamma distribution r(n, f). For each queue, subsequent\\n\\n\\x0cR. H. CRITES, A. G. BARTO\\n\\n1020\\n\\narrivals will generate the following expected penalties during the first b seconds after\\nthe hall button has been pressed:\\n\\nL Jor\\n\\nb\\n\\n00\\n\\nn=l\\n\\n(prob\\n\\nnth\\n\\narrival occurs at time r) . (penalty given arrival at time r) dr\\n\\n0\\n\\nThis integral can be solved by parts to yield expected penalties. We found that\\nusing online reinforcements actually produced somewhat better results than using\\nomniscient reinforcements, presumably because the algorithm was trying to learn\\naverage values anyway.\\nBecause elevator system events occur randomly in continuous time, the branching\\nfactor is effectively infinite, which complicates the use of algorithms that require\\nexplicit lookahead. Therefore, we employed a team of discrete-event Q-Iearning\\nagents, where each agent is responsible for controlling one elevator car. Q(:z:, a)\\nis defined as the expected infinite discounted return obtained by taking action a\\nin state :z: and then following an optimal policy (Watkins, 1989). Because of the\\nvast number of states, the Q-values are stored in feedforward neural networks. The\\nnetworks receive some state information as input, and produce Q-value estimates\\nas output. We have tested two architectures. In the parallel architecture, the agents\\nshare a single network, allowing them to learn from each other\\'s experiences and\\nforcing them to learn identical policies. In the fully decentralized architecture, the\\nagents have their own networks, allowing them to specialize their control policies.\\nIn either case, none of the agents have explicit access to the actions of the other\\nagents. Cooperation has to be learned indirectly via the global reinforcement signal.\\nEach agent faces added stochasticity and nonstationarity because its environment\\ncontains other learning agents. Other work on team Q-Iearning is described in\\n(Markey, 1994).\\nThe algorithm calls for each car to select its actions probabilistic ally using the\\nBoltzmann distribution over its Q-value estimates, where the temperature is lowered gradually during training. After every decision, error backpropagation is used\\nto train the car\\'s estimate of Q(:z:, a) toward the following target output:\\n\\nwhere action a is taken by the car from state :z: at time t x , the next decision by\\nthat car is required from state y at time ty, and TT and (3 are defined as above.\\ne-tJ(tv-t.) acts as a variable discount factor that depends on the amount of time\\nbetween events. The learning rate parameter was set to 0.01 or 0.001 and {3 was set\\nto 0.01 in the experiments described in this paper.\\nAfter considerable experimentation, our best results were obtained using networks\\nfor pure down traffic with 47 input units, 20 hidden sigmoid units, and two linear\\noutput units (one for each action value). The input units are as follows:\\n? 18 units: Two units encode information about each of the nine down hall\\nbuttons. A real-valued unit encodes the elapsed time if the button has\\nbeen pushed and a binary unit is on if the button has not been pushed.\\n\\n\\x0cImproving Elevator Performance Using Reinforcement Learning\\n\\n1021\\n\\n? 16 units: Each of these units represents a possible location and direction\\nfor the car whose decision is required. Exactly one of these units will be on\\nat any given time.\\n? 10 units: These units each represent one of the 10 floors where the other cars\\nmay be located. Each car has a \"footprint\" that depends on its direction\\nand speed. For example, a stopped car causes activation only on the unit\\ncorresponding to its current floor, but a moving car causes activation on\\nseveral units corresponding to the floors it is approachmg, with the highest\\nactivations on the closest floors.\\n? 1 unit: This unit is on if the car whose decision is required is at the highest\\nfloor with a waiting passenger.\\n? 1 unit: This unit is on if the car whose decision is required is at the floor\\nwith the passenger that has been waiting for the longest amount of time.\\n? 1 unit: The bias unit is always on.\\n\\n4\\n\\nRESULTS\\n\\nSince an optimal policy for the elevator dispatching problem is unknown, we measured the performance of our algorithm against other heuristic algorithms, including\\nthe best of which we were aware. The algorithms were: SECTOR, a sector-based\\nalgorithm similar to what is used in many actual elevator systems; DLB, Dynamic\\nLoad Balancing, attempts to equalize the load of all cars; HUFF, Highest Unanswered Floor First, gives priority to the highest floor with people waiting; LQF,\\nLongest Queue First, gives priority to the queue with the person who has been\\nwaiting for the longest amount of time; FIM, Finite Intervisit Minimization, a receding horizon controller that searches the space of admissible car assignments to\\nminimize a load function; ESA, Empty the System Algorithm, a receding horizon\\ncontroller that searches for the fastest way to \"empty the system\" assuming no new\\npassenger arrivals. ESA uses queue length information that would not be available\\nin a real elevator system. ESA/nq is a version of ESA that uses arrival rate information to estimate the queue lengths. For more details, see (Bao et al, 1994). These\\nreceding horizon controllers are very sophisticated, but also very computationally\\nintensive, such that they would be difficult to implement in real time. RLp and\\nRLd denote the RL controllers, parallel and decentralized. The RL controllers were\\neach trained on 60,000 hours of simulated elevator time, which took four days on a\\n100 MIPS workstation. The results are averaged over 30 hours of simulated elevator\\ntime. Table 2 shows the results for the traffic profile with down traffic only.\\nAlgorithm\\nSECTOR\\nDLB\\nBASIC HUFF\\nLQF\\nHUFF\\nFIM\\nESA/nq\\nESA\\nRLp\\nRLd\\n\\nI AvgWait I SquaredWait I SystemTime I Percent>60 secs I\\n21.4\\n19.4\\n19.9\\n19.1\\n16.8\\n16.0\\n15.8\\n15.1\\n14.8\\n14.7\\n\\n674\\n658\\n580\\n534\\n396\\n359\\n358\\n338\\n320\\n313\\n\\n47.7\\n53.2\\n47.2\\n46.6\\n48.6\\n47.9\\n47.7\\n47.1\\n41.8\\n41.7\\n\\n1.12\\n2.74\\n0.76\\n0.89\\n0.16\\n0.11\\n0.12\\n0.25\\n0.09\\n0.07\\n\\nTable 2: Results for Down-Peak Profile with Down Traffic Only\\n\\n\\x0cR.H.C~.A.G. BARTO\\n\\n1022\\n\\nTable 3 shows the results for the down-peak traffic profile with up and down traffic,\\nincluding an average of 2 up passengers per minute at the lobby. The algorithm\\nwas trained on down-only traffic, yet it generalizes well when up traffic is added\\nand upward moving cars are forced to stop for any upward hall calls.\\nAlgorithm\\nSECTOR\\nDLB\\nBASIC HUFF\\nLQF\\nHU ...?F\\nESA\\nFIM\\nRLp\\nRLd\\n\\nI AvgWait I Squared wait I SystemTime I Percent>60 secs I\\n27.3\\n21.7\\n22.0\\n21.9\\n19.6\\n18.0\\n17.9\\n16.9\\n16.9\\n\\n1252\\n826\\n756\\n732\\n608\\n524\\n476\\n476\\n468\\n\\n54.8\\n54.4\\n51.1\\n50.7\\n50.5\\n50.0\\n48.9\\n42.7\\n42.7\\n\\n9.24\\n4.74\\n3.46\\n2.87\\n1.99\\n1.56\\n0.50\\n1.53\\n1.40\\n\\nTable 3: Results for Down-Peak Profile with Up and Down Traffic\\nTable 4 shows the results for the down-peak traffic profile with up and down traffic,\\nincluding an average of 4 up passengers per minute at the lobby. This time there is\\ntwice as much up traffic, and the RL agents generalize extremely well to this new\\nsituation.\\nAlgorithm\\nSECTOR\\nHUFF\\nDLB\\nLQF\\nBASIC HUFF\\nFIM\\nESA\\nRLd\\nRLp\\n\\nI AvgWait I SquaredWait I SystemTime I Percent>60 secs I\\n30.3\\n22.8\\n22.6\\n23.5\\n23.2\\n20.8\\n20.1\\n18.8\\n18.6\\n\\n1643\\n884\\n880\\n877\\n875\\n685\\n667\\n593\\n585\\n\\n59.5\\n55.3\\n55.8\\n53.5\\n54.7\\n53.4\\n52.3\\n45.4\\n45.7\\n\\n13.50\\n5.10\\n5.18\\n4.92\\n4.94\\n3.10\\n3.12\\n2.40\\n2.49\\n\\nTable 4: Results for Down-Peak Profile with Twice as Much Up Traffic\\nOne can see that both the RL systems achieved very good performance, most notably as measured by system time (the sum of the wait and travel time), a measure\\nthat was not directly being minimized. Surprisingly, the decentralized RL system\\nwas able to achieve as good a level of performance as the parallel RL system. Better performance with nonstationary traffic profiles may be obtainable by providing\\nthe agents with information about the current traffic context as part of their input\\nrepresentation. We expect that an additional advantage of RL over heuristic controllers may be in buildings with less homogeneous arrival rates at each floor, where\\nRL can adapt to idiosyncracies in their individual traffic patterns.\\n\\n5\\n\\nCONCLUSIONS\\n\\nThese results demonstrate the utility of RL on a very large scale dynamic optimization problem. By focusing computation onto the states visited during simulated\\ntrajectories, RL avoids the need of conventional DP algorithms to exhaustively\\n\\n\\x0cImproving Elevator Performance Using Reinforcement Learning\\n\\n1023\\n\\nsweep the state set. By storing information in artificial neural networks, it avoids\\nthe need to maintain large lookup tables. To achieve the above results, each RL\\nsystem experienced 60,000 hours of simulated elevator time, which took four days\\nof computer time on a 100 MIPS processor. Although this is a considerable amount\\nof computation, it is negligible compared to what any conventional DP algorithm\\nwould require. The results also suggest that approaches to decentralized control\\nusing RL have considerable promise. Future research on the elevator dispatching\\nproblem will investigate other traffic profiles and further explore the parallel and\\ndecentralized RL architectures.\\nAcknowledgements\\nWe thank John McNulty, Christ os Cassandras, Asif Gandhi, Dave Pepyne, Kevin\\nMarkey, Victor Lesser, Rod Grupen, Rich Sutton, Steve Bradtke, and the ANW\\ngroup for assistance with the simulator and for helpful discussions. This research\\nwas supported by the Air Force Office of Scientific Research under grant F4962093-1-0269.\\nReferences\\nG. Bao, C. G. Cassandras, T. E. Djaferis, A. D. Gandhi, and D. P. Looze. (1994)\\nElevator Di,patcher, for Down Peale Traffic. Technical Report, ECE Department,\\nUniversity of Massachusetts, Amherst, MA.\\nS. J. Bradtke and M. O. Duff. (1995) Reinforcement Learning Methods for\\nContinuous-Time Markov Decision Problems. In: G. Tesauro, D. S. Touretzky\\nand T. K. Leen, eds., Advance, in Neural Information Procelling Sy,tem, 7, MIT\\nPress, Cambridge, MA.\\nJ. Lewis. (1991) A Dynamic Load Balancing Approach to the Control of Multuerver\\nPolling Sy,tem, with Applicationl to Elevator Syltem Dupatching. PhD thesis,\\nUniversity of Massachusetts, Amherst, MA.\\nK. L. Markey. (1994) Efficient Learning of Multiple Degree-of-Freedom Control\\nProblems with Quasi-independent Q-agents. In: M. C. Mozer, P. Smolensky,\\nD. S. Touretzky, J. L. Elman and A. S. Weigend, eds., Proceeding\\' of the 1993\\nConnectionilt Modell Summer SchooL Erlbaum Associates, Hillsdale, NJ.\\nG. Tesauro. (1992) Practical Issues in Temporal Difference Learning. Machine\\nLearning 8:257-277.\\nG. Tesauro. (1994) TO-Gammon, a Self-Teaching Backgammon Program, Achieves\\nMaster-Level Play. Neural Computation 6:215-219 .\\nG. Tesauro. (1995) Temporal Difference Learning and TD-Gammon. Communication, of the ACM 38:58-68.\\nC. J. C. H. Watkins. (1989) Learning from Delayed Reward,. PhD thesis, Cambridge University.\\n\\n\\x0c',\n",
       "     'pdf_name': '1073-improving-elevator-performance-using-reinforcement-learning.pdf',\n",
       "     'title': 'Improving Elevator Performance Using Reinforcement Learning',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1075',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1075',\n",
       "     'paper_text': 'Improving Committee Diagnosis with\\nResampling Techniques\\n\\nBambang Parmanto\\nDepartment of Information Science\\nUniversity of Pittsburgh\\nPittsburgh, PA 15260\\nparmanto@li6.pitt. edu\\n\\nPaul W. Munro\\nDepartment of Information Science\\nUniversity of Pittsburgh\\nPittsburgh, PA 15260\\nmunro@li6.pitt. edu\\n\\nHoward R. Doyle\\nPittsburgh Transplantation Institute\\n3601 Fifth Ave, Pittsburgh, PA 15213\\ndoyle@vesaliw. tu. med. pitt. edu\\n\\nAbstract\\nCentral to the performance improvement of a committee relative to\\nindividual networks is the error correlation between networks in the\\ncommittee. We investigated methods of achieving error independence between the networks by training the networks with different\\nresampling sets from the original training set. The methods were\\ntested on the sinwave artificial task and the real-world problems of\\nhepatoma (liver cancer) and breast cancer diagnoses.\\n\\n1\\n\\nINTRODUCTION\\n\\nThe idea of a neural net committee is to combine several neural net predictors\\nto perform collective decision making, instead of using a single network (Perrone,\\n1993). The potential of a committee in improving classification performance has\\nbeen well documented. Central to this improvement is the extent to which the\\nerrors tend to coincide. Committee errors occur where the misclassification sets of\\nindividual networks overlap. On the one hand, if all errors of committee members\\ncoincide, using a committee does not improve performance. On the other hand, if\\nerrors do not coincide, performance of the committee dramatically increases and\\nasymptotically approaches perfect performance. Therefore, it is beneficial to make\\nthe errors among the networks in the committee less correlated in order to improve\\nthe committee performance.\\n\\n\\x0cImproving Committee Diagnosis with Resampling Techniques\\n\\n883\\n\\nOne way of making the networks less correlated is to train them with different sets\\nof data. Decreasing the error correlation by training members of the committee\\nusing different sets of data is intuitively appealing. Networks trained with different\\ndata sets have a higher probability of generalizing differently and tend to make\\nerrors in different places in the problem space.\\nThe idea is to split the data used in the training into several sets. The sets are\\nnot necessarily mutually exclusive, they may share part of the set (overlap). This\\nidea resembles resampling methods such as cross-validation and bootstrap known\\nin statistics for estimating the error of a predictor from limited sets of available\\ndata. In the committee framework, these techniques are recast to construct different\\ntraining sets from the original training set. David Wolpert (1992) has put forward\\na general framework of training the committee using different partitions of the\\ndata known as stacked generalization. This approach has been adopted to the\\nregression environment and is called stacked regression (Breiman, 1992). Stacked\\nregression uses cross-validation to construct different sets of regression functions.\\nA similar idea of using a bootstrap method to construct different training sets has\\nbeen proposed by Breiman (1994) for classification and regression trees predictors.\\n\\n2\\n2.1\\n\\nTHE ALGORITHMS\\nBOOTSTRAP COMMITTEE (BOOTC)\\n\\nConsider a total of N items are available for training. The approach is to generate\\nK replicates from the original set, each containing the same number of item as the\\noriginal set. The replicates are obtained from the original set by drawing at random\\nwith replacement. See Efron & Tibshirani (1993) for background on bootstrapping.\\nUse each replicate to train each network in the committee.\\nUsing this bootstrap procedure, each replicate is expected to include roughly 36\\n% duplicates (due to replacement during sampling). Only the distinct fraction is\\nused for training and the leftover fraction for early stopping, if necessary (notice\\nslight difference from the standard bootstrapping and from Breiman\\'s bagging).\\nEarly stopping usually requires a fraction of the data to be taken from the original\\ntraining set, which might degrade the performance of the neural network. The\\nadvantage of a BOOTC is that the leftover sample is already available.\\nAlgorithm:\\n1. Generate bootstrap replicates Ll, ... , LK from the original set.\\n\\n2. For each bootstrap replicate, collect unsampled items into leftover sample\\n..\\nl*l , ... , l*K .\\nset s, gIVIng:\\n3. For each Lk, train a network. Use the leftover set l*k as validation stopping\\ncriteria if necessary. Giving K neural net predictors: f(~i Lk)\\n4. Build a committee from the bootstrap networks using a simple averaging\\nprocedure: fcom(~) =\\n~~=l f(~i Lk)\\n\\nic\\n\\nThere is no rule as to how many bootstrap replicates should be used to achieve a\\ngood performance. In error estimation, the number ranges from 20 to 200. It is\\nbeneficial to keep the number of replicates, hence the number of networks, small to\\nreduce training time. Unless the networks are trained on a parallel machine, training\\ntime increases proportionally to the number of networks in the committee. In this\\nexperiment, 20 bootstrap training replicates were constructed for 20 networks in\\n\\n\\x0c884\\n\\nB. PARMANTO, P. W. MUNRO, H. R. DOYLE\\n\\nthe committee. Twenty replicates were chosen since beyond this number there is\\nno significant improvement on the performance.\\n\\n2.2\\n\\nCROSS-VALIDATION COMMITTEE (CVC)\\n\\nThe algorithm is quite similar to the procedure used in prediction error estimation.\\nFirst, generate replicates from the original training set by removing a fraction of\\nthe data. Let D denote the original data, and D- V denote the data with subset\\nv removed. The procedure revolves so that each item is in the removed fraction\\nat least once. Generate replicates D11Jl , ??? Di/Ie and train each network in the\\ncommittee with one replicate.\\nAn important issue in the eve is the degree of data overlap between the replicates.\\nThe degree of overlap depends on the number of replicates and the size of a removed\\nfraction from the original sample. For example, if the committee consists of 5\\nnetworks and 0.5 of the data are removed for each replicate, the minimum fraction\\nof overlap is 0 (calculation: (v x 2) - 1.0) and the maximum is ~ (calculation:\\n1.0 -\\n\\nk)\\'\\n\\nAlgorithm:\\n\\n1. Divide data into v-fractions db . . . , dv\\n2. Leave one fraction die and train network fie with the rest of the data (D-d le ).\\n3. Use die as a validation stopping criteria, if necessary.\\n4. Build a committee from the networks using a simple averaging procedure.\\nThe fraction of data overlap determines the trade-off between the individual network\\nperformance and error correlation between the networks. Lower correlation can be\\nexpected if the networks train with less overlapped data, which means a larger\\nremoved fraction and smaller fraction for training. The smaller the training set\\nsize, the lower the individual network performance that can be expected.\\nWe investigated the effect of data overlap on the error correlations between the\\nnetworks and the committee performance. We also studied the effect of training\\nsize on the individual performance. The goal was to find an optimal combination\\nof data overlap and individual training size.\\n\\n3\\n\\nTHE BASELINE & PERFORMANCE EVALUATION\\n\\nTo evaluate the improvement of the proposed methods on the committee performance, they should be compared with existing methods as the baseline. The common method for constructing a committee is to train an ensemble of networks\\nindependently. The networks in the committee are initialized with different sets\\nof weights. This type of committee has been reported as achieving significant improvement over individual network performances in regression (Hashem, 1993) and\\nclassification tasks (Perrone, 1993; Parmanto et al., 1994).\\nThe baseline, BOOTe, and eve were compared using exactly the same architecture\\nand using the same pair of training-test sets. Performance evaluation was conducted\\nusing 4-fold exhaustive cross-validation where 0.25 fraction of the original data is\\nused for the test set and the remainder of the data is used for the training set. The\\nprocedure was repeated 4 times so that all items were once on the test set. The\\nperformance was calculated by averaging the results of 4 test sets. The simulations\\n\\n\\x0cImproving Committee Diagnosis with Resampling Techniques\\n\\n885\\n\\nwere conducted several times using different initial weights to exclude the possibility\\nthat the improvement was caused by chance.\\n\\n4\\n4.1\\n\\nEXPERIMENTS\\nSYNTHETIC DATA: SINWAVE CLASSIFICATION\\n\\nThe sinwave task is a classification problem with two classes, a negative class represented as 0 and a positive class represented as 1. The data consist of two input\\nvariables, x = (Xli X2). The entire space is divided equally into two classes with\\nthe separation line determined by the curve X2 = sin( 2: Xl). The upper half of the\\nrectangle is the positive class, while the lower half is the negative one (see Fig. 1).\\nGaussian noise along the perfect boundary with variance of 0.1 is introduced to\\nthe clean data and is presented in Fig. 1 (middle). Let z be a vector drawn from\\nthe Gaussian distribution with variance TI, then the classification rule is given by\\nequation:\\n(1)\\nA similar artificial problem is used to analyze the bias-variance trade-offs by Geman\\net al. (1992).\\n\\nFigure 1: Complete and clean data/without noise (top), complete data with noise\\n(middle), and a small fraction used for training (bottom).\\nThe population contains 3030 data items, since a grid of 0.1 is used for both Xl and\\nX2 . In the real world, we usually have no access to the entire population. To mimic\\nthis situation, the training set contained only a small fraction of the population.\\nFig. 1 (bottom) visualizes a training set that contains 200 items with 100 items for\\neach class. The training set is constructed by randomly sampling the population.\\nThe performance of the predictor is measured with respect to the test set. The\\npopulation (3030 items) is used as the test set.\\n\\n4.2\\n\\nHEPATOMA DETECTION\\n\\nHepatoma is a very important clinical problem in patients who are being considered\\nfor liver transplantation for its high probability of recurrence. Early hepatoma\\ndetection may improve the ultimate outlook of the patients since special treatment\\ncan be carried out. Unfortunately, early detection using non-invasive procedures\\n\\n\\x0c886\\n\\nB. PARMANTO, P. W. MUNRO, H. R. DOYLE\\n\\ncan be difficult, especially in the presence of cirrhosis. We have been developing\\nneural network classifiers as a detection system with minimum imaging or invasive\\nstudies (Parmanto et al., 1994).\\nThe task is to detect the presence or absence (binary output) of a hepatoma given\\nvariables taken from an individual patient. Each data item consists of 16 variables,\\n7 of which are continuous variables and the rest are binary variables, primarily\\nblood measurements.\\nFor this experiment, 1172 data items with their associated diagnoses are available.\\nOut of 1172 itmes, 693 items are free from missing values, 309 items contain missing\\nvalues only on the categorical variables, and 170 items contain missing values on\\nboth types of variables. For this experiment, only the fraction without missing\\nvalues and the fraction with missing values on the categorical variables were used,\\ngiving the total item of 1002. Out of the 1002 items, 874 have negative diagnoses\\nand the remaining 128 have positive diagnoses.\\n\\n4.3\\n\\nBREAST CANCER\\n\\nThe task is to diagnose if a breast cytology is benign or malignant based on cytological characteristics. Nine input variables have been established to differentiate\\nbetween the benign and malignant samples which include clump thickness, marginal\\nadhesion, the uniformity of cell size and shape, etc.\\nThe data set was originally obtained from the University of Wisconsin Hospitals\\nand currently stored at the UCI repository for machine learning (Murphy & Aha,\\n1994). The current size of the data set is 699 examples.\\n\\n5\\n\\nTHE RESULTS\\nCommittee Performance\\n\\nIndiv. Performance\\n\\n~ ~.::.:.:-:~~~?:\\n.: : ..::.::---.-.-.........---.. . .\\n\\n?\\n? :!\\n\\n---.... _---\\n\\n,I; N\\n\\n~ .....\\n\\no\\n\\n4\\n\\n6\\n\\n10\\n\\n12\\n\\n14\\n\\n16\\n\\n4\\n\\n8\\n\\nbas.an.\\n\\n0\\n\\n-\\n\\n?\\n\\n::-:::.\\n\\n&10...,,,,\\n\\n10\\n\\n12\\n\\n14\\n\\n/I hidden units\\n\\n/I hidden units\\n\\nCorrelation\\n\\nPercent Improvement\\n\\n16\\n\\n0r------------------.\\n\\n.... -... -\\n\\n-~-------~-------~\\n......-. ....-.........\\n-- ...._-. --_._..._---.\\n\" , --.\\n\\nQ\\n\\no\\n\\no~\\n\\n4\\n\\n-~. . . .\\n\\n& :::: li&>mr\",\\n________________\\n6\\n\\n10\\n\\n12\\n\\nII hidden units\\n\\n14\\n\\n~\\n\\n16\\n\\n8\\n\\n10\\n\\n/I hidden\\n\\n12\\n\\n14\\n\\n16\\n\\n...,its\\n\\nFigure 2: Results on the sinwave classif. task. Performances of individual nets\\nand the committee (top); error correlation and committee improvement (bottom).\\nFigure 2. (top) and Table 1. show that the performance of the committee is always\\nbetter than the average performance of individual networks in all three committees.\\n\\n\\x0cImproving Committee Diagnosis with Resampling Techniques\\n\\nTask\\n\\nMethods\\n\\nSmwave\\n(2 vars )\\n\\nBaseline\\nBOOTC\\nCVC\\nBaseline\\nBOOTC\\nCVC\\nBaSeline\\nBOOTC\\nCVC\\n\\nCancer\\n(9 vars)\\nHepatoma\\n(16 vars)\\n\\nIndiv. Nets\\n% error\\n13.31\\n12.85\\n15.72\\n2.7\\n3.14\\n3.2\\n25.95\\n26.00\\n26.90\\n\\nError\\nCorr\\n.87\\n.57\\n.33\\n.96\\n.83\\n.80\\n.89\\n.70\\n.55\\n\\nCommittee\\n% error\\n11.8\\n8.36\\n9.79\\n2.5\\n2.0\\n1.63\\n23.25\\n19.72\\n19.05\\n\\n887\\n\\nImprov.\\nto Indiv.\\n11 \\'70\\n35 %\\n38 %\\n5%\\n34 %\\n49 %\\n10.5 %\\n24 %\\n29 %\\n\\nImprov.\\nto baseline\\n\\n29 %\\n17 %\\n\\n20 %\\n35 %\\n\\n15.2 %\\n18 %\\n\\nTable 1: Error rate, correlation, and performance improvement calculated based on\\nthe best architecture for each method. Reduction of misclassification rates compare\\nto the baseline committee\\nCorrelation vs . Fraction of Data Overlap\\n\\n0r-----------------------____- .\\nm\\n\\n?\\n\\n.,.,\\nT\\n\\nN\\n\\no\\n\\n!\\n\\ni~ ,,\\nFraction 01data overlap\\n\\nFigure 3: Error correlation and fraction of overlap in training data (results from\\nthe sinwave classification task).\\n\\nThe CVC and BOOTC are always better than the baseline even when the individual\\nnetwork performance is worse. Figure 2 (bottom) and the table show that the\\nimprovement of a committee over individual networks is proportional to the error\\ncorrelation between the networks in the committee. The CVC consistently produces\\nsignificant improvement over its individual network performance due to the low error\\ncorrelation, while the baseline committee only produces modest improvement. This\\nresult confirms the basic assumption of this research: committee performance can\\nbe improved by decorrelating the errors made by the networks.\\nThe performance of a committee depends on two factors: individual performance of\\nthe networks and error correlation between the networks. The gain of using BOOTC\\nor CVC depends on how the algorithms can reduce the error correlations while still\\nmaintaining the individual performance as good as the individual performance of the\\nbaseline. The BOOTC produced impressive improvement (29 %) over the baseline\\non the sinwave task due to the lower correlation and good individual performance.\\nThe performances of the BOOTC on the other two tasks were not as impressive\\ndue to the modest reduction of error correlation and slight decrease in individual\\nperformance. The performances were still significantly better than the baseline\\ncommittee. The CVC, on the other hand, consistently reduced the correlation and\\n\\n\\x0c888\\n\\nB. PARMANTO, P. W. MUNRO, H. R. DOYLE\\n\\nimproved the committee performance. The improvement on the sinwave task was\\nnot as good as the BOOTC due to the low individual performance.\\nThe individual performance of the CVC and BOOTC in general are worse than the\\nbaseline. The individual performance of CVC is 18 % and 19 % lower than the\\nbaseline on the sinwave and cancer tasks respectively, while the BOOTC suffered\\nsignificant reduction of individual performance only on the cancer task (16 %). The\\ndegradation of individual performance is due to the smaller training set for each\\nnetwork on the CVC and the BOOTC. The detrimental effect of a small training\\nset, however, is compensated by low correlation between the networks. The effect\\nof a smaller training set depends on the size of the original training set. If the data\\nsize is large, using a smaller set may not be harmful. On the contrary, if the data set\\nis small, using an even smaller data set can significantly degrade the performance.\\nAnother interesting finding of this experiment is the relationship between the error\\ncorrelation and the overlap fraction in the training set. Figure 3 shows that small\\ndata overlap causes the networks to have low correlation to each other.\\n\\n6\\n\\nSUMMARY\\n\\nTraining committees of networks using different set of data resampled from the\\noriginal training set can improve committee performance by reducing the error correlation among the networks in the committee. Even when the individual network\\nperformances of the BOOTC and CVC degrade from the baseline networks, the\\ncommittee performance is still better due to the lower correlation.\\nAcknowledgement\\n\\nThis study is supported in part by Project Grant DK 29961 from the National\\nInstitutes of Health, Bethesda, MD. We would like to thank the Pittsburgh Transplantation Institute for providing the data for this study.\\nReferences\\n\\nBreiman, L, (1992) Stacked Regressions, TR 367, Dept. of Statistics., UC. Berkeley.\\nBreiman, L, (1994) Bagging Predictors, TR 421, Dept. of Statistics, UC. Berkeley.\\nEfron, B., & Tibshirani, R.J. (1993) An Introd. to the Bootstrap. Chapman & Hall.\\nHashem, S. (1994). Optimal Linear Combinations of Neural Networks. PhD Thesis,\\nPurdue University.\\nGeman, S., Bienenstock, E., and Doursat, R. (1992) Neural networks and the\\nbias/variance dilemma. Neural Computation, 4(1), 1-58.\\nMurphy, P. M., &. Aha, D. W. (1994). UCI Repository of machine learning databases\\n[ftp: ics.uci.edu/pub/machine-Iearning-databases/]\\nParmanto, B., Munro, P.W., Doyle, H.R., Doria, C., Aldrighetti, 1., Marino, I.R.,\\nMitchel, S., and Fung, J.J. (1994) Neural network classifier for hepatoma detectipn.\\nProceedings of the World Congress of Neural Networks 1994 San Diego, June 4-9.\\nPerrone, M.P. (1993) Improving Regression Estimation: Averaging Methods for\\nVariance Reduction with Eztension to General Convez Measure Optimization. PhD\\nThesis, Department of Physics, Brown University.\\nWolpert, D. (1992). Stacked generalization, Neural Networks, 5, 241-259.\\n\\n\\x0c',\n",
       "     'pdf_name': '1075-improving-committee-diagnosis-with-resampling-techniques.pdf',\n",
       "     'title': 'Improving Committee Diagnosis with Resampling Techniques',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '108',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '108',\n",
       "     'paper_text': '618\\n\\nNEURAL NETWORKS FOR MODEL\\nMATCHING AND PERCEPTUAL\\nORGANIZATION\\nGene Gindi\\nEE Department\\nYale University\\nNew Haven, CT 06520\\n\\nEric Mjolsness\\nCS Department\\nYale University\\nNew Haven, CT 06520\\n\\nP. Anandan\\nCS Department\\nYale University\\nNew Haven, CT 06520\\n\\nABSTRACT\\nWe introduce an optimization approach for solving problems in computer vision that involve multiple levels of abstraction. Our objective\\nfunctions include compositional and specialization hierarchies. We cast\\nvision problems as inexact graph matching problems, formulate graph\\nmatching in terms of constrained optimization, and use analog neural\\nnetworks to perform the optimization. The method is applicable to perceptual grouping and model matching. Preliminary experimental results\\nare shown.\\n\\n1\\n\\nIntroduction\\n\\nThe minimization of objective functions is an attractive way to formulate and solve\\nvisual recognition problems. Such formulations are parsimonious, being expressible\\nin several lines of algebra, and may be converted into artificial neural networks\\nwhich perform the optimization. Advantages of such networks including speed,\\nparallelism, cheap analog computing, and biological plausibility have been noted\\n[Hop field and Tank, 1985].\\nAccording to a common view of computational vision, recognition involves the construction of abstract descriptions of data governed by a data base of models. Abstractions serve as reduced descriptions of complex data useful for reasoning about\\nthe objects and events in the scene. The models indicate what objects and properties\\nmay be expected in the scene. The complexity of visual recognition demands that\\nthe models be organized into compositional hierarchies which express object-part\\nrelationships and specialization hierarchies which express object-class relationships.\\nIn this paper, we describe a methodology for expressing model-based visual recognition as the constrained minimization of an objective function. Model-specific\\nobjective functions are used to govern the dynamic grouping of image elements into\\nrecognizable wholes. Neural networks are used to carry out the minimization.\\n?This work was supported in part by AFOSR grant F49620-88-C-002S, and by DARPA grant\\nDAAAlS-87-K-OOOl, by ONR grant N00014-86-0310.\\n\\n\\x0cModel Matching and Perceptual Organization\\n\\nPrevious work on optimization in vision has typically been restricted to computations occuring at a single of level of abstraction and/or involving a single model\\n[Barrow and Popplestone, 1971,Hummel and Zucker, 1983,Terzopoulos, 1986]. For\\nexample, surface interpolation schemes, even when they include discontinuities\\n[Terzopoulos, 1986] do not include explicit models for physical objects whose surface\\ncharacteristics determine the expected degree of smoothness. By contrast, heterogeneous and hierarchical model-bases often occur in non-optimization approaches\\nto visual recognition [Hanson and Riseman, 1986] including some which use neural\\nnetworks [Ballard, 1986]. We attempt to obtain greater express ability and efficiency\\nby incorporating hierarchies of abstraction into the optimization paradigm.\\n\\n2\\n\\nCasting Model Matching as Optimization\\n\\nWe consider a type of objective function which, when minimized by a neural\\nnetwork, is capable of expressing many of the ideas found in Frame systems\\nin Artificial Intelligence [Minsky, 1975]. These \"Frameville\" objective functions\\n[Mjolsness et al., 1988,Mjolsness et al., 1989] are particularly well suited to applications in model-based vision, with frames acting as few-parameter abstractions of\\nvisual objects or perceptual groupings thereof. Each frame contains real-valued parameters, pointers to other frames, and pointers to predefined models (e.g. models\\nof objects in the world) which determine what portion of the objective function acts\\nupon a given frame.\\n\\n2.1\\n\\nModel Matching as Graph Matching\\n\\nModel matching involves finding a match between a set of frames, ultimately derived\\nfrom visual data, and the predefined static models. A set of pointers represent\\nobject-part relationships between frames, and are encoded as a graph or sparse\\nmatrix called ina. That is, inaij = 0 unless frame j is \"in\" frame i as one of its\\nparts, in which case inaij\\n1 is a \"pointer\" from j to i. The expected objectpart relationships between the corresponding models is encoded as a fixed graph\\nor sparse matrix INA. A form of inexact graph-matching is required: ina should\\nfollow INA as much as is consistent with the data.\\nA sparse match matrix M (0 < Meti < 1) of dynamic variables represents the\\ncorrespondence between model a and frame i. To find the best match between the\\ntwo graphs one can minimize a simple objective function for this match matrix, due\\nto Hopfield [Hopfield, 1984] (see also [Feldman et al., 1988,Malsburg, 1986]), which\\njust counts the number of consistent rectangles (see Figure 1a):\\n\\n=\\n\\nE(M)\\n\\n= - ~~INAet~inaijMaiM~j.\\net{3\\n\\n(1)\\n\\nij\\n\\nThis expression may be understood as follows: For model a and frame i, the match\\nvalue M eti is to be increased if the neighbors of a (in the INA graph) match to the\\nneighbors of i (in the ina graph).\\n\\n619\\n\\n\\x0c620\\n\\nMjolsness, Gindi and Anandan\\nM\\n\\n1??\\n\\nINA t.2\\n\\nData\\nside\\n\\nModel\\nside\\n\\nFigure 1: (a) Examples of Frameville rectangle rule. Shows the rectangle relationship between frames (triangles) representing a wing of a plane, and the plane\\nitself. Circles denote dynamic variables, ovals denote models, and triangles denote\\nframes. For the plane and wing models, the first few parameters of a frame are\\ninterpreted as position, length, and orientation. (b) Frameville sibling competition among parts. The match variables along the shaded lines (M3,9 and M 2,7)\\nare suppressed in favor of those along the solid lines (M2,9 and M 3,7)\\'\\nNote that E(M) as defined above can be trivially minimized by setting all the elements of the match matrix to unity. However, to do so will violate additional\\nsyntactic constraints of the form h(M) 0 which are imposed on the optimization,\\neither exactly (Platt and Barr, 1988] or as penalty terms (Hopfield and Tank, 1985]\\n~h2(M) added to the objective function. Originally the syntactic constraints\\nsimply meant that each frame should match one model and vice versa, as in\\n(Hopfield and Tank, 1985]. But in Frameville, a frame can match both a model\\nand one of its specializations (described later), and a single model can match any\\nnumber of instances or frames. In addition one can usually formulate constraints\\nstating that if a model matches a frame then two distinct parts of the same model\\nmust match two distinct part frames and vice-versa. \\\\Ve have found the following\\n\\n=\\n\\n\\x0cModel Matching and Perceptual Organization\\n\\nformulation to be useful:\\n\\n~ INAa{3Mai - ~ inaijM{3j\\na\\n\\n\"\\'p, i\\n\\n(2)\\n\\n0, Va,j\\n\\n(3)\\n\\n0,\\n\\nj\\n\\nE inaijMai - E 1NAa{3M{3j\\nJ\\n\\n{3\\n\\nwhere the first sum in each equation is necessary when several high-level models\\n(or frames) share a part. (It turns out that the first sums can be forced to zero\\nor one by other constraints.) The resulting competition is illustrated in Figure lb.\\nAnother constraint is that M should be binary-valued, i.e.,\\n(4)\\n\\nbut this constraint can also be handled by a special \"analog gain\" term in\\nthe objective function [Hopfield and Tank, 1985] together with a penalty term\\nc Eai Mai(l - Mai).\\nIn Frameville, the ina graph actually becomes variable, and is determined by a dynamic grouping or \"perceptual organization\" process. These new variables require\\nnew constraints, starting with inaij (1 - inaij) = 0, and including many high-level\\nconstraints which we now formulate.\\n\\n2.2\\n\\nFranles and Objective Functions\\n\\nFrames can be considered as bundles ~ of real-valued parameters Fip, where p\\nindexes the different parameters of a frame. For efficiency in computing complex\\narithmetic relationships, such as those involved in coordinate transformations, an\\nanalog representation of these parameters is used. A frame contains no information\\nconcerning its match criteria or control flow; instead, the match criteria are expressed as objective functions and the control flow is determined by the partiCUlar\\nchoice of a minimization technique.\\nIn Figure la, in order for the rectangle (1,4,9,2) to be consistent, the parameters\\nF 4p and F 9p should satisfy a criterion dictated by models 1 and 2, such as a restriction on the difference in angles appropriate for a mildly swept back wing. Such a\\nconstraint results in the addition of the following term to the objective function:\\n\\nL\\n\\nlNA a{3 inaij MaiM{3j Ha{3(~, Pj)\\n\\n(5)\\n\\ni,j,a,{3\\n\\nwhere Ha{3(~, Fj) measures the deviation of the parameters of the data frames from\\nthat demanded by the models. The term H can express coordinate transformation\\narithmetic (e.g. H a{3(Xi, Xj) = 1/2[xi - Xj - D.x a{3]2), and its action on a frame f;.\\nis selectively controlled or \"gated\" by M and ina variables. This is a fundamental\\nextension of the distance metric paradigm in pattern recognition; because of the\\ncomplexity of the visual world, we use an entire database of distance metrics H a {3.\\n\\n621\\n\\n\\x0c622\\n\\nMjolsness, Gindi and Anandan\\n\\nFigure 2: Frameville specialization hierarchy. The plane model specializes\\nalong 154. links to a propeller plane or a jet plane and correspondingly the wing\\nmodel specializes to prop-wing or jet-wing. Sibling match variables M 6 ,4 and M 4 ,4\\ncompete as do M7,9 and M S ,9. The winner in these competitions is determined by\\nthe consistency of the appropriate rectangles, e.g. if the 4-4-9-5 rectangle is more\\nconsistent than the 6-4-9-7 rectangle, then the jet model is favored over the prop\\nmodel.\\nWe index the models (and, indirectly, the data base of H metrics) by introducing\\na static graph of pointers I54. OI {j to act as both a specialization hierarchy and a\\ndiscrimination network for visual recognition. A frame may simultaneously match\\nto a model and just one of its specializations:\\n\\nMcxi -\\n\\nL I54.cx{jMf3i = o.\\n\\n(6)\\n\\nf3\\n\\nAs a result, 154. siblings compete for matches to a given frame (see Figure 2); this\\ncompetition allows the network to act as a discrimination tree.\\nFrameville networks have great expressive power, but have a potentially serious\\nproblem with cost: for n data frames and m models there may be O(nm + 71 2 )\\nneurons widely interconnected but sparsely activated. The number of connections\\nis at most the number of monomials in the polynomial objective function, namely\\nn2 m/, where / is the fan-out of the INA graph. One solution to the cost problem, used in the line grouping experiments reported in [Mjolsness et al., 1989], is to\\nrestrict the flexibility of the frame system by setting most M and ina neurons to\\nzero permanently. The few remaining variables can form an efficient data structure\\n\\n\\x0cModel Matching and Perceptual Organization\\n\\nsuch as a pyramid in vision. A more flexible solution might enforce the sparseness\\nconstraints on the M and ina neurons during minimization, as well as at the fixed\\npoint. Then large savings could result from using \"virtual\" neurons (and connections) which are created and destroyed dynamically. This and other cost-cutting\\nmethods are a subject of continuing research.\\n\\n3\\n\\nExperimental Results\\n\\nWe describe here experiments involving the recognition of simple stick figures.\\n(Other experiments involving the perceptual grouping of lines are reported in\\n[Mjolsness et al., 1989].) The input data (Figure 3(a)) are line segments parameterized by location x, y and orientation (), corresponding to frame parameters Fjp\\n(p\\n1,2,3). As seen in Figure 3(b), there are two high-level models, \"T\" and\\n\"L\" junctions, each composed of three low-level segments. The task is to recognize\\ninstances of \"T\", \"L\", and their parts, in a translation-invariant manner.\\nThe parameter check term H cx {3 of Equation 5 achieves translation invariance by\\nchecking the location and orientation of a given part relative to a designated main\\npart and is given by:\\n\\n=\\n\\nHa{3(~, ff;)\\n\\n= I)Fip -\\n\\nFjp - ~;{3)2\\n\\n(7)\\n\\nP\\n\\nHere Fjp and Fip are the slots of a low-level segment frame and a high-level main\\npart, respectively, and the quantity ~~{3 is model information that stores coordinate\\ndifferences. (Rotation invariance can also be formulated if a different parameterization is used.) It should be noted that absence of the main part does not preclude\\nrecognition of the high-level model.\\nWe used the unconstrained optimization technique in [Hopfield and Tank, 1985] and\\nachieved improved results by including terms demanding that at most one model\\nmatch a given frame, and that at most one high-level frame include a given low-level\\nframe as its part [Mjolsness et al., 1989].\\nFigure 3(c) shows results of attempts to recognize the junctions in Figure 3(a).\\nWhen initialized to random values, the network becomes trapped in unfavorable\\nlocal minima of the fifth-order objective function. (But with only a single high-level\\nmodel in the database, the system recognizes a shape amid noise.) If, however, the\\nnetwork is given a \"hint\" in the form of an initial state with mainparts and high-level\\nmatches set correctly, the network converges to the correct state.\\nThere is a great deal of unexploited freedom in the design of the model base and\\nits objective functions; there may be good design disciplines which avoid introducing spurious local minima. For example, it may be possible to use ISA and INA\\nhierarchies to guide a network to the desired local minimum.\\n\\n623\\n\\n\\x0c624\\n\\nMjolsness, Gindi and Anandan\\n\\n+ +\\n\\n+++++++t\\n+ + + + + + +\\n? + +\\n,+++++++\\n? + +\\n+ + + + + + + +\\n7\\n? + + + + +\\n+ + + + +\\n5+++++ ,+++++\\n4 + + + + +\\n+ + +\\n3 + + + + + t + + + + +\\n2 + + + + +\\n+ + + + +\\n1 + + + + + +--t--+ + + +\\no-;r+ + + + + + +--t-+\\' +\\n\\n10\\n\\no\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\nI\\n\\n7\\n\\n?\\n\\n9\\n\\n10\\n\\nMpj\\n\\nE; 0000.00000\\n\\n1\\n\\n2\\n\\n3\\n\\n123\\n1 ? ? ? 0000000\\n20000 ? ? ?000\\n30000000000\\ninaij\\n\\nB .000000000\\n8 00000.0000\\nE3 0.00000000\\nE1 00.0000000\\nB 000000l1li000\\n9 0000000000\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n12345678910\\n\\nFigure 3: (a) Input data consists of unit-length segments oriented horizontally or\\nvertically. The task is translation-invariant recognition of three segments forming a\\nliT\" junction (e.g. sticks 1,2,3) or an \"L\" (e.g. sticks 5,6,7) amid extraneous noise\\nsticks. (b) Structure of network. Models occur at two levels. INA links are\\nshown for a liT\". Each frame has three parameters: position x, y and orientation\\ne. Also shown are some match and ina links. The bold lines highlight a possible\\nconsistency rectangle. (c) Experhnental result. The value of each dynamical\\nvariable is displayed as the relative area of the shaded portion of a circle. Matrix\\nM{jj indicates low-level matches and MOti indicates high-level matches. Grouping\\nof low-level to high-level frames is indicated by the ina matrix. The parameters of\\nthe high-level frames are displayed in the matrix Fip of linear analog neurons. (The\\nparameters of the low-level frames, held fixed, are not displayed.) The few neurons\\ncircumscribed by a square, corresponding to correct matches for the main parts of\\neach model, are clamped to a value near unity. Shaded circles indicate the final\\ncorrect state.\\n\\n\\x0cModel Matching and Perceptual Organization\\n\\n4\\n\\nConclusion\\n\\nFrameville provides opportunities for integrating all levels of vision in a uniform notation which yields analog neural networks. Low-level models such as fixed convolution filters just require analog arithmetic for frame parameters, which is provided.\\nHigh-level vision typically requires structural matching, also provided. Qualitatively\\ndifferent models may be integrated by specifying their interactions, H cx /3.\\nAcknowledgements\\n\\nWe thank J. Utans, J. Ockerbloom and C. Garrett for the Frameville simulations.\\nReferences\\n[1] Dana Ballard. Cortical connections and parallel processing: structure and function.\\nBehavioral and Brain Sciences, vol 9:67-120, 1986.\\n[2] Harry G. Barrow and R. J. Popplestone. Relational descriptions in picture processing.\\nIn D. Mitchie, editor, Machine Intelligence 6, Edinborough University Press, 1971.\\n[3] Jerome A. Feldman, Mark A. Fanty, and Nigel H. Goddard. Computing with structured neural networks. IEEE Computer, 91, March 1988.\\n[4] Allen R. Hanson and E. M. Riseman. A methodology for the development of general\\nknowledge-based vision systems. In M. A. Arbib and A. R. Hanson, editors, Vision,\\nBrain, and Cooperative Computation, MIT Press, 1986.\\n[5] J. J. Hopfield. Personal communication. October 1984.\\n[6] J. J. Hopfield and D. W. Tank. \\'Neural\\' computation of decisions in optimization\\nproblems. Biological Cybernetics, vol. 52:141-152, 1985.\\n[7] Robert A. Hummel and S. W. Zucker. On the foundations of relaxation labeling\\nprocesses. IEEE Transactions on PAMI, vol. PAMI-5:267-287, May 1983.\\n[8] Marvin L. Minsky. A framework for representing knowledge. In P. H. Winston, editor,\\nThe Psychology of Computer Vision, McGraw-Hill, 1975.\\n[9] Eric Mjolsness, Gene Gindi, and P. Anandan. Optimization in Model Matching and\\nPerceptual Organization: A First Look. Technical Report YALEU /DCS/RR-634,\\nYale University, June 1988.\\n[10] Eric Mjolsness, Gene Gindi, and P. Anandan. Optimization in Model Matching and\\nPerceptual Organization. Neural Computation, to appear.\\n[11] John C. Platt and Alan H. Barr. Constraint methods for flexible models. Computer\\nGraphics, 22(4), August 1988. Proceedings of SIGGRAPH \\'88.\\n[12] Demitri Terzopoulos. Regularization of inverse problems involving discontinuities.\\nIEEE Transactions on PAMI, vol. PAMI-8:413-424, 1986.\\n[13] Christoph von der Malsburg and Elie Bienenstock. Statistical coding and short-term\\nsynaptic plasticity: a scheme for knowledge representation in the brain. In Disordered\\nSystems and Biological Organization, pages 247-252, Springer-Verlag, 1986.\\n\\n625\\n\\n\\x0c',\n",
       "     'pdf_name': '108-neural-networks-for-model-matching-and-perceptual-organization.pdf',\n",
       "     'title': 'Neural Networks for Model Matching and Perceptual Organization',\n",
       "     'year': '1988'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1082',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1082',\n",
       "     'paper_text': 'Prediction of Beta Sheets in Proteins\\nAnders Krogh\\nThe Sanger Centre\\nHinxton, Carobs CBIO IRQ, UK.\\nEmail: krogh@sanger.ac. uk\\n\\nS~ren Kamaric Riis\\nElectronics Institute, Building 349\\nTechnical University of Denmark\\n2800 Lyngby, Denmark\\nEmail: riis@ei.dtu.dk\\n\\nAbstract\\nMost current methods for prediction of protein secondary structure\\nuse a small window of the protein sequence to predict the structure\\nof the central amino acid. We describe a new method for prediction\\nof the non-local structure called ,8-sheet, which consists of two or\\nmore ,8-strands that are connected by hydrogen bonds. Since,8strands are often widely separated in the protein chain , a network\\nwith two windows is introduced. After training on a set of proteins\\nthe network predicts the sheets well, but there are many false positives. By using a global energy function the ,8-sheet prediction is\\ncombined with a local prediction of the three secondary structures\\na-helix, ,8-strand and coil. The energy function is minimized using\\nsimulated annealing to give a final prediction.\\n\\n1\\n\\nINTRODUCTION\\n\\nProteins are long sequences of amino acids. There are 20 different amino acids with\\nvarying chemical properties, e. g. , some are hydrophobic (dislikes water) and some\\nare hydrophilic [1]. It is convenient to represent each amino acid by a letter and\\nthe sequence of amino acids in a protein (the primary structure) can be written as\\na string with a typical length of 100 to 500 letters. A protein chain folds back on\\nitself, and the resulting 3D structure (the tertiary structure) is highly correlated to\\nthe function of the protein. The prediction of the 3D structure from the primary\\nstructure is one of the long-standing unsolved problems in molecular biology. As\\nan important step on the way a lot of work has been devoted to predicting the\\nlocal conformation of the protein chain, which is called the secondary structure.\\nNeural network methods are currently the most successful for predicting secondary\\nstructure. The approach was pioneered by Qian and Sejnowski [2] and Bohr et al.\\n[3], but later extended in various ways, see e.g. [4] for an overview. In most of this\\nwork, only the two regular secondary structure elements a-helix and ,8-strand are\\nbeing distinguished, and everything else is labeled coil. Thus, the methods based\\n\\n\\x0c918\\n\\nA. KROGH, S. K. RIIS\\n\\nH-\\\\\\n\\nt\\n\\no=c\\n\\nH-\\\\\\n,,=c!\"\\n\\n{-~\\n\\n{-H\\n\\n/=0\\nH- \\\\ \\' H-\\\\\\nf\\n\\no=c\\n\\nfa\\n\\n/o=c\\n\\n~-o ~-:\\n\\nFigure 1: Left: Anti-parallel,B-sheet. The vertical lines correspond to the backbone\\nof the protein. An amino acid consists of N-Ca-C and a side chain on the C a that\\nis not shown (the 20 amino acids are distinguished by different side chains). In the\\nanti-parallel sheet the directions of the strands alternate, which is here indicated\\nquite explicitly by showing the middle strand up-side down. The H-bonds between\\nthe strands are shown by 11111111. A sheet has two or more strands, here the antiparallel sheet is shown with three strands. Right: Parallel ,B-sheet consisting of two\\nstrands .\\non a local window of amino acids give a three-state prediction of the secondary\\nstructure of the central amino acid in the window.\\nCurrent predictions of secondary structure based on single sequences as input have\\naccuracies of about 65-66%. It is widely believed that this accuracy is close to\\nthe limit of what can be done from a local window (using only single sequences as\\ninput) [5], because interactions between amino acids far apart in the protein chain\\nare important to the structure. A good example of such non-local interactions\\nare the ,B-sheets consisting of two or more ,B-strands interconnected by H-bonds,\\nsee fig. 1. Often the ,B-strands in a sheet are widely separated in the sequence,\\nimplying that only part of the available sequence information about a ,B-sheet can\\nbe contained in a window of, say, 13 amino acids. This is one of the reasons why the\\naccuracy of ,B-strand predictions are generally lower than the accuracy of a-helix\\npredictions. The aim of this work is to improve prediction of secondary structures\\nby combining local predictions of a-helix, ,B-strand and coil with a non-local method\\npredicting ,B-sheets.\\nOther work along the same directions include [6] in which ,B-sheet predictions are\\ndone by linear methods and [7] where a so-called density network is applied to the\\nproblem.\\n\\n2\\n\\nA NEURAL NETWORK WITH TWO WINDOWS\\n\\nWe aim at capturing correlations in the ,B-sheets by using a neural network with\\ntwo windows, see fig. 2. While window 1 is centered around amino acid number i\\n(ai), window 2 slides along the rest of the chain. When the amino acids centered in\\neach of the two windows sit opposite each other in a ,B-sheet the target output is 1,\\nand otherwise O. After the whole protein has been traversed by window 2, window 1\\nis moved to the next position (i + 1) and the procedure is repeated. If the protein is\\nL amino acids long this procedure yields an output value for each of the L(L -1)/2\\n\\n\\x0cPrediction of Beta Sheets in Proteins\\n\\n919\\n\\nFigure 2: Neural network for predicting ,B-sheets. The network\\nemploys weight sharing to improve the encoding of the amino\\nacids and to reduce the number\\nof adjustable parameters.\\n\\npairs of amino acids. We display the output in a L x L gray-scale image as shown in\\nfig. 3. We assume symmetry of sheets, i.e., if the two windows are interchanged, the\\noutput does not change. This symmetry is ensured (approximately) during training\\nby presenting all inputs in both directions.\\nEach window of the network sees K amino acids. An amino acid is represented by a\\nvector of20 binary numbers all being zero, except one, which is 1. That is, the amino\\nacid A is represented by the vector 1,0,0, ... ,0 and so on. This coding ensures that\\nthe input representations are un correlated , but it is a very inefficient coding, since\\n20 amino acids could in principle be represented by only 5 bit. Therefore, we use\\nweight sharing [8] to learn a better encoding [4]. The 20 input units corresponding\\nto one window position are fully connected to three hidden units. The 3 x (20 + 1)\\nweights to these units are shared by all window positions, i.e., the activation of the\\n3 hidden units is a new learned encoding of the amino acids, so instead of being\\nrepresented by 20 binary values they are represented by 3 real values. Of course the\\nnumber of units for this encoding can be varied, but initial experiments showed that\\n3 was optimal [4]. The two windows of the network are made the same way with\\nthe same number of inputs etc .. The first layer of hidden units in the two windows\\nare fully connected to a hidden layer which is fully connected to the output unit, see\\nfig. 2. Furthermore, two structurally identical networks are used: one for parallel\\nand one for anti-parallel ,B-sheets.\\nThe basis for the training set in this study is the set of 126 non-homologous protein\\nchains used in [9], but chains forming ,B-sheets with other chains are excluded. This\\nleaves us with 85 proteins in our data set. For a protein of length L only a very small\\nfraction of the L(L - 1)/2 pairs are positive examples of ,B-sheet pairs. Therefore\\nit is very important to balance the positive and negative examples to avoid the\\nsituation where the network always predicts no ,B-sheet. Furthermore, there are\\nseveral types of negative examples with quite different occurrences: 1) two amino\\nacids of which none belong to a ,B-sheet; 2) one in a ,B-sheet and one which is not in\\na ,B-sheet; 3) two sitting in ,B-sheets, but not opposite to each other. The balancing\\nwas done in the following way. For each positive example selected at random a\\nnegative example from each of the three categories were selected at random.\\nIf the network does not have a second layer of hidden units, it turns out that the\\nresult is no better than a network with only one input window, i.e., the network\\ncannot capture correlations between the two windows. Initial experiments indicated\\nthat about 10 units in the second hidden layer and two identical input windows of\\nsize K = 9 gave the best results. In fig. 3(left) the prediction of anti-parallel sheets\\nis shown for the protein identified as 1acx in the Brookhaven Protein Data Bank\\n\\n\\x0cA. KROGH, S. K. RIIS\\n\\n920\\n\\n120\\n\\n100\\n\\n:g..\\n\\'\"o\\n\\n80\\n\\n.!:\\n\\n~ 60\\n\\n/\\n40\\n\\n\".\\n20\\n\\nFigure 3: Left: The prediction of anti-parallel ,8-sheets in the protein laex. In the\\nupper triangle the correct structure is shown by a black square for each ,8-sheet\\npair. The lower triangle shows the prediction by the two-window network. For\\nany pair of amino acids the network output is a number between zero (white) and\\none (black), and it is displayed by a linear gray-scale. The diagonal shows the\\nprediction of a-helices. Right: The same display for parallel ,8-sheets in the protein\\n4fxn. Notice that the correct structure are lines parallel to the diagonal, whereas\\nthey are perpendicular for anti-parallel sheets. For both cases the network was\\ntrained on a training set that did not contain the protein for which the result is\\nshown.\\n\\n[10]. First of all, one notices the checker board structure of the prediction of ,8sheets. This is related to the structure of ,8-sheets. Many sheets are hydrophobic\\non one side and hydrophilic on the other. The side chains of the amino acids in\\na strand alternates between the two sides of the sheet, and this gives rise to the\\nperiodicity responsible for the pattern.\\nAnother network was trained on parallel ,8-sheets. These are rare compared to\\nthe anti-parallel ones, so the amount of training data is limited. In fig. 3(right)\\nthe result is shown for protein 4fxn. This prediction seems better than the one\\nobtained for anti-parallel sheets, although false positive predictions still occurs at\\nsome positions with strands that do not pair. Strands that bind in parallel ,8-sheets\\nare generally more widely separated in the sequence than strands in anti-parallel\\nsheets. Therefore, one can imagine that the strands in parallel sheets have to be\\nmore correlated to find each other in the folding process, which would explain the\\nbetter prediction accuracy.\\nThe results shown in fig. 3 are fairly representative. The network misses some of the\\nsheets, but false positives present a more severe problem. By calculating correlation\\ncoefficients we can show that the network doe!> capture some correlations, but they\\nseem to be weak. Based on these results, we hypothesize that the formation of ,8sheets is only weakly dependent on correlations between corresponding ,8-strands.\\nThis is quite surprising. However weak these correlations are, we believe they can\\nstill improve the accuracy of the three state secondary structure prediction. In\\norder to combine local methods with the non-local ,8-sheet prediction, we introduce\\na global energy function as described below.\\n\\n\\x0c921\\n\\nPrediction of Beta Sheets in Proteins\\n\\n3\\n\\nA GLOBAL ENERGY FUNCTION\\n\\nWe use a newly developed local neural network method based on one input window\\n[4] to give an initial prediction of the three possible structures. The output from\\nthis network is constrained by soft max [11], and can thus be interpreted as the\\nprobabilities for each of the three structures. That is, for amino acid ai, it yields\\nthree numbers Pi,n, n = 1,2 or 3 indicating the probability of a-helix (Pi,l) , (3sheet (pi,2), or coil (pi,3). Define Si,n\\n1 if amino acid i is assigned structure n\\nand Si,n = 0 otherwise. Also define hi,n = 10gPi,n. We now construct the \\'energy\\nfunction\\'\\n(1)\\n\\n=\\ni\\n\\nn\\n\\nwhere weights Un are introduced for later usage. Assuming the probabilities Pi,n are\\nindependent for any two amino acids in a sequence, this is the negative log likelihood\\nof the assigned secondary structure represented by s, provided that Un = 1. As it\\nstands, alone, it is a fairly trivial energy function, because the minimum is the\\nassignment which corresponds to the prediction with the maximum Pi,n at each\\nposition i-the assignment of secondary structure that one would probably use\\nanyway.\\nFor amino acids ai and aj the logarithm of the output of the (3-sheet network\\ndescribed previously is called qfj for parallel (3-sheets and qfj for anti-parallel sheets.\\nWe interpret these numbers as the gain in energy if a (3-sheet pair is formed. (As\\nmore terms are added to the energy, the interpretation as a log-likelihood function\\nis gradually fading.) If the two amino acids form a pair in a parallel (3-sheet, we\\nset the variable T~ equal to 1, and otherwise to 0, and similarly with Tii for antiparallel sheets. Thus the Tii and T~ are sparse binary matrices. Now the total\\nenergy of the (3-sheets can be expressed as\\n\\nHf3(s, T a, TP) = - ~[CaqfjTij\\n\\n+ CpqfjT~],\\n\\n(2)\\n\\n\\'J\\n\\nwhere Ca and Cp determine the weights of the two terms in the function. Since\\nan amino acid can only be in one structure, the dynamic T and S variables are\\nconstrained: Only Tii or T~ can be 1 for the same (i, j), and if any of them is 1 the\\namino acids involved must be in a (3-sheet, so Si,2 = Sj,2 = 1. Also, Si ,2 can only be\\n1 if there exists a j with either Iii or T~ equal to 1. Because of these constraints\\nwe have indicated an S dependence of H f3.\\nThe last term in our energy function introduces correlations between neighboring\\namino acids. The above assumption that the secondary structure of the amino acids\\nare independent is of course a bad assumption, and we try to repair it with a term\\n\\nHn(s) =\\n\\nL: L: Jnm Si,n Si+l,m,\\ni\\n\\nnm\\n\\n(3)\\n\\nthat introduces nearest neighbor interactions in the chain. A negative J11, for\\ninstance, means that a following a is favored, and e.g., a positive h2 discourages\\na (3 following an a.\\nNow the total energy is\\n\\n(4)\\nSince (3-sheets are introduced in two ways, through h i ,2 and qij, we need the weights\\nUn in (1) to be different from 1.\\nThe total energy function (4) has some resemblance with a so-called Potts glass\\nin an external field [12]. The crucial difference is that the couplings between the\\n\\n\\x0cA. KROGH, S. K. RIIS\\n\\n922\\n\\n\\'spins\\' Si are dependent on the dynamic variables T. Another analogy of the energy\\nfunction is to image analysis, where couplings like the T\\'s are sometimes used as\\nedge elements.\\n\\n3.1\\n\\nPARAMETER ESTIMATION\\n\\nThe energy function contains a number of parameters, Un, Ca , C p and J nm . These\\nparameters were estimated by a method inspired by Boltzmann learning [13]. In\\nthe Boltzmann machine the estimation of the weights can be formulated as a minimization of the difference between the free energy of the \\'clamped\\' system and\\nthat of the \\'free-running\\' system [14]. If we think of our energy function as a free\\nenergy (at zero temperature), it corresponds to minimizing the difference between\\nthe energy of the correct protein structure and the minimum energy,\\n\\nwhere p is the total number of proteins in the training set. Here the correct structure\\nof protein J-l is called S(J-l) , Ta(J-l), TP(p), whereas s(J-l), Ta(J-l) , TP(J-l) represents the\\nstructure that minimizes the energy Htotal. By definition the second term of C is\\nless than the first, so C is bounded from below by zero.\\nThe cost function C is minimized by gradient descent in the parameters. This is\\nin principle straightforward, because all the parameters appear linearly in Htotal.\\nHowever, a problem with this approach is that C is minimal when all the parameters\\nare set to zero, because then the energy is zero. It is cured by constraining some of\\nthe parameters in Htotal. We chose the constraint l:n Un = 1. This may not be the\\nperfect solution from a theoretical point of view, but it works well. Another problem\\nwith this approach is that one has to find the minimum of the energy Htotal in the\\ndynamic variables in each iteration of the gradient descent procedure. To globally\\nminimize the function by simulated annealing each time would be very costly in\\nterms of computer time. Instead of using the (global) minimum of the energy for\\neach protein, we use the energy obtained by minimizing the energy from the correct\\nstructure. This minimization is done by a greedy algorithm in the following way.\\nIn each iteration the change in s, Ta, TP which results in the largest decrease in\\nHtotal is carried out. This is repeated until any change will increase Htotal. This\\nalgorithm works towards a local stability of the protein structures in the training\\nset. We believe it is not only an efficient way of doing it, but also a very sensible\\nway. In fact, the method may well be applicable in other models, such as Boltzmann\\nmachines.\\n\\n3.2\\n\\nSTRUCTURE PREDICTION BY SIMULATED ANNEALING\\n\\nAfter estimation of the parameters on which the energy function Htotal depends, we\\ncan proceed to predict the structure of new proteins. This was done using simulated\\nannealing and the EBSA package [15]. The total procedure for prediction is,\\n1. A neural net predicts a-helix, ,8-strand or coil. The logarithm of these\\npredictions give all the hi,n for that protein.\\n2. The two-window neural networks predict the ,8-sheets. The result is the qfj\\nfrom one network and the qfj from the other.\\n3. A random configuration of S, Ta, TP variables is generated from which the\\nsimulated annealing minimization of Htotal was started. During annealing,\\nall constraints on s, Ta, TP variables are strictly enforced.\\n\\n\\x0c923\\n\\nPrediction of Beta Sheets in Proteins\\n\\n4. The final minimum configuration s is the prediction of the secondary structure. The ,B-sheets are predicted by a and\\n\\nt\\n\\ntv.\\n\\nUsing the above scheme, an average secondary structure accuracy of 66.5% is obtained by seven-fold cross validation. This should be compared to 66.3% obtained\\nby the local neural network based method [4] on the same data set. Although these\\npreliminary results do not represent a significant improvement, we consider them\\nvery encouraging for future work. Because the method not only predicts the secondary structure, but also which strands actually binds to form ,B-sheets, even a\\nmodest result may be an important step on the way to full 3D predictions.\\n\\n4\\n\\nCONCLUSION\\n\\nIn this paper we introduced several novel ideas which may be applicable in other\\ncontexts than prediction of protein structure. Firstly, we described a neural network\\nwith two input windows that was used for predicting the non-local structure called\\n,B-sheets. Secondly, we combined local predictions of a-helix, ,B-strand and coil\\nwith the ,B-sheet prediction by minimization of a global energy function. Thirdly,\\nwe showed how the adjustable parameters in the energy function could be estimated\\nby a method similar to Boltzmann learning.\\nWe found that correlations between ,B-strands in ,B-sheets are surprisingly weak.\\nUsing the energy function to combine predictions improves performance a little.\\nAlthough we have not solved the protein folding problem, we consider the results\\nvery encouraging for future work. This will include attempts to improve the performance of the two-window network as well as experimenting with the energy function,\\nand maybe add more terms to incorporate new constraints.\\nAcknowledgments: We would like to thank Tim Hubbard, Richard Durbin and\\nBenny Lautrup for interesting comments on this work and Peter Salamon and\\nRichard Frost for assisting with simulated annealing. This work was supported\\nby a grant from the Novo Nordisk Foundation.\\n\\nReferences\\n[1] C. Branden and J. Tooze, Introduction to Protein Structure (Garland Publishing,\\n[2]\\n[3]\\n[4]\\n[5]\\n\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n\\n[11]\\n[12]\\n[13]\\n[14]\\n[15]\\n\\nInc., New York, 1991).\\nN. Qian and T. Sejnowski, Journal of Molecular Biology 202, 865 (1988).\\nH. Bohr et al., FEBS Letters 241, 223 (1988).\\nS. Riis and A. Krogh, Nordita Preprint 95/34 S, submitted to J. Compo BioI.\\nB. Rost, C. Sander, and R. Schneider, J Mol. BioI. 235, 13 (1994).\\nT. Hubbard, in Proc. of the 27th HICSS, edited by R. Lathrop (IEEE Computer Soc.\\nPress, 1994), pp. 336-354.\\nD. J. C. MacKay, in Maximum Entropy and Bayesian Methods, Cambridge 1994,\\nedited by J. Skilling and S. Sibisi (Kluwer, Dordrecht, 1995).\\nY. Le Cun et al., Neural Computation 1, 541 (1989).\\nB. Rost and C. Sander, Proteins 19, 55 (1994).\\nF. Bernstein et al., J Mol. BioI. 112,535 (1977).\\nJ. Bridle, in Neural Information Processing Systems 2, edited by D. Touretzky (Morgan Kaufmann, San Mateo, CA, 1990), pp. 211-217.\\nK. Fisher and J. Hertz, Spin glasses (Cambridge University Press, 1991).\\nD. Ackley, G. Hinton, and T. Sejnowski, Cognitive Science 9, 147 (1985).\\nJ. Hertz, A. Krogh, and R. Palmer, Introduction to the Theory of Neural Computation\\n(Addison-Wesley, Redwood City, 1991).\\nR. Frost, SDSC EBSA, C Library Documentation, version 2.1. SDSC Techreport.\\n\\n\\x0c',\n",
       "     'pdf_name': '1082-prediction-of-beta-sheets-in-proteins.pdf',\n",
       "     'title': 'Prediction of Beta Sheets in Proteins',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1084',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1084',\n",
       "     'paper_text': 'High-Speed Airborne Particle Monitoring\\nUsing Artificial Neural Networks\\n\\nAlistair Ferguson\\nERDC, Univ. of Hertfordshire\\nA.Ferguson@herts.ac.uk\\n\\nTheo Sabisch\\nDept. Electrical and Electronic Eng.\\nUniv. of Hertfordshire\\n\\nPaul Kaye\\nERDC, Univ. of Hertfordshire\\n\\nLaurence C. Dixon\\nNOC, Univ. of Hertfordshire\\n\\nHamid Bolouri\\nERDC, Univ. of Hertfordshire, Herts, ALtO 9AB, UK\\n\\nAbstract\\nCurrent environmental monitoring systems assume particles to be\\nspherical, and do not attempt to classify them. A laser-based system developed at the University of Hertfordshire aims at classifying airborne particles through the generation of two-dimensional\\nscattering profiles. The pedormances of template matching, and\\ntwo types of neural network (HyperNet and semi-linear units) are\\ncompared for image classification. The neural network approach is\\nshown to be capable of comparable recognition pedormance, while\\noffering a number of advantages over template matching.\\n\\n1\\n\\nIntroduction\\n\\nReliable identification of low concentrations of airborne particles requires high speed\\nmonitoring of large volumes of air, and incurs heavy computational overheads. An\\ninstrument to detect particle shape and size from spatial light scattering profiles has\\n\\n\\x0cHigh-speed Airborne Particle Monitoring Using Artificial Neural Networks\\n\\n981\\n\\npreviously been described [6]. The system constrains individual particles to traverse\\na laser beam. Thus, spatial distributions of the light scattered by individual particles\\nmay be recorded as two dimensional grey-scale images.\\nDue to their highly distributed nature, Artificial Neural Networks (ANNs) offer the\\npossibility of high-speed non-linear pattern classification. Their use in particulate\\nclassification has already been investigated. The work by Kohlus [7] used contour\\ndata extracted from microscopic images of particles, and so was not real-time. While\\nusing laser scattering data to allow real-time analysis, Bevan [2] used only three\\nphotomultipliers, from which very little shape information can be collected.\\nThis paper demonstrates the plausibility of particle classification based on shape\\nrecognition using an ANN. While capable of similar recognition rates, the neural\\nnetworks are shown to offer a number of advantages over template matching.\\n\\n2\\n\\nThe HyperN et Architecture\\n\\nHyperNet is the term used to denote the hardware model of a RAM-based sigma-pi\\nneural architecture developed by Gurney [5]. The architecture is similar in nature\\nto the pRAM of Gorse and Taylor (references in [4]). The amenability of these\\nnodes to hardware realisation has been extensively investigated, leading to custom\\nVLSI implementations of both nodes [3, 4]. Each HyperNet node is termed a multicube unit (MeU), and consists of a number of subunits, each with an arbitrary\\nnumber of inputs. j references the nodes, with i = 1, ... ,Ii indexing the subunits.\\nIJ denotes the site addresses, and is the set of bit strings 1J1, ... ,lJn wl1ere n denotes\\nthe number of inputs to the subunit. Zc refers to the cth real-valued input, with\\nZc E [0,1] and Zc == (1 - zc). For each of the 2n site store locations, two sets are\\nif IJc = 0; c E\\nif IJc = 1. The access probability p(lJii) for\\ndefined: c E\\nlocation IJ in subunit i of hidden layer node j is therefore\\n\\nM:!o\\n\\nM:!t\\n\\n(1)\\n\\nThe activation (ai ) is formed by accumulating the proportional site values (SIS\\';)\\nfrom every subunit. The activation is then passed through a sigmoidal transfer\\nfunction to yield the node output (yi).\\n(2)\\n.\\n.\\ny\\' = u(a1 ) =\\n\\n1\\n\\n.\\n1 + e a1 / p\\n\\n(3)\\n\\nwhere p is a positive parameter determining the steepness of the sigmoidal curve.\\nBy combining equations (1) and (2), it becomes apparent that the node is a higherorder or sigma-pi node [9]. A wide variety of learning algorithms have been tailored\\nfor these nodes, notably reward-penalty and back-propagation [5].\\n\\n\\x0c982\\n\\n3\\n\\nA. FERGUSON, T. SABISCH, P. KAYE. L. C. DIXON. H. BOWURJ\\n\\nDescription of the Particle Monitoring System\\n\\nThe instrument draws air through the laser scattering chamber at approximately\\n1.5 min- 1 , and is constrained to a column of approximately 0.8mm diameter at the\\nintersection with the laser beam. Light scattered into angles between 300 and 141 0\\nto the beam direction is reflected through the optics and onto the photocathode of\\nan intensified CCD (charge-coupled device), thus giving rise to the scattering profile.\\nThe imaging device used has a pixel resolution of 385 x 288, which is quantised into\\n2562 8-bit pixels by the frame grabbing processor card of the host computer.\\nData was collected on eight particle types, namely: long and short caffeine fibres;\\n31lm and 121lm micro-machined silicon dioxide fibres; copper flakes (2- 5Ilm in length\\nand O.lllm thick); 31lm and 4.31lm polystyrene spheres; and salt crystals. An exemplar profile for each class is given in figure 1. Almost all the image types are highly\\nvariable. In particular, the scattering profile obtained for a fibrous particle is affected by its orientation as it passes through the laser beam. The scattering profiles\\nare intrinsically centred, with the scaling giving important information regarding\\nthe size of the particle. The experiments reported here use 100 example scattering\\nprofiles for each of the eight particle classes. For each class, 50 randomly selected\\nimages were used to construct the templates or train the neural network (training\\nset), and the remainder used to test the performance of the pattern classifiers.\\n\\n4\\n\\nExperimental Results\\n\\nThe performance of template matching is compared to both HyperNet and networks\\nof semi-linear units. In all experiments, high-speed classification is emphasised by\\n\\n?\\n~.\\n\\n~\\n\\n\" l,: ~t..\\n,\\n\\n?\\'.,\\n\\n..\\n\\n.\\n\\n. ,\\n\\n~..,\\n\\n.\\nt \\',\\n\\n,\\n\\n.\\n\\n\\'.\\n\\n\\'l.J;\\n\\nFigure 1: Exemplar Image Profile For Each Of The Eight Benchmark Classes\\n\\n\\x0cHigh-speed Airborne Particle Monitoring Using Artificial Neural Networks\\n\\n983\\n\\navoiding image preprocessing operations such as transformation to the frequency\\ndomain, histogram equalisation, and other filtering operations. Furthermore, all\\nexperiments use the scatter profile image as input, and include no other information.\\nThe current monitoring system produces a 2562 8-bit pixel image. The sensitivity of\\nthe camera is such that a single pixel can represent the registration of a single photon\\nof light. Two possible methods of reducing computation, implementable through\\nthe use of a cheaper, less sensitive camera were investigated. The first grouped\\nneighbouring pixels to form a single average intensity value. The neighbourhood\\nsize was restricted to powers of two, producing images ranging in size from 2562 to\\n42 pixels. The second banded grey levels into groups, again in powers of two. Each\\npixel could therefore range from eight bits down to one.\\n\\n4.1\\n\\nTemplate Matching Results\\n\\nThe construction of reference templates is crucial to successful classification. Two\\napproaches to template construction were investigated\\n\\n<D Single reference image for each class. Various techniques were applied ranging from individual images, to mode, median, and mean averaged templates.\\nMean averaged templates were found to lead to the highest classification\\nrates. In this approach, each pixel location in the template takes on the\\naveraged value of that location across the 50 training images.\\n? Multiple templates per class. A K-means clustering algorithm [1] was used\\nto identify clusters of highly correlated images within each class. The initial\\ncluster centres were hand selected. The maximum number of clusters within\\neach class was limited to six. For each cluster, the reference template was\\nconstructed using the mean averaging approach above.\\nTables 1 and 2 summarise the recognition rates achieved using single, and multiple\\nmean averaged templates for each particle class. In both cases, the best average\\nrecognition rate using this approach was gained with 1282 3-bit pixel images. With\\na single template this lead to a recognition rate of 78.2%, increasing to 85.2% for\\nmultiple templates. However, the results for both 162 and 82 pixel images are\\nreasonable approximations of the best performance, and represent an acceptable\\ntrade-off between computational cost and performance. With few exceptions, multiple templates per class led to higher recognition rates than for the corresponding\\nsingle template results. This is attributable to the variability of the particles within\\na class. As expected, the effect of grey level quantisation is inversely proportional\\nto that of local averaging.\\nIn order to evaluate the efficiency of the template construction methods, every image\\n\\nin the training set was used as a reference template. 2562 8-bit, 1282 3-bit, and 642\\n2-bit pixel images were used for these experiments. However, the recognition rate\\ndid not exceed 85%, demonstrating the success of the template generation schemes\\npreviously employed.\\n\\n\\x0c984\\n\\nA. FERGUSON, T. SABISCH, P. KAYE, L. C. DIXON, H. BOLOURI\\n\\nTable 1: Single Template Per Class % Recognition Rates\\nimage size\\ngrey levels\\n42\\n2562 1282\\n64 2\\n32 2\\n162\\n82\\n73.5 75.0 74.7 74.7 74.7 75.0 67.2\\n256\\n128\\n73.5 75.0 74.7 74.7 74.5 75.0 68.5\\n64\\n73.0 75.0 74.5 74.5 74.2 74.7 66.2\\n32\\n73.0 74.7 75.2 75.5 74.7 74.2 66.5\\n16\\n74.0 76.0 76.7 76.0 75.0 15.5 56.0\\n15.5 18.2 11.5 11.5 16.0 73.7 38.7\\n8\\n68.4\\n69.7 71.0 70.7 69.7 58.5 18.7\\n4\\n2\\n69.7 68.7 65.5 66.2 46.2 23.0 16.6\\n\\nTable 2: Multiple Templates Per Class % Recognition Rates\\nimage size\\ngrey levels\\n256 2 1282\\n64 2\\n32 2\\n162\\n42\\n82\\n78.0 80.0 80.2 80.5 79.0 76.7 10.2\\n256\\n128\\n78.5 80.2 80.5 80.5 79.0 77.0 69.7\\n64\\n78.7 80.2 80.2 80.5 79.2 76.0 69.2\\n78.2 81.2 81.7 80.0 78.7 76.7 67.7\\n32\\n80.2 83.5 83.0 81.2 79.5 78.5 56.0\\n16\\n8\\n82.2 85.2 84.5 84.1 81.0 80.0 43.5\\n4\\n72.7 74.5 72.2 72.2 69.5 61.2 39.2\\n69.7 70.2 70.7 62.7 51.7 51.7 0.03\\n2\\n\\n4.2\\n\\nNeural Network Results\\n\\nA fully connected three layer feed-forward network was used in all experiments. The\\nnumber of hidden layer neurons was equal to the square root of the number of pixels.\\nThe target patterns were chosen to minimise the number of output layer nodes, while\\nensuring an equitable distribution of zeros and ones. Six output layer neurons were\\nused to give a minimum Hamming distance of two between target patterns. The\\nclassification of a pattern was judged to be the particle class whose target pattern\\nwas closest (lowest difference error). The HyperNet architecture was trained using\\nsteepest descent, though the line search was hardware based and inexact. The semilinear network was trained using a variety of back-propagation type algorithms, with\\nthe best results obtained reported. Both networks were randomly initialised. Due\\nto the enormous training overhead, only 162 and 82 pixel images were tried. The\\nrecognition rates achieved are given in table 3.\\nBoth neural networks are significantly better than the single, and some of the multiple template matching results. With optimisation of the network structures, it is\\nlikely that the ANNs could exceed the performance of multiple templates.\\n\\n\\x0cHigh-speed Airborne Particle Monitoring Using Artificial Neural Networks\\n\\n985\\n\\nTable 3: Neural Network % Recognition Rates\\nQuantisation Levels\\nClassifier\\n162 4 bit 1162 3-bit 82 4-bit 82 3-bit\\nHyperNet\\n82.3\\n83.0\\n76.8\\n83.8\\nSemi-linear\\n84.5\\n76.0\\n86.3\\n77.8\\n\\nI\\n\\nle+09\\n\\n,\\n,,\\n\\n,-.. le+08\\n\\'\"c::\\n\\'-\"\\n\\n,,\\n\\n,\\n\\n~ Ie+{)?\\n\\no~\\n\\nll!\\n0;;\\n\\n\\'\"\\n8\\n\\n~\\n\\nle+06\\n\\nle+05\\nle+04\\n\\n82\\n\\n162\\n\\n322\\n\\n642\\n\\n1282\\n\\n2562\\n\\nNumber of pixels\\n\\nFigure 2: Hardware classification speeds for a single pattern against image size\\n\\n5\\n\\nSpeed Considerations\\n\\nSingle processor, pipelined hardware implementations of the three classification\\ntechniques have been considered. A fast (45ns) multiply-accumulate chip (Logic\\nDevices Ltd, LMA201O) was utilised for semi-linear units. Both template matching\\nand HyperNet were implemented using the Logic Devices LGC381 ALU (26ns per\\naccumulate). The cost of these devices is approximately the same (?10-20). The\\nHyperNet implementation uses a bit-stream approach to eliminate the probability\\nmultiplications [8], with a stream length of 256 bits. Figure 2 plots single pattern\\nprocessing time for each classifier against image size.\\nFor small image resolutions, the semi-linear network offers the best performance, being almost three times faster than template matching. However, template matching\\nand HyperNet yield faster performance at higher image resolutions. At the optimum (indicated by template matching results (?4.1); 1282 pixels), HyperNet is\\nalmost seven times faster than the comparable implementation of semi-linear units.\\nWhile the hardware performance of template matching is similar to HyperNet, it\\nsuffers from a number of disadvantages to which the neural approaches are immune\\nCD Recognition rate is dependent on the choice of reference images.\\n\\n? Multiple reference images must be used to achieve good recognition rates\\n\\n\\x0c986\\n\\nA. FERGUSON, T. SABISCH, P. KAYE, L. C. DIXON, H. BOLOURI\\n\\nwhich drastically increases the amount of computation required.\\n\\n6\\n\\n@\\n\\nNew reference images must be found whenever a new class is introduced.\\n\\n@\\n\\nDifficult to make behaviour adaptive, ie. respond to changing conditions.\\n\\nConclusions\\n\\nThe feasibility of constructing an airborne particle monitoring system capable of\\nreliable particle identification at high speeds has been demonstrated. Template\\nmatching requires multiple reference images and is cumbersome to develop. The\\nneural networks offer easier training procedures and equivalent recognition rates. In\\naddition, HyperNet has the advantage of high speed operation at large image sizes.\\nAcknowledgements\\nThe authors would like to thank Dr. Eric Dykes and Dr. Edwin Hirst at the University of Hertfordshire, Dr. Kevin Gurney at BruneI University, and the EPSRC\\nand the Royal Society for financial support.\\n\\nReferences\\n[1] Stephen Banks. Signal Processing, Image Processing, and Pattern Recognition.\\nPrentice Hall, 1990.\\n[2] A V Bevan et al. The application of neural networks to particle shape classification. Journal of Aerosol Science, 23(Suppl. 1):329-332, 1992.\\n[3] Hamid Bolouri et al. Design, manufacture, and evaluation of a scalable highperformance neural system. Electronics Letters, 30(5):426-427, 3 March 1994.\\n[4] T G Clarkson et al. The pRAM: An adaptive VLSI chip. IEEE \\'Ihmsactions\\non Neural Networks, 4(3):408-412, May 1993.\\n[5] Kevin N Gurney. Learning in networks of structured hypercubes. PhD thesis,\\nDepartment of Electrical Engineering, UK, 1995.\\n[6] Paul H Kaye et al. Airborne particle shape and size classification from spatial\\nlight scattering profiles. Journal of Aerosol Science, 23(6):597--611, 1992.\\n[7] R Kohlus et al. Particle shape analysis as an example of knowledge extraction\\nby neural nets. Part. Part. Syst. Charact., 10:275-278, 1993.\\n[8] Paul Morgan et al. Hardware implementation of a real-valued sigma-pi network.\\nIn Artificial Neural Networks 5, volume 2, pages 351-356, North-Holland, 1995.\\n[9] David E Rumelhart et al. Parallel Distributed Processing: Explorations in the\\nMacrostructure of Cognition, volume 1. MIT Press, 1986.\\n\\n\\x0cPART IX\\nCONTROL\\n\\n\\x0c\\x0c',\n",
       "     'pdf_name': '1084-high-speed-airborne-particle-monitoring-using-artificial-neural-networks.pdf',\n",
       "     'title': 'High-Speed Airborne Particle Monitoring Using Artificial Neural Networks',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2372',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '2372',\n",
       "     'paper_text': 'Bounded Finite State Controllers\\nPascal Poupart\\nDepartment of Computer Science\\nUniversity of Toronto\\nToronto, ON M5S 3H5\\nppoupart@cs.toronto.edu\\n\\nCraig Boutilier\\nDepartment of Computer Science\\nUniversity of Toronto\\nToronto, ON M5S 3H5\\ncebly@cs.toronto.edu\\n\\nAbstract\\nWe describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the\\nspace of bounded-size, stochastic finite state controllers, combining several advantages of gradient ascent (efficiency, search through restricted\\ncontroller space) and policy iteration (less vulnerability to local optima).\\n\\n1 Introduction\\nFinite state controllers (FSCs) provide a simple, convenient way of representing policies\\nfor partially observable Markov decision processes (POMDPs). Two general approaches\\nare often used to construct good controllers: policy iteration (PI) [7] and gradient ascent\\n(GA) [10, 11, 1]. The former is guaranteed to converge to an optimal policy, however, the\\nsize of the controller often grows intractably. In contrast, the latter restricts its search to\\ncontrollers of a bounded size, but may get trapped in a local optimum.\\nWhile locally optimal solutions are often acceptable, for many planning problems with a\\ncombinatorial flavor, GA can easily get trapped by simple policies that are far from optimal. Consider a system engaged in preference elicitation, charged with discovering optimal\\nquery policy to determine relevant aspects of a user?s utility function. Often no single question yields information of much value, while a sequence of queries does. If each question\\nhas a cost, a system that locally optimizes the policy by GA may determine that the best\\ncourse of action is to ask no questions (i.e., minimize cost given no information gain).\\nWhen an optimal policy consists of a sequence of actions any small perturbation to which\\nresults in a bad policy, there is little hope of finding this sequence using methods that\\ngreedily perform local perturbations such as those employed by GA.\\nIn general, we would like the best of both worlds: bounded controller size and convergence to a global optimum. While achieving both is NP-hard for the class of deterministic\\ncontrollers [10], one can hope for a tractable algorithm that at least avoids obvious local optima. We propose a new anytime algorithm, bounded policy iteration (BPI) that improves a\\npolicy much like Hansen?s PI [7] while keeping the size of the controller fixed. Whenever\\nthe algorithm gets stuck in a local optimum, the controller is allowed to slightly grow by\\nintroducing one (or a few) node(s) to escape the local optimum.\\nFollowing a brief review of FSCs (Sec. 2), we extend PI to stochastic controllers (Sec. 3),\\nthus admitting smaller, high quality controllers. We then derive the BPI algorithm by ensuring that the number of nodes remains unchanged (Sec. 4). We analyze the structure of\\n\\n\\x0clocal optima for BPI (Sec. 5), relate this analysis to GA, and use it to justify a new method\\nto escape local optima. Finally, we report some preliminary experiments (Sec. 6).\\n\\n2 Finite State Controllers for POMDPs\\n\\n\\x01\\n\\n\\x02\\n\\x12\\x14\\x13\\x15\\x04\\x07\\x06\\x0f\\x0e\\x17\\x16 \\x06\\t\\x08\\x0b\\n\\x15\\x10\\n\\x12\\x14\\x13\\x1e\\x04\\x07\\x1c\\x0c\\x16 \\x06\\t\\x08\\x0b\\n\\x15\\x10\\n\\x1f\\n\\x1f \\x04\\x07\\x06\\t\\x08\\n\\x1e\\x10\\n\\n\\n\\nA POMDP is defined by a set of states ; a set of actions ; a set of observations ;\\na transition function , where\\ndenotes the transition probabilities\\n;\\nan observation function , where\\ndenotes the probability\\nof making\\nobservation in state after taking action ; and a reward function , where\\ndenotes the immediate reward associated with state when executing ation . We assume\\ndiscrete state, action and observation sets and we focus on discounted, infinite horizon\\n. Since states are not directly observable in\\nPOMDPs with discount factor\\nPOMDPs, we define a belief state\\nto be a distribution over states. Belief\\nstate can be updated in response to a action-observation pair\\nusing Bayes rule.\\n\\n\\x03\\n\\n\\x1c\\n\\n\\x18\\n\\n\\x06\\n\\n\\x03\\x05\\x04\\x07\\x06\\t\\x08\\x0b\\n\\x0c\\x08\\n\\x06\\x0f\\x0e\\x11\\x10\\n\\x18\\x19\\x04\\x1a\\x06\\x1b\\x08\\n\\x1c\\x1d\\x10\\n\\n\\n\\n\\n\\x06\\n\\n!#\"%$\\'&)(\\n*\\x0f\\x04\\x1a\\x06+\\x10-,.\\x12\\x14\\x13\\x15\\x04\\x1a\\x06+\\x10\\n\\n*\\n\\n/0\\n\\x0c\\x08\\x0b\\x1c\\x1e1\\n\\n23,4/657\\x0898:1\\n\\nPolicies represented by FSCs are defined by a (possibly cyclic) directed graph\\n,\\nwhere each node\\nis labeled by an action and each edge\\nby an observation\\n. Each node has one outward edge per observation. The FSC can be viewed as a policy\\n, where action strategy associates each node with an action\\n,\\nand observation strategy associates each node and observation with a successor node\\n(corresponding to the edge from labeled with ). A policy is executed\\nby taking the action associated with the ?current node,? and updating the current node by\\nfollowing the edge labeled by the observation made.\\n\\n;=<>5\\n\\n\\x1c\\n24,A/0BC\\x089D:1\\nDG\\x040;H\\x08\\x0b\\x1c\\x1e\\x10I<J5\\n\\n\\n\\n\\nB\\n\\nD\\n\\n;\\n\\n?-<@8\\n\\n;\\n\\n;\\n\\n\\x1c\\n\\nBC\\x040;E\\x10F<7\\x01\\n\\n\\x1c\\n\\nKML\\n2\\n2\\nK L 0\\x04 ;H\\x08\\n\\x06N\\x10G,O\\x1f \\x1a\\x04 \\x06\\x1b\\x08\\nBC\\x040;E\\x109\\x10EPQ$\\x14R\\x1bS4\\x12\\x14\\x13\\x15\\x04\\x07\\x06 \\x0e \\x16 \\x06\\x1b\\x08\\nBC\\x040;E\\x109\\x109\\x12\\x14\\x13\\x1e\\x04\\x07\\x1c\\x0c\\x16 \\x06 \\x0e \\x08\\x0bBT\\x04U;E\\x109\\x10\\x0bK L \\x040DG\\x04U;H\\x08\\n\\x1c\\x1d\\x10V\\x08\\n\\x06 \\x0e \\x10 (1)\\nGiven an initial belief state * , an FSC?s value at node ; is simply the expectation\\nKW_\\x0f\\x040;H`b\\x08\\na *V\\x10X,ZY#[\\\\*\\x0f\\x04\\x1a\\x06+\\x10\\x0bKW\\x040;H\\x08\\n\\x06N\\x10 ; the best starting node for a given * is determined by K]\\x04\\x07*V\\x10X,\\n^K-\\x04U;H\\x08c*V\\x10 . As a result, the value KW\\x04U;H\\x08c*V\\x10 of each node ; is linear with respect to the\\n\\nThe value function\\nof an FSC is the expected discounted sum of rewards for executing\\nits policy , and can be computed by solving a set of linear equations:\\n\\nbelief state; hence the value function of the controller is piecewise-linear and convex. In\\nFig. 1(a), each linear segment corresponds to the value function of a node and the upper\\nsurface of these segments forms the controller value function. The optimal value function\\nsatisfies Bellman?s equation:\\n\\nK\\x05d\\n\\n^ f _\\x0f` \\x1f \\x04\\x1a*N\\x08\\x0b\\n\\x15\\x10EPg$\\x14R\\x1bS4\\x12\\x14\\x13\\x15\\x040\\x1c\\\\\\x16 *N\\x08\\n\\x15\\x109KW\\x04\\x1a* Sf \\x10\\nK d \\x04\\x07*V\\x10e, -\\n\\n(2)\\n\\nPolicy iteration (PI) [7] incrementally improves a controller by alternating between two\\nsteps, policy improvement and policy evaluation, until convergence to an optimal policy.\\nPolicy evaluation solves Eq. 1 for a given policy. Policy improvement adds nodes to the\\ncontroller by dynamic programming (DP) and removes other nodes. A DP backup applies\\nthe r.h.s. of Eq. 2 to the value function ( in Fig. 2(a)) of the current controller to obtain a\\nnew, improved value function ( in Fig. 2(a)). Each linear segment of\\ncorresponds to a\\nnew node added to the controller. Several algorithms can be used to perform DP backups,\\nwith incremental pruning [4] perhaps being the fastest. After the new nodes created by\\nDP have been added, old nodes that are now pointwise dominated are removed. A node\\nis pointwise dominated when its value is less than that of some other node at all belief\\nstates (e.g., is pointwise dominated by\\nin Fig. 2(a)). The inward edges of a pointwise\\ndominated node are re-directed to the dominating node since it offers better value (e.g.,\\ninward arcs of\\nare redirected to\\nin Fig. 2(c)). The controller resulting from this\\npolicy improvement step is guaranteed to offer higher value at all belief states. On the\\nother hand, up to\\nnew nodes may be added with each DP backup, so the size of\\nthe controller quickly becomes intractable in many POMDPs.\\n\\nK\\n\\nK\\x19\\x0e\\n\\n;ih\\n\\nK\\x19\\x0e\\n\\n;Ej\\n\\n;h\\n\\n;j\\n\\n\\x16 \\x01k\\x16\\x11\\x16 54\\x16\\x11l mCl\\n\\n\\x0cvalue function:\\nbacked up value function:\\nn3\\nvalue\\n\\nvalue\\n\\nupper surface:\\nconvex combination:\\nn2\\n\\nn1\\nbelief space\\n\\nb?\\n\\na)\\n\\nb\\nb)\\n\\nFigure 1: a) Value function example; b) BPI local optimum: each linear segment of the\\nvalue function is tangent to the backed up value function\\nV:\\n\\nV?:\\n\\nvalue\\n\\nn4\\n\\nn3\\n\\nn2\\n\\nn1\\n\\na\\n\\nn3\\n\\nn1\\n\\nb\\n\\na\\n\\nc\\n\\nn4\\n\\nbelief space\\n\\nn3\\n\\nn2\\n\\na)\\n\\nb\\n\\nb)\\n\\nK\\n\\nc\\n\\nn4\\n\\nn2\\nc)\\n\\nK \\x0e\\n\\nFigure 2: a) Value function and the backed-up\\nobtained by DP; b) original controller\\n( and \\x01 ) with nodes added (\\x03\\x02 and ) by DP; c) new controller once pointwise dominated node\\nis removed and its inward arcs a, b, c are redirected to\\n\\n;ih\\n\\n;\\n\\n;\\n\\n;h\\n\\n;j\\n\\n;j\\n\\n3 Policy Iteration for Stochastic Controllers\\nPolicy iteration only prunes nodes that are pointwise dominated, rather than all dominated\\nnodes. This is because the algorithm is designed to produce controllers with deterministic\\nobservation strategies. A pointwise-dominated node can safely be pruned since its inward\\narcs are redirected to the dominating node (which has value at least as high as the dominated\\nnode at each state). In contrast, a node jointly dominated by several nodes (e.g., \\x01 in\\nFig. 2(b) is jointly dominated by \\x04\\x02 and ) cannot be pruned without its inward arcs being\\nredirected to different nodes depending on the current belief state.\\n\\n;\\n\\n;\\n\\n;j\\n\\nThis problem can be circumvented by allowing stochastic observation strategies. We revise\\nthe notion of observation strategy\\n, defining a distribution over\\nsuccessor nodes\\nfor each\\n-pair. If the stochastic strategy is chosen carefully, the\\ncorresponding convex combination of dominating nodes may pointwise dominate the node\\nwe would like to prune. In Fig. 1(a),\\nis dominated by \\x01 and \\x02 together (but neither of\\nthem alone). Convex combinations of \\x01 and \\x03\\x02 correspond to all lines that pass through\\nthe intersection of \\x01 and \\x05\\x02 . The dotted line illustrates one convex combination of \\x01 and\\n\\x02 that pointwise dominates : consequently, can be safely removed and its inward\\narcs re-directed to reflect this convex combination by setting the observation probabilities\\naccordingly. In general, when a node is jointly dominated by a group of nodes, there exists\\na pointwise-dominating convex combination of this group.\\n\\n;E\\x0e\\n\\n;H\\x08\\x0b\\x1c\\n\\nDC\\x04U;H\\x08\\x0b\\x1c \\x089;i\\x0e\\x11\\x10\\x19, \\x12\\x14\\x13\\x15\\x04U; \\x0e \\x16 ;H\\x08\\n\\x1c\\x1d\\x10\\n; h\\n\\n;\\n\\n;\\n\\n;\\n\\n;\\n\\n;h\\n\\nK \\x04U;H\\x08 \\x10\\n]\\n;:hN\\x08\\nKW\\x040;H\\x08 \\x10\\n\\n;\\n\\n;\\n\\n;h\\n;\\n\\n;\\n\\n;\\n\\n\\x07\\x06 of a node is jointly dominated by the value funcTheorem 1 The value function\\ntions\\n\\x08\\x06 \\x07\\t\\x08\\t\\x08\\t\\n\\x03\\n \\x08\\x06 of nodes \\x07\\t\\x08\\t\\x08\\t \\x03\\n if and only if there is a convex combination\\n\\x0b \\x08\\x06 that dominates\\n\\x08\\x06 .\\n\\x0c\\x0b\\x0e\\n \\x0b\\n\\nY\\n\\nK]\\x04U;ih \\x08 \\x10 \\x08\\nKW\\x04U; \\x08 \\x10\\n\\n\\x08\\nK-\\x040; \\x1e\\x08 \\x10\\n\\n\\x089;\\n\\n\\x0c^\\x01\\x03\\x02 \\x04\\n\\ns.t.\\n\\nY [ \\x0f* \\x04\\x07\\x06N\\x10\\x0bKW\\x04U;H\\x08c\\x06N\\x10 P \\x04\\x06\\x05\\x05 Y [ *\\x0f\\x04\\x07\\x06N\\x109K]\\x04U; \\x0b \\n\\x08 \\x06N\\x10 \\x08\\x08\\x07\\n\\t\\nY [ \\x0f* \\x04\\x07\\x06N\\x10e,4(\\x0c\\x0b\\'*\\x0f\\x04\\x07\\x06N\\x10 !b\\n#\\n\\x08 \\x07\\x06\\n\\nKW\\x04U;H\\x08\\x07\\x06 \\x10 is jointly dominated by KW\\x040;:h\\x0f\\x08\\x08\\x06 \\x10 \\x08\\x08\\t\\x07\\t\\x08\\t \\x08\\nKW\\x040;\\x03\\n\\x1e\\x08\\x08\\x06 \\x10\\n^-_\\x0f` \\x04 s.t. KW\\x040;H\\x08\\n\\x06N\\x10EP \\x04 \" Y \\x0b\\x0e\\n \\x0b K]\\x04U; \\x0b \\x08c\\x06N\\x10 \\x08\\x08\\x07 \\x06\\x14<\\nY \\x0b\\x0e\\n \\x0b , (\\x0c\\x0b \\n \\x0b \\x05 !b\\x08\\x08\\x07\\n\\t\\n\\nTable 1: Primal LP:\\n\\nTable 2: Dual LP: convex combination\\n\\nKW\\x04U;H\\x08 \\x10\\n\\nY\\n\\nwhen\\n\\n\\x04\\x06\\x05 ! .\\n\\n\\x0b \\n \\x0b KW\\x040; \\x0b \\x07\\x08 \\x06 \\x10 dominates KW\\x04U;H\\x08\\x07\\x06 \\x10 when \\x04\\x0e\\x05\\n\\n!.\\n\\nKW\\x04U; hN\\x08 \\x10 \\x08 \\x08\\nK]\\x04U; \\x1e\\x08 \\x10\\n*\\nKW\\x040; h \\x08\\n*V\\x10V\\x08 V\\x08cKW\\x04U; \\n\\x08 *V\\x10\\nKW\\x04U;H\\x08 \\x10\\n\\x0f\\n\\nProof:\\n\\x07\\x06 is dominated by\\n\\x08\\x06 \\x07\\t\\x08\\t\\x07\\t\\n\\x03\\n \\x07\\x06 when the objective of the LP in\\nTable 1 is positive. This LP finds the belief state that minimizes the difference between\\n\\x08\\t\\x07\\t\\x08\\t\\nand the max of\\n\\n . It turns out that the dual LP (Table 2) finds\\nthe most dominating convex combination parallel to\\n\\x08\\x06 . Since the dual has positive\\nobjective value when the primal does, the theorem follows.\\n\\nKW\\x040;H\\x08\\n*V\\x10\\n\\nAs argued in the proof of Thm. 1, the LP in Table 1 gives us an algorithm to find the most\\ndominating convex combination parallel to a dominated node. In summary, by considering\\nstochastic controllers, we can extend PI to prune all dominated nodes (pointwise or jointly)\\nin the policy improvement step. This provides two advantages: controllers can be made\\nsmaller while improving their decision quality.\\n\\n4 Bounded Policy Iteration\\nAlthough pruning all dominated nodes helps to keep the controller small, it may still grow\\nsubstantially with each DP backup. Several heuristics are possible to bound the number of\\nnodes. Feng and Hansen [6] proposed that one prunes all nodes that dominate the value\\nfunction by less than some after each DP backup. Alternatively, instead of growing the\\ncontroller with each backup and then pruning, we can do a partial DP backup that generates\\nonly a subset of the nodes using Cheng?s algorithm [5], the witness algorithm [9], or other\\nheuristics [14]. In order to keep the controller bounded, for each node created in a partial\\nDP backup, one node must be pruned and its inward arcs redirected to some dominating\\nconvex combination. In the event where no node is dominated, we can still prune a node\\nand redirect its arcs to a good convex combination, but the resulting controller may have\\nlesser value at some belief states. We now propose a new algorithm called bounded policy iteration (BPI) that guarantees monotonic value improvement at all belief states while\\nkeeping the number of nodes fixed.\\n\\n\\x04\\n\\nBPI considers one node at a time and tries to improve it while keeping all other nodes\\nfixed. Improvement is achieved by replacing each node by a good convex combination of\\nthe nodes normally created by a DP backup, but without actually performing a backup.\\nSince the backed up value function must dominate the controller?s current value function,\\nthen by Thm. 1 there must exist a convex combination of the backed up nodes that pointwise dominates each node of the controller. Combining this idea with Eq. 2, we can directly\\ncompute such convex combinations with the LP in Table 3. This LP has\\nvariables corresponding to the probabilities of the convex combination as well as the variable\\nmeasuring the value improvement. We can significantly reduce the number of variables\\nby pushing the convex combination variables as far as possible into the DP backup, resulting in the LP shown in Table 4. The key here is to realize that we can aggregate many\\nvariables since we only care about the marginals \\nand\\n\\n.\\n\\n\\n\\n\\x16 \\x01]\\x16 \\x16 54\\x04 \\x16 l mCl\\n\\nf \\x12 a\\x11\\x1d , Y a \\x10 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a\\x11\\x1d\\x1f\\x1e \\x10 \\x12 a\\x0c\\x1d! \\x10 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a\\x19\\x18 \\x1a\\x1b\\x18 f \\x12 a \\x10 \\x12 a \\x14 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a \\x18 \\x1a\\x1b\\x18\\n\\nf ,\\'Y a\\x11\\x10\\x13\\x12 a\\x15\\x14\\x13\\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a\\x19\\x18 \\x1a\\x1b\\x18 f \\x12 a \\x10 \\x12 a \\x14 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a\\x19\\x18 \\x1a\\x1c\\x18\\n\\n\\x0c^-_\\x0f` \\x04\\ns.t.\\n\\nK]\\x04U;H\\x08\\n\\x06N\\x10EP \\x04 \"OY f \\x12 a \\x10 S\\x12 a \\x14 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a \\x18 \\x1a\\x1b\\x18 \\n f \\x12 a \\x10\\x1f\\x12 a\\x15\\x14 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a \\x18 \\x1a\\x1b\\x18 \\x01 \\x1f \\x04\\x07\\x06\\tS \\x08\\n\\x1e\\x10\\nP\\n$ Y [\\x03\\x02 \\x12 \\x12\\x14\\x13\\x15\\x04\\x07\\x06+\\x0e \\x16 \\x06\\t\\x08\\n\\x1e\\x109\\x12\\x14\\x13\\x15\\x040\\x1c\\\\\\x16 \\x06+\\x0e\\x1a\\x08\\x0b\\n\\x15\\x109KW\\x04U; \\x08\\n\\x06+\\x0e \\x10\\x05\\x04\\x17\\x08 \\x07 \\x14\\x06 <\\nY f \\x12 a \\x10 \\x12 a \\x14 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a \\x18 \\x1a\\x1b\\x18 \\n f \\x12 a\\x11\\x10\\x1f\\x12 a\\x0c\\x14\\x13\\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a\\x19\\x18 \\x1a\\x1b\\x18 , (\\x11\\x0b \\n f \\x12 a\\x11\\x10\\x1f\\x12 a\\x15\\x14 \\x12\\x17\\x16\\x17\\x16\\x17\\x16 \\x12 a\\x19\\x18 \\x1a\\x1c\\x18 \\x05 !b\\x08 \\x07 \\x0c\\n \\x089; h \\x08\\x0b; \\x01 \\x08\\x08 \\t\\x07\\t\\x08\\t \\x089; l mCl\\n;\\n\\nTable 3: Naive LP to find a convex combination of backed up nodes that dominate .\\n\\n^-_ ` \\x04\\ns.t.\\n\\nS\\nS\\nKW\\x04U;H\\x08c\\x06N\\x10 P \\x04 \" Y f \\x01 \\n f \\x1f \\x12 a\\x04\\x07\\x06\\t\\x1d \\x08\\x0b\\n\\x15\\x10 Pg$ Y [ \\x02 \\x12 \\x12\\x14\\x05\\x13\\x1e\\x04\\x1a\\x06+\\x0e\\x17\\x16 \\x06\\t\\x08\\x0b\\n\\x15\\x10 \\x12\\x14\\x13\\x15\\x04\\x07\\x1c\\x0c\\x12 a \\x16 \\x1d \\x06+\\x0e0\\x05 \\x08\\n\\x1e\\x10 \\n f \\x12 a \\x1d KW\\x040; \\x08\\n\\x06+\\x0e \\x10\\x03\\x04\\x1a\\x08!\\x07 \\x06\\n!b\\x08 \\x07 \\n\\x0c\\x08\\x0b\\x1c\\nY f \\n f ,7(\\x11\\x0b Y \\x0ca \\x1d \\n f , \\n f \\x08 \\x07 \\x1b\\n \\x0b \\n f !b\\x08 \\x07\\\\\\n\\x1b\\x0b \\n f\\n;\\n\\nTable 4: Efficient LP to find a convex combination of backed up nodes that dominate .\\n\\nf \\x12 a\\x0c\\x1d\\n\\n\\x16 \\x01]\\x16 \\x16 \\x02>\\x16\\x11\\x16 54\\x16 Pg\\x16 \\x01]\\x16 P (\\n\\nThe efficient LP in Table 4 has only\\nvariables.1 Furthermore, the varihave an intuitive interpretation w.r.t. the action and observation strategies\\nables \\n and \\nfor the improved node. Each \\n variable indicates the probability of executing action (i.e.,\\nvariable indicates the (unnormalized) probability of\\n\\n ). Similarly, each \\nafter executing and observing (i.e.,\\nreaching node\\n\\n \\x07\\x06 \\n ). Note\\nthat we now use probabilistic action strategies and have extended probabilistic observation\\nstrategies to depend on the action executed.\\n\\nf\\n\\nf\\n\\nBT\\x04U;H\\x08\\x0b\\n\\x15\\x10 , f S\\n;\\n\\nf \\x12a \\x1d\\n\\n\\n\\n\\x1c\\n\\nS\\n\\nDG\\x04U;H\\x08\\n \\x08\\n\\x1c \\x08\\x0b; \\x10T, f \\x12 a \\x1d f\\n\\n\\n\\n\\nTo summarize, BPI alternates between policy evaluation and improvement as in regular PI,\\nbut the policy improvement step simply tries to improve each node by solving the LP in\\nTable 4. The \\n and \\nvariables are used to set the probabilistic action and observation\\nstrategies of the new improved node.\\n\\nf\\n\\nf \\x12 a\\x0c\\x1d\\n\\n5 Local Optima\\nBPI is a simple, efficient alternative to standard PI that monotonically improves an FSC\\nwhile keeping its size constant. Unfortunately, it is only guaranteed to converge to a local\\noptimum. We now characterize BPI?s local optima and propose a method to escape them.\\n5.1 Characterization\\nThm. 2 gives a necessary and sufficient condition characterizing BPI?s local optima. Intuitively, a controller is a local optimum when each linear segment touches from below, or is\\ntangent to, the controller?s backed up value function (see Fig. 1(b)).\\nTheorem 2 BPI has converged to a local optimum if and only if each node?s value function\\nis tangent to the backed up value function.\\n\\n\\x04\\n\\nProof: Since the objective function of the LP in Table 4 seeks to maximize the improvement , the resulting convex combination must be tangent to the upper surface of the\\nbacked up value function. Conversely, the only time when the LP won?t be able to improve\\na node is when its vector is already tangent to the backed up value function.\\n\\n\\x0f\\n\\n1\\n\\nActually, we don?t need the \\x08\\n\\t variables since they can be derived from the \\x08\\n\\t\\x0c\\x0b \\nsumming out \\x0e\\x10\\x0f , so the number of variables can be reduced to \\x11 \\x12\\x13\\x11\\x14\\x11 \\x15\\x13\\x11\\x14\\x11 \\x16\\x17\\x11\\x19\\x18\\x1b\\x1a .\\n\\n\\x1d\\n\\nvariables by\\n\\n\\x0cInterestingly, tangency is a necessary (but not sufficient) condition for GA?s local optima.\\nCorollary 1 If GA has converged to a local optimum, then the value function of each node\\nreachable from the initial belief state is tangent to the backed up value function.\\nProof: GA seeks to monotonically improve a controller in the direction of steepest ascent.\\nThe LP of Table 4 also seeks a monotonically improving direction. Thus if BPI can\\nimprove a controller by finding a direction of improvement using the LP of Table 4, then\\nGA will also find it or will find a steeper one. Conversely, when a controller is a local\\noptimum for GA, then there is no monotonic improvement possible in any direction. Since\\nBPI can only improve a controller by following a direction of monotonic improvement,\\nGA?s local optima are a subset of BPI?s local optima. Thus, tangency is a necessary, but\\nnot sufficient, condition of GA?s local optima.\\n\\n\\x0f\\n\\nIn the proof of Corollary 1, we argued that GA?s local optima are a subset of BPI?s local\\noptima. This suggests that BPI is inferior to GA since it can be trapped by more local\\noptima than GA. However we will describe in the next section a simple technique that\\nallows BPI to easily escape from local optima.\\n5.2 Escape Technique\\nThe tangency condition characterizing local optima can be used to design an effective escape method for BPI. It essentially tells us that such tangent belief states are ?bottlenecks?\\nfor further policy improvement. If we could improve the value at the tangent belief state(s)\\nof some node, then we could break out of the local optimum. A simple method for doing\\nso consists of a one-step lookahead search from the tangent belief states. Figure 1(b) illustrates how belief state can be reached in one step from tangent belief state , and how\\nthe backed up value function improves ?s current value. Thus, if we add a node to the\\ncontroller that maximizes the value of , its improved value can subsequently be backed\\nup to the tangent belief state , breaking out of the local optimum.\\n\\n*V\\x0e\\n\\n*\\n\\n*\\x0e\\n*\\x0e\\n\\n*\\n\\nOur algorithm is summarized as follows: perform a one-step lookahead search from each\\ntangent belief state; when a reachable belief state can be improved, add a new node to the\\ncontroller that maximizes that belief state?s value. Interestingly, when no reachable belief\\nstate can be improved, the policy must be optimal at the tangent belief states.\\nTheorem 3 If the backed up value function does not improve the value of any belief state\\nreachable in one step from any tangent belief state, then the policy is optimal at the tangent\\nbelief states.\\nProof: By definition, belief states for which the backed up value function provides no\\nimprovement are tangent belief states. Hence, when all belief states reachable in one step\\nare themselves tangent belief states, then the set of tangent belief states is closed under\\nevery policy. Since there is no possibility of improvement, the current policy must be\\noptimal at the tangent belief states.\\n\\n\\x0f\\n\\nAlthough Thm 3 guarantees an optimal solution only at the tangent belief states, in practice,\\nthey rarely form a proper subset of the belief space (when none of the reachable belief states\\ncan be improved). Note also that the escape algorithm assumes knowledge of the tangent\\nbelief states. Fortunately, the solution to the dual of the LP in Table 4 is a tangent belief\\nstate. Since most commercial LP solvers return both the solution of the primal and dual, a\\ntangent belief state is readily available for each node.2\\n2\\n\\nA node may have more than one tangent belief state when an interval of its linear segment is\\n\\n\\x0cMaze400\\n\\nTag?Avoid\\n\\nExpected Rewards\\n\\nExpected Rewards\\n\\n55\\n50\\n45\\n40\\n\\n?10\\n?20\\n?30\\n?40\\n?50\\n\\n35\\n0\\n\\n500\\n\\n1000\\n\\n1500\\n\\n0\\n\\n500\\n\\n1000\\n\\n1500\\n\\nNumber of nodes\\n\\nNumber of nodes\\n\\nMaze400\\n\\nTag?Avoid\\n\\nExpected Rewards\\n\\nExpected Rewards\\n\\n55\\n50\\n45\\n40\\n\\n?10\\n?20\\n?30\\n?40\\n?50\\n\\n35 0\\n10\\n\\n1\\n\\n2\\n\\n10\\n\\n10\\n\\n3\\n\\n10\\n\\n4\\n\\n10\\n\\n5\\n\\n10\\n\\n1\\n\\n10\\n\\n2\\n\\n10\\n\\nTime (seconds)\\n\\n3\\n\\n10\\n\\n4\\n\\n5\\n\\n10\\n\\n10\\n\\n6\\n\\n10\\n\\nTime (seconds)\\n\\nFigure 3: Experimental results for the maze and tag-avoid problems.\\n\\n6 Experiments\\nWe report some preliminary experiments with BPI and the escape method to assess their\\nrobustness against local optima, as well as their scalability to relatively large POMDPs.\\nIn a first experiment, we ran BPI with escape on a preference elicitation problem and a\\nmodified version of the Heaven-and-Hell problem described in [3]. It consistently found\\nthe optimal policy, whereas GA settles for a local optimum for both problems.\\nIn a second experiment, we report the running time and decision quality of the controllers found for two large grid-world problems. The first is a\\n-state extention of\\nHauskrecht?s [8] \\x01 -state maze problem, and the second Pineau et al.?s [12] \\x02\\x04\\x03 -state tagavoid problem. In Figure 3, we report the expected return achieved w.r.t. time and number\\nof nodes. For the maze problem, the expected return is averaged over all 400 states since\\nBPI tries to optimize the policy for all belief states simultaneously. For comparison purposes, the expected return for the tag-avoid problem is measured at the same initial belief\\nstate used in [12] even though BPI doesn?t tailor its policy exclusively to that belief state.\\nIn contrast, many point-based algorithms including PBVI [12] (which is perhaps the best\\nsuch algorithm) optimize the policy for a single initial belief state, capitalizing on a hopefully small reachable belief region. BPI found a \\x05\\x06 -node controller in \\x07\\x06\\x05\\x04\\x03\\x08\\x03\\x06\\x01 with the\\nsame expected return of \\t\\n\\x05 \\t \\x0b\\x02 achieved by PBVI in \\x0b\\x02 \\x08\\x02\\x0c\\x02\\nwith a policy of \\x0e\\n\\x0c\\n\\x06 linear\\nsegments. This suggests that most of the belief space is reachable in tag-avoid. We also\\n\\n\\t!\\x1b!\\n\\n!\\n\\n\\x11(\\n\\n\\t!\\n\\n( \\x1b! \\x1b!\\x1d\\x06\\n\\n\\x0f!\\n\\n\\t\\x06\\n(\\n\\ntangent to the backed up value function, indicating that it is identical to some backed up node.\\n\\n\\x0cran BPI on the tiger-grid, hallway and hallway2 benchmark problems [12] and obtained\\nand \\x01\\x0c\\x03 \\x01\\x08\\x02\\nachieving expected returns of\\n\\x0b\\x07 -node controllers in \\x01\\x08\\n\\x06\\x04\\x01 , \\x01 \\x0c\\x05\\x04\\x03 \\n\\t \\x02 , \\t \\x07 , \\t \\x01\\x06\\x02 at the same initial belief states used in [12], but without using them to\\ntailor the policy. In contrast, PBVI achieved expected returns of \\x01 \\t \\x01\\x08\\x07 , \\t \\x07\\x06\\n and \\t \\n\\x08 in\\nwith policies of \\x04\\x03 , \\x02\\x02 and \\x05 \\x07 linear segments tailored to those\\n\\n\\x06\\x0c\\x0c\\x02 , \\x01\\x08\\x02\\x08\\x02 and \\n\\x02\\ninitial belief states. This suggests that only a small portion of the belief space is reachable.\\n\\n( \\x1b!\\x1b!\\n( b( ! \\x15( !\\n\\x1d\\x06 \\t\\x06\\n\\n(\\n\\n\\t!\\t\\x06\\n\\n!\\x1d\\x06\\n\\n\\t!\\t\\x06\\n!\\n\\n\\x1b!\\t\\x06\\n\\n!\\n\\n!\\n\\n7 Conclusion\\nWe have introduced the BPI algorithm, which guarantees monotonic improvement of the\\nvalue function while keeping controller size fixed. While quite efficient, the algorithm may\\nget trapped in local optima. An analysis of such local optima reveals that the value function\\nof each node is tangent to the backed up value function. This property can be successfully\\nexploited in an algorithm that escapes local optima quite robustly.\\nThis research can be extented in a number of directions. State aggregation [2] and belief\\ncompression [13] techniques could be easily integrated with BPI to scale to problems with\\nlarge state spaces. Also, since stochastic GA [11, 1] can tackle model free problems (which\\nBPI cannot) it would be interesting to see if tangent belief states could be computed for\\nstochastic GA and used to design a heuristic to escape local optima similar to the one\\nproposed for BPI.\\nAcknowledgements We thank Darius Braziunas for his help with the implementation and the anonymous reviewers for the helpful comments.\\n\\nReferences\\n[1] D. Aberdeen and J. Baxter. Scaling internal-state policy-gradient methods for POMDPs. Proc.\\nICML-02, pp.3?10, Sydney, Australia, 2002.\\n[2] C. Boutilier and D. Poole. Computing optimal policies for partially observable decision processes using compact representations. Proc. AAAI-96, pp.1168?1175, Portland, OR, 1996.\\n[3] D. Braziunas. Stochastic local search for POMDP controllers. Master?s thesis, University of\\nToronto, Toronto, 2003.\\n[4] A. R. Cassandra, M. L. Littman, and N. L. Zhang. Incremental pruning: A simple, fast, exact\\nmethod for POMDPs. Proc.UAI-97, pp.54?61, Providence, RI, 1997.\\n[5] H.-T. Cheng. Algorithms for Partially Observable Markov Decision Processes. PhD thesis,\\nUniversity of British Columbia, Vancouver, 1988.\\n[6] Z. Feng and E. A. Hansen. Approximate planning for factored POMDPs. Proc. ECP-01, Toledo,\\nSpain, 2001.\\n[7] E. A. Hansen. Solving POMDPs by searching in policy space. Proc. UAI-98, pp.211?219,\\nMadison, Wisconsin, 1998.\\n[8] M. Hauskrecht. Value-function approximations for partially observable Markov decision processes. Journal of Artificial Intelligence Research, 13:33?94, 2000.\\n[9] L. P. Kaelbling, M. Littman, and A. R. Cassandra. Planning and acting in partially observable\\nstochastic domains. Artificial Intelligence, 101:99?134, 1998.\\n[10] N. Meuleau, K.-E. Kim, L. P. Kaelbling, and A. R. Cassandra. Solving POMDPs by searching\\nthe space of finite policies. Proc. UAI-99, pp.417?426, Stockholm, 1999.\\n[11] N. Meuleau, L. Peshkin, K.-E. Kim, and L. P. Kaelbling. Learning finite-state controllers for\\npartially observable environments. Proc. UAI-99, pp.427?436, Stockholm, 1999.\\n[12] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: an anytime algorithm for\\nPOMDPs. In Proc. IJCAI-03, Acapulco, Mexico, 2003.\\n[13] P. Poupart and C. Boutilier. Value-directed compressions of POMDPs. Proc. NIPS-02, pp.1547?\\n1554, Vancouver, Canada, 2002.\\n[14] N. L. Zhang and W. Zhang. Speeding up the convergence of value-iteration in partially observable Markov decision processes. Journal of Artificial Intelligence Research, 14:29?51, 2001.\\n\\n\\x0c',\n",
       "     'pdf_name': '2372-bounded-finite-state-controllers.pdf',\n",
       "     'title': 'Bounded Finite State Controllers',\n",
       "     'year': '2003'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2373',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '2373',\n",
       "     'paper_text': 'Minimax embeddings\\nMatthew Brand\\nMitsubishi Electric Research Labs\\nCambridge MA 02139 USA\\n\\nAbstract\\nSpectral methods for nonlinear dimensionality reduction (NLDR) impose\\na neighborhood graph on point data and compute eigenfunctions of a\\nquadratic form generated from the graph. We introduce a more general\\nand more robust formulation of NLDR based on the singular value decomposition (SVD). In this framework, most spectral NLDR principles\\ncan be recovered by taking a subset of the constraints in a quadratic form\\nbuilt from local nullspaces on the manifold. The minimax formulation\\nalso opens up an interesting class of methods in which the graph is ?decorated? with information at the vertices, offering discrete or continuous\\nmaps, reduced computational complexity, and immunity to some solution instabilities of eigenfunction approaches. Apropos, we show almost\\nall NLDR methods based on eigenvalue decompositions (EVD) have a solution instability that increases faster than problem size. This pathology\\ncan be observed (and corrected via the minimax formulation) in problems\\nas small as N < 100 points.\\n\\n1\\n\\nNonlinear dimensionality reduction (NLDR)\\n\\n.\\nSpectral NLDR methods are graph embedding problems where a set of N points X =\\n[x1 , ? ? ? , xN ] ? RD?N sampled from a low-dimensional manifold in a ambient space RD is\\nreparameterized by imposing a neighborhood graph G on X and embedding the graph with\\nminimal distortion in a ?parameterization? space Rd , d < D. Typically the graph is sparse\\nand local, with edges connecting points to their immediate neighbors. The embedding must\\nkeep these edges short or preserve their length (for isometry) or angles (for conformality).\\nThe graph-embedding problem was first introduced as a least-squares problem by Tutte [1],\\nand as an eigenvalue problem by Fiedler [2]. The use of sparse graphs to generate metrics\\nfor least-squares problems has been studied intensely in the following three decades (see\\n[3]). Modern NLDR methods use graph constraints to generate a metric in a space of embeddings RN . Eigenvalue decomposition (EVD) gives the directions of least or greatest variance\\nunder this metric. Typically a subset of d extremal eigenvectors gives the embedding of N\\npoints in Rd parameterization space. This includes the IsoMap family [4], the locally linear\\nembedding (LLE) family [5,6], and Laplacian methods [7,8]. Using similar methods, the\\nAutomatic Alignment [6] and Charting [9] algorithms embed local subspaces instead of\\npoints, and by combining subspace projections thus obtain continuous maps between RD\\nand Rd .\\nThis paper introduces a general algebraic framework for computing optimal embeddings\\ndirectly from graph constraints. The aforementioned methods can can be recovered as special cases. The framework also suggests some new methods with very attractive properties,\\nincluding continuous maps, reduced computational complexity, and control over the degree\\n\\n\\x0cof conformality/isometry in the desired map. It also eliminates a solution instability that is\\nintrinsic to EVD-based approaches. A perturbational analysis quantifies the instability.\\n\\n2\\n\\nMinimax theorem for graph embeddings\\n\\nWe begin with neighborhood graph specified by a nondiagonal weighted adjacency matrix\\nM ? RN?N that has the data-reproducing property XM = X (this can be relaxed to XM ?\\nX in practice). The graph-embedding and NLDR literatures offer various constructions of\\nM, each appropriate to different sets of assumptions about the original embedding and\\nits sampling X (e.g., isometry, local linearity, noiseless samples, regular sampling, etc.).\\nTypically Mi j 6= 0 if points i, j are nearby on the intrinsic manifold and |Mi j | is small or\\nzero otherwise. Each point is taken to be a linear or convex combination of its neighbors,\\nand thus M specifies manifold connectivity in the sense that any nondegenerate embedding\\nY that satisfies YM ? Y with small residual kYM ? YkF will preserve this connectivity\\nand the structure of local neighborhoods. For example, in barycentric embeddings, each\\npoint is the average of its neighbors and thus Mi j = 1/k if vertex i is connected to vertex j\\n(of degree k). We will also consider three optional constraints on the embedding :\\n1. A null-space restriction, where the solution must be outside to the column-space\\nof C ? RN?M , M < N. For example, it is common to stipulate that the solution Y\\nbe centered, i.e., YC = 0 for C = 1, the constant vector.\\n2. A basis restriction, where the solution must be a linear combination of the rows\\nof basis Z ? RK?N , K ? N. This can be thought of as information placed at the\\nvertices of the graph that serves as example inputs for a target NLDR function. We\\nwill use this to construct dimension-reducing radial basis function networks.\\n3. A metric ? ? RN?N that determines how error is distributed over the points. For\\nexample, it might be important that boundary points have less error. We assume\\nthat ? is symmetric positive definite and has factorization ? = AA> (e.g., A could\\nbe a Cholesky factor of ?).\\nIn most settings, the optional matrices will default to the identity matrix. In this context,\\nwe define the per-dimension embedding error of row-vector yi ? rows(Y) to be\\n.\\n\\nEM (yi ) =\\n\\nmax\\n\\nyi\\n\\n?range(Z),, K?RM?N\\n\\nk(yi (M + CD) ? yi )Ak\\nkyi Ak\\n\\n(1)\\n\\nwhere D is a matrix constructed by an adversary to maximize the error. The optimizing yi\\nis a vector inside the subspace spanned by the rows of Z and outside the subspace spanned\\nby the columns of C, for which the reconstruction residual yi M?yi has smallest norm\\nw.r.t. the metric ?. The following theorem identifies the optimal embedding Y for any\\nchoice of M, Z, C, ?:\\nMinimax solution: Let Q ? SK?P be a column-orthonormal basis of the null-space of the\\nrows of ZC, with P = K ? rank(C). Let B ? RP?P be a square factor satisfying B> B =\\nQ> Z?Z> Q, e.g., a Cholesky factor (or the ?R? factor in QR-decomposition of (Q> ZA)> ).\\nCompute the left singular vectors U ? SN?N of Udiag(s)V> = B?> Q> Z(I ? M)A, with\\n.\\nsingular values s> = [s1 , ? ? ? , sP ] ordered s1 ? s2 ? ? ? ? ? s p . Using the leading columns\\n?> Q> Z.\\nU1:d of U, set Y = U>\\n1:d B\\nTheorem 1. Y is the optimal (minimax) embedding in Rd with error k[s1 , ? ? ? , sd ]k2 :\\n.\\n?> >\\nY = U>\\nQ Z = arg min\\n? EM (yi )2 with EM (yi ) = si .\\n1:d B\\nY?Rd?N y ?rows(Y)\\ni\\n\\n(2)\\n\\n\\x0cAppendix A develops the proof and other error measures that are minimized.\\nLocal NLDR techniques are easily expressed in this framework. When Z = A = I, C = [],\\nand M reproduces X through linear combinations with M> 1 = 1, we recover LLE [5].\\nWhen Z = I, C = [], I ? M is the normalized graph Laplacian, and A is a diagonal matrix\\nof vertex degrees, we recover Laplacian eigenmaps [7]. When further Z = X we recover\\nlocally preserving projections [8].\\n\\n3\\n\\nAnalysis and generalization of charting\\n\\nThe minimax construction of charting [9] takes some development, but offers an interesting insight into the above-mentioned methods. Recall that charting first solves for a set\\nof local affine subspace axes S1 ? RD?d , S2 , ? ? ? at offsets ?1 ? RD , ?2 , ? ? ? that best cover\\nthe data and vary smoothly over the manifold. Each subspace offers a chart?a local parameterization of the data by projection onto the local axes. Charting then constructs a\\nweighted mixture of affine projections that merges the charts into a global parameterization. If the data manifold is curved, each projection will assign a point a slightly different\\nembedding, so the error is measured as the variance of these proposed embeddings about\\ntheir mean. This maximizes consistency and tends to produce isometric embeddings; [9]\\ndiscusses ways to explicitly optimize the isometry of the embedding.\\nUnder the assumption of isometry, the charting error is equivalent to the sumsquared displacements of an embedded point relative to its immediate neighbors\\n(summed over all neighborhoods). To construct the same error criteria in the minimax setting, let xi?k , ? ? ? , xi , ? ? ? , xi+k denote points in the ith neighborhood and let\\nthe columns of Vi ? R(2k+1)?d be an orthonormal basis of rows of the local parameterization S>\\ni [xi?k , ? ? ? , xi , ? ? ? , xi+k ]. Then a nonzero reparameterization will satisfy\\n[yi?k , ? ? ? , yi , ? ? ? , yi+k ]Vi V>\\ni = [yi?k , ? ? ? , yi , ? ? ? , yi+k ] if and only if it preserves the relative\\nposition of the points in the local parameterization. Conversely, any relative displacements\\nof the points are isolated by the formula [yi?k , ? ? ? , yi , ? ? ? , yi+k ](I ? Vi V>\\ni ). Minimizing\\nthe Frobenius norm of this expression is thus equivalent to minimizing the local error in\\ncharting. We sum these constraints over all neighborhoods to obtain the constraint matrix\\n>\\nth\\nth\\nM = I ? ?i Fi (I ? Vi V>\\ni )Fi , where (Fi )k j = 1 iff the j point of the i neighborhood is\\n>\\nthe kth point of the dataset. Because Vi V>\\ni and (I ? Vi Vi ) are complementary, it follows\\nthat the error criterion of any local NLDR method (e.g., LLE, Laplacian eigenmaps, etc.)\\nmust measure the projection of the embedding onto some subspace of (I ? Vi V>\\ni ).\\nTo construct a continuous map, charting uses an overcomplete radial basis function (RBF)\\nrepresentation Z = [z(x1 ), z(x2 ), ? ? ? z(xN )], where z(x) is a vector that stacks z1 (x), z2 (x),\\netc., and\\n\\x14\\n\\x15\\npm (x)\\n. K>\\nm (x ? ?m )\\n,\\n(3)\\nzm (x) =\\n1\\n?m pm (x)\\n> ?1\\n.\\npm (x) = N (x|?m , ?m ) ? e?(x??m ) ?m (x??m )/2\\n(4)\\nand Km is any local linear dimensionality reducer, typically Sm itself. Each column of Z\\ncontains many ?views? of the same point that are combined to give its low-dimensional\\nembedding.\\nFinally, we set C = 1, which forces the embedding of the full data to be centered.\\nApplying the minimax solution to these constraints yields the RBF network mixing ma.\\n?> Q> z(x). Theorem 1 guarantees that the resulting embedding is leasttrix, f (x) = U>\\n1:d B\\nsquares optimal w.r.t. Z, M, C, A at the datapoints f (xi ), and because f (?) is an affine transform of z(?) it smoothly interpolates the embedding between points.\\nThere are some interesting variants:\\n\\n\\x0cKernel embeddings of the twisted swiss roll\\ngeneralized EVD\\nminimax SVD\\n\\nUR corner detail\\n\\nLL corner detail\\n\\nFig. 1. Minimax and generalized EVD solution for kernel eigenmap of a non-developable\\nswiss roll. Points are connected into a grid which ideally should be regular. The EVD solution shows substantial degradation. Insets detail corners where the EVD solution crosses\\nitself repeatedly. The border compression is characteristic of Laplacian constraints.\\nOne-shot charting: If we set the local dimensionality reducers to the identity matrix (all\\nKm = I), then the minimax method jointly optimizes the local dimensionality reduction to\\ncharts and the global coordination of the charts (under any choice of M). This requires that\\nrows(Z) ? N for a fully determined solution.\\nDiscrete isometric charting: If Z = I then we directly obtain a discrete isometric embedding of the data, rather than a continuous map, making this a local equivalent of IsoMap.\\nReduced basis charting: Let Z be constructed using just a small number of kernels randomly placed on the data manifold, such that rows(Z) \\x1c N. Then the size of the SVD\\nproblem is substantially reduced.\\n\\n4\\n\\nNumerical advantage of minimax method\\n\\nNote that the minimax method projects the constraint matrix M into a subspace derived\\nfrom C and Z and decomposes it there. This suppresses unwanted degrees of freedom\\n(DOFs) admitted by the problem constraints, for example the trivial R0 embedding where\\nall points are mapped to a single point yi = N ?1/2 . The R0 embedding serves as a translational DOF in the solution. LLE- and eigenmap-based methods construct M to have a\\nconstant null-space so that the translational DOF will be isolated in the EVD as null eigenvalue paired to a constant eigenvector, which is then discarded. However, section 4.1 shows\\nthat this construction makes the EVD increasingly unstable as problem size grows and/or the\\ndata becomes increasing amenable to low-residual embeddings, ultimately causing solution\\ncollapse. As the next paragraph demonstrates, the problem is exacerbated when embedding\\nw.r.t. a basis Z (via the equivalent generalized eigenproblem), partly because the eigenvector associated with the unwanted DOF can have arbitrary structure. In all cases the problem\\ncan be averted by using the minimax formulation with C = 1 to suppress the DOF.\\nA 2D plane was embedded in 3D with a curl, a twist, and 2.5% Gaussian noise, then regularly sampled at 900 points. We computed a kernelized Laplacian eigenmap using 70 random points as RBF centers, i.e., a continous map using M derived from the graph Laplacian\\nand Z constructed as above. The map was computed both via the minimax (SVD) method\\nand via the equivalent generalized eigenproblem, where the translational degree of freedom\\nmust be removed by discarding an eigenvector from the solution. The two solutions are algebraically equivalent in every other regard. A variety of eigensolvers were tried; we took\\n\\n\\x0c?5\\n\\nexcess energy\\n\\nx 10\\n\\nEigen spectrum compared to minimax spectrum\\n\\n15\\n10\\n5\\n0\\n\\n?5\\n\\nEigen spectrum compared to minimax spectrum\\n\\n2\\n\\n15\\ndeviation\\n\\nexcess energy\\n\\nx 10\\n\\n10\\n5\\n\\n100\\n200\\neigenvalue\\nError in null embedding\\n\\n?5\\n\\nx 10\\n0\\n?2\\n?4\\n?6\\n?8\\n\\n0\\n\\n100\\n\\n?5\\n\\neigenvalue\\nError in null embedding\\n\\n200\\n\\n100\\n\\n200\\n\\n300\\n\\n400 500\\npoint\\n\\n600\\n\\n700\\n\\n800\\n\\n900\\n\\nFig. 2. Excess energy in the eigenspectrum indicates that the translational DOF has contam2\\ninated\\nmany eigenvectors. If the EVD had successfully isolated the unwanted DOF, then its\\n0\\nremaining\\neigenvalues should be identical to those derived from the minimax solution. The\\n?2\\n?4 at left shows the difference in the eigenspectra. The graph at right shows the EVD\\ngraph\\n?6\\nsolution?s\\ndeviation from the translational vector y0 = 1 ? N ?1/2 ? .03333. If the numer?8\\nics were100\\nperfect\\nflat,800but900in practice the deviation is significant enough\\n200 the\\n300 line\\n400 would\\n500 600 be700\\npoint\\n(roughly 1% of the diameter\\nof the embedding) to noticably perturb points in figure 1.\\ndeviation\\n\\nx 10\\n\\nthe best result. Figure 1 shows that the EVD solution exhibits many defects, particularly a\\nfolding-over of the manifold at the top and bottom edges and at the corners. Figure 2 shows\\nthat the noisiness of the EVD solution is due largely to mutual contamination of numerically\\nunstable eigenvectors.\\n4.1\\n\\nNumerical instability of eigen-methods\\n\\nThe following theorem uses tools of matrix perturbation theory to show that as the problem size increases, the desired and unwanted eigenvectors become increasingly wobbly\\nand gradually contaminate each other, leading to degraded solutions. More precisely, the\\nlow-order eigenvalues are ill-conditioned and exhibit multiplicities that may be true (due\\nto noiseless samples from low-curvature manifolds) or false (due to numerical noise). Although in many cases some post-hoc algebra can ?filter? the unwanted components out\\nof the contaminated eigensolution, it is not hard to construct cases where the eigenvectors\\ncannot be cleanly separated. The minimax formulation is immune to this problem because\\nit explicitly suppresses the gratuitous component(s) before matrix decomposition.\\nTheorem 2. For any finite numerical precision, as the number of points N increases, the\\nFrobenius norm of numerical noise in the null eigenvector v0 can grow as O(N 3/2 ), and\\nthe eigenvalue problem can approach a false multiplicity at a rate as fast as O(N 3/2 ),\\nat which point the eigenvectors of interest?embedding and translational?are mutually\\ncontaminated and/or have an indeterminate eigenvalue ordering.\\nPlease see appendix B for the proof. This theorem essentially lower-bounds an upperbound on error; examples can be constructed in which the problem is worse. For example, it can be shown analytically that when embedding points drawn from the simple curve\\nxi = [a, cos ?a]> , a ? [0, 1] with K = 2 neighbors, instabilities cannot be bounded better\\nthan O(N 5/2 ); empirically we see eigenvector mixing with N < 100 points and we see\\nit grow at the rate ? O(N 4 )?in many different eigensolvers. At very large scales, more\\npernicious instabilities set in. E.g., by N = 20000 points, the solution begins to fold over.\\nAlthough algebraic multiplicity and instability of the eigenproblem is conceptually a minor\\noversight in the algorithmic realizations of eigenfunction embeddings, as theorem 2 shows,\\nthe consequences are eventually fatal.\\n\\n5\\n\\nSummary\\n\\nOne of the most appealing aspects of the spectral NLDR literature is that algorithms are\\nusually motivated from analyses of linear operators on smooth differentiable manifolds,\\ne.g., [7]. Understandably, these analysis rely on assumptions (e.g., smoothness or isometry\\n\\n\\x0cor noiseless sampling) that make it difficult to predict what algorithmic realizations will do\\nwhen real, noisy data violates these assumptions. The minimax embedding theorem provides a complete algebraic characterization of this discrete NLDR problem, and provides\\na solution that recovers numerically robustified versions of almost all known algorithms.\\nIt offers a principled way of constructing new algorithms with clear optimality properties\\nand good numerical conditioning?notably the construction of a continuous NLDR map (an\\nRBF network) in a one-shot optimization ( SVD ). We have also shown how to cast several\\nlocal NLDR principles in this framework, and upgrade these methods to give continuous\\nmaps. Working in the opposite direction, we sketched the minimax formulation of isometric charting and showed that its constraint matrix contains a superset of all the algebraic\\nconstraints used in local NLDR techniques.\\n\\nReferences\\n1. W.T. Tutte. How to draw a graph. Proc. London Mathematical Society, 13:743?768, 1963.\\n2. Miroslav Fiedler. A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory. Czech. Math. Journal, 25:619?633, 1975.\\n3. Fan R.K. Chung. Spectral graph theory, volume 92 of CBMS Regional Conference Series in\\nMathematics. American Mathematical Society, 1997.\\n4. Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for\\nnonlinear dimensionality reduction. Science, 290:2319?2323, December 22 2000.\\n5. Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear\\nembedding. Science, 290:2323?2326, December 22 2000.\\n6. Yee Whye Teh and Sam T. Roweis. Automatic alignment of hidden representations. In Proc.\\nNIPS-15, 2003.\\n7. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data\\nrepresentation. volume 14 of Advances in Neural Information Processing Systems, 2002.\\n8. Xiafei He and Partha Niyogi. Locality preserving projections. Technical Report TR-2002-09,\\nUniversity of Chicago Computer Science, October 2002.\\n9. Matthew Brand. Charting a manifold. volume 15 of Advances in Neural Information Processing\\nSystems, 2003.\\n10. G.W. Stewart and Ji-Guang Sun. Matrix perturbation theory. Academic Press, 1990.\\n\\nA\\n\\nProof of minimax embedding theorem (1)\\n\\nThe burden of this proof is carried by supporting lemmas, below. To emphasize the proof\\nstrategy, we give the proof first; supporting lemmas follow.\\nProof. Setting yi = l>\\ni Z, we will solve for li ? columns(L). Writing the error in terms of li ,\\n\\nEM (li ) = max\\n\\nK?RM?N\\n\\n>\\nkl>\\nkl>\\ni Z(I ? M ? CK)Ak\\ni Z(I ? M)A ? li ZCKAk\\n=\\nmax\\n.\\nkl>\\nkl>\\nK?RM?N\\ni ZAk\\ni ZAk\\n\\n(5)\\n\\n>\\nThe term l>\\ni ZCKA produces infinite error unless li ZC = 0, so we accept this as a constraint and seek\\nkl>\\ni Z(I ? M)Ak\\nmin\\n.\\n(6)\\n>\\nkl>\\nli ZC=0\\ni ZAk\\nBy lemma 1, that orthogonality is satisfied by solving the problem in the space orthogonal\\n.\\nto ZC; the basis for this space is given by columns of Q = null((ZC)> ).\\n\\nBy lemma 2, the denominator of the error specifies the metric in solution space to be\\nZAA> Z> ; when the problem is projected into the space orthogonal to ZC it becomes\\nQ> (ZAA> Z> )Q. Nesting the ?orthogonally-constrained-SVD? construction of lemma 1\\n\\n\\x0cinside the ?SVD-under-a-metric? lemma 2, we obtain a solution that uses the correct metric\\nin the orthogonal space:\\nB> B = Q> ZAA> Z> Q\\n>\\n\\n?>\\n\\nUdiag(s)V = B\\n\\n{Q(Z(I ? M)A)}\\n\\n(7)\\n(8)\\n\\nL = QB?1 U\\n(9)\\nwhere braces indicate the nesting of lemmas. By the ?best-projection? lemma (#3), if we\\norder the singular values by ascending magnitude,\\nq\\n(10)\\nL1:d = arg min\\n?ji ?cols(J) (kj> Z(I ? M)Ak/kjkZ?Z> )2\\nJ?RN?d\\n\\nThe proof is completed by making the substitutions L> Z ? Y> and kx> Ak ? kxk? (for\\n? = AA> ), and leaving off the final square root operation to obtain\\n\\x10\\n\\x112\\n(Y> )1:d = arg min ?ji ?cols(J) kj> (I ? M)k? /kjk? .\\n(11)\\nJ?RN?d\\n\\nLemma 1. Orthogonally constrained SVD: The left singular vectors L of matrix M under\\n.\\nSVD\\nthe constraint U> C = 0 are calculated as Q = null(C> ), Udiag(s)V> ? Q> M, L = QU.\\nProof. First observe that L is orthogonal to C: By definition, the null-space basis satisfies\\nQ> C = 0, thus L> C = U> Q> C = 0. Let J be an orthonormal basis for C, with J> J = I\\nand Q> J = 0. Then Ldiag(s)V> = QQ> M = (I ? JJ> )M, the orthogonal projector of C\\napplied to M, proving that the SVD captures the component of M that is orthogonal to C.\\nLemma 2. SVD with respect to a metric: The vectors li ? L, vi ? V that diagonalize matrix\\nM with respect to positive definite column-space metric ? are calculated as B> B ? ?,\\nSVD\\n.\\nUdiag(s)V> ? B?> M, L = B?1 U satisfy kl>\\ni Mk/kli k? = si and extremize this form for\\nthe extremal singular values smin , smax .\\nProof. By construction, L and V diagonalize M:\\nL> MV = (B?1 U)> MV = U> (B?> M)V = diag(s)\\ndiag(s)V>\\n\\n(12)\\n\\nB?> M.\\n\\nand\\n=\\nForming the gram matrices of both sides of the last line, we\\nobtain the identity Vdiag(s)2 V> = M> B?1 B?> M = M> ??1 M, which demonstrates that\\nsi ? s are the singular values of M w.r.t. column-space metric ?. Finally, L is orthonormal\\nw.r.t. the metric ?, because kLk2? = L> ?L = U> B?> B> BB?1 U = I. Consequently,\\nkl> Mk/klk? = kl> Mk/1 = ksi v>\\ni k = si .\\nand by the Courant-Hilbert theorem,\\nsmax = max kl> Mk/klk? ;\\n\\nsmin = min kl> Mk/klk? .\\nl\\n\\nl\\n\\n(13)\\n(14)\\n\\nLemma 3. Best projection: Taking L and s from lemma 2, let the columns of L and elements of s be sorted so that s1 ? s2 ? ? ? ? ? sN . Then for any dimensionality 1 ? d ? N,\\n.\\nL1:d = [l1 , ? ? ? , ld ] = arg max kJ> Mk(J> ?J)?1\\n(15)\\nJ?RN?d\\n\\n= arg\\n\\nmax\\n\\nJ?RN?d |J> ?J=I\\n\\n= arg max\\n\\nJ?RN?d\\n\\nkJ> MkF\\n\\nq\\n?ji ?cols(J) (kj> Mk/kjk? )2\\n\\n(16)\\n(17)\\n\\nwith the optimum value of all right hand sides being (?di=1 s2i )1/2 . If the sort order is reversed, the minimum of this form is obtained.\\n\\n\\x0cProof. By the Eckart-Young-Mirsky theorem, if U> MV = diag(s) with singular values\\n.\\nsorted in descending order, then U1:d = [u1 , ? ? ? , ud ] = arg maxU?SN?d kU> MkF . We first\\nextend this to a non-orthonogonal basis J under a Mahalonobis norm:\\nmaxJ?RN?d kJ> Mk(J> J)?1 = maxU?SN?d kU> MkF\\n(18)\\nbecause\\n\\nkJ> Mk2(J> J)?1 = trace(M> J(J> J)?1 J> M) = trace(M> JJ+ (JJ+ )> M) =\\n\\nk(JJ+ )Mk2F = kUU> Mk2F = kU> Mk2F since JJ+ is a (symmetric) orthogonal projector having binary eigenvalues ? ? {0, 1} and therefore it is the gram of an thin\\northogonal matrix. We then impose a metric ? on the column-space of J to obtain\\nthe first criterion (equation 15), which asks what maximizes variance in J> M while\\nminimizing the norm of J w.r.t. metric ?. Here it suffices to substitute in the leading\\n(resp., trailing) columns of L and verify that the norm is maximized (resp., mini2\\n>\\n> >\\n?1 >\\nmized). Expanding, kL>\\n1:d Mk(L> ?L )?1 = trace((L1:d M) (L1:d ?L1:d ) (L1:d M)) =\\n1:d\\n\\n1:d\\n\\n>\\n>\\n> >\\n>\\n2\\ntrace((L>\\n1:d M) I(L1:d M)) = trace((diag(s1:d )V1:d ) (diag(s1:d )V1:d )) = ks1:d k . Again,\\nby the Eckart-Young-Mirsky theorem, these are the maximal variance-preserving projections, so the first criterion is indeed maximized by setting J to the columns in L\\ncorresponding to the largest values in s.\\n\\nCriterion #2 restates the first criterion with the set of candidates for J restricted to (the hyperelliptical manifold of) matrices that reduce the metric on the norm to the identity matrix\\n(thereby recovering the Frobenius norm). Criterion #3 criterion merely expands the above\\ntrace by individual singular values. Note that the numerator and denominator can have different metrics because they are norms in different spaces, possibly of different dimension.\\nFinally, that the trailing d eigenvectors minimize these criteria follows directly from the fact\\nthat leading N ? d singular values account for the maximal part of the variance.\\n\\nB\\n\\nProof of instability theorem (2)\\n\\nProof. When generated from a sparse graph with average degree K, weighted connectivity\\nmatrix W is sparse and has O(NK) entries. Since the graph vertices represent samples from\\na smooth manifold, increasing the sampling density N does not change the distribution of\\nmagnitudes in W. Consider a perturbation of the nonzero values in W, e.g., W ? W + E\\ndue to numerical noise E created by finite machine precision. By the weak law\\n? of large\\nnumbers, the Frobenius norm of the sparse perturbation grows as kEkF ? O( N). However the t th -smallest nonzero eigenvalue ?t (W) grows as ?t (W) = vt> Wvt ? O(N ?1 ), because elements of corresponding eigenvector vt grow as O(N ?1/2 ) and only K of those\\nelements are multiplied by nonzero values to form each element of Wvt . In sum, the perturbation kEkF grows while the eigenvalue ?t (W) shrinks. In linear embedding algorithms,\\n.\\nthe eigengap of interest is ?gap = ?1 ? ?0 . The tail eigenvalue ?0 = 0 by construction but\\nit is possible that ?0 > 0 with numerical error, thus ?gap ? ?1 . Combining these facts,\\nthe ratio between the perturbation and the eigengap grows as kEkF /?gap ? O(N 3/2 ) or\\nfaster. Now consider the shifted eigenproblem I ? W with leading (maximal) eigenvalues 1 ? ?0 ? 1 ? ?1 ? ? ? ? and unchanged eigenvectors. From matrix perturbation the.\\nory [10, thm. V.2.8], when W is perturbed to W0 = W + E, the\\n? change in the lead0 is bounded as |?0 ? ? | ?\\ning eigenvalue from\\n1\\n?\\n?\\nto\\n1\\n?\\n?\\n2kEkF and similarly\\n0\\n0\\n0\\n0\\n?\\n?\\n1 ? ?01 ? 1 ? ?1 + 2kEkF . Thus ?0gap ? ?gap ? 2kEkF . Since kEkF /?gap ? O(N 3/2 ),\\nthe right hand side of the gap bound goes negative at a supralinear rate, implying that the\\neigenvalue ordering eventually becomes unstable with the possibility of the first and second\\neigenvalue/vector pairs being swapped. Mutual contamination of the eigenvectors happens\\nwell before: Under general (dense) conditions, the change in the eigenvector v0 is bounded\\n?F\\nas kv00 ? v0 k ? |? ??4kEk\\n[10, thm. V.2.8]. (This bound is often tight enough to serve\\n0\\n1 |? 2kEkF\\nas a good approximation.) Specializing this to the\\nsparse embedding\\nmatrix, we find that\\n?\\n?\\nO( N) ?\\nO( N)\\n0\\n?1/2\\nthe bound weakens to kv0 ? 1 ? N\\nk ? O(N ?1 )?O( N) > O(N ?1 ) = O(N 3/2 ).\\n\\n\\x0c',\n",
       "     'pdf_name': '2373-minimax-embeddings.pdf',\n",
       "     'title': 'Minimax Embeddings',\n",
       "     'year': '2003'},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': 7241},\n",
       " 'timed_out': False,\n",
       " 'took': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res = es.search(index = \"nips_papers\", body = { \"size\": 25})\n",
    "sample_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-set analysis: each year in aggregation of years contains an aggregation of significant terms\n",
    "sig_terms_res = es.search(index = \"nips_papers\", body = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"keywords_by_years\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"year\",\n",
    "                \"size\": 50\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"keywords\": {\n",
    "                    \"significant_terms\": {\n",
    "                        \"field\": \"title\",\n",
    "                        \"size\": 50\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "lst = [] #flatten the dictionary into a list\n",
    "\n",
    "sig_dict = sig_terms_res['aggregations']['keywords_by_years']['buckets']\n",
    "\n",
    "for year in sig_dict:\n",
    "    words = year['keywords']['buckets']\n",
    "    for word in words:\n",
    "        line = {\n",
    "            'score': word['score'],\n",
    "            'year': year['key'],\n",
    "            'doc_count': word['doc_count'],\n",
    "            'keyword': word['key']\n",
    "        }\n",
    "        lst.append(line)\n",
    "\n",
    "with open('sig_terms_res.csv', 'w', newline='') as f:\n",
    "    header_present = False\n",
    "    for doc in lst:\n",
    "        if not header_present:\n",
    "            w = csv.DictWriter(f, doc.keys())\n",
    "            w.writeheader()\n",
    "            header_present = True\n",
    "        w.writerow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-set analysis: each year in aggregation of years contains an aggregation of general (total) terms\n",
    "tot_terms_res = es.search(index = \"nips_papers\", body = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"keywords_by_years\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"year\",\n",
    "                \"size\": 50\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"keywords\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"title\",\n",
    "                        \"size\": 50\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "lst2 = [] #flatten the dictionary into a list\n",
    "\n",
    "tot_dict = tot_terms_res['aggregations']['keywords_by_years']['buckets']\n",
    "\n",
    "for year in tot_dict:\n",
    "    words = year['keywords']['buckets']\n",
    "    for word in words:\n",
    "        line = {\n",
    "            'year': year['key'],\n",
    "            'doc_count': word['doc_count'],\n",
    "            'keyword': word['key']\n",
    "        }\n",
    "        lst2.append(line)\n",
    "\n",
    "with open('tot_terms_res.csv', 'w', newline='') as f:\n",
    "    header_present = False\n",
    "    for doc in lst2:\n",
    "        if not header_present:\n",
    "            w = csv.DictWriter(f, doc.keys())\n",
    "            w.writeheader()\n",
    "            header_present = True\n",
    "        w.writerow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
