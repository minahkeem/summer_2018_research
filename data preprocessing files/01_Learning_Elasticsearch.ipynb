{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch([{'host':'nyuvis-web.poly.edu', 'port': 80, 'url_prefix':'es'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '1001',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1001',\n",
       "     'paper_text': 'Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders Krogh\"\\nNordita\\nBlegdamsvej 17\\n2100 Copenhagen, Denmark\\n\\nJesper Vedelsby\\nElectronics Institute, Building 349\\nTechnical University of Denmark\\n2800 Lyngby, Denmark\\n\\nAbstract\\nLearning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity\\nis defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among\\nthe networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble\\ngeneralization error, and how this type of ensemble cross-validation\\ncan sometimes improve performance. It is shown how to estimate\\nthe optimal weights of the ensemble members using unlabeled data.\\nBy a generalization of query by committee, it is finally shown how\\nthe ambiguity can be used to select new training data to be labeled\\nin an active learning scheme.\\n\\n1\\n\\nINTRODUCTION\\n\\nIt is well known that a combination of many different predictors can improve predictions. In the neural networks community \"ensembles\" of neural networks has been\\ninvestigated by several authors, see for instance [1, 2, 3]. Most often the networks\\nin the ensemble are trained individually and then their predictions are combined.\\nThis combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .\\n.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk\\n\\n\\x0c232\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nAt the workshop after the last NIPS conference (December, 1993) an entire session\\nwas devoted to ensembles of neural networks ( \"Putting it all together\", chaired by\\nMichael Perrone) . Many interesting papers were given, and it showed that this area\\nis getting a lot of attention .\\nA combination of the output of several networks (or other predictors) is only useful\\nif they disagree on some inputs. Clearly, there is no more information to be gained\\nfrom a million identical networks than there is from just one of them (see also\\n[2]). By quantifying the disagreement in the ensemble it turns out to be possible\\nto state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the\\ndisagreement (called the ensemble ambiguity) and the generalization error is the\\nbasis for this paper, so we will derive it with no further delay.\\n\\n2\\n\\nTHE BIAS-VARIANCE TRADEOFF\\n\\nAssume the task is to learn a function J from RN to R for which you have a sample\\nof p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples\\nare assumed to be drawn randomly from the distribution p(x) . Anything in the\\nfollowing is easy to generalize to several output variables.\\nThe ensemble consists of N networks and the output of network a on input x is\\ncalled va (x). A weighted ensemble average is denoted by a bar , like\\n\\nV(x) =\\n\\nL Wa Va(x).\\n\\n(1)\\n\\na\\n\\nThis is the final output of the ensemble. We think of the weight Wa as our belief in\\nnetwork a and therefore constrain the weights to be positive and sum to one. The\\nconstraint on the sum is crucial for some of the following results.\\nThe ambiguity on input x of a single member of the ensemble is defined as aa (x)\\n(V a(x) - V(x))2 . The ensemble ambiguity on input x is\\n\\na(x)\\n\\n= Lwaaa(x) = LWa(va(x) a\\n\\nV(x))2 .\\n\\n=\\n\\n(2)\\n\\na\\n\\nIt is simply the variance of the weighted ensemble around the weighed mean, and\\nit measures the disagreement among the networks on input x. The quadratic error\\nof network a and of the ensemble are\\n\\n(J(x) - V a(x))2\\n(J(x) - V(X))2\\n\\n(3)\\n(4)\\n\\nrespectively. Adding and subtracting J( x) in (2) yields\\n\\na(x)\\n\\n=L\\n\\nWafa(X) - e(x)\\n\\n(5)\\n\\na\\n\\n(after a little algebra using that the weights sum to one) . Calling the weighted\\naverage of the individual errors ?( x) = La Wa fa (x) this becomes\\n\\ne(x)\\n\\n= ?(x) -\\n\\na(x).\\n\\n(6)\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\n233\\n\\nAll these formulas can be averaged over the input distribution . Averages over the\\ninput distribution will be denoted by capital letter, so\\n\\nJ dxp(xVl! (x)\\nJ dxp(x)aa(x)\\nJ dxp(x)e(x).\\n\\nE\\n\\n(7)\\n(8)\\n(9)\\n\\nThe first two of these are the generalization error and the ambiguity respectively\\nfor network n , and E is the generalization error for the ensemble. From (6) we then\\nfind for the ensemble generalization error\\n(10)\\nThe first term on the right is the weighted average of the generalization errors of\\nthe individual networks (E = La waEa), and the second is the weighted average\\nof the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.\\nThe beauty of this equation is that it separates the generalization error into a term\\nthat depends on the generalization errors of the individual networks and another\\nterm that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is\\nrequired of the real function to be approximated. The term \"unlabeled example\" is\\nborrowed from classification problems, and in this context it means an input x for\\nwhich the value of the target function f( x) is unknown.\\nEquation (10) expresses the tradeoff between bias and variance in the ensemble ,\\nbut in a different way than the the common bias-variance relation [4] in which the\\naverages are over possible training sets instead of ensemble averages. If the ensemble\\nis strongly biased the ambiguity will be small , because the networks implement very\\nsimilar functions and thus agree on inputs even outside the training set. Therefore\\nthe generalization error will be essentially equal to the weighted average of the\\ngeneralization errors of the individual networks. If, on the other hand , there is a\\nlarge variance , the ambiguity is high and in this case the generalization error will\\nbe smaller than the average generalization error . See also [5].\\nFrom this equation one can immediately see that the generalization error of the\\nensemble is always smaller than the (weighted) average of the ensemble errors,\\nE < E. In particular for uniform weights:\\n\\nE\\n\\n~ ~ \\'fEcx\\n\\n(11)\\n\\nwhich has been noted by several authors , see e.g. [3] .\\n\\n3\\n\\nTHE CROSS-VALIDATION ENSEMBLE\\n\\nFrom (10) it is obvious that increasing the ambiguity (while not increasing individual\\ngeneralization errors) will improve the overall generalization. We want the networks\\nto disagree! How can we increase the ambiguity of the ensemble? One way is to\\nuse different types of approximators like a mixture of neural networks of different\\ntopologies or a mixture of completely different types of approximators. Another\\n\\n\\x0c234\\n\\nAnders Krogh, Jesper Vedelsby\\n\\n.\\n\\n:~\\n\\n1. -\\n\\nt\\n\\n-\\n\\n,\\',\\n\\n.. ,\\n\\nE o...... -\\' \\'.- .. \\' ........ ....,.\\n\\n.\\'\\n\\n..... , ...\\n\\nv \\'. --:\\n\\n,\\n\\n.~.--c??\\n\\n__ .. -.tI\"\\n\\n.\\n\\n. -- - -\\\\\\\\\\n\\n\\'1\\n\\n-\\n\\n.~\\n\\n~.\\n\\n, . _ ? .\" ?\\n\\n.. - .....\\n\\n_._ ..... .\\'-._._.1\\n\\n,\\n\\n-\\n\\n>\\n\\n-\\n\\n-1.k!\\n~\\n\\n-4\\n\\n.t.\\n\\nf.\\n\\n1\\\\.1\\n\\n:\\\\,\\'. - ?-.l\\n\\n:--,____\\n..\\n\\n~~\\n.\\n\\n~.\\n\\n,\\n\\n,\\'\\n\\n-2\\n\\n.~\\n\\nIf\\n\\no\\n\\n2\\n\\n\\\\.\\n~\\n:\\n?\\n\\n\\' 0\\'\\n\\n~:\\n\\n4\\n\\nx\\n\\nFigure 1: An ensemble of five networks were trained to approximate the square\\nwave target function f(x). The final ensemble output (solid smooth curve) and\\nthe outputs of the individual networks (dotted curves) are shown. Also the square\\nroot of the ambiguity is shown (dash-dot line) _ For training 200 random examples\\nwere used, but each network had a cross-validation set of size 40, so they were each\\ntrained on 160 examples.\\n\\nobvious way is to train the networks on different training sets. Furthermore, to be\\nable to estimate the first term in (10) it would be desirable to have some kind of\\ncross-validation. This suggests the following strategy.\\nChose a number K :::; p. For each network in the ensemble hold out K examples for\\ntesting, where the N test sets should have minimal overlap, i. e., the N training sets\\nshould be as different as possible. If, for instance, K :::; piN it is possible to choose\\nthe K test sets with no overlap. This enables us to estimate the generalization error\\nE(X of the individual members of the ensemble, and at the same time make sure\\nthat the ambiguity increases . When holding out examples the generalization errors\\nfor the individual members of the ensemble, E(X, will increase, but the conjecture\\nis that for a good choice of the size of the ensemble (N) and the test set size\\n(K), the ambiguity will increase more and thus one will get a decrease in overall\\ngeneralization error.\\nThis conjecture has been tested experimentally on a simple square wave function\\nof one variable shown in Figure 1. Five identical feed-forward networks with one\\nhidden layer of 20 units were trained independently by back-propagation using 200\\nrandom examples. For each network a cross-validation set of K examples was held\\nout for testing as described above. The \"true\" generalization and the ambiguity were\\nestimated from a set of 1000 random inputs. The weights were uniform, w(X\\n1/5\\n(non-uniform weights are addressed later).\\n\\n=\\n\\nIn Figure 2 average results over 12 independent runs are shown for some values of\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\nFigure 2: The solid line shows the generalization error for uniform weights as\\na function of K, where K is the size\\nof the cross-validation sets. The dotted\\nline is the error estimated from equation (10) . The dashed line is for the\\noptimal weights estimated by the use of\\nthe generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.\\nThe bottom solid line is the generalization error one would obtain if the individual generalization errors were known\\nexactly (the best possible weights).\\n\\n0.08\\n\\n235\\n\\n,-----r----,--~---r-----,\\n\\no\\n\\nt=\\nw\\n0.06\\n\\nc\\n\\no\\n~\\n\\n.!::!\\n\\nco...\\n\\n~ 0.04\\n\\nQ)\\n\\n(!)\\n\\n0 .02 \\'---_---1_ _---\\'-_ _--\\'-_ _-----\\'\\no\\n20\\n40\\n60\\n80\\nSize of CV set\\n\\nK (top solid line) . First, one should note that the generalization error is the same\\nfor a cross-validation set of size 40 as for size 0, although not lower, so it supports\\nthe conjecture in a weaker form. However, we have done many experiments, and\\ndepending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,\\nonly ensembles with at least four converging networks out of five were used . If all\\nthe ensembles were kept, the error would have been significantly higher at ]{ = a\\nthan for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set\\nwas used. Thus it is still unclear under which circumstances one can expect a drop\\nin generalization error when using cross-validation in this fashion.\\n\\nThe dotted line in Figure 2 is the error estimated from equation (10) using the\\ncross-validation sets for each of the networks to estimate Ea, and one notices a\\ngood agreement.\\n\\n4\\n\\nOPTIMAL WEIGHTS\\n\\nThe weights Wa can be estimated as described in e.g. [3]. We suggest instead\\nto use unlabeled data and estimate them in such a way that they minimize the\\ngeneralization error given in (10) .\\nThere is no analytical solution for the weights , but something can be said about\\nthe minimum point of the generalization error. Calculating the derivative of E as\\ngiven in (10) subject to the constraints on the weights and setting it equal to zero\\nshows that\\nEa - Aa\\nE or Wa = O.\\n(12)\\n\\n=\\n\\n(The calculation is not shown because of space limitations, but it is easy to do.)\\nThat is, Ea - Aa has to be the same for all the networks. Notice that Aa depends\\non the weights through the ensemble average of the outputs. It shows that the\\noptimal weights have to be chosen such that each network contributes exactly waE\\n\\n\\x0c236\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nto the generalization error. Note, however, that a member of the ensemble can have\\nsuch a poor generalization or be so correlated with the rest of the ensemble that it\\nis optimal to set its weight to zero.\\nThe weights can be \"learned\" from unlabeled examples, e.g. by gradient descent\\nminimization of the estimate of the generalization error (10). A more efficient\\napproach to finding the optimal weights is to turn it into a quadratic optimization\\nproblem. That problem is non-trivial only because of the constraints on the weights\\n(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,\\nC af3\\n\\n=\\n\\nf\\n\\ndxp(x)V a (x)V f3 (x) .\\n\\n(13)\\n\\nThen, using that the weights sum to one, equation (10) can be rewritten as\\nE\\n\\n=\\n\\nL\\na\\n\\nwa Ea\\n\\n+ L w a C af3 w f3 - L\\naf3\\n\\nwaCaa .\\n\\n(14)\\n\\na\\n\\nHaving estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation\\nmatrix can be estimated from unlabeled data to any accuracy needed (provided that\\nthe input distribution p is known).\\nIn Figure 2 the results from an experiment with weight optimization are shown.\\nThe dashed curve shows the generalization error when the weights are optimized as\\ndescribed above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the\\nerrors Ea are known exactly, so it shows the lowest possible error. The performance\\nimprovement is quite convincing when the cross-validation estimates are used.\\nIt is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual\\nnetworks do not overfit, one might even use the training errors as estimates for\\nEa (see [3]). It is also possible to use some kind of regularization in (14), if the\\ncross-validation sets are small.\\n\\n5\\n\\nACTIVE LEARNING\\n\\nIn some neural network applications it is very time consuming and/or expensive\\nto acquire training data, e.g., if a complicated measurement is required to find the\\nvalue of the target function for a certain input. Therefore it is desirable to only use\\nexamples with maximal information about the function. Methods where the learner\\npoints out good examples are often called active learning.\\nWe propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by\\ncommittee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those\\npoints we would benefit the most from including in the training set.\\nSince the generalization error is always non-negative, we see from (6) that the\\nweighted average of the individual network errors is always larger than or equal to\\nthe ensemble ambiguity,\\nf(X) 2:: a(x),\\n(15)\\n\\n\\x0cNeural Network Ensembles. Cross Validation. and Active Learning\\n\\n237\\n\\n2.5 r\"\\':\":\\'T---r--\"T\"\"--.-----r---,\\n\\n.\\n\\n.\\n\\n.\\n\\n:\\n\\n0.5\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\nTraining set size\\n\\n40\\n\\n50\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\nTraining set size\\n\\nFigure 3: In both plots the full line shows the average generalization for active\\nlearning, and the dashed line for passive learning as a function of the number of\\ntraining examples. The dots in the left plot show the results of the individual\\nexperiments contributing to the mean for the active learning. The dots in right plot\\nshow the same for passive learning.\\n\\nwhich tells us that the ambiguity is a lower bound for the weighted average of the\\nsquared error. An input pattern that yields a large ambiguity will always have a\\nlarge average error. On the other hand, a low ambiguity does not necessarily imply\\na low error. If the individual networks are trained to a low training error on the\\nsame set of examples then both the error and the ambiguity are low on the training\\npoints. This ensures that a pattern yielding a large ambiguity cannot be in the close\\nneighborhood of a training example. The ambiguity will to some extent follow the\\nfluctuations in the error. Since the ambiguity is calculated from unlabeled examples\\nthe input-space can be scanned for these areas to any detail. These ideas are well\\nillustrated in Figure 1, where the correlation between error and ambiguity is quite\\nstrong, although not perfect.\\nThe results of an experiment with the active learning scheme is shown in Figure 3.\\nAn ensemble of 5 networks was trained to approximate the square-wave function\\nshown in Figure 1, but in this experiments the function was restricted to the interval\\nfrom - 2 to 2. The curves show the final generalization error of the ensemble in a\\npassive (dashed line) and an active learning test (solid line). For each training set\\nsize 2x40 independent tests were made, all starting with the same initial training\\nset of a single example. Examples were generated and added one at a time. In the\\npassive test examples were generated at random, and in the active one each example\\nwas selected as the input that gave the largest ambiguity out of 800 random ones.\\nFigure 3 also shows the distribution of the individual results of the active and\\npassive learning tests. Not only do we obtain significantly better generalization by\\nactive learning, there is also less scatter in the results. It seems to be easier for the\\nensemble to learn from the actively generated set.\\n\\n\\x0c238\\n\\n6\\n\\nAnders Krogh. Jesper Vedelsby\\n\\nCONCLUSION\\n\\nThe central idea in this paper was to show that there is a lot to be gained from\\nusing unlabeled data when training in ensembles. Although we dealt with neural\\nnetworks, all the theory holds for any other type of method used as the individual\\nmembers of the ensemble.\\nIt was shown that apart from getting the individual members of the ensemble to\\ngeneralize well, it is important for generalization that the individuals disagrees as\\nmuch as possible, and we discussed one method to make even identical networks\\ndisagree. This was done by training the individuals on different training sets by\\nholding out some examples for each individual during training. This had the added\\nadvantage that these examples could be used for testing, and thereby one could\\nobtain good estimates of the generalization error.\\nIt was discussed how to find the optimal weights for the individuals of the ensemble.\\nFor our simple test problem the weights found improved the performance of the\\nensemble significantly.\\n\\nFinally a method for active learning was described, which was based on the method\\nof query by committee developed for classification problems. The idea is that if the\\nensemble disagrees strongly on an input, it would be good to find the label for that\\ninput and include it in the training set for the ensemble. It was shown how active\\nlearning improves the learning curve a lot for a simple test problem.\\nAcknowledgements\\n\\nWe would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank\\nLars Kai Hansen for many discussions and great insights, and David Wolpert for\\nvaluable comments.\\n\\nReferences\\n[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.\\n[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.\\n[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method\\nfor neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image\\nprocessing. Chapman-Hall, 1993.\\n[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance\\ndilemma. Neural Computation, 4(1):1-58, Jan. 1992.\\n[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least\\nsquares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.\\n[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of\\nthe Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,\\nCA, 1992. Morgan Kaufmann.\\n[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query\\nby committee. In Advances in Neural Information Processing Systems, volume 5, San\\nMateo, California, 1993. Morgan Kaufmann.\\n\\n\\x0c',\n",
       "     'pdf_name': '1001-neural-network-ensembles-cross-validation-and-active-learning.pdf',\n",
       "     'title': 'Neural Network Ensembles, Cross Validation, and Active Learning',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1004',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1004',\n",
       "     'paper_text': 'ICEG Morphology Classification using an\\nAnalogue VLSI Neural Network\\n\\nRichard Coggins, Marwan Jabri, Barry Flower and Stephen Pickard\\nSystems Engineering and Design Automation Laboratory\\nDepartment of Electrical Engineering J03,\\nUniversity of Sydney, 2006, Australia.\\nEmail: richardc@sedal.su.oz.au\\n\\nAbstract\\nAn analogue VLSI neural network has been designed and tested\\nto perform cardiac morphology classification tasks. Analogue techniques were chosen to meet the strict power and area requirements\\nof an Implantable Cardioverter Defibrillator (ICD) system. The robustness of the neural network architecture reduces the impact of\\nnoise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer percept ron with on chip digital weight\\nstorage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit\\nat the output. The network was trained in loop and included a\\ncommercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better\\nthan 90% true positive and true negative detections for dangerous\\nrhythms which cannot be detected by present ICDs. The chip was\\nimplemented in 1.2um CMOS and consumes less than 200nW maximum average power in an area of 2.2 x 2.2mm2.\\n\\n1\\n\\nINTRODUCTION\\n\\nTo the present time, most ICDs have used timing information from ventricular\\nleads only to classify rhythms which has meant some dangerous rhythms can not\\nbe distinguished from safe ones, limiting the use of the device. Even two lead\\n\\n\\x0c732\\n\\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\n4.00\\n\\nHO\\n3.00\\n\\n2.00\\n\\nI.SO\\n\\n_ _ _:::::::!\\n\\nQ\\n1.00\\n\\nO.SO\\n\\nFigure 1: The Morphology of ST and VT retrograde 1:1.\\n\\natrial/ventricular systems fail to distinguish some rhythms when timing information alone is used [Leong and Jabri, 1992]. A case in point is the separation of Sinus Tachycardia (ST) from Ventricular Tachycardia with 1:1 retrograde conduction.\\nST is a safe arrhythmia which may occur during vigorous exercise and is characterised by a heart rate of approximately 120 beats/minute. VT retrograde 1:1 also\\noccurs at the same low rate but can be a potentially fatal condition. False negative\\ndetections can cause serious heart muscle injury while false positive detections deplete the batteries, cause patient suffering and may lead to costly transplantation\\nof the device. Figure 1 shows however, the way in which the morphology changes\\non the ventricular lead for these rhythms. Note, that the morphology change is\\npredominantly in the \"QRS complex\" where the letters QRS are the conventional\\nlabels for the different points in the conduction cycle during which the heart is\\nactually pumping blood.\\nFor a number of years, researchers have studied template matching schemes in order\\nto try and detect such morphology changes. However, techniques such as correlation\\nwaveform analysis [Lin et. al., 1988], though quite successful are too computationally intensive to meet power requirements. In this paper, we demonstrate that\\nan analogue VLSI neural network can detect such morphology changes while still\\nmeeting the strict power and area requirements of an implantable system. The\\nadvantages of an analogue approach are born out when one considers that an energy efficient analogue to digital converter such as [Kusumoto et. al., 1993] uses\\n1.5nJ per conversion implying 375nW power consumption for analogue to digital\\nconversion of the ICEG alone. Hence, the integration of a bucket brigade device and\\nanalogue neural network provides a very efficient way of interfacing to the analogue\\ndomain. Further, since the network is trained in loop with the ICD in real time,\\nthe effects of device offsets, noise, QRS detection jitter and signal distortion in the\\nanalogue circuits are largely alleviated.\\nThe next section discusses the chip circuit designs. Section 3 describes the method\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\n733\\n\\nAowAcId. . .\\n\\n1axl Syna.... AIRy\\n\\n\"-\\n\\nColumn\\nAoIcIr.-\\n\\nI\\n\\no.ta Reglsl...\\n\\nIClkcMmux\\n\\nI\\n\\nBu1I...\\n\\nI WTAI\\n\\n10 DOD DO\\n\\nFigure 2: Floor Plan and Photomicrograph of the chip\\nused to train the network for the morphology classification task. Section 4 describes\\nthe classifier performance on seven patients with arrhythmia which can not be\\ndistinguished using the heart rate only. Section 5 summarises the results, remaining\\nproblems and future directions for the work .\\n\\n2\\n\\nARCHITECTURE\\n\\nThe neural network chip consists of a 10:6:3 multilayer perceptron, an input bucket\\nbrigade device (BBD) and a winner take all (WTA) circuit at the output. A floor\\nplan and photomicrograph of the chip appears in figure 2. The BBD samples the\\nincoming ICEG at a rate of 250Hz. For three class problems, the winner take all\\ncircuit converts the winning class to a digital signal. For the two class problem\\nconsidered in this paper , a simple thresholding function suffices. The following\\nsubsections briefly describe the functional elements of the chip . The circuit diagrams\\nfor the chip building blocks appear in figure 3.\\n\\n2.1\\n\\nBUCKET BRIGADE DEVICE\\n\\nOne stage of the bucket brigade circuit is shown in figure 3. The BBD uses a\\ntwo phase clock to shift charge from cell to cell and is based on a design by\\nLeong [Leong, 1992] . The BBD operates by transferring charge deficits from S\\nto D in each of the cells. PHIl and PHI2 are two phase non-overlapping clocks.\\nThe cell is buffered from the synapse array to maintain high charge transfer efficiency. A sample and hold facility is provided to store the input on the gates of the\\nsynapses. The BBD clocks are generated off chip and are controlled by the QRS\\ncomplex detector in the lCD.\\n\\n2.2\\n\\nSYNAPSE\\n\\nThis synapse has been used on a number of neural network chips previously.\\ne.g . [Coggins et. al., 1994] . The synapse has five bits plus sign weight storage which\\n\\n\\x0c734\\n\\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\nNEURON\\n\\n.-----------------------------------------------------------,,,\\n,,\\n~ !\\nBUJIOIII\\'\\n\\n00\\n\\nBUCKET BRIGADE ClLL\\n\\n\"\\n\\nFigure 3: Neuron, Bucket Brigade and Synapse Circuit Diagrams.\\nsets the bias to a differential pair which performs the multiplication. The bias references for the weights are derived from a weighted current source in the corner of\\nthe chip. A four quadrant multiplication is achieved by the four switches at the top\\nof the differential pair.\\n\\n2.3\\n\\nNEURON\\n\\nDue to the low power requirements, the bias currents of the synapse arrays are of\\nthe order of hundreds of nano amps, hence the neurons must provide an effective\\nresistance of many mega ohms to feed the next synapse layer while also providing\\ngain control. Without special high resistance polysilicon, simple resistive neurons\\nuse prohibitive area, However, for larger networks with fan-in much greater than\\nten, an additional problem of common mode cancellation is encountered, That is,\\nas the fan-in increases, a larger common mode range is required or a cancellation\\nscheme using common mode feedback is needed.\\nThe neuron of figure 3 implements such a cancellation scheme, The mirrors MO/M2\\nand Ml/M3 divide the input current and facilitate the sum at the drain of M7.\\nM7/M8 mirrors the sum so that it may be split into two equal currents by the\\nmirrors formed by M4, M5 and M6 which are then subtracted from the input\\ncurrents. Thus, the differential voltage vp - Vm is a function of the transistor\\ntransconductances, the common mode input current and the feedback factor , The\\ngain of the neuron can be controlled by varying the width to length ratio of the\\nmirror transistors MO and Ml. The implementation in this case allows seven gain\\ncombinations, using a three bit RAM cell to store the gain,\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\n735\\n\\nImplantable\\nC.cio?erlor\\n\\nDefibrillalOr\\n\\nRunnngMUME\\n\\nNe .....1\\nNelwa\\'1<\\nChip\\n\\nFigure 4: Block Diagram of the Training and Testing System.\\nThe importance of a common mode cancellation scheme for large networks can\\nbe seen when compared to the straight forward approach of resistive or switched\\ncapacitor neurons. This may be illustrated by considering the energy usage of\\nthe two approaches. Firstly, we need to define the required gain of the neuron\\nas a function of its fan-in . If we assume that useful inputs to the network are\\nmostly sparse, i.e. with a small fraction of non-zero values, then the gain is largely\\nindependent of the fan-in, yet the common mode signal increases linearly with fanin. For the case of a neuron which does not cancel the common mode, the power\\nsupply voltage must be increased to accommodate the common mode signal, thus\\nleading to a quadratic increase in energy use with fan-in. A common mode cancelling\\nneuron on the other hand , suffers only a linear increase in energy use with fan-in\\nsince extra voltage range is not required and the increased energy use arises only\\ndue to the linear increase in common mode current.\\n\\n3\\n\\nTRAINING SYSTEM\\n\\nThe system used to train and test the neural network is shown in figure 4. Control\\nof training and testing takes place on the PC. The PC uses a PC-LAB card to\\nprovide analogue and digital I/O . The PC plays the ICEG signal to the input of\\nthe commercial ICD in real time. Note, that the PC is only required for initially\\ntraining the network and in this case as a source of the heart signal. The commercial\\nICD performs the function of QRS complex detection using analogue circuits. The\\nQRS complex detection signal is then used to freeze the BBD clocks of the chip, so\\nthat a classification can take place.\\nWhen training, a number of examples of the arrhythmia to be classified are selected\\nfrom a single patient data base recorded during an electrophysiological study and\\npreviously classified by a cardiologist. Since most of the morphological information\\nis in the QRS complex, only these segments of the data are repeatedly presented to\\n\\n\\x0c736\\n\\nRichard Coggins. Marwan Jabri. Barry Flower. Stephen Pickard\\n\\nPatient\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n% Training Attempts Converged\\nRun ~\\nRun 1\\n\\nH=3\\n80\\n80\\n0\\n60\\n100\\n100\\n80\\n\\nH= 6\\n10\\n100\\n0\\n10\\n80\\n40\\n100\\n\\nH=3\\n60\\n0\\n0\\n40\\n0\\n60\\n40\\n\\nH=6\\n60\\n10\\n10\\n40\\n60\\n60\\n100\\n\\nAverage\\nIterations\\n62\\n86\\n101\\n77\\n44\\n46\\n17\\n\\nTable 1: Training Performance of the system on seven patients.\\nthe network. The weights are adjusted according to the training algorithm running\\non the PC using the analogue outputs of the network to reduce the output error .\\nThe PC writes weights to the chip via the digital I/Os of the PC-LAB card and the\\nserial weight bus of network. The software package implementing the training and\\ntesting, called MUME [Jabri et. al ., 1992], provides a suite of training algorithms\\nand control options. Online training was used due to its success in training small\\nnetworks and because the presentation of the QRS complexes to the network was\\nthe slowest part of the training procedure. The algorithm used for weight updates\\nin this paper was summed weight node perturbation [Flower and Jabri, 1993].\\nThe system was trained on seven different patients separately all of whom had\\nVT with 1: 1 retrograde conduction. Note, that patient independent training has\\nbeen tried but with mixed results [Tinker, 1992] . Table 1 summarises the training\\nstatistics for the seven patients. For each patient and each architecture, five training\\nruns were performed starting from a different random initial weight set. Each\\nof the patients was trained with eight of each class of arrhythmia. The network\\narchitecture used was 10:H:1, where H is the number of hidden layer neurons and\\nthe unused neurons being disabled by setting their input weights to zero. Two sets\\nof data were collected denoted Run 1 and Run 2. Run 1 corresponded to output\\ntarget values of ?0.6V within margin 0.45V and Run 2 to output target values of\\n?0.2V within margin 0.05V. A training attempt was considered to have converged\\nwhen the training set was correctly classified within two hundred training iterations.\\nOnce the morphologies to be distinguished have been learned for a given patient,\\nthe remainder of the patient data base is played back in a continuous stream and\\nthe outputs of the classifier at each QRS complex are logged and may be compared\\nto the classifications of a cardiologist. The resulting generalisation performance is\\ndiscussed in the next section.\\n\\n4\\n\\nMORPHOLOGY CLASSIFIER GENERALISATION\\nPERFORMANCE\\n\\nTable 2 summarises the generalisation performance of the system on the seven\\npatients for the training attempts which converged. Most of the patients show a\\ncorrect classification rate better than 90% for at least one architecture on one of the\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\nPatient\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nNo. of\\nComplexes\\nST\\nVT\\n440\\n61\\n57\\n94\\n67\\n146\\n166\\n65\\n61\\n96\\n61\\n99\\n28\\n80\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n440\\n94\\n67\\n166\\n61\\n61\\n28\\n\\n61\\n57\\n146\\n65\\n96\\n99\\n80\\n\\n737\\n\\n% Correct Classifications Run 1\\nH = 6\\nH - i3\\nVT\\nST\\nST\\nVT\\n89?10 89?3\\n58?0\\n99?0\\n99?1\\n99?1\\n100?0 99?1\\n66?44 76?37\\n99?1\\n50?3\\n82?1 75?13\\n89?9\\n94?6\\n84?8\\n97?1\\n90?5\\n99?1\\n97?3\\n98?5\\n99?1\\n99?1\\n% Correct Classifications Run 2\\n86?14 99?1\\n88?2\\n99?1\\n94?6\\n94?3\\n84?2\\n99?1\\n76?18 59?2\\n87?7 100?0\\n88?2\\n49?5\\n84?1\\n82?5\\n92?6 90?10\\n99?1\\n99?1\\n94?3\\n99?0\\n94?3\\n92?3\\n\\nTable 2: Generalisation Performance of the system on seven patients.\\nruns, whereas, a timing based classifier can not separate these arrhythmia at all.\\nFor each convergent weight set the network classified the test set five times. Thus,\\nthe \"% Correct\" columns denote the mean and standard deviation of the classifier\\nperformance with respect to both training and testing variations. By duty cycling\\nthe bias to the network and buffers, the chip dissipates less than 200n W power for\\na nominal heart rate of 120 beats/minute during generalisation.\\n\\n5\\n\\nDISCUSSION\\n\\nReferring to table 1 we see that the patient 3 data was relatively difficult to train.\\nHowever, for the one occasion when training converged generalisation performance\\nwas quite acceptable. Inspection of this patients data showed that typically, the\\nmorphologies of the two rhythms were very similar. The choice of output targets,\\nmargins and architecture appear to be patient dependent and possibly interacting\\nfactors. Although larger margins make training easier for some patients they appear\\nto also introduce more variability in generalisation performance. This may be due\\nto the non-linearity of the neuron circuit. Further experiments are required to\\noptimise the architecture for a given patient and to clarify the effect of varying\\ntargets, margins and neuron gain. Penalty terms could also be added to the error\\nfunction to minimise the possibility of missed detections of the dangerous rhythm.\\nThe relatively slow rate of the heart results in the best power consumption being\\nobtained by duty cycling the bias currents to the synapses and the buffers. Hence,\\nthe bias settling time of the weighted current source is the limiting factor for reducing power consumption further for this design. By modifying the connection of the\\ncurrent source to the synapses using a bypassing technique to reduce transients in\\n\\n\\x0cRiclulrd Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\n738\\n\\nthe weighted currents, still lower power consumption could be achieved.\\n\\n6\\n\\nCONCLUSION\\n\\nThe successful classification of a difficult cardiac arrhythmia problem has been\\ndemonstrated using. an analogue VLSI neural network approach. Furthermore, the\\nchip developed has shown very low power consumption of less than 200n W, meeting the requirements of an implantable system. The chip has performed well, with\\nover 90% classification performance for most patients studied and has proved to be\\nrobust when the real world influence of analogue QRS detection jitter is introduced\\nby a commercial implantable cardioverter defibrillator placed in the signal path to\\nthe classifier.\\nAcknowledgements\\n\\nThe authors acknowledge the funding for the work in this paper provided under\\nAustralian Generic Technology Grant Agreement No. 16029 and thank Dr. Phillip\\nLeong of the University of Sydney and Dr. Peter Nickolls of Telectronics Pacing\\nSystems Ltd., Australia for their helpful suggestions and advice.\\nReferences\\n\\n[Castro et. al., 1993] H.A. Castro, S.M. Tam, M.A. Holler, \"Implementation and\\nPerformance of an analogue Nonvolatile Neural Network,\" Analogue Integrated\\nCircuits and Signal Processing, vol. 4(2), pp. 97-113, September 1993.\\n[Lin et. al., 1988] D. Lin, L.A. Dicarlo, and J .M. Jenkins, \"Identification of Ventricular Tachycardia using Intracavitary Electrograms: analysis of time and frequency domain patterns,\" Pacing (3 Clinical Electrophysiology, pp. 1592-1606,\\nNovember 1988.\\n[Leong, 1992] P.H.W. Leong, Arrhythmia Classification Using Low Power VLSI,\\nPhD Thesis, University of Sydney, Appendix B, 1992.\\n[ Kusumoto et. al., 1993] K. Kusumoto et. al., \"A lObit 20Mhz 30mW Pipelined\\nInterpolating ADC,\" ISSCC, Digest of Technical Papers, pp. 62-63, 1993.\\n[Leong and Jabri, 1992] P.H.W. Leong and M. Jabri, \"MATIC - An Intracardiac Tachycardia Classification System\", Pacing (3 Clinical Electrophysiology,\\nSeptember 1992.\\n[Coggins et. al., 1994] R.J. Coggins and M.A. Jabri, \"WATTLE: A Trainable Gain\\nAnalogue VLSI Neural Network\", NIPS6, Morgan Kauffmann Publishers, 1994.\\n[Jabri et. al., 1992] M.A. Jabri, E.A. Tinker and L. Leerink, \"MUME- A MultiNet-Multi-Architecture Neural Simulation Environment\", Neural Network Simulation Environments, Kluwer Academic Publications, January, 1994.\\n[Flower and Jabri, 1993] B. Flower and M. Jabri, \"Summed Weight Neuron Perturbation: an O(N) improvement over Weight Perturbation,\" NIPS5, Morgan\\nKauffmann Publishers, pp. 212-219, 1993.\\n[Tinker, 1992] E.A. Tinker, \"The SPASM Algorithm for Ventricular Lead Timing and Morphology Classification,\" SEDAL ICEG-RPT-016-92, Department of\\nElectrical Engineering, University of Sydney, 1992.\\n\\n\\x0c',\n",
       "     'pdf_name': '1004-iceg-morphology-classification-using-an-analogue-vlsi-neural-network.pdf',\n",
       "     'title': 'ICEG Morphology Classification using an Analogue VLSI Neural Network',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1006',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1006',\n",
       "     'paper_text': 'Real-Time Control of a Tokamak Plasma\\nUsing Neural Networks\\n\\nChris M Bishop\\nNeural Computing Research Group\\nDepartment of Computer Science\\nAston University\\nBirmingham, B4 7ET, U.K.\\nc.m .bishop@aston .ac .uk\\n\\nPaul S Haynes, Mike E U Smith, Tom N Todd,\\nDavid L Trotman and Colin G Windsor\\nAEA Technology, Culham Laboratory,\\nOxfordshire OX14 3DB\\n(Euratom/UKAEA Fusion Association)\\n\\nAbstract\\nThis paper presents results from the first use of neural networks\\nfor the real-time feedback control of high temperature plasmas in\\na tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen\\nplasmas, at temperatures of up to 100 Million K, are confined\\nby strong magnetic fields. Accurate control of the position and\\nshape of the plasma boundary requires real-time feedback control\\nof the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural\\nnetwork approach can give significantly better performance than\\nthe linear technique currently used on most tokamak experiments.\\nThe practical application of the neural network approach requires\\nhigh-speed hardware, for which a fully parallel implementation of\\nthe multilayer perceptron, using a hybrid of digital and analogue\\ntechnology, has been developed.\\n\\n\\x0c1008\\n\\n1\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nINTRODUCTION\\n\\nFusion of the nuclei of hydrogen provides the energy source which powers the sun.\\n\\nIt also offers the possibility of a practically limitless terrestrial source of energy.\\nHowever, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a\\nhigh temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the\\nRussian for \\'toroidal magnetic chamber\\') as illustrated schematically in Figure 1.\\nAt these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks\\nhad plasmas with circular cross-sections, for which feedback control of the plasma\\nposition and shape is relatively straightforward. However, recent tokamaks, such as\\nthe COMPASS experiment at Culham Laboratory, as well as most next-generation\\ntokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy\\nconfinement properties and thereby significantly enhance the performance of the\\ntokamak.\\n\\nz\\n\\nR\\n\\nFigure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma\\n(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded\\nas axisymmetric about the Z-axis, and so the plasma boundary can\\nbe described by its cross-sectional shape at one particular toroidal\\nlocation.\\nUnlike circular cross-section plasmas, highly non-circular shapes are more difficult to\\nproduce and to control accurately, since currents through several control coils must\\nbe adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape\\nmust evolve, usually from some initial near-circular shape. Due to uncertainties\\nin the current and pressure distributions within the plasma, the desired accuracy\\nfor plasma control can only be achieved by making real-time measurements of the\\nposition and shape of the boundary, and using error feedback to adjust the currents\\nin the control coils.\\nThe physics of the plasma equilibrium is determined by force balance between the\\n\\n\\x0c1009\\n\\nReal-Time Control of Tokamak Plasma Using Neural Networks\\n\\ncircle\\n\\nellipse\\n\\nO-shape\\n\\nbean\\n\\nFigure 2: Cross-sections of the COMPASS vacuum vessel showing\\nsome examples of potential plasma shapes. The solid curve is the\\nboundary of the vacuum vessel, and the plasma is shown by the\\nshaded regions.\\n\\nthermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms\\nof solutions of a non-linear partial differential equation called the Grad-Shafranov\\n(GS) equation. Due to the non-linear nature of this equation, a general analytic\\nsolution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the\\nexternal control coils which surround the vacuum vessel. On the tokamak itself it\\nis changes in these currents which are used to alter the position and cross-sectional\\nshape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used\\nto generate the training dataset for the neural network, as described in the next\\nsection. However , this approach is computationally very intensive and is therefore\\nunsuitable for feedback control purposes.\\nFor real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a\\nvariety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.\\nMost tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these\\nmagnetic signals collectively as a vector m .\\nFor a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,\\ngiven by\\n\\nR(B)\\nZ(B)\\n\\nRo + a cos(B + 8 sinB)\\nZo + a/\\\\,sinB\\n\\nwhere we have defined the following parameters\\n\\n(1)\\n\\n\\x0c1010\\n\\nRo\\nZo\\na\\nK\\n\\n6\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nradial distance of the plasma center from the major axis of the torus,\\nvertical distance of the plasma center from the torus midplane,\\nminor radius measured in the plane Z = Zo,\\nelongation,\\ntriangularity.\\n\\nWe denote these parameters collectively by Yk. The basic problem which has to be\\naddressed, therefore, is to find a representation for the (non-linear) mapping from\\nthe magnetic signals m to the values of the geometrical parameters Yk, which can\\nbe implemented in suitable hardware for real-time control.\\nThe conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical\\nparameters by a single linear transformation. However, the intrinsic non-linearity\\nof the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;\\nBishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the\\ncontrol loop for the neural network approach to tokamak equilibrium control.\\nNeural\\n\\nNetwork\\n\\nFigure 3: Block diagram of the control loop used for real-time\\nfeedback control of plasma position and shape.\\n\\n2\\n\\nSOFTWARE SIMULATION RESULTS\\n\\nThe dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base\\ncurrently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes\\nseveral minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled\\nwith the appropriate values of the shape parameters. Of the 120 magnetic signals\\navailable on COMPASS which could be used to provide inputs to the network, a\\n\\n\\x0c1011\\n\\nReal-Time Control o/Tokamak PLasma Using Neural Networks\\n\\nsubset of 16 has been chosen using sequential forward selection based on a linear\\nrepresentation for the mapping (discussed below) .\\nIt is important to note that the transformation from magnetic signals to flux surface\\nparameters involves an exact linear invariance. This follows from the fact that, if all\\nof the currents are scaled by a constant factor, then the magnetic fields will be scaled\\nby this factor, and the geometry of the plasma boundary will be unchanged . It is\\nimportant to take advantage of this prior knowledge and to build it into the network\\nstructure, rather than force the network to learn it by example. We therefore\\nnormalize the vector m of input signals to the network by dividing by a quantity\\nproportional to the total plasma current. Note that this normalization has to be\\nincorporated into the hardware implementation of the network, as will be discussed\\nin Section 3.\\n1.2\\n\\n4\\n01\\n\\n2\\n\\n2\\n\\n01\\n\\nc\\n\\nc\\n.5.\\n\\n0-\\n\\n~\\n:E\\n\\n.5.\\nCIS\\n\\n:E\\n\\n1iI\\n\\n~\\n::J\\n\\n?\\n\\n1iI\\nCD\\n\\n-2\\n\\ngo.8\\n\\n.5.\\n0-\\n\\n?\\n\\nCIS\\n\\n:E\\n\\n1iI 0 .4\\nCD\\n\\nc\\n::J\\n\\nc\\n\\n::J\\n\\n-2\\n\\n-4\\nDatabase\\n\\n?\\nDatabase\\n\\n1.2\\n\\n4\\n~\\n\\n~CD\\n\\nZ\\n\\n~\\n:::I\\n\\nCD\\n\\nz\\n\\n.2\\nDatabase\\n\\n2\\n\\n~O.8\\n~\\n\\n?\\n\\nCD\\n\\nz\\n\\n~O.4\\n\\n-2\\n\\n:::I\\n\\nCD\\n\\nZ\\n\\n?\\n\\n-4\\nDatabase\\n\\nDatabase\\n\\n.2\\nDatabase\\n\\nFigure 4: Plots of the values from the test set versus the values\\npredicted by the linear mapping for the 3 equilibrium parameters,\\ntogether with the corresponding plots for a neural network with 4\\nhidden units.\\n\\nThe results presented in this paper are based on a multilayer perceptron architecture\\nhaving a single layer of hidden units with \\'tanh\\' activation functions , and linear\\noutput units. Networks are trained by minimization of a sum-of-squares error using\\na standard conjugate gradients optimization algorithm, and the number of hidden\\n\\n\\x0cJ012\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nunits is optimized by measuring performance with respect to an independent test\\nset. Results from the neural network mapping are compared with those from the\\noptimal linear mapping, that is the single linear transformation which minimizes\\nthe same sum-of-squares error as is used in the neural network training algorithm,\\nas this represents the method currently used on a number of present day tokamaks .\\nInitial results were obtained on networks having 3 output units, corresponding to\\nthe values of vertical position ZQ, major radius RQ, and elongation K; these being\\nparameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.\\nBy comparison, the optimal linear mapping gave a normalized test set error of 18.3.\\nThis represents a reduction in error of about 30% in going from the linear mapping\\nto the neural network. Such an improvement, in the context of this application , is\\nvery significant.\\nFor the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we\\nconsider the results from this network in more detail. Figure 4 shows plots of the\\nnetwork predictions for various parameters versus the corresponding values from\\nthe test set portion of the database. Analogous plots for the optimal linear map\\npredictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,\\neven for this sub-optimal network topology.\\n\\n3\\n\\nHARDWARE IMPLEMENTATION\\n\\nThe hardware implementation of the neural network must have a bandwidth of 2:\\n20 kHz in order to cope with the fast timescales of the plasma evolution. It must\\nalso have an output precision of at least (the the analogue equivalent of) 8 bits in\\norder to ensure that the final accuracy which is attainable will not be limited by the\\nhardware system. We have chosen to develop a fully parallel custom implementation\\nof the multilayer perceptron, based on analogue signal paths with digitally stored\\nsynaptic weights (Bishop et al., 1993). A VME-based modular construction has\\nbeen chosen as this allows flexibility in changing the network architecture, ease of\\nloading network weights, and simplicity of data acquisition. Three separate types\\nof card have been developed as follows:\\n? Combined 16-input buffer and signal normalizer.\\nThis provides an analogue hardware implementation of the input normalization described earlier.\\n? 16 x 4 matrix multiplier\\nThe synaptic weights are produced using 12 bit frequency-compensated\\nmultiplying DACs (digital to analogue converters) which can be configured\\nto allow 4-quadrant multiplication of analogue signals by a digitally stored\\nnumber.\\n? 4-channel sigmoid module\\nThere are many ways to produce a sigmoidal non-linearity, and we have\\nopted for a solution using two transistors configured as along-tailed-pair,\\n\\n\\x0cReal-Time Control of Tokamak Plasma Using Neural Networks\\n\\n1013\\n\\nto generate a \\'tanh \\' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the\\nappearance of temperature in the denominator of the exponential transistor\\ntransfer characteristic. An elegant solution to this problem has been found\\nby exploiting a chip containing 5 transistors in close thermal contact. Two\\nof the transistors form the long-tailed pair, one of the transistors is used\\nas a heat source, and the remaining two transistors are used to measure\\ntemperature. External circuitry provides active thermal feedback control,\\nand stability to changes in ambient temperature over the range O?C to 50?C\\nis found to be well within the acceptable range.\\nThe complete network is constructed by mounting the appropriate combination\\nof cards in a VME rack and configuring the network topology using front panel\\ninterconnections. The system includes extensive diagnostics, allowing voltages at\\nall key points within the network to be monitored as a function of time via a series\\nof multiplexed output channels.\\n\\n4\\n\\nRESULTS FROM REAL-TIME FEEDBACK CONTROL\\n\\nFigure 5 shows the first results obtained from real-time control of the plasma in\\nthe COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time\\nduring a plasma pulse. Here the desired elongation has been preprogrammed to\\nfollow a series of steps as a function of time. The remaining 2 network outputs\\n(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,\\nbut were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the\\npost-shot reconstruction of the elongation obtained from a simple \\'filament\\' code,\\nwhich gives relatively rapid post-shot plasma shape reconstruction but with limited\\naccuracy. The circles denote the elongation values given by the much more accurate\\nreconstructions obtained from the full equilibrium code. The graph clearly shows\\nthe network generating the required elongation signal in close agreement with the\\nreconstructed values. The typical residual error is of order 0.07 on elongation values\\nup to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is\\ncurrently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units\\navailable with the initial hardware configuration. While these results represent the\\nfirst obtained using closed loop control, it is clear from earlier software modelling of\\nlarger network architectures (such as 32- 16-4) that residual errors of order a few %\\nshould be attainable. The implementation of such larger networks is being persued,\\nfollowing the successes with the smaller system.\\nAcknowledgements\\nWe would like to thank Peter Cox, Jo Lister and Colin Roach for many useful\\ndiscussions and technical contributions. This work was partially supported by the\\nUK Department of Trade and Industry.\\n\\n\\x0cC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\n1014\\n\\n1.8\\nshot 9576\\n\\nc:\\n\\no\\n\\n~\\n14\\nC)\\n?\\nc:\\n\\no\\n\\nas\\n1.0\\n0.0\\n\\n0.1\\n\\n0.2\\n\\ntime (sec.)\\nFigure 5: Plot of the plasma elongation K. as a function of time\\nduring shot no. 9576 on the COMPASS tokamak, during which the\\nelongation was being controlled in real-time by the neural network.\\n\\nReferences\\n\\nBishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman\\nD L, 1992. A neural network approach to tokamak equilibrium control. In Neural\\nNetwork Applications, Ed. J G Taylor, Springer Verlag, 114-128.\\nBishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.\\n1993. Hardware implementation of a neural network for plasma position control in\\nCOMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,\\nItaly. 2 997-1001.\\nLagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi\\nM, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time\\ncalculations of plasma equilibrium parameters for PBX-M, In Proceedings of the\\n17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.\\nLister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma\\nparameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.\\n\\n\\x0cPulsestream Synapses with Non-Volatile\\nAnalogue Amorphous-Silicon Memories.\\n\\nA.J. Holmes, A.F. Murray, S. Churcher and J. Hajto\\nDepartment of Electrical Engineering\\nUniversity of Edinburgh\\nEdinburgh, EH9 3JL\\nM. J. Rose\\nDept. of Applied Physics and Electronics,\\nDundee University\\nDundee DD14HN\\n\\nAbstract\\nA novel two-terminal device, consisting of a thin lOooA layer of p+\\na-Si:H sandwiched between Vanadium and Chromium electrodes,\\nexhibits a non-volatile, analogue memory action. This device stores\\nsynaptic weights in an ANN chip, replacing the capacitor previously\\nused for dynamic weight storage. Two different synapse designs are\\ndiscussed and results are presented.\\n\\n1\\n\\nINTRODUCTION\\n\\nAnalogue hardware implementations of neural networks have hitherto been hampered by the lack of a straightforward (local) analogue memory capability. The\\nideal storage mechanism would be compact, non-volatile, easily reprogrammable,\\nand would not interfere with the normal silicon chip fabrication process.\\nTechniques which have been used to date include resistors (these are not generally\\nreprogrammable, and suffer from being large and difficult to fabricate with any accuracy), dynamic capacitive storage [4] (this is compact, reprogrammable and simple,\\nbut implies an increase in system complexity, arising from off-chip refresh circuitry),\\n\\n\\x0c764\\n\\nA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\nEEPROM (\"floating gate\") memory [5] (which is compact, reprogrammable, and\\nnon-volatile, but is slow, and cannot be reprogrammed in situ), and local digital\\nstorage (which is non-volatile, easily programmable and simple, but consumes area\\nhorribly).\\nAmorphous silicon has been used for synaptic weight storage [1, 2], but only as\\neither a high-resistance fixed weight medium or a binary memory.\\nIn this paper, we demonstrate that novel amorphous silicon memory devices can be\\nincorporated into standard CMOS synapse circuits, to provide an analogue weight\\nstorage mechanism which is compact, non-volatile, easily reprogrammable, and simple to implement.\\n\\n2\\n\\na-Si:H MEMORY DEVICES\\n\\nThe a-Si:H analogue memory device [3] comprises a lOooA thick layer of amorphous\\nsilicon (p+ a-Si:H) sandwiched between Vanadium and Chromium electrodes.\\nThe a-Si device takes the form of a two-terminal, programmable resistor. It is an\\n\"add-on\" to a conventional CMOS process, and does not demand that the normal\\nCMOS fabrication cycle be disrupted. The a-Si device sits on top of the completed\\nchip circuitry, making contact with the CMOS arithmetic elements via holes cut in\\nthe protective passivation layer, as shown in Figure 1.\\n\\nCMOS Passivation\\nFigure 1: The construction of a-Si:H Devices on a CMOS chip\\nAfter fabrication a number of electronic procedures must be performed in order to\\nprogram the device to a given resistance state.\\nProgramming, and Pre-Programming Procedures\\n\\nBefore the a-Si device is usable, the following steps must be carried out:\\n? Forming: This is a once-only process, applied to the a-Si device in its\\n\"virgin\" state, where it has a resistance of several MO. A series of 300ns\\npulses, increasing in amplitude from 5v to 14v, is applied to the device\\nelectrodes. This creates a vertical conducting channel or filament whose\\napproximate resistance is 1KO. This filament can then be programmed to\\na value in the range lKO to 1 MO . The details of the physical mechanisms\\nare not yet fully established, but it is clear that conduction occurs through\\na narrow (sub-micron) conducting channel.\\n\\n\\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\\n\\n765\\n\\n? Write: To decrease the device\\'s resistance, negative \"Write\", pulses are\\napplied.\\n? Erase: To increase the device\\'s resistance, positive\" Erase\" , pulses are applied.\\n? Usage: Pulses below O.5v do not change the device resistance. The resistance can therefore be utilised as a weight storage medium using a voltage\\nof less than O.5v without causing reprogramming.\\nProgramming pulses, which range between 2v and 5v, are typically 120ns in duration. Programming is therefore much faster than for other EEPROM (floating\\ngate) devices used in the same context, which use a series of 100jls pulses to set the\\nthreshold voltage [5].\\nThe following sections describe synapse circuits using the a-Si:H devices. These\\nsynapses use the reprogrammable a-Si:H resistor in the place of a storage capacitor\\nor EEPROM cell. These new synapses were implemented on a chip referred to as\\nASiTEST2, consisting of five main test blocks, each comprising of four synapses\\nconnected to a single neuron.\\n\\n3\\n\\nThe EPSILON based synapse\\n\\nThe first synapse to be designed used the a-Si:H resistor as a direct replacement for\\nthe storage capacitor used in the EPSILON [4] synapse.\\n\\n+Sv\\n\\nNeuron\\n\\n1\\nI\\n\\nV\\n\\n..\\n\\nt.\\n\\n~:l:\\n\\n><!:\\n\\n~\\n\\nMirror Set\\n\\nE\\n\\n30\\n\\na-Si => Vw\\n\\nCircuitry\\n\\nOriginal\\nStorage\\nCapacitor\\n\\nO.5v\\n\\n<0-----------.,...\\n__\\n\\n...\\n\\nEPSILON Synapse\\n\\nFigure 2: The EPSILON Synapse with a-Si:H weight storage\\n\\nIn the original EPSILON chip the weight voltage was stored as a voltage on a\\ncapacitor. In this new synapse design, shown in Figure 2, the a-Si:H resistance is\\nset such that the voltage drop produced by Iset is equivalent to the original weight\\nvoltage, Vw, that was stored dynamically on the capacitor.\\nA new, simpler, synapse, which can be operated from a single +5v supply, was also\\nbe included on the ASiTEST2 chip.\\n\\n\\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\n766\\n\\n4\\n\\nThe MkII synapse\\n\\nThe circuit is shown in Figure 3. The a-Si:H memory is used to store a current,\\nIasi. This current is subtracted from a zero current, Isy...:z\" to give a weight current\\n, +/-Iw, which adds or subtracts charge from the activity capacitor, Cact, thus\\nimplementing excitation or inhibition respectively.\\nFor the circuit to function correctly we must limit the voltage on the activity capacitor to the range [1.5v,3.5v], to ensure that the transistors mirroring Isy_z and\\nIasi remain in saturation. As Figure 3 shows, there are few reference signals and\\nthe circuit operates from a single +5v power supply rail, in sharp contrast to many\\nearlier analogue neural circuits, including our own.\\n\\n1v\"\"\\n\\n+5vPWm\\n\\nII .\\n\\n~\\n\\n881\\n\\nVsel\\n\\n.r--\\\\.\\n\\n-.L\\n\\n....L\\n\\n*\\n\\nComparator\\nPWout\\n..rL\\n\\nCact\\n\\nVramp\\n\\n~\\n\\nOv\\nE\\n;.\\nMirror Set\\n\\n\"\\'E~-----------:~~\\n\\nE\\n\\nSynapse\\n\\nPower Supplies\\nV5_0=5.Ov\\n\\nReferences\\nVrstv?2.5v\\nIsy_z=5uA\\n\\n;.\\n\\nNeuron\\n\\nTail Currents\\n\\nIneu=4uA\\n\\nFigure 3: The MkII synapse\\n\\nOn first inspection the main drawback of this design would appear to be a reliance\\non the accuracy with which the zero current Isy...:z, is mirrored across an entire chip.\\nThe variation in this current means that two cells with the same synapse resistance\\ncould produce widely differing values of Iw. However, during programming we\\ndo not use the resistance of the a-Si:H device as a target value. We monitor the\\nvoltage on Cact for a given PWin signal, increasing or decreasing the resistance\\nof the a-Si:H device until the desired voltage level is achieved.\\nExample: To set a weight to be the maximum positive value, we adjust the a-Si\\nresistance until a PWin signal of 5us, the maximum input signal, gives a voltage of\\n3.5v on the integration capacitor.\\nWe are able to set the synapse weight using the whole integration range of [1.5v,3.5v]\\nby only closing Vsel for the desired synapse during programming. In normal operating mode all four Vsel switches will be closed so that the integration charge is\\nsummed over all four local capacitors.\\n\\n\\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\\n\\n4.1\\n\\n767\\n\\nExample - Stability Test\\n\\nAs an example of the use of integration voltage as means of monitoring the resistance\\nof a particular synapse we have included a stability test. This was carried out on\\none of the test chips which contained the MkII synapse.\\nThe four synapses on the test chip were programmed to give different levels of\\nactivation. The chip was then powered up for 30mins each day during a 7-day\\nperiod, and the activation levels for each synapse were measured three times.\\n3.5\\n\\nStability Test - PWin = 3us\\n\\n,----~-_._--;--__..___r...:....,.-_._,.__-,.--.,..,.-__..__:___.,\\n\\ntestl\\n\\ntest2\\n\\ntest4\\n\\ntest3\\n\\ntestS\\n\\ntest7\\n\\ntest6\\n\\n3\\n\\n?\\n,.\\nI\\n\\nt:\\'\"\\n-~\\n\\n1:1\\n?\\n\\n\\'t\\n\\n,\\n\\n?\\n\\n~\\n\\n?\\n\\n- ~ -:- - ~ -:- -\\n\\n25\\n?\\n\\n2\\n\\n?\\n\\n~.\\n\\n?\\n\\n-:- - - . of - ~-:- -\\n\\n..\\n\\n.\\n\\nI\\n\\nI\\n\\n?\\n\\n?\\n\\n{,o-:- - .\\n- ~-s4\\n.\\n\\n,\\n.\\n.\\n\\'.\\n.?\\n.?\\n.?\\n.?\\n- - ~ - - ~ - - --:- - - ~ - - -~- - - w. -- --r s2\\n- - - .;. \\'\" - -: -011>- - :.. ~ - ~ -oGii - -:- - - - i-- ~ - ..; -sl\\n?\\n.\\n.\\n.\\n.\\n.\\n?\\n\\n?\\n\\n~ - - ~ - - -~- - - ~ - ?\\n\\nI\\n\\n?\\n\\nI\\nI\\n\\n,\\nI\\n\\n?\\n?\\n\\n?\\n\\n,\\n\\n?\\n?\\nI\\n\\nI\\n?\\n?\\n\\n10\\n\\n20\\n\\n30\\n\\n-.\\nL---L- -- -~\\n\\n~ -s3\\n\\n,\\n\\n?\\n\\n,\\n,\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n,\\n\\nI\\n\\n,\\n\\n?\\n\\n?\\nI\\n:\\n\\nI\\n?\\n:\\n\\n?\\nI\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n\\n40\\n\\n50\\n\\n60\\n\\n,\\n\\n70\\n\\n80\\n\\n?\\n\\n90\\n\\nMeasurement Index\\n\\nFigure 4: ASiTEST2- Stability Test\\nAs figure 4 shows, the memories remain in the same resistance state (i.e retain their\\nprogrammed weight value) over the whole 7-day period. Separate experiments on\\nisolated devices indicate much longer hold times - of the order of months at least.\\n\\n5\\n\\nASiTEST3\\n\\nRecently we have received our latest, overtly neural, a-Si:H based test chip. This\\ncontains an 8x8 array of the MkII synapses.\\nThe circuit board for this device has been constructed and partially tested while\\nthe ASiTEST3 chips are awaiting the deposition of the a-Si:H layers. We have been\\nable to use an ASiTEST2 chip containing two of the MkII synapse test blocks i.e.\\n8 synapses and 2 neurons to exercise much of the board\\'s functionality.\\nThe test board contains a simple state machine which has four different states:\\n? State 0: Load Input Pulsewidths into SRAM from PC.\\n? State 1: Apply Input Pulsewidth signals to chipl.\\n? State 2: Use Vramp to generate threshold function for chipl. The resulting\\nPulsewidth outputs are used as the inputs to chip2, as well as being stored\\n\\n\\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\n768\\n\\nin SRAM .\\n? State 3: Use Vramp to generate threshold function for chip2. Read resulting\\nPulsewidth Outputs into SRAM .\\n? State 0: Read Output Pulsewidths from SRAM into PC.\\nThe results obtained during a typical test cycle are shown in Figure 5.\\nIE-- Statel --;,,;,,*1E E - - State2\\n\\n-----;>~i~\\n\\nState3 ---;!>~I\\n\\n~r-IF~~~--r-------~---------l\\n4v\\n3v\\n\\nPWin_O\\n\\n2v\\nIv\\n\\nOv~ . . . . . . . . .\\n\\n3.5v r;;;;;\"\"-~,\"\",,,or;:::::;;:;::;;;;:;;;::;;t---~----t--------,\\n\\n~:~\\n\\n....... ~;a~;\"\"\"\\'\"\\n\\n2.Ov\\n\\n~\\'Sig~;,id\"\"\"\"\"\\n... ~\"Li~\"\"\"\"\\n......\\n\\n. . . . . . . . . . . . . . . . . . . . . . . ,.\\n\\n....... .\\n\\n.... .....\\n\\n..........\\n\\n.. ..\\n\\nl.~\\ne?_e\\n\\n_____ ...... ___\\n\\n?\\n\\n_____\\n\\n?\\n\\n____________ . . . . . . . .\\n\\n_____\\n\\n????\\n\\n____\\n\\n..........\\n\\n.\\n\\n5v\\n4v\\n3v\\n\\n2v\\n\\n:;; ~,...,................-..--t;?.~...:...~__--! ..........~~.n-.-~~~~~~......J\\nIS.\\n\\n10.\\n\\nFigure 5: ASiTEST3 Board Scope Waveforms\\nAs this figure shows different ramp signals, corresponding to different threshold\\nfunctions, can be applied to chipl and chip2 neurons.\\n10.0\\n\\nSingle Buffer PulscWidth Sweeps\\n\\n.----.,..----r------.----r----.------,\\n\\n9.0\\n8.0\\n\\n!\\n\\n7.0 -\\n\\n~\\n\\n5\\'O~~~~~~~-~~-+++~~~~N~~~\\n\\n~\\n\\ni6.o~\\n\\n~----\\n\\nJ::\\n2D\\n\\nNeal-Syal\\nN~~JIII\\n\\n1.0\\n\\nN~~ya2\\n\\no.oL---~--~- --~--?\\n\\no\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n2.5\\n\\n3.0\\n\\nPulscwldlh Input [WI)\\n\\nFigure 6: ASiTEST3 Board - MkII Synapse Characteristic\\nWhile the signals shown in Figure 5 appear noisy the multiplier characteristic that\\nthe chip produces is still admirably linear, as shown in Figure 6. In this experiment\\nall eight synapses on a test chip were programmed into different resistance states\\nand PWin was swept from 0 to 3us.\\n\\n\\x0cPulsestream Synapses with Non- Volatile Analogue Amorphous-Silicon Memories\\n\\n6\\n\\n769\\n\\nConclusions\\n\\nWe have demonstrated the use of novel a-Si:H analogue memory devices as a means\\nof storing synaptic weights in a Pulsewidth ANN. We have also demonstrated the\\noperation of an interface board which allows two 8x8 ANN chips, operating as a\\ntwo layer network, to be controlled by a simple PC interface card.\\nThis technology is most suitable for small networks in, for example, remote control and other embedded-system applications where cost and power considerations\\nfavour a single all-inclusive ANN chip with non-volatile, but programmable weights.\\nAnother possible application of this technology is in large networks constructed\\nusing Thin Film Technology(TFT). If TFT\\'s were used in place of the CMOS transistors then the area constraint imposed by crystalline silicon would be removed,\\nallowing truly massively parallel networks to be integrated.\\nIn summary - the a-Si:H analogue memory devices described in this paper provide a\\nroute to an analogue, non-volatile and fast synaptic weight storage medium. At the\\npresent time neither the programming nor storage mechanisms are fully understood\\nmaking it difficult to compare this new device with more established technologies\\nsuch as the ubiquitous Floating-Gate EEPROM technique. Current research is\\nfocused on firstly, improving the yield on the a-Si:H device which is unacceptably\\nlow at present, a demerit that we attribute to imperfections in the a-Si fabrication\\nprocess and secondly, improving understanding of the device physics and hence the\\nprogramming and storage mechanisms.\\nAcknowledgements\\nThis research has been jointly funded by BT, and EPSRC (formerly SERC), the\\nEngineering and Physical Sciences Research Council.\\n\\nReferences\\n[1] W. Hubbard et al.(1986) Electronic Neural Networks AlP Conference Proceedings - Snowbird 1986 :227-234\\n[2] H.P. Graf (1986) VLSI Implementation of a NN memory with several hundreds\\nof neurons AlP Conference Proceedings - Snowbird 1986 :182-187.\\n[3] M.J. Rose et al (1989) Amorphous Silicon Analogue Memory Devices Journal\\nof Non-Crystalline Solids 1(115):168-170\\n[4] A.Hamilton et al. (1992) Integrated Pulse-Stream Neural Networks - Results,\\nIssues and Pointers IEEE Transactions on N.N.s 3(3):385-393\\n[5] M.Holler, S.Tam, H.Castro and R.Benson (1989) An Electrically Trainable ANN\\nwith 10240 Floating Gate Synapses. Int Conf on N.N.s Proc :191-196\\n[6] A.F.Murray and A.V.W.Smith.(1987) Asynchronous Arithmetic for VLSI Neural Systems. Electronics Letters 23(12):642-643\\n[7] A.J. Holmes et al. (1993) Use of a-Si:H Memory Devices for Non-volatile Weight\\nStorage in ANNs. Proc lCAS 15 :817-820\\n\\n\\x0c\\x0c',\n",
       "     'pdf_name': '1006-pulsestream-synapses-with-non-volatile-analogue-amorphous-silicon-memories.pdf',\n",
       "     'title': 'Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1007',\n",
       "     'paper_text': 'Learning To Play the Game of Chess\\n\\nSebastian Thrun\\nUniversity of Bonn\\nDepartment of Computer Science III\\nRomerstr. 164, 0-53117 Bonn, Germany\\nE-mail: thrun@carbon.informatik.uni-bonn.de\\n\\nAbstract\\nThis paper presents NeuroChess, a program which learns to play chess from the final\\noutcome of games. NeuroChess learns chess board evaluation functions, represented\\nby artificial neural networks. It integrates inductive neural network learning, temporal\\ndifferencing, and a variant of explanation-based learning. Performance results illustrate\\nsome of the strengths and weaknesses of this approach.\\n\\n1 Introduction\\nThroughout the last decades, the game of chess has been a major testbed for research on\\nartificial intelligence and computer science. Most oftoday\\'s chess programs rely on intensive\\nsearch to generate moves. To evaluate boards, fast evaluation functions are employed which\\nare usually carefully designed by hand, sometimes augmented by automatic parameter tuning\\nmethods [1]. Building a chess machine that learns to play solely from the final outcome of\\ngames (win/loss/draw) is a challenging open problem in AI.\\nIn this paper, we are interested in learning to play chess from the final outcome of games.\\nOne of the earliest approaches, which learned solely by playing itself, is Samuel\\'s famous\\nchecker player program [10]. His approach employed temporal difference learning (in short:\\nTO) [14], which is a technique for recursively learning an evaluation function . Recently,\\nTesauro reported the successful application of TO to the game of Backgammon, using\\nartificial neural network representations [16]. While his TO-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go\\n[12] and chess have been less successful. For example, Schafer [11] reports a system just\\nlike Tesauro\\'s TO-Gammon, applied to learning to play certain chess endgames. Gherrity [6]\\npresented a similar system which he applied to entire chess games. Both approaches learn\\npurely inductively from the final outcome of games. Tadepalli [15] applied a lazy version\\nof explanation-based learning [5, 7] to endgames in chess. His approach learns from the\\nfinal outcome, too, but unlike the inductive neural network approaches listed above it learns\\nanalytically, by analyzing and generalizing experiences in terms of chess-specific knowledge.\\n\\n\\x0c1070\\n\\nSebastian Thrun\\n\\nThe level of play reported for all these approaches is still below the level of GNU-Chess, a\\npublicly available chess tool which has frequently been used as a benchmark. This illustrates\\nthe hardness of the problem of learning to play chess from the final outcome of games.\\nThis paper presents NeuroChess, a program that learns to play chess from the final outcome\\nof games. The central learning mechanisms is the explanation-based neural network (EBNN)\\nalgorithm [9, 8]. Like Tesauro\\'s TD-Gammon approach, NeuroChess constructs a neural\\nnetwork evaluation function for chess boards using TO. In addition, a neural network version\\nof explanation-based learning is employed, which analyzes games in terms of a previously\\nlearned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate\\nsome of its strengths and weaknesses.\\n\\n2\\n\\nTemporal Difference Learning in the Domain of Chess\\n\\nTemporal difference learning (TO) [14] comprises a family of approaches to prediction in\\ncases where the event to be predicted may be delayed by an unknown number of time steps.\\nIn the context of game playing, TD methods have frequently been applied to learn functions\\nwhich predict the final outcome of games. Such functions are used as board evaluation\\nfunctions.\\nThe goal of TO(O), a basic variant of TO which is currently employed in the NeuroChess\\napproach, is to find an evaluation function, V, which ranks chess boards according to their\\ngoodness: If the board S is more likely to be a winning board than the board Sf, then\\nV(s) > V(Sf). To learn such a function, TO transforms entire chess games, denoted by\\na sequence of chess boards So, SI, s2, . . . , StunaJ\\' into training patterns for V. The TO(O)\\nlearning rule works in the following way. Assume without loss of generality we are learning\\nwhite\\'s evaluation function. Then the target values for the final board is given by\\n{\\n\\nI,\\n0,\\n-1,\\n\\nif Stu.?tI is a win for white\\nif StUnaJ is a draw\\nif StonaJ is a loss for white\\n\\nand the targets for the intermediate chess boards So, SI , S2, . .. , Stu.?tI-2 are given by\\nVt.1fget( St)\\nI? V (St+2)\\n\\n=\\n\\n(1)\\n\\n(2)\\n\\nThis update rule constructs V recursively. At the end of the game, V evaluates the final\\noutcome of the game (Eq. (l In between, when the assignment of V -values is less obvious,\\nV is trained based on the evaluation two half-moves later (Eq. (2?. The constant I (with\\no ~ I ~ 1) is a so-called discount factor. It decays V exponentially in time and hence\\nfavors early over late success. Notice that in NeuroChess V is represented by an artificial\\nneural network, which is trained to fit the target values vtarget obtained via Eqs. (l) and (2)\\n(cj [6, 11, 12, 16]).\\n\\n?.\\n\\n3\\n\\nExplanation-Based Neural Network Learning\\n\\nIn a domain as complex as chess, pure inductive learning techniques. such as neural network Back-Propagation, suffer from enormous training times. To illustrate why, consider\\nthe situation of a knight fork. in which the opponent\\'s knight attacks our queen and king\\nsimultaneously. Suppose in order to save our king we have to move it, and hence sacrifice\\nour queen. To learn the badness of a knight fork, NeuroChess has to discover that certain\\nboard features (like the position of the queen relative to the knight) are important, whereas\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n1071\\n\\nFigure 1: Fitting values and slopes in EBNN: Let V be the target function for which three\\nexamples (s\\\\, V(S\\\\)), (S2\\' V(S2)), and (S3, V(S3)) are known. Based on these points the\\nS2)OS2, and a~;:3) are\\nlearner might generate the hypothesis V\\'. If the slopes a~;:I),\\nalso known, the learner can do much better: V\".\\n\\nar\\n\\nothers (like the number of weak pawns) are not. Purely inductive learning algorithms such\\nas Back-propagation figure out the relevance of individual features by observing statistical\\ncorrelations in the training data. Hence, quite a few versions of a knight fork have to be\\nexperienced in order to generalize accurately. In a domain as complex as chess, such an\\napproach might require unreasonably large amounts of training data.\\nExplanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training\\ndata. They rely instead on the availability of domain knowledge, which they use for explaining\\nand generalizing training examples. For example, in the explanation of a knight fork, EBL\\nmethods employ knowledge about the game of chess to figure out that the position of the\\nqueen is relevant, whereas the number of weak pawns is not. Most current approaches to\\nEBL require that the domain knowledge be represented by a set of symbolic rules. Since\\nNeuroChess relies on neural network representations, it employs a neural network version\\nof EBL, called explanation-based neural network learning (EBNN) [9]. In the context of\\nchess, EBNN works in the following way: The domain-specific knowledge is represented\\nby a separate neural network, called the chess model M. M maps arbitrary chess boards St\\nto the corresponding expected board St+2 two half-moves later. It is trained prior to learning\\nV, using a large database of grand-master chess games. Once trained, M captures important\\nknowledge about temporal dependencies of chess board features in high-quality chess play.\\nEBNN exploits M to bias the board evaluation function V. It does this by extracting slope\\nconstraints for the evaluation function V at all non-final boards, i.e., all boards for which V\\nis updated by Eq. (2). Let\\nwith\\n\\nt E\\n\\n{a, 1,2, ... , tlioa\\\\ - 2}\\n\\ndenote the target slope of V at St, which, because\\nEq. (2), can be rewritten as\\n\\noV target ( St)\\n\\n=\\n\\n\\'Y.\\n\\noV( St+2) OSt+2\\n._OSt+2\\nOSt\\n\\nvtarget ( St)\\n\\n(3)\\n\\nis set to \\'Y V (St+2) according\\n(4)\\n\\nusing the chain rule of differentiation. The rightmost term in Eq. (4) measures how infinitesimal small changes of the chess board St influence the chess board St+2. It can be\\napproximated by the chess model M:\\n\\novtarget(St)\\nOSt\\n\\n~\\n\\n\\'Y.\\n\\nOV(St+2) oM(st)\\n.\\nOSt+2\\nOSt\\n\\n(5)\\n\\nThe right expression is only an approximation to the left side, because M is a trained neural\\n\\n\\x0cSebastian Thrun\\n\\n1072\\n\\n~\\n\\nbmrd at time\\n\\nf\\n\\n(W\"T\"~)\\n\\n~\\n\\nboard attime 1+ I\\n(black to move)\\n\\n~\\n\\nboard at time 1+2\\n\\n(w\"\\'?ro~)\\n\\npredictive model network M\\n\\n165 hidden unit,\\n\\nV(1+2)\\n\\nFigure 2: Learning an evaluation function in NeuroChess. Boards are mapped into a\\nhigh-dimensionalJeature vector, which forms the input for both the evaluation network V\\nand the chess model M. The evaluation network is trained by Back-propagation and the\\nTD(O) procedure. Both networks are employed for analyzing training example in order to\\nderive target slopes for V.\\nnetwork and thus its first derivative might be erroneous. Notice that both expressions on\\nthe right hand side of Eq. (5) are derivatives of neural network functions, which are easy to\\ncompute since neural networks are differentiable.\\nThe result of Eq . (5) is an estimate of the slope of the target function V at 8t . This slope\\nadds important shape information to the target values constructed via Eq. (2). As depicted in\\nFig. 1, functions can be fit more accurately if in addition to target values the slopes of these\\nvalues are known. Hence, instead of just fitting the target values vtarget ( 8t), NeuroChess also\\nfits these target slopes. This is done using the Tangent-Prop algorithm [13].\\nThe complete NeuroChess learning architecture is depicted in Fig. 2. The target slopes\\nprovide a first-order approximation to the relevance of each chess board feature in the\\ngoodness of a board position. They can be interpreted as biasing the network V based on\\nchess-specific domain knowledge, embodied in M . For the relation ofEBNN and EBL and\\nthe accommodation of inaccurate slopes in EBNN see [8].\\n\\n4\\n\\nTraining Issues\\n\\nIn this section we will briefly discuss some training issues that are essential for learning good\\nevaluation functions in the domain of chess. This list of points has mainly been produced\\nthrough practical experience with the NeuroChess and related TD approaches. It illustrates\\nthe importance of a careful design of the input representation, the sampling rule and the\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n1073\\n\\nparameter setting in a domain as complex as chess.\\nSampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for\\nexample, the opponent leads by more than a queen and a rook, one is most likely to loose.\\nWithout an appropriate sampling method there is the danger that the learner spends most\\nof its time learning from uninteresting examples. Therefore, NeuroChess interleaves selfplay and expert play for guiding the sampling process. More specifically, after presenting\\na random number of expert moves generated from a large database of grand-master games,\\nNeuroChess completes the game by playing itself. This sampling mechanism has been found\\nto be of major importance to learn a good evaluation function in a reasonable amount of time.\\nQuiescence. In the domain of chess certain boards are harder to evaluate than others. For\\nexample, in the middle of an ongoing material exchange, evaluation functions often fail to\\nproduce a good assessment. Thus, most chess programs search selectively. A common\\ncriterion for determining the depth of search is called quiescence. This criterion basically\\ndetects material threats and deepens the search correspondingly. NeuroChess\\' search engine\\ndoes the same. Consequently, the evaluation function V is only trained using quiescent\\nboards.\\nSmoothness. Obviously, using the raw, canonical board description as input representation is\\na poor choice. This is because small changes on the board can cause a huge difference in value,\\ncontrasting the smooth nature of neural network representations. Therefore, NeuroChess\\nmaps chess board descriptions into a set of board features . These features were carefully\\ndesigned by hand.\\nDiscounting. The variable \\'Y in Eq. (2) allows to discount values in time. Discounting has\\nfrequently been used to bound otherwise infinite sums of pay-off. One might be inclined to\\nthink that in the game of chess no discounting is needed, as values are bounded by definition.\\nIndeed, without discounting the evaluation function predicts the probability for winning-in\\nthe ideal case. In practice, however, random disturbations of the evaluation function can\\nseriously hurt learning, for reasons given in [4, 17]. Empirically we found that learning\\nfailed completely when no discount factor was used. Currently, NeuroChess uses \\'Y = 0.98.\\nLearning rate. TO approaches minimize a Bellman equation [2]. In the NeuroChess\\ndomain, a close-to-optimal approximation of the Bellman equation is the constant function\\nV(s) == O. This function violates the Bellman equation only at the end of games (Eq. (1?,\\nwhich is rare if complete games are considered. To prevent this, we amplified the learning\\nrate for final values by a factor of20, which was experimentally found to produce sufficiently\\nnon-constant evaluation functions.\\nSoftware architecture. Training is performed completely asynchronously on up to 20\\nworkstations simultaneously. One of the workstations acts as a weight server, keeping track\\nof the most recent weights and biases of the evaluation network. The other workstations\\ncan dynamically establish links to the weight server and contribute to the process of weight\\nrefinement. The main process also monitors the state of all other workstations and restarts\\nprocesses when necessary. Training examples are stored in local ring buffers (1000 items\\nper workstation).\\n\\n5\\n\\nResults\\n\\nIn this section we will present results obtained with the NeuroChess architecture. Prior to\\nlearning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)\\nis trained using a database of 120,000 expert games. NeuroChess then learns an evaluation\\n\\n\\x0c1074\\n\\nI. e2e3 b8c6\\n2. dlf3 c6e5\\n3. f3d5 d7d6\\n4. flb5 c7c6\\n5. b5a4 g8f6\\n6. d5d4 c8f5\\n7. f2f4 e5d7\\n8. ele2d8a5\\n9. a4b3 d7c5\\n10. b I a3 c5b3\\n11 . a2b3 e7e5\\n12. f4e5 f6e4\\n13. e5d6 e8c8\\n14. b3b4 a5a6\\n15. b4b5 a6a5\\n\\nSebastian Thrun\\n\\n16. b2b4 a5a4\\n17. b5c6 a4c6\\n18. gl f3 d8d6\\n19. d4a7 f5g4\\n20. c2c4 c8d7\\n21. b4b5 c6c7\\n22. d2d3 d6d3\\n23. b5b6 c7c6\\n24. e2d3 e4f2\\n25. d3c3 g4f3\\n26. g2f3 f2h 1\\n27. clb2 c6f3\\n28. a7a4 d7e7\\n29. a3c2 hi f2\\n30. b2a3 e7f6\\n\\n31 . a3f8 f2e4\\n32. c3b2 h8f8\\n33. a4d7 f3f5\\n34. d7b7 f5e5\\n35. b2cl f8e8\\n36. b7d5 e5h2\\n37. ala7 e8e6\\n38. d5d8 f6g6\\n39. b6b7 e6d6\\n40. d8a5 d6c6\\n41 . a5b4 h2b8\\n42. a7a8 e4c3\\n43. c2d4 c6f6\\n44. b4e7 c3a2\\n45. cldl a2c3\\n\\n46. d I c2 b8h2\\n47. c2c3 f6b6\\n48. e7e4 g6h6\\n49. d4f5 h6g5\\n50. e4e7 g5g4\\n51. f5h6 g7h6\\n52. e7d7 g4h5\\n53. d7d I h5h4\\n54. d I d4 h4h3\\n55. d4b6 h2e5\\n56. b6d4 e5e6\\n57. c3d2 e6f5\\n58. e3e4 f5 g5\\n59. d4e3 g5e3\\n60. d2e3 f7f5\\n\\n61 . e4f5 h3g4 65. a8e8 e6d7\\n62. f5f6 h6h5\\n66. e8e7 d7d8\\n63. b7b8q g4f5 67. f4c7\\n64. b8f4 f5e6\\nfinal board\\n\\nFigure 3: NeuroChess against GNU-Chess. NeuroChess plays white. Parameters: Both\\nplayers searched to depth 3, which could be extended by quiescence search to at most 11.\\nThe evaluation network had no hidden units. Approximately 90% of the training boards\\nwere sampled from expert play.\\n\\nnetwork V (175 input units, 0 to 80 hidden units, and one output units). To evaluate the level\\nof play, NeuroChess plays against GNU-Chess in regular time intervals. Both players employ\\nthe same search mechanism which is adopted from GNU-Chess. Thus far, experiments lasted\\nfor 2 days to 2 weeks on I to 20 SUN Sparc Stations.\\nA typical game is depicted in Fig. 3. This game has been chosen because it illustrates both\\nthe strengths and the shortcomings of the NeuroChess approach. The opening of NeuroChess\\nis rather weak. In the first three moves NeuroChess moves its queen to the center of the\\nboard.\\' NeuroChess then escapes an attack on its queen in move 4, gets an early pawn\\nadvantage in move 12, attacks black\\'s queen pertinaciously through moves 15 to 23, and\\nsuccessfully exchanges a rook. In move 33, it captures a strategically important pawn, which,\\nafter chasing black\\'s king for a while and sacrificing a knight for no apparent reason, finally\\nleads to a new queen (move 63). Four moves later black is mate. This game is prototypical.\\nAs can be seen from this and various other games, NeuroChess has learned successfully to\\nprotect its material, to trade material, and to protect its king. It has not learned, however, to\\nopen a game in a coordinated way, and it also frequently fails to play short.endgames even\\nif it has a material advantage (this is due to the short planning horizon). Most importantly, it\\nstill plays incredibly poor openings, which are often responsible for a draw or a loss. Poor\\nopenings do not surprise, however, as TD propagates values from the end of a game to the\\nbeginning.\\nTable I shows a performance comparison of NeuroChess versus GNU-Chess, with and\\nwithout the explanation-based learning strategy. This table illustrates that NeuroChess wins\\napproximately 13% of all games against GNU-Chess, if both use the same search engine. It\\n\\'This is because in the current version NeuroChess still heavily uses expert games for sampling.\\nWhenever a grand-master moves its queen to the center of the board, the queen is usually safe, and there\\nis indeed a positive correlation between having the queen in the center and winning in the database.\\nNeuroChess falsely deduces that having the queen in the center is good. This effect disappears when\\nthe level of self-play is increased, but this comes at the expense of drastically increased training time,\\nsince self-play requires search.\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n# of games\\n100\\n200\\n500\\n1000\\n1500\\n2000\\n2400\\n\\nGNU depth 2, NeuroChess depth 2\\nBack-propagation\\nEBNN\\n1\\n0\\n6\\n2\\n35\\n13\\n73\\n85\\n130\\n135\\n190\\n215\\n239\\n316\\n\\n1075\\n\\nGNU depth 4, NeuroChess depth 2\\nBack-propagation\\nEBNN\\n0\\n0\\n0\\n0\\nI\\n0\\n2\\n1\\n3\\n3\\n3\\n8\\nII\\n3\\n\\nTable 1: Performance ofNeuroChess vs. GNU-Chess during training. The numbers show the\\ntotal number of games won against GNU-Chess using the same number of games for testing\\nas for training. This table also shows the importance of the explanation-based learning\\nstrategy in EBNN. Parameters: both learners used the original GNU-Chess features, the\\nevaluation network had 80 hidden units and search was cut at depth 2, or 4, respectively (no\\nquiescence extensions).\\nalso illustrates the utility of explanation-based learning in chess.\\n\\n6 Discussion\\nThis paper presents NeuroChess, an approach for learning to play chess from the final\\noutcomes of games. NeuroChess integrates TD, inductive neural network learning and\\na neural network version of explanation-based learning. The latter component analyzes\\ngames using knowledge that was previously learned from expert play. Particular care has\\nbeen taken in the design of an appropriate feature representation, sampling methods, and\\nparameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in\\nseveral hundreds of games. However, the level of play still compares poorly to GNU-Chess\\nand human chess players.\\nDespite the initial success, NeuroChess faces two fundamental problems which both might\\nweB be in the way of excellent chess play. Firstly, training time is limited, and it is to\\nbe expected that excellent chess skills develop only with excessive training time. This is\\nparticularly the case if only the final outcomes are considered. Secondly, with each step of\\nTO-learning NeuroChess loses information. This is partially because the features used for\\ndescribing chess boards are incomplete, i.e., knowledge about the feature values alone does\\nnot suffice to determine the actual board exactly. But, more importantly, neural networks have\\nnot the discriminative power to assign arbitrary values to all possible feature combinations.\\nIt is therefore unclear that a TD-like approach will ever, for example, develop good chess\\nopenmgs.\\nAnother problem of the present implementation is related to the trade-off between knowledge\\nand search. It has been well recognized that the ul timate cost in chess is determi ned by the ti me\\nit takes to generate a move. Chess programs can generally invest their time in search, or in the\\nevaluation of chess boards (search-knowledge trade-off) [3] . Currently, NeuroChess does a\\npoor job, because it spends most of its time computing board evaluations. Computing a large\\nneural network function takes two orders of magnitude longer than evaluating an optimized\\nlinear evaluation function (like that of GNU-Chess). VLSI neural network technology offers\\na promising perspective to overcome this critical shortcoming of sequential neural network\\nsimulations.\\n\\n\\x0c1076\\n\\nSebastian Thrun\\n\\nAcknowledgment\\nThe author gratefully acknowledges the guidance and advise by Hans Berliner, who provided\\nthe features for representing chess boards, and without whom the current level of play would\\nbe much worse. He also thanks Tom Mitchell for his suggestion on the learning methods,\\nand Horst Aurisch for his help with GNU-Chess and the database.\\n\\nReferences\\n[I] Thomas S. Anantharaman. A Statistical Study of Selective Min-Max Search in Computer Chess.\\nPhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA, 1990.\\nTechnical Report CMU-CS-90-173.\\n[2] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.\\n[3] Hans J. Berliner, Gordon Goetsch, Murray S. Campbell, and Carl Ebeling. Measuring the\\nperformance potential of chess programs. Artificial Intelligence, 43:7-20, 1990.\\n[4] Justin A. Boyan. Generalization in reinforcement learning: Safely approximating the value\\nfunction. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information\\nProcessing Systems 7, San Mateo, CA, 1995. Morgan Kaufmann. (to appear).\\n[5] Gerald Dejong and Raymond Mooney. Explanation-based learning: An alternative view. Machine Learning, 1(2): 145-176, 1986.\\n[6] Michael Gherrity. A Game-Learning Machine. PhD thesis, University of California, San Diego,\\n1993.\\n[7] Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. Explanation-based generalization: A\\nunifying view. Machine Learning, 1(1 ):47-80, 1986.\\n[8] Tom M. Mitchell and Sebastian Thrun. Explanation based learning: A comparison of symbolic\\nand neural network approaches. In Paul E. Utgoff, editor, Proceedings of the Tenth International\\nConference on Machine Learning, pages 197-204, San Mateo, CA, 1993. Morgan Kaufmann.\\n[9] Tom M. Mitchell and Sebastian Thrun. Explanation-based neural network learning for robot\\ncontrol. In S. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information\\nProcessing Systems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.\\n[10] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal on\\nresearch and development, 3:210-229, 1959.\\n[11] Johannes Schafer. Erfolgsorientiertes Lemen mit Tiefensuche in Bauemendspielen. Technical\\nreport, UniversiUit Karlsruhe, 1993. (in German).\\n[12] Nikolaus Schraudolph, Pater Dayan, and Terrence J. Sejnowski. Using the TD(lambda) algorithm\\nto learn an evaluation function for the game of go. In Advances in Neural Information Processing\\nSystems 6, San Mateo, CA, 1994. Morgan Kaufmann.\\n[13] Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. Tangent prop -a formalism for\\nspecifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P.\\nLippmann, editors, Advances in Neural Information Processing Systems 4, pages 895-903, San\\nMateo, CA, 1992. Morgan Kaufmann.\\n[14] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\\n3,1988.\\n[15] Prasad Tadepalli. Planning in games using approximately learned macros. In Proceedings of the\\nSixth International Workshop on Machine Learning, pages 221-223, Ithaca, NY, 1989. Morgan\\nKaufmann.\\n[16] Gerald J. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8, 1992.\\n[17] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors,\\nProceedings of the 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Erlbaum\\nAssociates.\\n\\n\\x0c',\n",
       "     'pdf_name': '1007-learning-to-play-the-game-of-chess.pdf',\n",
       "     'title': 'Learning to Play the Game of Chess',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1013',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1013',\n",
       "     'paper_text': 'Ocular Dominance and Patterned Lateral\\nConnections in a Self-Organizing Model of the\\nPrimary Visual Cortex\\nJoseph Sirosh and Risto Miikkulainen\\n\\nDepartment of Computer Sciences\\nUniversity of Texas at Austin, Austin, \\'IX 78712\\nemail:\\n\\nsirosh.risto~cs.utexas.edu\\n\\nAbstract\\nA neural network model for the self-organization of ocular dominance and\\nlateral connections from binocular input is presented. The self-organizing\\nprocess results in a network where (1) afferent weights of each neuron organize into smooth hill-shaped receptive fields primarily on one of the retinas, (2) neurons with common eye preference form connected, intertwined\\npatches, and (3) lateral connections primarily link regions of the same eye\\npreference. Similar self-organization of cortical structures has been observed experimentally in strabismic kittens. The model shows how patterned lateral connections in the cortex may develop based on correlated\\nactivity and explains why lateral connection patterns follow receptive field\\nproperties such as ocular dominance.\\n\\n1 Introduction\\nLateral connections in the primary visual cortex have a patterned structure that closely\\nmatches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.1993).\\nFor example, in the normal visual cortex, long-range lateral connections link areas with similar orientation preference (Gilbert and Wiesel 1989). Like cortical response properties, the\\nconnectivity pattern is highly plastic in early development and can be altered by experience\\n(Katz and Callaway 1992). In a cat that is brought up squint-eyed from birth, the lateral connections link areas with the same ocular dominance instead of orientation (Lowel and Singer\\n1992). Such patterned lateral connections develop at the same time as the orientation selectivity and ocular dominance itself (Burkhalter et al.1993; Katz and Callaway 1992). Together,\\n\\n\\x0c110\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\nthese observations suggest that the same experience-dependent process drives the development of both cortical response properties and lateral connectivity.\\nSeveral computational models have been built to demonstrate how orientation preference,\\nocular dominance, and retinotopy can emerge from simple self-organizing processes (e.g.\\nGoodhill1993; Miller 1994; Obermayer et al.1992; von der Malsburg 1973). These models\\nassume that the neuronal response properties are primarily determined by the afferent connections, and concentrate only on the self-organization of the afferent synapses to the cortex. Lateral interactions between neurons are abstracted into simple mathematical functions\\n(e.g. Gaussians) and assumed to be uniform throughout the network; lateral connectivity is not\\nexplicitly taken into account. Such models do not explicitly replicate the activity dynamics\\nof the visual cortex, and therefore can make only limited predictions about cortical function.\\nWe have previously shown how Kohonen\\'s self-organizing feature maps (Kohonen 1982)\\ncan be generalized to include self-organizing lateral connections and recurrent activity dynamics (the Laterally Interconnected Synergetically Self-Organizing Map (LISSOM); Sirosh\\nand Miikkulainen 1993, 1994a), and how the algorithm can model the development of ocular dominance columns and patterned lateral connectivity with abstractions of visual input.\\nLISSOM is a low-dimensional abstraction of cortical self-organizing processes and models a\\nsmall region of the cortex where all neurons receive the same input vector. This paper shows\\nhow realistic, high-dimensional receptive fields develop as part of the self-organization, and\\nscales up the LISSOM approach to large areas of the cortex where different parts of the cortical network receive inputs from different parts of the receptor surface. The new model shows\\nhow (1) afferent receptive fields and ocular dominance columns develop from simple retinal images, (2) input correlations affect the wavelength of the ocular dominance columns and\\n(3) lateral connections self-organize cooperatively and simultaneously with ocular dominance\\nproperties. The model suggests new computational roles for lateral connections in the cortex,\\nand suggests that the visual cortex maybe maintained in a continuously adapting equilibrium\\nwith the visual input by co adapting lateral and afferent connections.\\n\\n2\\n\\nThe LISSOM Model of Receptive Fields and Ocular Dominance\\n\\nThe LISSOM network is a sheet of interconnected neurons (figure 1). Through afferent connections, each neuron receives input from two \"retinas\". In addition, each neuron has reciprocal excitatory and inhibitory lateral connections with other neurons. Lateral excitatory connections are short-range, connecting only close neighbors. Lateral inhibitory connections run\\nfor long distances, and may even implement full connectivity between neurons in the network.\\nNeurons receive afferent connections from broad overlapping patches on the retina called\\nanatomical receptive fields, or RFs. The N x N network is projected on to each retina of\\nR x R receptors, and each neuron is connected to receptors in a square area of side s around\\nthe projections. Thus, neurons receive afferents from corresponding regions of each retina.\\nDepending on the location of the projection, the number of afferents to a neuron from each\\nretina could vary from\\nx ~s (at the comers) to s x s (at the center).\\n\\nts\\n\\nThe external and lateral weights are organized through an unsupervised learning process. At\\neach training step, neurons start out with zero activity. The initial response TJij of neuron (i, j)\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\nLoft _ . .\\n\\n111\\n\\nfllgIIl Roll . .\\n\\nFigure 1: The Receptive-Field LISSOM architecture. The afferent and lateral connectionsof a single\\nneuron in the liSSOM network are shown. All connection weights are positive.\\n\\nis based on the scalar product\\nTJij\\n\\n=\\n\\n(T\\n\\n(L\\n\\neabJJij ,ab\\n\\n+\\n\\na,b\\n\\nL\\n\\n(1)\\n\\neCdJJij,Cd) ,\\n\\nc,d\\n\\nwhere eab and ecd are the activations of retinal receptors (a, b) and (c, d) within the receptive\\nfields of the neuron in each retina, JJij,ab and JJij,cd are the corresponding afferent weights,\\nand (T is a piecewise linear approximation of the familiar sigmoid activation function. The\\nresponse evolves over time through lateral interaction. At each time step, the neuron combines the above afferent activation I:: eJJ with lateral excitation and inhibition:\\nTJij(t)\\n\\n=\\n\\n(T\\n\\n(L eJJ + L\\n\"Ie\\n\\nEij,kITJkl(t -\\n\\n1) - L\\n\"Ii\\n\\nk,1\\n\\nIij,klTJkl(t -\\n\\n1)) ,\\n\\n(2)\\n\\nk,1\\n\\nwhere Eij,kl is the excitatory lateral connection weight on the connection from neuron (k, l)\\nto neuron (i, j), Iij,kl is the inhibitory connection weight, and TJkl (t - 1) is the activity of\\nneuron (k, I) during the previous time step. The constants \"Ie and \"Ii determine the relative\\nstrengths of excitatory and inhibitory lateral interactions. The activity pattern starts out diffuse and spread over a substantial part of the map, and converges iteratively into stable focused\\npatches of activity, or activity bubbles. After the-activity has settled, typically in a few iterations of equation 2, the connection weights of each neuron are modified. Both afferent and\\nlateral weights adapt according to the same mechanism: the Hebb rule, normalized so that the\\nsum of the weights is constant:\\n(\\n\\nWij,mn t\\n\\nr ) _\\n\\n+ vt\\n\\n-\\n\\n+\\n\\nWij,mn(t)\\nCtTJijXmn\\n\\'\"\"\\n( )\\nwmn [Wij ,mn t\\nCtTJijXmn\\n\\n+\\n\\n1\\'\\n\\n(3)\\n\\nwhere TJij stands for the activity of neuron (i, j) in the final activity bubble, Wij,mn is the afferent or lateral connection weight (JJ, E or I), Ct is the learning rate for each type of connection\\n(Ct a for afferent weights, Ct E for excitatory, and Ct I for inhibitory) and X mn is the presynaptic\\nactivity for afferent, TJ for lateral).\\n\\n(e\\n\\n\\x0cJoseph Sirosh, Risto Miikkulainen\\n\\n112\\n\\n\"\\n(a) Random Initial Weights\\n\\n(b) Monocular RF\\n\\n(c) Binocular RF\\n\\nFigure 2: Self-organization of the afferent input weights into receptive fields. The afferent weights\\nof a neuron at position (42,39) in a 60 x 60 network are shown before (a) and after self-organization\\n(b). This particular neuron becomes monocular with strong connections to the right eye, and weak connections to the left. A neuron at position (38, 23) becomes binocular with appoximately equal weights\\nto both eyes (c).\\nBoth excitatory and inhibitory lateral connections follow the same Hebbian learning process and strengthen by correlated activity. The short-range excitation keeps the activity of\\nneighboring neurons correlated, and as self-organization progresses, excitation and inhibition strengthen in the vicinity of each neuron. At longer distances, very few neurons have\\ncorrelated activity and therefore most long-range connections become weak. Such weak connections are eliminated, and through weight normalization, inhibition concentrates in a closer\\nneighborhood of each neuron. As a result, activity bubbles become more focused and local,\\nweights change in smaller neighborhoods, and receptive fields become better tuned to local\\nareas of each retina.\\nThe input to the model consists of gaussian spots of \"light\" on each retina:\\nt\\n_\\n((x\\n<\"x,y - exp -\\n\\n- xd 2 + (y - Yi)2)\\nu2\\n\\n(4)\\n\\nwhere ex,y is the activation of receptor (x, V), u 2 is a constant determining the width of the\\nspot, and (Xi,Yi): 0 ~ xi, Yi < R its center. At each input presentation, one spot is randomly\\nplaced at (Xi ,Yi) in the left retina, and a second spot within a radius of p x RN of (Xi, yd\\nin the right retina. The parameter p E [0, 1] specifies the spatial correlations between spots\\nin the two retinas, and can be adjusted to simulate different degrees of correlations between\\nimages in the two eyes.\\n\\n3\\n\\nSimulation results\\n\\nTo see how correlation between the input from the two eyes affects the columnar structures\\nthat develop, several simulations were run with different values of p. The afferent weights of\\nall neurons were initially random (as shown in figure 2a), with the total strength to both eyes\\nbeing equal.\\nFigures 2b,c show the final afferent receptive fields of two typical neurons in a simulation\\nwith p = 1. In this case, the inputs were uncorrelated, simulating perfect strabismus. In\\nthe early stages of such simulation, some of the neurons randomly develop a preference for\\none eye or the other. Nearby neurons will tend to share the same preference because lateral\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\n(a) Connections of a Monocular Neuron\\n\\n113\\n\\n(b) Connections of a Binocular Neuron\\n\\nFigure 3: Ocular dominance and lateral connection patterns. The ocular dominance of a neuron is\\nmeasured as the difference in total afferent synaptic weight from each eye to the neuron. Each neuron\\nis labeled with a grey-scale value (black ~ white) that represents continuously changing eye preference from exclusive left through binocular to exclusive right. Small white dots indicate the lateral input\\nconnections to the neuron marked with a big white dot. (a) The surviving lateral connections of a left\\nmonocular neuron predominantly link areas of the same ocular dominance. (b) The lateral connections\\nof a binocular neuron come from both eye regions.\\n\\nexcitation keeps neural activity partially correlated over short distances. As self-organization\\nprogresses, such preferences are amplified, and groups of neurons develop strong weights to\\none eye. Figure 2b shows the afferent weights of a typical monocular neuron.\\nThe extent of activity correlations on the network detennines the size of the monocular neuronal groups. Farther on the map, where the activations are anticorrelated due to lateral inhibition, neurons will develop eye preferences to the opposite eye. As a result, alternating\\nocular dominance patches develop over the map, as shown in figure 3. 1 In areas between ocular dominance patches, neurons will develop approximately equal strengths to both eyes and\\nbecome binocular, like the one shown in figure 2e.\\nThe width and number of ocular dominance columns in the network (and therefore, the wavelength of ocular dominance) depends on the input correlations (figure 4). When inputs in the\\ntwo eyes become more correlated (p < 1), the activations produced by the two inputs in the\\nnetwork overlap closely and activity correlations become shorter range. By Hebbian adaptation, lateral inhibition concentrates in the neighborhood of each neuron, and the distance at\\nwhich activations becomes anticorrelated decreases. Therefore, smaller monocular patches\\ndevelop, and the ocular dominance wavelength decreases. Similar dependence was very recently observed in the cat primary visual cortex (LoweI1994). The LISSOM model demonstrates that the adapting lateral interactions and recurrent activity dynamics regulate the wavelength, and suggests how these processes help the cortex develop feature detectors at a scale\\n1 For a thorough treatment of the mathematical principles underlying the development of ocular dominance columns, see (GoodhillI993; Miller et al.1989; von der Malsburg and Singer 1988).\\n\\n\\x0c114\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\n-0\\n-0\\n\\n(a) Strabismic case\\n\\n(b ) Normal case\\n\\nFigure 4: Ocular dominance wavelength in strabismic and normal models. In the strabismic case,\\nthere are no between-eye correlations (p = 1), and broad ocular dominance columns are produced (a) .\\nWith normal, partial between-eye correlations (p = 0.45 in this example), narrower stripes are formed\\n(b). As a result, there are more ocular dominance columns in the normal case and the ocular dominance\\nwavelength is smaller.\\n\\nthat matches the input correlations.\\nAs eye preferences develop, left or right eye input tends to cause activity only in the left or\\nright ocular dominance patches. Activity patterns in areas of the network with the same ocular dominance tend to be highly correlated because they are caused by the same input spot.\\nTherefore, the long-range lateral connections between similar eye preference areas become\\nstronger, and those between opposite areas weaker. After the weak lateral connections are\\neliminated, the initially wide-ranging connections are pruned, and eventually only connect\\nareas of similar ocular dominance as shown in figure 3. Binocular neurons between ocular\\ndominance patches will see some correlated activity in both the neigbboring areas, and maintain connections to both ocular dominance columns (figure 3b).\\nThe lateral connection patterns shown above closely match observations in the primary visual cortex. Lowel and Singer (1992) observed that when between-eye correlations are abolished in kittens by surgically induced strabismus, long-range lateral connections primarily\\nlink areas of the same ocular dominance. However, binocular neurons, located between ocular dominance columns, retained connections to both eye regions. The receptive field model\\nconfinns that such patterned lateral connections develop based on correlated neuronal activity,\\nand demonstrates that they can self-organize simultaneously with ocular dominance columns.\\nThe model also predicts that the long-range connections have an inhibitory function.\\n\\n4 Discussion\\nIn LISSOM, evolving lateral interactions and dynamic activity patterns are explicitly modeled. Therefore, LISSOM has several novel properties that set it apart from other selforganizing models of the cortex.\\nPrevious models (e.g. Goodhill1993; Milleret al.1989; Obermayer et al.1992; von der Malsburg 1973) have concentrated only on forming ordered topographic maps where clusters of\\nadjacent neurons assume similar response properties such as ocular dominance or orientation\\npreference. The lateral connections in LISSOM, in addition, adapt to encode correlations be-\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\n115\\n\\ntween the responses. 2 This property can be potentially very useful in models of cortical function. While afferent connections learn to detect the significant features in the input space (such\\nas ocularity or orientation), the lateral connections can learn correlations between these features (such as Gestalt principles), and thereby form a basis for feature grouping.\\nAs an illustration, consider a single spot of light presented to the left eye. The spot causes disjoint activity patterns in the left-eye-dominant patches. How can these multiple activity patterns be recognized as representing the same spatially coherent entity? As proposed by Singer\\net al. (1990), the long-range lateral connections between similar ocular dominance columns\\ncould synchronize cortical activity, and form a coherently firing assembly of neurons. The\\nspatial coherence of the spot will then be represented by temporal coherence of neural activity. LISSOM can be potentially extended to model such feature binding.\\nEven after the network has self-organized, the lateral and afferent connections remain plastic\\nand in a continuously-adapting dynamic equilibrium with the input. Therefore, the receptive\\nfield properties of neurons can dynamically readapt when the activity correlations in the network are forced to change. For example, when a small area of the cortex is set inactive (or\\nlesioned), the sharply-tuned afferent weight profiles of the neurons surrounding that region\\nexpand in size, and neurons begin to respond to the stimuli that previously activated only the\\nlesioned area (Sirosh and Miikkulainen 1994b, 1994c). This expansion of receptive fields is\\nreversible, and when the lesion is repaired, neurons return to their original tuning. Similar\\nchanges occur in response to retinal lesions as well. Such dynamic expansions of receptive\\nfields have been observed in the visual cortex (Pettet and Gilbert 1992). The LISSOM model\\ndemonstrates that such plasticity is a consequence of the same self-organizing mechanisms\\nthat drive the development of cortical maps.\\n\\n5\\n\\nConclusion\\n\\nThe LISSOM model shows how a single local and unsupervised self-organizing process can\\nbe responsible for the development of both afferent and lateral connection structures in the primary visual cortex. It suggests that this same developmental mechanism also encodes higherorder visual information such as feature correlations into the lateral connections. The model\\nforms a framework for future computational study of cortical reorganization and plasticity, as\\nwell as dynamic perceptual processes such as feature grouping and binding.\\nAcknowledgments\\n\\nThis research was supported in part by National Science Foundation under grant #IRI9309273. Computer time for the simulations was provided by the Pittsburgh Supercomputing\\nCenter under grants IRI930005P and TRA940029P.\\n\\nReferences\\nBurkhalter, A., Bernardo, K. L., and Charles, V. (1993). Development of local circuits in\\nhuman visual cortex. Journalo/Neuroscience, 13:1916-1931.\\nGilbert, C. D., and Wiesel, T. N. (1989). Columnar specificity of intrinsic horizontal and\\ncorticocortical connections in cat visual cortex. Journal 0/ Neuroscience, 9:2432-2442.\\n2Tbe idea was conceived by von der Malsburg and Singer (1988), but not modeled.\\n\\n\\x0c116\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\nGoodhill, G. (1993). Topography and ocular dominance: a model exploring positive correlations. Biological Cybernetics, 69:109-118.\\nKatz, L. C., and Callaway, E. M. (1992). Development of local circuits in mammalian visual\\ncortex. Annual Review o/Neuroscience, 15:31-56.\\nKohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biolog-\\n\\nical Cybernetics, 43:59-69.\\nLowel, S. (1994). Ocular dominance column development: Strabismus changes the spacing\\nof adjacent columns in cat visual cortex. Journal 0/ Neuroscience, 14(12):7451-7468.\\nLowel, S., and Singer, W. (1992). Selection of intrinsic horizontal connections in the visual\\ncortex by correlated neuronal activity. Science, 255:209-212.\\nMalach, R., Amir, Y., Harel, M., and Grinvald, A (1993). Relationship between intrinsic\\nconnections and functional architecture revealed by optical imaging and in vivo targeted\\nbiocytin injections in the primate striate cortex. Proceedings o/the National Academy\\n\\no/Sciences, USA,90:10469-10473.\\nMiller, K. D. (1994). A model for the development of simple cell receptive fields and the\\nordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs. Journalo/Neuroscience, 14:409-441.\\nMiller, K. D., Keller, 1. B., and Stryker, M. P. (1989). Ocular dominance column development:\\nAnalysis and simulation. Science, 245:605-615.\\nObermayer, K., Blasdel, G. G., and Schulten, K. J. (1992). Statistical-mechanical analysis of\\nself-organization and pattern formation during the development of visual maps. Physical\\n\\nReview A, 45:7568-7589.\\nPettet, M. W., and Gilbert, C. D. (1992). Dynamic changes in receptive-field size in cat primary visual cortex. Proceedings o/the NationalAcademy 0/ Sciences, USA,89:83668370.\\nSinger, W., Gray, C., Engel, A, Konig, P., Artola, A, and Bracher, S. (1990). Formation of\\ncortical cell assemblies. In Cold Spring Harbor Symposia on Quantitative Biology, Vol.\\nLV, 939-952. Cold Spring Harbor, NY: Cold Spring Harbor Laboratory.\\nSirosh, J., and Miikkulainen, R. (1993). How lateral interaction develops in a self-organizing\\nfeature map. In Proceedings o/the IEEE International Conference on Neural Networks\\n(San Francisco, CA), 1360--1365. Piscataway, NJ: IEEE.\\nSirosh, J., and Miikkulainen, R. (1994a). Cooperative self-organization of afferent and lateral\\nconnections in cortical maps. Biological Cybernetics, 71(1):66--78.\\nSirosh, 1., and Miikkulainen, R. (1994b). Modeling cortical plasticity based on adapting lateral interaction. In The Neurobiologyo/Computation: Proceedings o/the Annual ComputationalNeuroscience Meeting. Dordrecht; Boston: Kluwer. In Press.\\nSirosh, J., and Miikkulainen, R. (1994c). A neural network model oftopographic reorganization following cortical lesions. In Proceedings o/the World Congress on Computational\\nMediCine, Public Health and BioteChnology (Austin, TX). World Scientific. In Press.\\nvon der Malsburg, C. (1973). Self-organization of orientation-sensitive cells in the striate\\ncortex. Kybernetik, 15:85-100.\\nvon der Malsburg, C., and Singer, W. (1988). Principles of cortical network organization. In\\nRakic, P., and Singer, W., editors, Neurobiology 0/Neocortex, 69-99. New York: Wiley.\\n\\n\\x0c',\n",
       "     'pdf_name': '1013-ocular-dominance-and-patterned-lateral-connections-in-a-self-organizing-model-of-the-primary-visual-cortex.pdf',\n",
       "     'title': 'Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1032',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1032',\n",
       "     'paper_text': 'VLSI Model of Primate Visual Smooth Pursuit\\n\\nRalph Etienne-Cummings\\n\\nJan Van der Spiegel\\n\\nDepartment of Electrical Engineering,\\nSouthern Illinois University, Carbondale,\\nIL 62901\\n\\nMoore School of Electrical Engineering,\\nUniversity of Pennsylvania, Philadelphia,\\nPA 19104\\n\\nPaul Mueller\\nCorticon, Incorporated,\\n3624 Market Str, Philadelphia,\\nPA 19104\\n\\nAbstract\\nA one dimensional model of primate smooth pursuit mechanism has\\nbeen implemented in 2 11m CMOS VLSI. The model consolidates\\nRobinson\\'s negative feedback model with Wyatt and Pola\\'s positive\\nfeedback scheme, to produce a smooth pursuit system which zero\\'s the\\nvelocity of a target on the retina. Furthermore, the system uses the\\ncurrent eye motion as a predictor for future target motion. Analysis,\\nstability and biological correspondence of the system are discussed. For\\nimplementation at the focal plane, a local correlation based visual\\nmotion detection technique is used. Velocity measurements, ranging\\nover 4 orders of magnitude with < 15% variation, provides the input to\\nthe smooth pursuit system. The system performed successful velocity\\ntracking for high contrast scenes. Circuit design and performance of the\\ncomplete smooth pursuit system is presented.\\n\\n1 INTRODUCTION\\nThe smooth pursuit mechanism of primate visual systems is vital for stabilizing a region\\nof the visual field on the retina. The ability to stabilize the image of the world on the\\nretina has profound architectural and computational consequences on the retina and visual\\ncortex, such as reducing the required size, computational speed and communication\\nhardware and bandwidth of the visual system (Bandera, 1990; Eckert and Buchsbaum,\\n1993). To obtain similar benefits in active machine vision, primate smooth pursuit can\\nbe a powerful model for gaze control. The mechanism for smooth pursuit in primates\\nwas initially believed to be composed of a simple negative feedback system which\\nattempts to zero the motion of targets on the fovea, figure I (a) (Robinson, 1965).\\nHowever, this scheme does not account for many psychophysical properties of smooth\\n\\n\\x0c707\\n\\nVLSI Model of Primate Visual Smooth Pursuit\\n\\npursuit, which led Wyatt and Pola (1979) to proposed figure l(b), where the eye\\nmovement signal is added to the target motion in a positive feed back loop. This\\nmechanism results from their observation that eye motion or apparent target motion\\nincreases the magnitude of pursuit motion even when retinal motion is zero or constant.\\nTheir scheme also exhibited predictive qualities, as reported by Steinbach (1976). The\\nsmooth pursuit model presented in this paper attempts the consolidate the two models\\ninto a single system which explains the findings of both approaches.\\nTarget\\nMoticn\\n\\nEye\\nMotion\\n\\nRetinal\\nMotion\\n\\ne~\\n\\nlee\\n\\nG\\n\\nee = e t G+l\\n~;\\n\\n>\\n\\nI\\nG ~ co G r\\n\\nTarget\\nMotion\\n\\nEye\\nMotion\\n\\ne~~\\n\\n>\\n\\n=0\\n(b)\\n\\n(a)\\n\\nFigure I: System Diagrams of Primate Smooth Pursuit Mechanism.\\n(a) Negative feedback model by Robinson (1965). (b) Positive\\nfeedback model by Wyatt and Pola (1979).\\nThe velocity based smooth pursuit implemented here attempts to zero the relative velocity\\nof the retina and target. The measured retinal velocity, is zeroed by using positive\\nfeedback to accumulate relative velocity error between the target and the retina, where the\\naccumulated value is the current eye velocity. Hence, this model uses the Robinson\\napproach to match target motion, and the Wyatt and Pola positive feed back loop to\\nachieve matching and to predict the future velocity of the target. Figure 2 shows the\\nsystem diagram of the velocity based smooth pursuit system. This system is analyzed\\nand the stability criterion is derived. Possible computational blocks for the elements in\\nfigure I (b) are also discussed. Furthermore, since this entire scheme is implemented on a\\nsingle 2 /lm CMOS chip, the method for motion detection, the complete tracking circuits\\nand the measured results are presented.\\nRetinal\\nMotion\\n\\nEye\\nMotion\\n\\ner\\n\\nFigure 2: System Diagram of VLSI Smooth Pursuit Mechanism.\\nis target velocity in space, Bt is projected target velocity, Be is the eye\\nvelocity and Br is the measured retinal velocity.\\n\\n2 VELOCITY BASED SMOOTH PURSUIT\\nAlthough figure I (b) does not indicate how retinal motion is used in smooth pursuit, it\\nprovides the only measurement of the projected target motion. The very process of\\ncalculating retinal motion realizes negative feed back between the eye movement and the\\ntarget motion, since retinal motion is the difference between project target and eye\\nmotion. If Robinson\\'s model is followed, then the eye movement is simply the\\namplified version of the retinal motion. If the target disappears from the retina, the eye\\nmotion would be zero. However, Steinbach showed that eye movement does not cea~\\nwhen the target fades off and on, indicating that memory is used to predict target motion.\\nWyatt and Palo showed a direct additive influence of eye movement on pursuit. However,\\nthe computational blocks G\\' and a of their model are left unfilled.\\n\\n\\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\n708\\n\\nIn figure 2, the gain G models the internal gain of the motion detection system , and the\\ninternal representation of retinal velocity is then Vr. Under zero-slip tracking, the retinal\\nvelocity is zero. This is obtained by using positive feed back to correct the velocity error\\nand eye,\\nThe delay element represents a memory of the last eye\\nbetween target,\\nvelocity while the current retinal motion is measured. If the target disappears, the eye\\nmotion continues with the last value, as recorded by Steinbach, thus anticipating the\\nposition of the target in space. The memory also stores the current eye velocity during\\nperfect pursuit. The internal representation of eye velocity, Ve , is subsequently amplified\\nby H and used to drive the eye muscles. The impulse response of the system is given in\\nequations (I). Hence, the relationship between eye velocity and target velocity is recursive\\nand given by equations (2). To prove the stability of this system, the retinal velocity can\\nbe expressed in terms of the target motion as given in equations (3a). The ideal condition\\nfor accurate performance is for GH = 1. However, in practice, gains of different amplifiers\\n\\ner,\\n\\n()\\n\\nz-)\\n\\n=GH--_-)\\n\\n-.f..(z)\\n\\n1- Z\\n\\n(}r\\n\\nee.\\n\\n()\\n\\n(a); ~(I1)\\n(}r\\n\\n=GH[-8(11) + u(n)]\\n\\n(I)\\n\\n(b)\\nn-)\\n\\n(}e(n)\\n\\n= (},(n) -\\n\\n(}r(n)\\n\\n=GH[-8(n) + u(n)] * (}r(n) = GHL(},.(k)\\n\\n(2)\\n\\nk=O\\n() r ( 11)\\n\\n() r (n)\\n\\n= (),( n ) (1\\n11\\n\\n~\\n\\n00\\n\\n)\\n\\n11\\n\\n- GH)\\n0\\n\\nif 11 -\\n\\n=> () r( 1l )\\n\\nI\\n\\nGH < 1\\n\\n= 0 if\\n\\nGH\\n\\n= 1 =>\\n\\n() in)\\n\\n= (),( 11 )\\n\\n=> 0 < GH < 2 for stability\\n\\n(\\n\\na)\\n\\n(3)\\n\\n( b)\\n\\nare rarely perfectly matched. Equations (3b) shows that stability is assured for O<GH< 2.\\nFigure 3 shows a plot of eye motion versus updates for various choices of GH. At each\\nupdate, the retinal motion is computed. Figure 3(a) shows the eye\\'s motion at the on-set\\nof smooth pursuit. For GH = 1, the eye movement tracks the target\\'s motion exactly,\\nand lags slightly only when the target accelerates. On the other hand, if GH? I, the\\neye\\'s motion always lags the target\\'s. If GH -> 2, the system becomes increasing\\nunstable, but converges for GH < 2. The three cases presented correspond to the smooth\\npursuit system being critically, over and under damped, respectively.\\n\\n3 HARDWARE IMPLEMENTATION\\nUsing the smooth pursuit mechanism described, a single chip one dimensional tracking\\nsystem has been implemented. The chip has a multi-layered computational architecture,\\nsimilar to the primate\\'s visual system. Phototransduction, logarithmic compression,\\nedge detection, motion detection and smooth pursuit control has been integrated at the\\nfocal-plane. The computational layers can be partitioned into three blocks, where each\\nblock is based on a segment of biological oculomotor systems.\\n\\n3.1\\n\\nIMAGING AND PREPROCESSING\\n\\nThe first three layers of the system mimics the photoreceptors, horizontal cells arx:l\\nbipolar cells of biological retinas. Similar to previous implementations of silicon\\nretinas, the chip uses parasitic bipolar transistors as the photoreceptors. The dynamic\\nrange of photoreceptor current is compressed with a logarithmic response in low light arx:l\\nsquare root response in bright light. The range compress circuit represents 5-6 orders of\\nmagnitude of light intensity with 3 orders of magnitude of output current dynamic range.\\nSubsequently, a passive resistive network is used to realize a discrete implementation of a\\nLaplacian edge detector. Similar to the rods and cones system in primate retinas, the\\nresponse time, hence the maximum detectable target speed, is ambient intensity dependent\\n(160 (12.5) Ils in 2.5 (250) IlW/cm2). However, this does prevent the system from\\nhandling fast targets even in dim ambient lighting.\\n\\n\\x0cVLSI Model of Primate Visual Smooth Pursuit\\n\\n~\\n\\ng\\n\\nu\\n>\\n\\n709\\n\\n20\\n\\n20\\n\\n15\\n\\n15\\n\\n10\\n\\n10\\n\\n5\\n\\n~\\n\\n5\\n\\n0\\n\\n]\\n\\n-5\\n\\n>\" -5\\n\\n- 10\\n\\n?\\n\\n? 10\\n\\nTarget\\n\\n- -Eye: GH=I 99\\n- E ye GH=IOO\\n__ . Eye: GH=O_IO\\n\\n-15\\n\\n0\\n\\n? 15\\n-20\\n\\n-20\\n100\\n\\n50\\n\\n0\\n\\n150\\n\\n500\\n\\n600\\n\\nUpdates\\n\\n(a)\\n\\n700\\n800\\nUpdates\\n\\n900\\n\\n1000\\n\\n(b)\\n\\nFigure 3: (a) The On-Set of Smooth Pursuit for Various GH Values.\\n(b) Steady-State Smooth Pursuit.\\n\\n3.2\\n\\nMOTION MEASUREMENT\\n\\nThis computational layer measures retinal motion. The motion detection technique\\nimplemented here differs from those believed to exist in areas V 1 and MT of the primate\\nvisual cortex. Alternatively, it resembles the fly\\'s and rabbit\\'s retinal motion detection\\nsystem (Reichardt, 1961; Barlow and Levick, 1965; Delbruck, 1993). This is not\\ncoincidental, since efficient motion detection at the focal plane must be performed in a\\nsmall areas and using simple computational elements in both systems.\\nThe motion detection scheme is a combination of local correlation for direction\\ndetermination, and pixel transfer time measurement for speed. In this framework, motion\\nis defined as the disappearance of an object, represented as the zero-crossings of its edges,\\nat a pixel , followed by its re-appearance at a neighboring pixel. The (dis)appearance of\\nthe zero-crossing is determined using the (negative) positive temporal derivative at the\\npixel. Hence, motion is detected by AND gating the positive derivative of the zerocrossing of the edge at one pixel with the negative derivative at a neighboring pixel. The\\ndirection of motion is given by the neighboring pixel from which the edge disappeared.\\nProvided that motion has been detected at a pixel, the transfer time of the edge over the\\npixel\\'s finite geometry is inversely proportional to its speed.\\nEquation (4) gives the mathematical representation of the motion detection process for an\\nobject moving in +x direction. In the equation. f,(l.\\'k ,y.t) is the temporal response of\\npixel k as the zero crossing of an edge of an object passes over its 2a aperture. Equation\\n(4) gives the direction of motion, while equation (5) gives the speed. The schematic of\\n\\nmotion _ x = [\\n\\nf f,( l: k, y, t) > 0] [ f f t(l.\\' k + J, y, t) < 0] =0\\n\\nmotion+x=[~f,(l.\\'k-J,y,t)<O][~f/l.\\'k , y,t?O]\\n\\n= 8[t\\nMotion.\\' t m =\\n\\nSpeed + x\\n\\n=\\n\\nt\\n\\n-\\n\\n(b)\\n\\n(4)\\n\\n2a(k-n)-a\\nv\\n]8[x - 2ak]\\nx\\n\\n2a(k -n) -a\\nvx\\n\\nJ\\n- t\\n\\n( a)\\n\\nvx\\n\\n2a\\n\\nDisappear .\\' t d\\n\\n2a(k -n) +a\\n\\n= --~--?\\nvx\\n\\n(5)\\n\\nd\\nm\\nthe VLSI circuit of the motion detection model is shown in figure 4(a). Figure 4(b)\\nshows reciprocal of the measured motion pulse-width for 1 D motion. The on-chip speed,\\net, is the projected target speed. The measured pulse-widths span 3-4 orders magnitude,\\n\\n\\x0c710\\n\\nR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\nOne-Over Pulse-Width vs On-Chip Speed\\n\\n?\\n\\nO.R\\n\\n~\\n\\n0.4\\n\\n\"\\n\\n~ -0.0 +--------::II~-----__+\\nM\\n~ -0.4\\n\\n---e-- \\\\IPW_Lefi\\n\\n-0 .8\\n\\n- - . - - IIPW_ Rlght\\n\\n- 1.2 +-\\'----\\'--\\'\\'-+--\\'--\\'--\\'--t---\\'--\\'\\'--\\'-+-\\'--\\'--\\'-t-\\'--\\'--\\'-t---\\'--\\'--\\'-+\\n-40\\n00\\n4.0\\n8.0\\n12.0\\n-12.0\\n-R.O\\nOn-Chip Speed rcml~J\\n\\nRight\\n\\nLeft\\n\\n(b)\\n\\n(a)\\n\\nFigure 4: (a) Schematic of the Motion Detection Circuit.\\nMeasured Output of the Motion Detection Circuit.\\n\\n(b)\\n\\ndepending on the ambient lighting, and show less than 15% variation between chips,\\npixels, and directions (Etienne-Cummings, 1993).\\n\\n3.3\\n\\nTHE SMOOTH PURSUIT CONTROL SYSTEM\\n\\nThe one dimensional smooth pursuit system is implemented using a 9 x I array of\\nmotion detectors. Figure 5 shows the organization of the smooth pursuit chip. In this\\nsystem, only diverging motion is computed to reduce the size of each pixel. The outputs\\nof the motion detectors are grouped into one global motion signal per direction. This\\ngrouping is performed with a simple, but delayed, OR, which prevents pulses from\\nneighboring motion cells from overlapping. The motion pulse trains for each direction\\nare XOR gated, which allows a single integrator to be used for both directions, thus\\nlimiting mis-match_ The final value of the integrator is inversely proportional to the\\ntarget\\'s speed. The OR gates conserve the direction of motion. The reciprocal of the\\nintegrator voltage is next computed using the linear mode operation of a MOS transistor\\n(Etienne-Cummings, 1993). The unipolar integrated pulse allows a single inversion\\ncircuit to be used for both directions of motion, again limiting mis-match. The output of\\nthe \"one-over\" circuit is amplified, and the polarity of the measured speed is restored.\\nThis analog voltage is proportional to retinal speed.\\nThe measured retinal speed is subsequently ailed to the stored velocity. Figure 6 shows\\nthe schematic for the retinal velocity accumulation (positive feedback) and storage (analog\\nWave Forms\\n\\nMotion Pulse Integration\\nand \"One-Over\"\\nV = GIRetinal Velocityl\\n\\nPolarity\\nRestoration\\n\\nRetinal Velocity\\nAccumulation\\nand Sample/Hold\\n\\nFigure 5: Architecture of the VLSI Smooth Pursuit System. Sketches\\nof the wave forms for a fast leftward followed by a slow rightward\\nretinal motion are shown.\\n\\n\\x0c711\\n\\nVLSI Model of Primate Visual Smooth Pursuit\\n\\nmemory). The output of the XOR gate in figure 5 is used by the sample-and-hold circuit\\nto control sampling switches S I and S2. During accumulation, the old stored velocity\\nvalue, which is the current eye velocity, is isolated from the summed value. At the\\nfalling edge of the XOR output, the stored value on C2 is replaced by the new value on\\nCl. This stored value is amplified using an off chip motor driver circuit, and used to\\nmove the chip. The gain of the motor driver can be finely controlled for optimal\\noperation.\\n\\nMotor\\n\\nRetinal\\nVelocity\\n\\nSystem\\n\\nAccumulatiun\\n\\nTarget\\nVelocity\\n\\nTwo Phase Sample/Hold\\n\\nFigure 6: Schematic Retinal Velocity Error Accumulation, Storage and\\nMotor Driver Systems.\\nFigure 7(a) shows a plot of one-over the measured integrated voltage as a function of on\\nchip target speed. Due to noise in the integrator circuit, the dynamic range of the motion\\ndetection system is reduced to 2 orders of magnitude. However, the matching between left\\nand right motion is unaffected by the integrator. The MaS \"one-over\" circuit, used to\\ncompute the analog reciprocal of the integrated voltage, exhibits only 0.06% deviation\\nfrom a fitted line (Etienne-Cummings, 1993b). Figure 7(b) shows the measured\\nincrements in stored target velocity as a function of retinal (on-chip) speed. This is a test\\nof all the circuit components of the tracking system. Linearity between retinal velocity\\nincrements and target velocity is observed, however matching between opposite motion\\nhas degraded. This is caused by the polarity restoration circuit since it is the only\\nlocation where different circuits are used for opposite motion. On average, positive\\nincrements are a factor of 1.2 times larger than negative increments. The error bars shows\\nthe variation in velocity increments for different motion cells and different Chips. The\\ndeviation is less than 15 %. The analog memory has a leakage of 10 mV/min and an\\nasymmetric swing of 2 to -1 V, caused by the buffers. The dynamic range of the\\ncomplete smooth pursuit system is measured to be 1.5 orders magnitude. The maximum\\nspeed of the system is adjustable by varying the integrator charging time. The maximum\\nspeed is ambient intensity dependent and ranges from 93 cmls to 7 cm/s on-chip speed in\\nVelocity Error Increment vs On-Chip Speed\\n\\nIntegrated Pulse vs On-Chip Speed\\n1.4\\n24\\n\\n~\\n\\n16\\n\\n~\\n\\n8\\n\\n~\\n\\n0\\n\\nil\\n?\\noS\\n\\n.\\'\\n._\\n\\n1.2\\n\\n~\\n\\nl\\'! 1.0\\n\\n\"e~\\nu\\n\\n-t--------\",/II!...------+\\n\\n-8\\n\\n.s\\n\\nO.R\\n\\ng 0 .6\\n\\nLLl\\n\\n.::;.\\n\\ng 04\\n\\n:: -16\\n-e--lnlPuI~_l..xft\\n\\n-24\\n\\n_ _? _\\n\\nJntPlllo;e_Rl~hl\\n\\n-32 -t-\\'---\\'---\\'-\\'--+-\\'--~~-t--\\'\"-\\'-~_t_--\"--\\'\\'---\\'---\"-t\\n10.0\\n-100\\n-5.0\\n0.0\\n5.0\\nOn-Chip Speed lemlsl\\n\\n(a)\\n\\nOJ\\n\\n>\\n\\n- - - - . Nc~_ Jn c rt~nl\\n\\n02\\n\\n__ ? _ _Po,,_Incremclll\\n\\n0.0\\n0\\n\\n4\\n6\\nOn-Chip Speed lem/s)\\n\\n(b)\\n\\nFigure 7. (a) Measured integrated motion pulse voltage. (b) Measured\\noutput for the complete smooth pursuit system.\\n\\n10\\n\\n\\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\n712\\n\\nbright (250 JlW/cm 2) and dim (2.5 JlW/cm 2) lighting, respectively. However, for any\\nmaximum speed chosen, the minimum speed is a factor of 0.03 slower. The minimum\\nspeed is limited by the discharge time of the temporal differentiators in the motion\\ndetection circuit to 0.004 cmls on chip. The contrast sensitivity of this system proved to\\nbe the stumbling block, and it can not track objects in normal indoor lighting. However,\\nall circuits components tested successfully when a light source is used as the target.\\nAdditional measured data can be found in (Etienne-Cummings, 1995). Further work will\\nimprove the contrast sensitivity, combat noise and also consider two dimensional\\nimplementations with target acquisition (saccades) capabilities.\\n\\n4\\n\\nCONCLUSION\\n\\nA model for biological and silicon smooth pursuit has been presented. It combines the\\nnegative feed back and positive feedback models of Robinson and Wyatt and Pola. The\\nsmooth pursuit system is stable if the gain product of the retinal velocity detection\\nsystem and the eye movement system is less than 2. VLSI implementation of this\\nsystem has been performed and tested. The performance of the system suggests that wide\\nrange (92.9 - 0.004 cmls retinal speed) target tracking is possible with a single chip focal\\nplane system. To improve this chip\\'s performance, care must be taken to limit noise,\\nimprove matching and increase contrast sensitivity. Future design should also include a\\nsaccadic component to re-capture escaped targets, similar to biological systems.\\n\\nReferences\\nC. Bandera, \"Foveal Machine Vision Systems\", Ph.D. Thesis, SUNY Buffalo, New\\nYork, ]990\\nH. Barlow and W. Levick, \\'The Mechanism for Directional Selective Units in Rabbit\\' s\\nRetina\", Journal of Physiology, Vol. 178, pp. 477-504, ]965\\nT. Delbruck, \"Silicon Retina with Correlation-Based, Velocity-Tuned Pixels \", IEEE\\nTransactions on Neural Networks, Vol. 4:3, pp. 529-41, 1993\\n\\nM. Eckert and G. Buchsbaum, \"Effect of Tracking Strategies on the Velocity Structure of\\nTwo-Dimensional Image Sequences\", J. Opt. Soc. Am., Vol. AIO:7, pp. 1582-85, 1993\\nR. Etienne-Cummings et at., \"A New Temporal Domain Optical Flow Measurement\\nTechnique for Focal Plane VLSI Implementation\", Proceedings of CAMP 93, M.\\nBayoumi, L. Davis and K. Valavanis (Eds.), pp. 24]-25] , 1993\\nR. Etienne-Cummings, R. Hathaway and J. Van der Spiegel, \"An Accurate and Simple\\nCMOS \\'One-Over\\' Circuit\", Electronic Letters, Vol. 29-18, pp. ]618-]620, 1993b\\nR. Etienne-Cummings et aI., \"Real-Time Visual Target Tracking: Two Implementations\\nof Velocity Based Smooth Pursuit\", Visual Information Processing IV, SPIE Vol. 2488,\\nOrlando, 17-18 April 1995\\n\\nW. Reichardt, \"Autocorrelation, A Principle for the Evaluation of Sensory Information by\\nthe Central Nervous System\", Sensory Communication, Wiley, New York, 1961\\nD. Robinson, \"The Mechanism of Human Smooth Pursuit Eye Movement\", Journal of\\nPhysiology ( London) Vol. 180, pp. 569-591 , 1965\\nM. Steinbach, \"Pursuing the Perceptual Rather than the Retinal Stimuli\", Vision\\nResearch, Vol. 16, pp. 1371-1376,1976\\nH. Wyatt and J. Pola, \"The Role of Perceived Motion in Smooth Pursuit Eye\\nMovements\", Vision Research, Vol. 19, pp. 613-618, 1979\\n\\n\\x0c',\n",
       "     'pdf_name': '1032-vlsi-model-of-primate-visual-smooth-pursuit.pdf',\n",
       "     'title': 'VLSI Model of Primate Visual Smooth Pursuit',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1033',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1033',\n",
       "     'paper_text': 'Gradient and Hamiltonian Dynamics\\nApplied to Learning in Neural Networks\\nJames W. Howse\\n\\nChaouki T. Abdallah\\n\\nGregory L. Heileman\\n\\nDepartment of Electrical and Computer Engineering\\nUniversity of New Mexico\\nAlbuquerque, NM 87131\\n\\nAbstract\\nThe process of machine learning can be considered in two stages: model\\nselection and parameter estimation. In this paper a technique is presented\\nfor constructing dynamical systems with desired qualitative properties. The\\napproach is based on the fact that an n-dimensional nonlinear dynamical\\nsystem can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and\\nHamiltonian portions appropriately so that a certain behavior is obtainable.\\nTo estimate the parameters, a stably convergent learning rule is presented.\\nThis algorithm has been proven to converge to the desired system trajectory\\nfor all initial conditions and system inputs. This technique can be used to\\ndesign neural network models which are guaranteed to solve the trajectory\\nlearning problem.\\n\\n1\\n\\nIntroduction\\n\\nA fundamental problem in mathematical systems theory is the identification of dynamical systems. System identification is a dynamic analogue of the functional approximation problem. A set of input-output pairs {u(t), y(t)} is given over some time\\ninterval t E [7i, 1j]. The problem is to find a model which for the given input sequence\\nreturns an approximation of the given output sequence. Broadly speaking, solving an\\nidentification problem involves two steps. The first is choosing a class of identification models which are capable of emulating the behavior of the actual system. The\\nsecond is selecting a method to determine which member of this class of models best\\nemulates the actual system. In this paper we present a class of nonlinear models and\\na learning algorithm for these models which are guaranteed to learn the trajectories\\nof an example system. Algorithms to learn given trajectories of a continuous time\\nsystem have been proposed in [6], [8], and [7] to name only a few. To our knowledge,\\nno one has ever proven that the error between the learned and desired trajectories\\nvanishes for any of these algorithms. In our trajectory learning system this error is\\nguaranteed to vanish. Our models extend the work in [1] by showing that Cohen\\'s\\nsystems are one instance of the class of models generated by decomposing the dynamics into a component normal to some surface and a set of components tangent to the\\nsame surface. Conceptually this formalism can be used to design dynamical systems\\nwith a variety of desired qualitative properties. Furthermore, we propose a provably\\nconvergent learning algorithm which allows the parameters of Cohen\\'s models to be\\nlearned from examples rather than being programmed in advance. The algorithm is\\n\\n\\x0c275\\n\\nGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\nconvergent in the sense that the error between the model trajectories and the desired trajectories is guaranteed to vanish. This learning procedure is related to one\\ndiscussed in [5] for use in linear system identification.\\n\\n2\\n\\nConstructing the Model\\n\\nFirst some terminology will be defined. For a system of n first order ordinary differential equations, the phase space of the system is the n-dimensional space of all state\\ncomponents. A solution trajectory is a curve in phase space described by the differential equations for one specific starting point. At every point on a trajectory there\\nexists a tangent vector. The space of all such tangent vectors for all possible solution\\ntrajectories constitutes the vector field for this system of differential equations.\\nThe trajectory learning models in this paper are systems of first order ordinary differential equations. The form of these equations will be obtained by considering the\\nsystem dynamics as motion relative to some surface. At each point in the state space\\nan arbitrary system trajectory will be decomposed into a component normal to this\\nsurface and a set of components tangent to this surface. This approach was suggested\\nto us by the results in [4], where it is shown that an arbitrary n-dimensional vector\\nfield can be decomposed locally into the sum of one gradient vector field and (n - 1)\\nHamiltonian vector fields. The concept of a potential function will be used to define these surfaces. A potential function V(:z:) is any scalar valued function of the\\nsystem states :z: = [Xl, X2, ??? , Xn.] t which is at least twice continuously differentiable\\n(Le. V(:z:) E or : r ~ 2). The operation [.]t denotes the transpose of the vector. If\\nthere are n components in the system state, the function V{:z:), when plotted with\\nrespect all of the state components, defines a surface in an (n + 1)-dimensional space.\\nThere are two curves passing through every point on this potential surface which are\\nof interest in this discussion, they are illustrated in Figure 1(a). The dashed curve is\\n(z - zo)t \\\\7 ... v (z)l ...o = 0\\n\\n(a)\\n\\n(b)\\n\\nV(z) = K-\\n\\nFigure 1: (a) The potential function V(z) = X~ (Xl _1)2 +x~ plotted versus its two dependent variables Xl and X2. The dashed curve is called a level surface and is given\\nby V(z) = 0.5. The solid curve follows the path of steepest descent through Zo.\\n(b) The partitioning of a 3-dimensional vector field at the point Zo into a 1dimensional portion which is normal to the surface V(z) = K- and a 2-dimensional\\nportion which is tangent to V(z) = K-. The vector -\\\\7 ... V(z) 1\"\\'0 is the normal vector to the surface V(z) = K- at the point Zo. The plane (z - zo)t \\\\7 ... V (z) 1\"\\'0 = 0\\ncontains all of the vectors which are tangent to V(z) = K- at Zo. Two linearly\\nindependent vectors are needed to form a basis for this tangent space, the pair\\nQ2(z) \\\\7 ... V (z)l ... o and Q3(Z) \\\\7 ... V (z)l ... o that are shown are just one possibility.\\nreferred to as a level surface, it is a surface along which V(:z:) = K for some constant\\nK. Note that in general this level surface is an n-dimensional object. The solid curve\\n\\n\\x0c276\\n\\nJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\nmoves downhill along V (X) following the path of steepest descent through the point\\nXo. The vector which is tangent to this curve at Xo is normal to the level surface\\nat Xo. The system dynamics will be designed as motion relative to the level surfaces\\nof V(x). The results in [4] require n different local potential functions to achieve\\narbitrary dynamics. However, the results in [1] suggest that a considerable number\\nof dynamical systems can be achieved using only a single global potential function.\\nA system which is capable of traversing any downhill path along a given potential\\nsurface V(x), can be constructed by decomposing each element of the vector field\\ninto a vector normal to the level surface of V(x) which passes through each point\\nand a set of vectors tangent to the level surface of V(x) which passes through the\\nsame point. So the potential function V(x) is used to partition the n-dimensional\\nphase space into two subspaces. The first contains a vector field normal to some\\nlevel surface V(x) = }( for }( E IR, while the second subspace holds a vector field\\ntangent to V(x) = IC. The subspace containing all possible normal vectors to the\\nn-dimensional level surface at a given point, has dimension one. This is equivalent\\nto the statement that every point on a smooth surface has a unique normal vector.\\nSimilarly, the subspace containing all possible tangent vectors to the level surface at\\na given point has dimension (n - 1). An example of this partition in the case of a\\n3-dimensional system is shown in Figure 1(b). Since the space of all tangent vectors\\nat each point on a level surface is (n - I)-dimensional, (n - 1) linearly independent\\nvectors are required to form a basis for this space.\\nMathematically, there is a straightforward way to construct dynamical systems which\\neither move downhill along V(x) or remain at a constant height on V(x). In this\\npaper, dynamical systems which always move downhill along some potential surface\\nare called gradient-like systems. These systems are defined by differential equations\\nof the form\\nx = -P(x) VII:V(x),\\n(1)\\nwhere P(x) is a matrix function which is symmetric (Le. pt = P) and positive\\n:z~]f. These systems\\ndefinite at every point x, and where VIII V(x) =\\nare similar to the gradient flows discussed in [2]. The trajectories of the system\\nformed by Equation (1) always move downhill along the potential surface defined by\\nV(x). This can be shown by taking the time derivative of V(x) which is V(x) =\\n-[VII: V (x)]t P(x) [VII: V(x)] :5 O. Because P(x) is positive definite, V(x) can only be\\nzero where V II: V (x) = 0, elsewhere V(x) is negative. This means that the trajectories\\nof Equation (1) always move toward a level surface of V(x) formed by \"slicing\" V(x)\\nat a lower height, as pointed out in [2]. It is also easy to design systems which remain\\nat a constant height on V(x). Such systems will be denoted Hamiltonian-like systems.\\nThey are specified by the equation\\nx = Q(x) VII: V(x),\\n(2)\\nwhere Q(x) is a matrix function which is skew-symmetric (Le. Qt = -Q) at every\\npoint x. These systems are similar to the Hamiltonian systems defined in [2]. The\\nelements of the vector field defined by Equation (2) are always tangent to some level\\nsurface of V (x). Hence the trajectories ofthis system remain at a constant height on\\nthe potential surface given by V(x). Again this is indicated by the time derivative\\nof V(x), which in this case is V(x) = [VII: V(x)]f Q(x)[VII: V(x)] = o. This indicates\\nthat the trajectories of Equation (2) always remain on the level surface on which the\\nsystem starts. So a model which can follow an arbitrary downhill path along the\\npotential surface V(x) can be designed by combining the dynamics of Equations (1)\\nand (2) . The dynamics in the subspace normal to the level surfaces of V(x) can be\\n\\n[g;: , g;: ,... ,\\n\\n\\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\n277\\n\\ndefined using one equation of the form in Equation (1). Similarly the dynamics in the\\nsubspace tangent to the level surfaces of Vex) can be defined using (n - 1) equations\\nof the form in Equation (2). Hence the total dynamics for the model are\\nn\\n\\nz= -P(x)VIDV(x) + LQi(X)VIDV(x).\\n\\n(3)\\n\\ni=2\\n\\nFor this model the number and location of equilibria is determined by the function\\nVex), while the manner in which the equilibria are approached is determined by the\\nmatrices P(x) and Qi(x).\\nIf the potential function Vex) is bounded below (i.e. Vex) > Bl V x E IRn , where\\nBl is a constant), eventually increasing (i.e. limlllDlI-+oo Vex) ~ 00) , and has only\\na finite number of isolated local maxima and minima (i.e. in some neighborhood\\nof every point where V III V (x) = 0 there are no other points where the gradient\\nvanishes), then the system in Equation (3) satisfies the conditions of Theorem 10\\nin [1]. Therefore the system will converge to one of the points where V ID Vex) = 0,\\ncalled the critical points of Vex), for all initial conditions. Note that this system\\nis capable of all downhill trajectories along the potential surface only if the (n - 1)\\nvectors Qi(X) V ID Vex) V i = 2, ... , n are linearly independent at every point x. It\\nis shown in [1] that the potential function\\n\\nV(z) = C (\\n\\n1:., (-y) d-y +\\n\\nt, [~\\n\\n(XI - I:.,(xd)\\'\\n\\n+~\\n\\nJ:\\'\\n\\n1:., h )II:.: (-y)]\\' d-y\\n\\n1\\n\\n(4)\\n\\nsatisfies these three criteria. In this equation ?.i(Xt} Vi = 1, ... , n are interpolation\\npolynomials, C is a real positive constant, Xi Vi = 1, ... , n are real constants chosen\\nso that the integrals are positive valued, and ?.Hxt} ==\\n\\nf:-.\\n\\n3\\n\\nThe Learning Rule\\n\\nIn Equation (3) the number and location of equilibria can be controlled using the\\npotential function Vex), while the manner in which the equilibria are approached can\\nbe controlled with the matrices P(x) and Qi(X). If it is assumed that the locations\\nof the equilibria are known, then a potential function which has local minima and\\nmaxima at these points can be constructed using Equation (4). The problem of\\ntrajectory learning is thereby reduced to the problem of parameterizing the matrices\\nP(x) and Qi(x) and finding the parameter values which cause this model to best\\nemulate the actual system. If the elements P(x) and Qi(x) are correctly chosen,\\nthen a learning rule can be designed which makes the model dynamics converge to\\nthat of the actual system. Assume that the dynamics given by Equation (3) are a\\nparameterized model of the actual dynamics. Using this model and samples of the\\nactual system states, an estimator for states of the actual system can be designed. The\\nbehavior of the model is altered by changing its parameters, so a parameter estimator\\nmust also be constructed. The following theorem provides a form for both the state\\nand parameter estimators which guarantees convergence to a set of parameters for\\nwhich the error between the estimated and target trajectories vanishes.\\nTheorem 3.1. Given the model system\\nk\\n\\nZ = LAili(x) +Bg(u)\\n\\n(5)\\n\\ni=l\\n\\nwhere Ai E IRnxn and BE IRnxm are unknown, and li(\\') and g(.) are known smooth\\nfunctions such that the system has bounded solutions for bounded inputs u(t). Choose\\n\\n\\x0cJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\n278\\n\\na state estimator of the form\\nk\\n\\n~ = \\'R. B (x - x) +\\n\\nL Ai fi(x) + iJ g(u)\\n\\n(6)\\n\\ni=1\\n\\nwhere\\'R. B is an (n x n) matrix of real constants whose eigenvalues must all be in the\\nleft half plane, and Ai and iJ are the estimates of the actual parameters. Choose\\nparameter estimators of the form\\n~\\nt\\nAi = -\\'R.p (x - x) [fi(x)] V i = 1, ... , k\\n(7)\\n= -\\'R.p (x - x) [g(u)]t\\n\\nB\\n\\nwhere \\'R. p is an (n x n) matrix of real constants which is symmetric and positive\\ndefinite, and (x - x) [.]t denotes an outer product. For these choices of state and\\nparameter estimators limt~oo(x(t) -x(t? = 0 for all initial conditions. Furthermore,\\nthis remains true if any of the elements of Ai or iJ are set to 0, or if any of these\\nmatrices are restricted to being symmetric or skew-symmetric.\\nThe proof of this theorem appears in [3]. Note that convergence of the parameter\\nestimates to the actual parameter values is not guaranteed by this theorem. The\\nmodel dynamics in Equation (3) can be cast in the form of Equation (5) by choosing\\neach element of P(x) and Qi(X) to have the form\\nI-I\\n\\nn\\n\\nn\\n\\nI-I\\n\\n= LL~rBjkt?k(Xj)\\n\\nand\\nQrB = LLArBjk ek(Xj),\\n(8)\\nj=1 k=O\\nj=1 k=O\\nwhere {t?o(Xj), t?1 (Xj), ... ,t?I-1 (Xj)} and {eo(Xj), el (Xj), ... ,el-l (Xj)} are a set of 1\\northogonal polynomials which depend on the state Xj\\' There is a set of such polynomials for every state Xj, j = 1,2, ... , n. The constants ~rBjk and ArBjk determine\\nthe contribution of the kth polynomial which depends on the jth state to the value\\nof Prs and Qrs respectively. In this case the dynamics in Equation (3) become\\nPrB\\n\\n:i:\\n\\n=\\n\\nt. ~ {\\n\\nS;. [11.(x;) V. V (z)j\\n\\n+\\n\\nt,\\n\\nA;;. [e;.(x;)\\n\\nv. V(z)j } + T g(u(t))\\n\\n(9)\\n\\nwhere 8 jk is the (n x n) matrix of all values ~rsjk which have the same value of j and\\nk. Likewise A ijk is the (n x n) matrix of all values Arsjk, having the same value of\\nj and k, which are associated with the ith matrix Qi(X). This system has m inputs,\\nwhich may explicitly depend on time, that are represented by the m-element vector\\nfunction u(t). The m-element vector function g(.) is a smooth, possibly nonlinear,\\ntransformation of the input function. The matrix Y is an (n x m) parameter matrix\\nwhich determines how much of input S E {I, ... , m} effects state r E {I, ... , n}.\\nAppropriate state and parameter estimators can be designed based on Equations (6)\\nand (7) respectively.\\n\\n4\\n\\nSimulation Results\\n\\nNow an example is presented in which the parameters of the model in Equation (9)\\nare trained, using the learning rule in Equations (6) and (7), on one input signal and\\nthen are tested on a different input signal. The actual system has three equilibrium\\npoints, two stable points located at (1,3) and (3,5), and a saddle point located at\\n(2 - ~,4 + ~). In this example the dynamics of both the actual system and the\\nmodel are given by\\n\\n(~1) =\\nZ2\\n\\nZ~\\n\\nZ~\\n\\nO\\n\\n(1\\'1 + 1\\'2\\n+:3\\n2)\\n0 1\\'4 + 1\\'5 Z1 + 1\\'6 Z2\\n\\n(:~)\\n+ (0 - {1\\'7 + 1\\'8 Z1 + 1\\'9 Z2}) (:~ ) + (1\\'10) u(t)\\n8Y\\n\\'P7 + \\'P8 ZI + 1\\'9 Z2\\n8Y\\n0\\n\\n8Z2\\n\\n0\\n\\n8Z2\\n\\n(10)\\n\\n\\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\n279\\n\\nwhere V(x) is defined in Equation (4) and u(t) is a time varying input. For the actual\\nsystem the parameter values were \\'PI = \\'P4 = -4, \\'P2 = \\'Ps = -2, \\'P3 = \\'P6 = -1,\\n\\'P7 = 1, \\'Ps = 3, \\'P9 = 5, and \\'PIO = 1. In the model the 10 elements \\'Pi are\\ntreated as the unknown parameters which must be learned. Note that the first matrix\\nfunction is positive definite if the parameters \\'PI-\\'P6 are all negative valued. The\\nsecond matrix function is skew-symmetric for all values of \\'P7-\\'P9. The two input\\nsignals used for training and testing were Ul = 10000 (sin! 1000t + sin ~ 1000t) and\\nU2 = 5000 sin 1000 t. The phase space responses of the actual system to the inputs UI\\nand U2 are shown by the solid curves in Figures 3(b) and 3(a) respectively. Notice that\\nboth of these inputs produce a periodic attractor in the phase space of Equation (10).\\nIn order to evaluate the effectiveness of the learning algorithm the Euclidean distance\\nbetween the actual and learned state and parameter values was computed and plotted\\nversus time. The results are shown in Figure 2. Figure 2(a) shows these statistics when\\n{1I~zll, II~\\'PII}\\n\\n{1I~zll, II~\\'PII}\\n\\n17.5\\n15\\n15\\n12.5\\n12.5\\n10\\n\\n7.5\\n\\ni\\n\\n----\\n\\n,., ~--.----... ... .......\\n\\n- --\\n\\n2.5\\n\\n150\\n200\\n250\\n300 t\\n50\\n100\\n150\\n200\\n250\\n300 t\\n(a)\\n(b)\\nFigure 2: (a) The state and parameter errors for training using input signal Ut. The solid\\ncurve is the Euclidean distance between the state estimates and the actual states\\nas a function of time. The dashed curve shows the distance between the estimated\\nand actual parameter values versus time.\\n(b) The state and parameter errors for training using input signal U2.\\n50\\n\\n100\\n\\ntraining with input UI, while Figure 2(b) shows the same statistics for input U2. The\\nsolid curves are the Euclidean distance between the learned and actual system states,\\nand the dashed curves are the distance between the learned and actual parameter\\nvalues. These statistics have two noteworthy features. First, the error between the\\nlearned and desired states quickly converges to very small values, regardless of how\\nwell the actual parameters are learned. This result was guaranteed by Theorem 3.1.\\nSecond, the final error between the learned and desired parameters is much lower when\\nthe system is trained with input UI. Intuitively this is because input Ul excites more\\nfrequency modes of the system than input U2. Recall that in a nonlinear system the\\nfrequency modes excited by a given input do not depend solely on the input because\\nthe system can generate frequencies not present in the input. The quality of the\\nlearned parameters can be qualitatively judged by comparing the phase plots using\\nthe learned and actual parameters for each input, as shown in Figure 3. In Figure 3(a)\\nthe system was trained using input Ul and tested with input U2, while in Figure 3(b)\\nthe situation was reversed. The solid curves are the system response using the actual\\nparameter values, and the dashed curves are the response for the learned parameters.\\nThe Euclidean distance between the target and test trajectories in Figure 3(a) is in\\nthe range (0,0.64) with a mean distance of 0.21 and a standard deviation of 0.14. The\\ndistance between the the target and test trajectories in Figure 3(b) is in the range\\n(0,4.53) with a mean distance of 0.98 and a standard deviation of 1.35. Qualitatively,\\nboth sets of learned parameters give an accurate response for non-training inputs.\\n\\n\\x0c280\\n\\n1. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\n5\\nI\\n\\no\\n\\n{i\\n\\n-------r-- -- ----- --- -- I\\n\\n-5\\n\\n-10\\n\\n-15\\n\\n-l\\n\\n-1\\n\\n1\\n\\n-2\\n\\n-1\\n\\n4\\n\\nXl\\n\\n(a)\\n(b)\\nFigure 3: (a) A phase plot of the system response when trained with input UI and tested\\nwith input U2. The solid line is the response to the test input using the actual\\nparameters. The dotted line is the system response using the learned parameters.\\n(b) A phase plot of the system response when trained with input U2 and tested\\nwith input UI.\\n\\nNote that even when the error between the learned and actual parameters is large,\\nthe periodic attractor resulting from the learned parameters appears to have the same\\n\"shape\" as that for the actual parameters.\\n\\n5\\n\\nConclusion\\n\\nWe have presented a conceptual framework for designing dynamical systems with\\nspecific qualitative properties by decomposing the dynamics into a component normal\\nto some surface and a set of components tangent to the same surface. We have\\npresented a specific instance of this class of systems which converges to one of a finite\\nnumber of equilibrium points. By parameterizing these systems, the manner in which\\nthese equilibrium points are approached can be fitted to an arbitrary data set. We\\npresent a learning algorithm to estimate these parameters which is guaranteed to\\nconverge to a set of parameter values for which the error between the learned and\\ndesired trajectories vanishes.\\n\\nAcknowledgments\\nThis research was supported by a grant from Boeing Computer Services under Contract\\nW-300445. The authors would like to thank Vangelis Coutsias, Tom Caudell, and Bill\\nHome for stimulating discussions and insightful suggestions.\\n\\nReferences\\n[1] M.A. Cohen. The construction of arbitrary stable dynamics in nonlinear neural networks.\\nNeural Networks, 5(1):83-103, 1992.\\n[2] M.W. Hirsch and S. Smale. Differential equations, dynamical systems, and linear algebra,\\nvolume 60 of Pure and Applied Mathematics. Academic Press, Inc., San Diego, CA, 1974.\\n[3] J.W. Howse, C.T. Abdallah, and G.L. Heileman. A gradient-hamiltonian decomposition\\nfor designing and learning dynamical systems. Submitted to Neural Computation, 1995.\\n[4] R.V. Mendes and J .T. Duarte. Decomposition of vector fields and mixed dynamics.\\nJournal of Mathematical Physics, 22(7):1420-1422, 1981.\\n[5] K.S. Narendra and A.M. Annaswamy. Stable adaptitJe systems. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1989.\\n[6] B.A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural\\nComputation, 1(2):263-269, 1989.\\n[7] D. Saad. Training recurrent neural networks via trajectory modification. Complex Systems, 6(2) :213-236, 1992.\\n[8] M.-A. Sato. A real time learning algorithm for recurrent analog neural networks. Biological Cybernetics, 62(2):237-241, 1990.\\n\\n\\x0c',\n",
       "     'pdf_name': '1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf',\n",
       "     'title': 'Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1035',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1035',\n",
       "     'paper_text': 'A Dynamical Model of Context Dependencies for the\\nVestibulo-Ocular Reflex\\nTerrence J. Sejnowskit\\n\\nOlivier J.M.D. Coenen*\\n\\nComputational Neurobiology Laboratory\\nHoward Hughes Medical Institute\\nThe Salk Institute for Biological Studies\\n10010 North Torrey Pines Road\\nLa Jolla, CA 92037, U.S.A.\\nDepartments oftBiology and *tPhysics\\nUniversity of California, San Diego\\nLa Jolla, CA 92093, U.S.A\\n\\n{olivier,terry}@salk.edu\\n\\nAbstract\\nThe vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid\\nhead motions. The gain of the VOR (the ratio of eye to head rotation velocity)\\nis typically around -1 when the eyes are focused on a distant target. However, to\\nstabilize images accurately, the VOR gain must vary with context (eye position,\\neye vergence and head translation). We first describe a kinematic model of the\\nVOR which relies solely on sensory information available from the semicircular\\ncanals (head rotation), the otoliths (head translation), and neural correlates of eye\\nposition and vergence angle. We then propose a dynamical model and compare it\\nto the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and\\nsuggests one way to combine the required neural signals within the cerebellum and\\nthe brain stem. It also makes predictions for the responses of neurons to multiple\\ninputs (head rotation and translation, eye position, etc.) in the oculomotor system.\\n\\n1 Introduction\\nThe VOR stabilizes images on the retina during rapid head motions: Rotations and translations of\\nthe head in three dimensions must be compensated by appropriate rotations of the eye. Because the\\nhead\\'s rotation axis is not the same as the eye\\'s rotation axis, the calculations for proper image stabilization of an object must take into account diverse variables such as object distance from each eye,\\n\\n\\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\\n\\n90\\n\\ngaze direction, and head translation (Viire et al., 1986). The stabilization is achieved by integrating\\ninfonnation from different sources: head rotations from the semicircular canals of the inner ear, head\\ntranslations from the otolith organs, eye positions, viewing distance, as well as other context infonnation, such as posture (head tilts) or activity (walking, running) (Snyder and King, 1992; Shelhamer\\net al.,1992; Grossman et al., 1989). In this paper we concentrate on the context modulation of the\\nVOR which can be described by the kinematics of the reflex, i.e. eye position, eye vergence and\\nhead translation.\\n\\n2\\n\\nThe Vestibulo-Ocular Reflex: Kinematic Model\\nDefinition of Vectors\\n\\nTarget Object\\n\\nCoordinate System\\n\\nGaze Vector\\n\\nGaze Angle\\nInterocular\\nDistance\\nEye position\\nVector\\n\\nRotation Axis\\n\\nSemicircular\\nCanals and\\nOtoliths\\nHead\\nTop View\\n\\n?\\n\\n~_--+_\\n\\nOrigin of coordinate\\nsyste,,-, (arbitrary)\\n\\nFigure 1: Diagram showing the definition of the vectors used in the equation of the kinematic model of the\\nvestibulo-ocular reflex.\\n\\nThe ideal VOR response is a compensatory eye movement which keeps the image fixed on the retina\\nfor any head rotations and translations. We therefore derived an equation for the eye rotation velocity\\nby requiring that a target remains stationary on the retina. The velocity of the resulting compensatory\\neye rotation can be written as (see fig. 1):\\n\\nw= -Oe + 1:1\\n\\nx [Dej x\\n\\nOe - To;]\\n\\n(1)\\n\\nwhere Oe is the head rotation velocity sensed by the semicircular canals, TOj is the head translation\\nvelocity sensed by the otoliths, Dej == (e - OJ), eis a constant vector specifying the location of an\\neye in the head, OJ is the position of either the left or right otolith, fJ and Igl are the unit vector and\\namplitude of the gaze vector: fJ gives the eye position (orientation of the eye relative to the head),\\nand Igl gives the distance from the eye to the object, and the symbol x indicates the cross-product\\nbetween two vectors. wand Oe are rotation vectors which describe the instantaneous angUlar velocity\\nof the eye and head, respectively. A rotation vector lies along the instantaneous axis of rotation;\\nits magnitude indicates the speed of rotation around the axis, and its direction is given by the righthand screw rule. A motion of the head combining rotation (0) and translation (T) is sensed as the\\ncombination of a rotation velocity Oe measured by the semicircular canals and a translation velocity\\nTo sensed by the otoliths. The rotation vectors are equal (0 = Oe), and the translation velocity vector\\nas measured by the otoliths is given by: TOj = OOj x 0 + T, where OOj == (a - OJ), and a is the\\nposition vector of the axis of rotation.\\n\\n\\x0c91\\n\\nA Dynarnical Model of Context Dependencies for the Vestibula-Ocular Reflex\\n\\nThe special case where the gaze is horizontal and the rotation vector is vertical (horizontal head rotation) has been studied extensively in the literature. We used this special case in the sirnulations.\\nIn that case rnay be sirnplify by writing its equation with dot products. Since 9 and\\nare then\\nperpendicular (9 . fie = 0). the first term of the following expression in brackets is zero:\\n\\nw\\n\\nslc\\n\\n(2)\\n\\nThe sernicircular canals decornpose and report acceleration and velocity of head rotation fi by its\\ncornponents along the three canals on each side of the head fie : horizontal. anterior and posterior.\\nThe two otolith organs on each side report the dynamical inertial forces generated during linear rnotion (translation) in two perpendicular plane. one vertical and the other horizontal relative to the head.\\nHere we assurne that a translation velocity signal (To) derived frorn or reported by the otolith afferents is available. The otoliths encode as well the head orientation relative to the gravity vector force.\\nbut was not included in this study.\\nTo cornplete the correspondence between the equation and a neural correlate. we need to determine\\nThe eye position 9 is assurned to be given by the output of the\\na physiological source for 9 and\\nvelocity-to-position transformation or so-called \"neural integrator\" which provides eye position information and which is necessary for the activation of the rnotoneuron to sustain the eye in a fixed\\nposition. The integrator for horizontal eye position appears to be located in the nucleus prepositus\\nhypoglossi in the pons. and the vertical integrator in the rnidbrain interstitial nucleus of Cajal. (Crawford. Cadera and Vilis. 1991; Cannon and Robinson. 1987). We assurne that the eye position is given\\nas the coordinates of the unit vector 9 along the ~ and 1; of fig. 1. The eye position depends on the\\neye velocity according to\\n= 9 x w. For the special case w(t) = w(t)z. i.e. for horizontal head\\nrotation. the eye position coordinates are given by:\\n\\nI!I.\\n\\n\\'*\\n\\n91 (t) =\\n\\n91 (0) + f~ iJ2( r )w( r) dr\\n\\n92(t) =\\n\\n92(0) - f~ 91(r)w(r)dr\\n\\n(3)\\n\\nThis is a set of two negatively coupled integrators. The \"neural integrator\" therefore does not integrate the eye velocity directly but a product of eye position and eye velocity. The distance frorn eye\\nto target\\ncan be written using the gaze angles in the horizontal plane of the head:\\n\\nI!I\\n\\n1\\n\\n(4)\\n\\n1\\n\\n(5)\\n\\nRight eye:\\n\\n19RT\\n\\nLeft eye:\\n\\n19LT\\n\\nwhere ?()R - () L) is the vergence angle. and I is the interocular distance; the angles are rneasured frorn\\na straight ahead gaze. and take on negative values when the eyes are turned towards the right. Within\\nthe oculornotor systern. the vergence angle and speed are encoded by the rnesencephalic reticular\\nformation neurons (Judge and Curnrning. 1986; Mays. 1984). The nucleus reticularis tegrnenti pontis\\nwith reciprocal connections to the flocculus. oculornotor vermis. paravermis of the cerebellurn also\\ncontains neurons which activity varies linearly with vergence angle (Gamlin and Clarke. 1995).\\nWe conclude that it is possible to perform the cornputations needed to obtain an ideal VOR with signals known to be available physiologically.\\n\\n\\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\\n\\n92\\nDynamical Model Overview\\n\\nNod_\\nPftpoIItao\\n\\nIIyposIoooI\\n\\nFigure 2: Anatomical connections considered in the dynamical model. Only the left side is shown, the right\\nside is identical and connected to the left side only for the calculation of vergence angle. The nucleus prepositus\\nhypoglossi and the nucleus reticularis tegmenti pontis are meant to be representative of a class of nuclei in the\\nbrain stem carrying eye position or vergence signal. All connections are known to exist except the connection\\nbetween the prepositus nucleus to the reticularis nucleus which has not been verified. Details of the cerebellum\\nare in fig. 3 and of the vestibular nucleus in fig. 4.\\n\\n3 Dynamical Model\\nSnyder & King (1992) studied the effect of viewing distance and location of the axis of rotation on\\nthe VOR in monkeys; their main results are reproduced in fig. 5. In an attempt to reproduce their\\ndata and to understand how the signals that we have described in section 2 may be combined in time,\\nwe constructed a dynamical model based on the kinematic model. Its basic anatomical structure is\\nshown in fig. 2. Details of the model are shown in fig. 3, and fig . 4 where all constants are written\\nusing a millisecond time scale. The results are presented in fig. 5. The dynamical variables represent\\nthe change of average firing rate from resting level of activity. The firing rate of the afferents has a\\ntonic component proportional to the velocity and a phasic component proportional to the acceleration\\nof movement. Physiologically, the afferents have a wide range of phasic and tonic amplitudes. This\\nis reflected by a wide selection of parameters in the numerators in the boxes of fig. 3 and fig. 4. The\\nLaplace transform of the integration operator in equation (3) of the eye position coordinates is ~.\\nFollowing Robinson (1981), we modeled the neural integrator with a gain and a time constant of\\n20 seconds. We therefore replaced the pure integrator ~ with 20~~~~1 in the calculations of eye\\nposition. The term 1 in fig. 3 is calculated by using equations (4) and (5), and by using the integrator\\n9\\n\\n20~o:!~1 on the eye velocity motor command to find the angles (h and (JR.\\n\\nThe dynamical model is based on the assumption that the cerebellum is required for context modulation, and that because of its architecture, the cerebellum is more likely to implement complex functions of multiple signals than other relevant nuclei. The major contributions of vergence and eye\\nposition modulation on the VOR are therefore mediated by the cerebellum. Smaller and more transient contributions from eye position are assumed to be mediated through the vestibular nucleus as\\nshown in fig. 4. The motivation for combining eye position as in fig . 4 are, first, the evidence for eye\\nresponse oscillations; second, the theoretical consideration that linear movement information (To) is\\nuseless without eye position information for proper VOR.\\nThe parameters in the dynamical model were adjusted by hand after observing the behavior of the different components of the model and noting how these combine to produce the oscillations observed\\n\\n\\x0c93\\n\\nA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\\nVestibular\\nSemicirtular\\n\\nCerebellum\\n\\nc..l\\n\\nO-\\n\\n- - - - t 401+1 r-----?--f--..j\\n\\nx\\n\\n300+1\\n\\nOIolith\\n0Igan\\n\\nVHlibabr\\nNuc1tul\\n\\nFigure 3: Contribution of the cerebellum to the dynamical model. Filtered velocity inputs from the canals and\\notoliths are combined with eye position according to equation (2). These calculations could be performed either\\noutside the cerebellum in one or multiple brain stem nuclei (as shown) or possibly inside the cerebellum. The\\nonly output is to the vestibular nucleus. The Laplace notation is used in each boxes to represent a leaky integrator\\nwith a time constant. input derivative and input gain. The term oe are the coordinates of the vector oe shown\\nin fig. 1. The x indicates a multiplication. The term! multiplies each inputs individually. The open arrows\\nindicate inhibitory (negative) connections.\\nCere... lIum\\n\\nVHlibalu\\n\\nSemicimtlu\\n\\nc.w\\n\\nO--\\'----t~l---+--?----t~~\\n\\nX\\nFigure 4: Contribution of the vestibular nucleus to the dynamical model. Three pathways in the vestibular nucleus process the canal and otolith inputs to drive the eye. The first pathway is modulated by the output of the\\ncerebellum through a FIN (Flocculus Target Neuron). The second and third pathways report transient information from the inputs which are combined with eye position in a manner identical to fig. 3. The location of these\\ncalculations is hypothetical.\\n\\nin the data. Even though the number of parameters in the model is not small. it was not possible to\\nfit any single response in fig. 5 without affecting most of the other eye responses. This puts severe\\nlimits on the set of parameters allowed in the model.\\nThe dynamical model suggests that the oscillations present in the data reflect: 1) important acceleration components in the neural signals. both rotational and linear, 2) different time delays between the\\ncanal and otolith signal processing. and 3) antagonistic or synergistic action of the canal and otolith\\nsignals with different axes of rotation, as described by the two terms in the bracket of equation (2).\\n\\n4 Discussion\\nBy fitting the dynamical model to the data, we tested the hypothesis that the VOR has a response\\nclose to ideal taking into account the time constraints imposed by the sensory inputs and the neural\\nnetworks performing the computations. The vector computations that we used in the model may not\\n\\n\\x0c94\\n\\nO. J. M. D. COENEN, T. J. SEJNOWSKI\\nDynamical Model Responses vs Experimental Data\\n80\\n\\n80\\nLOMtIOftof\\n.... 01 rotMIon\\n\\n-,a.-om\\n\\n.-\\n\\nT..........~\\n\\n60\\n\\n40\\n20\\n\\n~\\nw\\n\\n-20\\n\\n-20\\n\\n-400~----~5~0------~\\n10\\n=0\\n~\\nTime (m.)\\n\\n-40oL-----~\\n5~\\n0 ----~1~\\n0~\\n0-?\\n\\nTime (m.)\\n\\nFigure 5: Comparison between the dynamical model and monkey data. The dotted lines show the effect of\\nviewing distance and location of the axis of rotation on the VOR as recorded by Snyder & King (1992) from\\nmonkeys in the dark. The average eye velocity response (of left and right eye) to a sudden change in head velocity is shown for different target distances (left) and rotational axes (right). On the left, the location of the axis\\nof rotation was in the midsagittal plane 12.5 cm behind the eyes (-12.5 cm), and the target distance was varied\\nbetween 220 cm and 9 cm. On the right, the target di stance was kept constant at 9 cm in front of the eye, and the\\nlocation of the axis of rotation was varied from 14 cm behind t04cm in front of the eyes (-14cm to 4cm) in the\\nmidsagittal plane. The solid lines show the model responses. The model replicates many characteristics of the\\ndata. On the left the model captures the eye velocity fluctuations between 20-50 ms, followed by a decrease and\\nan increase which are both modulated with target distance (50-80 ms). The later phase of the response (80-100\\nms) is almost exact for 220 cm, and one peak is seen at the appropriate location for the other distances. On the\\nright the closest fits were obtained for the 4 cm and 0 cm locations. The mean values are in good agreement and\\nthe waveforms are close, but could be shifted in time for the other locations of the axis of rotations. Finally, the\\nlatest peak (..... lOOms) in the data appears in the model for -14 cm and 9 cm location.\\n\\nbe the representation used in the oculomotor system. Mathematically, the vector representation is\\nonly one way to describe the computations involved. Other representations exist such as the quaternion representation which has been studied in the context of the saccadic system (Tweed and Vilis,\\n1987; see also Handzel and Flash, 1996 for a very general representation). Detailed comparisons\\nbetween the model and recordings from neurons will be require to settle this issue.\\nDirect comparison between Purkinje cell recordings (L.H. Snyder & W.M. King, unpublished data)\\nand predictions of the model could be used to determine more precisely the different inputs to some\\nPurkinje cells. The model can therefore be an important tool to gain insights difficult to obtain directly with experiments.\\nThe question of how the central nervous system learns the transformations that we described still\\nremains. The cerebellum may be one site of learning for these transformations, and its output may\\nmodulate the VOR in real time depending on the context. This view is compatible with the results\\nof Angelaki and Hess (1995) which indicate that the cerebellum is required to correctly perform an\\notolith transformation. It is also consistent with adaptation results in the VOR. To test this hypothesis,\\nwe have been working on a model of the cerebellum which learns to anticipate sensory inputs and\\nfeedbacks, and use these signals to modulate the VOR. The learning in the cerebellum and vestibular\\nnuclei is mediated by the climbing fibers which report a reinforcement signal of the prediction error\\n(Coenen and Sejnowski. in preparation).\\n\\n\\x0cA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\\n\\n95\\n\\n5 Conclusion\\nMost research on the VOR has assumed forward gaze focussed at infinity. The kinematics of offcenter gaze and fixation at finite distance necessitates nonlinear corrections that require the integration of a variety of sensory inputs. The dynamical model studied here is a working hypothesis for\\nhow these corrections could be computed and is generally consistent with what is known about the\\ncerebellum and brain stem nuclei. We are, however, far from knowing the mechanisms underlying\\nthese computations, or how they are learned through experience.\\n\\n6 Acknowledgments\\nThe first author was supported by a McDonnell-Pew Graduate Fellowship during this research. We\\nwould like to thank Paul Viola for helpful discussions.\\nReferences\\nAngelaki, D. E. and Hess, B. J. (1995). Inertial representation of angular motion in the vestibular system of rhesus monkeyus. II. Otolith-controlled transformation that depends on an intact cerebellar nodulus. Journal\\nof Neurophysiology, 73(5): 1729-1751.\\nCannon, S. C. and Robinson, D. A. (1987). Loss of the neural integrator of the oculomotor system from brain\\nstem lesions in monkey. Journal of Neurophysiology, 57(5):1383-1409.\\nCrawford, J. D., Cadera, W., and Vilis, T. (1991). Generation of torsional and vertical eye position signals by\\nthe interstitial nucleus of Cajal. Science, 252:1551-1553.\\nGamlin, P. D. R. and Clarke, R. J. (1995). Single-unit activity in the primate nucleus reticularis tegmenti pontis\\nrelated to vergence and ocular accomodation. Journal of Neurophysiology, 73(5):2115-2119.\\nGrossman, G. E., Leigh, R. J., Bruce, E. N., Huebner, W. P.,and Lanska, D.J. (1989). Performanceofthe human\\nvestibu1oocu1ar reflex during locomotion. Journal of Neurophysiology, 62(1 ):264-272.\\nHandzel, A. A. and Flash, T. (1996). The geometry of eye rotations and listing\\'s law. In Touretzky, D., Mozer,\\nM., and Hasselmo, M., editors, Advances in Neural Information Processing Systems 8, Cambridge, MA.\\nMIT Press.\\nJudge, S. J. and Cumming, B. G. (1986). Neurons in the monkey midbrain with activity related to vergence eye\\nmovement and accomodation. Journal of Neurophysiology, 55:915-930.\\nMays, L. E. (1984). Neural control of vergence eye movements: Convergence and divergence neurons in midbrain. Journal of Neurophysiology, 51:1091-1108.\\nRobinson, D. A. (1981). The use of control systems analysis in the neurophysiology of eye movements. Ann.\\nRev. Neurosci., 4:463-503.\\nShelhamer, M., Robinson, D. A., and Tan, H. S. (1992). Context-specific adaptation of the gain of the vestibuloocular reflex in humans. Journal of Vestibular Research, 2:89-96.\\nSnyder, L. H. and King, W. M. (1992). Effect of viewing distance and location ofthe axis of head rotation on the\\nmonkey\\'s vestibuloocular reflex I. eye movement response. Journal of Neurophysiology, 67(4):861-874.\\nTweed, D. and Vilis, T. (1987). Implications of rotational kinematics for the oculomotor system in three dimensions. Journal of Neurophysiology, 58(4):832-849.\\nViire, E., Tweed, D., Milner, K., and Vilis, T. (1986). A reexamination of the gain ofthe vestibuloocular reflex.\\nJournal of Neurophysiology, 56(2):439-450.\\n\\n\\x0c',\n",
       "     'pdf_name': '1035-a-dynamical-model-of-context-dependencies-for-the-vestibulo-ocular-reflex.pdf',\n",
       "     'title': 'A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1036',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1036',\n",
       "     'paper_text': 'Improved Gaussian Mixture Density\\nEstimates Using Bayesian Penalty Terms\\nand Network Averaging\\n\\nDirk Ormoneit\\nInstitut fur Informatik (H2)\\nTechnische Universitat Munchen\\n80290 Munchen, Germany\\normoneit@inJormatik.tu-muenchen.de\\n\\nVolker Tresp\\nSiemens AG\\nCentral Research\\n81730 Munchen, Germany\\nVolker. Tresp@zJe.siemens.de\\n\\nAbstract\\n\\nWe compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density\\nestimates. The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules\\nwhich maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation.\\nThis includes Breiman\\'s \"bagging\" , which recently has been found\\nto produce impressive results for classification networks.\\n\\n1\\n\\nIntroduction\\n\\nGaussian mixture models have recently attracted wide attention in the neural network community. Important examples of their application include the training of\\nradial basis function classifiers, learning from patterns with missing features, and\\nactive learning. The appeal of Gaussian mixtures is based to a high degree on the\\napplicability of the EM (Expectation Maximization) learning algorithm, which may\\nbe implemented as a fast neural network learning rule ([Now91], [Orm93]). Severe\\nproblems arise, however, due to singularities and local maxima in the log-likelihood\\nfunction. Particularly in high-dimensional spaces these problems frequently cause\\nthe computed density estimates to possess only relatively limited generalization capabilities in terms of predicting the densities of new data points. As shown in this\\npaper, considerably better generalization can be achieved using regularization.\\n\\n\\x0c543\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nWe will compare two regularization methods. The first one uses a Bayesian prior\\non the parameters. By using conjugate priors we can derive EM learning rules\\nfor finding the MAP (maximum a posteriori probability) parameter estimate. The\\nsecond approach consists of averaging the outputs of ensembles of Gaussian mixture\\ndensity estimators trained on identical or resampled data sets. The latter is a form\\nof \"bagging\" which was introduced by Breiman ([Bre94]) and which has recently\\nbeen found to produce impressive results for classification networks. By using the\\nregularized density estimators in a Bayes classifier ([THA93], [HT94], [KL95]) , we\\ndemonstrate that both methods lead to density estimates which are superior to the\\nunregularized Gaussian mixture estimate.\\n\\n2\\n\\nGaussian Mixtures and the EM Algorithm\\n\\nConsider the lroblem of estimating the probability density of a continuous random\\nvector x E \\'R based on a set x* = {x k 11 S k S m} of iid. realizations of x. As a density model we choose the class of Gaussian mixtures p(xle) = L:7=1 Kip(xli, pi, E i ),\\nwhere the restrictions Ki ~ 0 and L:7=1 Kj = 1 apply. e denotes the parameter\\nvector (Ki\\' Iti, E i )i=1. The p(xli, Pi, E i ) are multivariate normal densities:\\np( xli , Pi , Ei) = (271\")- 41Ei 1- 1 / 2 exp [-1/2(x - Pi)tEi 1 (x - Iti)] .\\nThe Gaussian mixture model is well suited to approximate a wide class of continuous\\nprobability densities. Based on the model and given the data x*, we may formulate\\nthe log-likelihood as\\n\\nlee)\\n\\n= log [rr mk=l p(xkle)] = \",m\\nlog \"\\'~ Kip(xkli, Pi, Ei) .\\n.L...\".k=1\\n.L...\".J=l\\n\\ne\\n\\nMaximum likelihood parameter estimates may efficiently be computed with the\\nEM (Expectation Maximization) algorithm ([DLR77]) . It consists of the iterative\\napplication of the following two steps:\\n1. In the E-step, based on the current parameter estimates, the posterior\\nprobability that unit i is responsible for the generation of pattern xk is\\n\\nestimated as\\n(1)\\n\\n2. In the M-step, we obtain new parameter estimates (denoted by the prime):\\n,\\n\\nK ?\\nJ\\n\\n= -m1 L mk=1 h?k\\nJ\\n\\n~m\\n\\n,\\n\\n(2)\\n\\n=\\n\\nPi\\n\\nwk-l\\n\\n~m\\n\\nhki X k\\nhi\\n\\nwl=l\\n\\n~.\\' _ L:~1 hf(x k - pD(x k - pDt\\nL.J J\\n\\n-\\n\\nm\\n\\nI\\n\\n(3)\\n\\ni\\n\\n(4)\\n\\nL:l=l hi\\nNote that K~ is a scalar , whereas p~ denotes a d-dimensional vector and E/\\nis a d x d matrix.\\nIt is well known that training neural networks as predictors using the maximum\\nlikelihood parameter estimate leads to overfitting. The problem of overfitting is\\neven more severe in density estimation due to singularities in the log-likelihood\\nfunction. Obviously, the model likelihood becomes infinite in a trivial way if we\\nconcentrate all the probability mass on one or several samples of the training set.\\n\\n\\x0c544\\n\\nD. ORMONEIT, V. TRESP\\n\\nIn a Gaussian mixture this is just the case if the center of a unit coincides with\\none of the data points and E approaches the zero matrix. Figure 1 compares the\\ntrue and the estimated probability density in a toy problem. As may be seen,\\nthe contraction of the Gaussians results in (possibly infinitely) high peaks in the\\nGaussian mixture density estimate. A simple way to achieve numerical stability\\nis to artificially enforce a lower bound on the diagonal elements of E. This is a\\nvery rude way of regularization, however, and usually results in low generalization\\ncapabilities. The problem becomes even more severe in high-dimensional spaces.\\nTo yield reasonable approximations, we will apply two methods of regularization,\\nwhich will be discussed in the following two sections.\\n\\nFigure 1: True density (left) and unregularized density estimation (right).\\n\\n3\\n\\nBayesian Regularization\\n\\nIn this section we propose a Bayesian prior distribution on the Gaussian mixture\\nparameters, which leads to a numerically stable version of the EM algorithm. We\\nfirst select a family of prior distributions on the parameters which is conjugate*.\\nSelecting a conjugate prior has a number of advantages. In particular, we obtain\\nanalytic solutions for the posterior density and the predictive density. In our case,\\nthe posterior density is a complex mixture of densities t . It is possible, however, to\\nderive EM-update rules to obtain the MAP parameter estimates.\\nA conjugate prior of a single multivariate normal density is a product of a normal\\ndensity N(JLilft,1]-lE i ) and a Wishart density Wi(E;lla,,8) ([Bun94]). A proper\\nconjugate prior for the the mixture weightings \\'\" = (\"\\'1, ... , \"\\'n) is a Dirichlet density\\nD(\"\\'hV. Consequently, the prior of the overall Gaussian mixture is the product\\nD(\",lr)\\nN(JLilil, 71- 1Ei)Wi(E;1I a , ,8). Our goal is to find the MAP parameter\\nestimate, that is parameters which assume the maximum of the log-posterior\\n\\nil7=1\\n\\nIp(S)\\n\\n2:=~=1 log 2:=;=1 \"\\'iP(X k Ii, JLi, Ei ) + log D(\"\\'lr)\\n\\n+ 2:=;=1 [logN(JLilft, 71- 1Ei) + log Wi(E;lla, ,8)].\\nAs in the unregularized case, we may use the EM-algorithm to find a local maximum\\n? A family F of probability distributions on 0 is said to be conjugate if, for every 1r E F,\\nthe posterior 1r(0Ix) also belongs to F ([Rob94]).\\ntThe posterior distribution can be written as a sum of nm simple terms.\\ntThose densities are defined as follows (b and c are normalizing constants):\\n\\nbII n\\n\\nD(1I:17)\\n\\n.=1\\n\\n~.=l\\n\\n(21r)-i 11,-IE;I-l/2 exp [-~(Il\\' -\\n\\nN(Il.lp,1,-IE.)\\nW i(Ei l la,,8)\\n\\n11:7,-1, with 11:, ~ 0 and \",n\\n\\n=\\n\\ncIEillo-Cd+l)/2 exp [-tr(,8Ei 1 )]\\n\\n11:.\\n\\n=1\\n\\nMt Ei 1 (1l\\' - M]\\n?\\n\\n\\x0c545\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nof Ip(8). The E-step is identical to (1). The M-step becomes\\n,\\nL..\"k-l hki + ri - 1\\n(5)\\n,L..\"k=l hki x k + \\'1J1.\\n\"\\'i\\nJ1.i\\nhi\\nm + L..\"i=l ri - n\\nL..,,1=1 i + 11\\n\\n=\\n\\n\"m\\n\\nE~ =\\n\\n\"m\\n\\n\"n\\n\\n2:;-1 hf(x k -\\n\\nA\\n\\n= \"m\\n\\n+ 11(J1.i 2:~1 h~ + 20: - d\\n\\nJ1.D(xk - J1.D t\\n\\nI\\n\\njJ.)(J1.i - jJ.)t\\n\\n(6)\\n\\n+ 2f3\\n\\n(7)\\n\\nAs typical for conjugate priors, prior knowledge corresponds to a set of artificial\\ntraining data which is also reflected in the EM-update equations. In our experiments, we focus on a prior on the variances which is implemented by f3 =F 0, where\\no denotes the d x d zero matrix. All other parameters we set to \"neutral\" values:\\n\\nri=l\\'v\\'i : l::;i::;n,\\n\\n0:= (d+I)/2,\\n\\n11=0,\\n\\nf3=iJl d\\n\\nld is the d x d unity matrix. The choice of 0: introdu~es a bias which favors large\\nvariances?. The effect of various values of the scalar f3 on the density estimate is\\nillustrated in figure 2. Note that if iJ is chosen too small, overfitting still occurs. If\\nit is chosen to large , on the other hand, the model is too constraint to recognize the\\nunderlying structure.\\n\\nFigure 2: Regularized density estimates (left:\\n\\niJ =\\n\\n0.05, right: \\'iJ = 0.1).\\n\\nTypically, the optimal value for iJ is not known a priori. The simplest procedure\\nconsists of using that iJ which leads to the best performance on a validation set,\\nanalogous to the determination of the optimal weight decay parameter in neural\\nnetwork training. Alternatively, iJ might be determined according to appropriate\\nBayesian methods ([Mac9I]). Either way, only few additional computations are\\nrequired for this method if compared with standard EM.\\n\\n4\\n\\nAveraging Gaussian Mixtures\\n\\nIn this section we discuss the averaging of several Gaussian mixtures to yield improved probability density estimation. The averaging over neural network ensembles\\nhas been applied previously to regression and classification tasks ([PC93]) .\\nThere are several different variants on the simple averaging idea. First, one may\\ntrain all networks on the complete set of training data. The only source of disagreement between the individual predictions consists in different local solutions\\nfound by the likelihood maximization procedure due to different starting points.\\nDisagreement is essential to yield an improvement by averaging, however, so that\\nthis proceeding only seems advantageous in cases where the relation between training data and weights is extremely non-deterministic in the sense that in training,\\n?If A is distributed according to Wi(AIO\\', (3), then E[A- 1 ] = (0\\' - (d + 1)/2)-1 {3. In our\\ncase A is B;-I, so that E[Bi] -+ 00 ? {3 for 0\\' -+ (d + 1)/2.\\n\\n\\x0c546\\n\\nD. ORMONEIT, V. TRESP\\n\\ndifferent solutions are found from different random starting points. A straightforward way to increase the disagreement is to train each network on a resampled\\nversion of the original data set. If we resample the data without replacement, the\\nsize of each training set is reduced, in our experiments to 70% of the original. The\\naveraging of neural network predictions based on resampling with replacement has\\nrecently been proposed under the notation \"bagging\" by Breiman ([Bre94]), who\\nhas achieved dramatic.ally improved results in several classification tasks. He also\\nnotes, however, that an actual improvement of the prediction can only result if the\\nestimation procedure is relatively unstable. As discussed, this is particularly the\\ncase for Gaussian mixture training. We therefore expect bagging to be well suited\\nfor our task.\\n\\n5\\n\\nExperiments and Results\\n\\nTo assess the practical advantage resulting from regularization, we used the density\\nestimates to construct classifiers and compared the resulting prediction accuracies\\nusing a toy problem and a real-world problem. The reason is that the generalization error of density estimates in terms of the likelihood based on the test data\\nis rather unintuitive whereas performance on a classification problem provides a\\ngood impression of the degree of improvement. Assume we have a set of N labeled\\ndata z* = {(xk, lk)lk = 1, ... , N}, where lk E Y = {I, ... , C} denotes the class label\\nof each input xk . A classifier of new inputs x is yielded by choosing the class I\\nwith the maximum posterior class-probability p(llx). The posterior probabilities\\nmay be derived from the class-conditional data likelihood p(xll) via Bayes theorem:\\np(llx) = p(xll)p(l)/p(x) ex p(xll)p(l) . The resulting partitions ofthe input space are\\noptimal for the true p(llx). A viable way to approximate the posterior p(llx) is to\\nestimate p(xll) and p(l) from the sample data.\\n5.1\\n\\nToy Problem\\n\\nIn the toy classification problem the task is to discriminate the two classes of circulatory arranged data shown in figure 3. We generated 200 data points for each class\\nand subdivided them into two sets of 100 data points. The first was used for training, the second to test the generalization performance. As a network architecture\\nwe chose a Gaussian mixture with 20 units. Table 1 summarizes the results, beginning with the unregularized Gaussian mixture which is followed by the averaging\\nand the Bayesian penalty approaches. The three rows for averaging correspond to\\nthe results yielded without applying resampling (local max.), with resampling with-\\n\\nFigure 3: Toy Classification Task.\\n\\n\\x0c547\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nout replacement (70% subsets), and with resampling with replacement (bagging).\\nThe performances on training and test set are measured in terms of the model loglikelihood. Larger values indicate a better performance. We report separate results\\nfor dass A and B, since the densities of both were estimated separately. The final\\ncolumn shows the prediction accuracy in terms of the percentage of correctly classified data in the test set. We report the average results from 20 experiments. The\\nnumbers in brackets denote the standard deviations u of the results. Multiplying u\\nwith T19;95%/v\\'20 = 0.4680 yields 95% confidence intervals. The best result in each\\ncategory is underlined.\\nAlgorithm\\n\\nLog- Likelihood\\n\\nunreg.\\nAveraging:\\nlocal max.\\n70% subset\\nbagging\\nPenalty:\\n[3 = 0.01\\n[3 = 0.02\\n[3 = 0.05\\n[3 = 0.1\\n\\nAccuracy\\n\\nA\\n-120.8 (13.3)\\n\\n-120.4 (10.8)\\n\\nTest\\nA\\nB\\n-224.9 (32.6) -241.9 (34.1)\\n\\n-115.6 (6.0)\\n-106.8 (5.8)\\n-83.8 (4.9)\\n\\n-112.6 (6.6)\\n-105.1 (6.7)\\n-83.1 (7.1)\\n\\n-200.9 (13.9)\\n-188.8 (9.5)\\n-194.2 (7.3)\\n\\n-209.1 (16.3)\\n-196.4 (11.3)\\n-200.1 (11.3)\\n\\n81.8% (3.1)\\n83.2% (2.9)\\n82.6% (3.4)\\n\\n-149.3\\n-156.0\\n-173.9\\n-183.0\\n\\n-146.5 (5.9)\\n-153.0 (4.8)\\n-167.0 (15.8)\\n-181.9 (21.1)\\n\\n-186.2\\n-177.1\\n-182.0\\n-184.6\\n\\n-182.9 (11.6)\\n-174.9 (7.0)\\n-173.9 (14.3)\\n-182.5 (21.1)\\n\\n83.1%\\n84.4%\\n81.5%\\n78.5%\\n\\nTraining\\n\\n(18.5)\\n(16.5)\\n(24.3)\\n(21.9)\\n\\nB\\n\\n(13.9)\\n(11.8)\\n(20.1)\\n(21.0)\\n\\nI\\n80.6\\'70 (2.8)\\n\\n(2.9)\\n(6.3)\\n(5.9)\\n(5.1)\\n\\nTable 1: Performances in the toy classification problem .\\nAs expected, all regularization methods outperform the maximum likelihood approach in terms of correct classification. The performance of the Bayesian regularization is hereby very sensitive to the appropriate choice of the regularization\\nparameter (3. Optimality of (3 with respect to the density prediction and oytimality\\nwith respect to prediction accuracy on the test set roughly coincide (for (3 = 0.02).\\nA veraging is inferior to the Bayesian approach if an optimal {3 is chosen.\\n5.2\\n\\nBUPA Liver Disorder Classification\\n\\nAs a second task we applied our methods to a real-world decision problem from\\nthe medical environment. The problem is to detect liver disorders which might\\narise from excessive alcohol consumption. Available information consists of five\\nblood tests as well as a measure of the patients\\' daily alcohol consumption. We\\nsubdivided the 345 available samples into a training set of 200 and a test set of 145\\nsamples. Due to the relatively few data we did not try to determine the optimal\\nregularization parameter using a validation process and will report results on the\\ntest set for different parameter values.\\nAlgorithm\\nunregularized\\nBayesian penalty ({3 = 0.05)\\nBayesian penalty ?(3 = 0.10)\\nBayesian penal ty (3 = 0.20\\naveraging local maxima\\naveraging (70 % subset)\\naveraging (bagging)\\n\\nAccuracy\\n64.8 %\\n65.5 %\\n66.9 %\\n61.4 %\\n65 .5 0\\n72.4 %\\n71.0 %\\n\\nTable 2: Performances in the liver disorder classification problem.\\n\\n\\x0c548\\n\\nD. ORMONEIT. V. TRESP\\n\\nThe results of our experiments are shown in table 2. Again, both regularization\\nmethods led to an improvement in prediction accuracy. In contrast to the toy problem, the averaged predictor was superior to the Bayesian approach here. Note that\\nthe resampling led to an improvement of more than five percent points compared\\nto unresampled averaging.\\n\\n6\\n\\nConclusion\\n\\nWe proposed a Bayesian and an averaging approach to regularize Gaussian mixture\\ndensity estimates. In comparison with the maximum likelihood solution both approaches led to considerably improved results as demonstrated using a toy problem\\nand a real-world classification task. Interestingly, none of the methods outperformed\\nthe other in both tasks. This might be explained with the fact that Gaussian mixture density estimates are particularly unstable in high-dimensional spaces with\\nrelatively few data. The benefit of averaging might thus be greater in this case.\\nA veraging proved to be particularly effective if applied in connection with resampIing of the training data, which agrees with results in regression and classification\\ntasks. If compared to Bayesian regularization, averaging is computationally expensive. On the other hand, Baysian approaches typically require the determination of\\nhyper parameters (in our case 13), which is not the case for averaging approaches.\\n\\nReferences\\n[Bre94]\\n\\nL. Breiman. Bagging predictors. Technical report , UC Berkeley, 1994.\\n\\n[Bun94]\\n\\nW . Buntine. Operations for learning with graphical models. Journal of Artificial\\nIntelligence Research, 2:159-225, 1994.\\n\\n[DLR77] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from\\nincomplete data via the EM algorithm. J. Royal Statistical Society B, 1977.\\n[HT94]\\n\\nT. Hastie and R. Tibshirani. Discriminant analysis by gaussian mixtures. Technical report , AT&T Bell Labs and University of Toronto, 1994.\\n\\n[KL95]\\n\\nN. Kambhatla and T. K. Leen. Classifying with gaussian mixtures and clusters.\\nIn Advances in Neural Information Processing Systems 7. Morgan Kaufman,\\n1995.\\n\\n[Mac91]\\n\\nD. MacKay. Bayesian Modelling and Neural Networks. PhD thesis, California\\nInstitute of Technology, Pasadena, 1991.\\n\\n[Now91] S. J. Nowlan. Soft Competitive Adaption: Neural Network Learning Algorithms\\nbased on Fitting Statistical Mixtures. PhD thesis, School of Computer Science,\\nCarnegie Mellon University, Pittsburgh, 1991.\\n[Orm93] D. Ormoneit. Estimation of probability densities using neural networks. Master\\'s\\nthesis, Technische Universitiit Munchen, 1993.\\n[PC93]\\n\\nM. P. Perrone and L. N. Cooper. When networks disagree: Ensemble methods for\\nhybrid Neural networks. In Neural Networks for Speech and Image Processing.\\nChapman Hall, 1993.\\n\\n[Rob94]\\n\\nC. P. Robert. The Bayesian Choice. Springer-Verlag, 1994.\\n\\n[THA93] V. Tresp, J. Hollatz, and S. Ahmad. Network structuring and training using\\nrule-based knowledge. In Advances in Neural Information Processing Systems 5.\\nMorgan Kaufman, 1993.\\n\\n\\x0c',\n",
       "     'pdf_name': '1036-improved-gaussian-mixture-density-estimates-using-bayesian-penalty-terms-and-network-averaging.pdf',\n",
       "     'title': 'Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1044',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1044',\n",
       "     'paper_text': 'Learning with ensembles: How\\nover-fitting can be useful\\n\\nPeter Sollich\\nDepartment of Physics\\nUniversity of Edinburgh, U.K.\\nP.SollichGed.ac.uk\\n\\nAnders Krogh\\'\"\\nNORDITA, Blegdamsvej 17\\n2100 Copenhagen, Denmark\\nkroghGsanger.ac.uk\\n\\nAbstract\\nWe study the characteristics of learning with ensembles. Solving\\nexactly the simple model of an ensemble of linear students, we\\nfind surprisingly rich behaviour. For learning in large ensembles,\\nit is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can\\nbe obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble\\nweights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide\\nrange of regularization parameters makes this improvement robust\\nagainst changes in the unknown level of noise in the training data.\\n\\n1\\n\\nINTRODUCTION\\n\\nAn ensemble is a collection of a (finite) number of neural networks or other types\\nof predictors that are trained for the same task. A combination of many different predictors can often improve predictions, and in statistics this idea has been\\ninvestigated extensively, see e.g. [1, 2, 3]. In the neural networks community, ensembles of neural networks have been investigated by several groups, see for instance\\n[4, 5, 6, 7]. Usually the networks in the ensemble are trained independently and\\nthen their predictions are combined.\\nIn this paper we study an ensemble of linear networks trained on different but\\noverlapping training sets. The limit in which all the networks are trained on the\\nfull data set and the one where all the data sets are different has been treated in\\n[8] . In this paper we treat the case of intermediate training set sizes and overlaps\\n?Present address: The Sanger Centre, Hinxton, Cambs CBIO IRQ, UK.\\n\\n\\x0cLearning with Ensembles: How Overfitting Can Be Useful\\n\\n191\\n\\nexactly, yielding novel insights into ensemble learning. Our analysis also allows us to\\nstudy the effect of regularization and of having different predictors in an ensemble.\\n\\n2\\n\\nGENERAL FEATURES OF ENSEMBLE LEARNING\\n\\nWe consider the task of approximating a target function fo from RN to R. It\\nwill be assumed that we can only obtain noisy samples of the function, and the\\n(now stochastic) target function will be denoted y(x) . The inputs x are taken\\nto be drawn from some distribution P(x). Assume now that an ensemble of K\\nindependent predictors fk(X) of y(x) is available. A weighted ensemble average is\\ndenoted by a bar, like\\n(1)\\nlex) = L,wkfk(X),\\nk\\n\\nwhich is the final output of the ensemble. One can think of the weight Wk as the\\nbelief in predictor k and we therefore constrain the weights to be positive and sum\\nto one. For an input x we define the error of the ensemble c(x), the error of the\\nkth predictor ck(X), and its ambiguity ak(x)\\nc(x)\\nck(X)\\n\\n(y(x) -lex)?\\n(y(x) - fk(X)?\\n(fk(X) -1(x?2.\\n\\n=\\n\\n(2)\\n(3)\\n(4)\\n\\n=\\n\\nThe ensemble error can be written as c(x)\\nlex) - a(x) [7], where lex)\\nL,k Wkck(X) is the average error over the individual predictors and a(x) =\\nL,k Wkak(X) is the average of their ambiguities, which is the variance of the output\\nover the ensemble. By averaging over the input distribution P(x) (and implicitly\\nover the target outputs y(x?, one obtains the ensemble generalization error\\n(5)\\nwhere c(x) averaged over P(x) is simply denoted c, and similarly for land a. The\\nfirst term on the right is the weighted average of the generalization errors of the individual predictors, and the second is the weighted average of the ambiguities, which\\nwe refer to as the ensemble ambiguity. An important feature of equation (5) is that\\nit separates the generalization error into a term that depends on the generalization\\nerrors of the individual students and another term that contains all correlations between the students. The latter can be estimated entirely from unlabeled data, i. e.,\\nwithout any knowledge of the target function to be approximated. The relation (5)\\nalso shows that the more the predictors differ, the lower the error will be, provided\\nthe individual errors remain constant.\\n\\nIn this paper we assume that the predictors are trained on a sample of p examples\\nof the target function, (xt\\',yt\\'), where yt\\' = fo(xt\\') + TJt\\' and TJt\\' is some additive\\nnoise (Jl. = 1, ... ,p). The predictors, to which we refer as students in this context\\nbecause they learn the target function from the training examples, need not be\\ntrained on all the available data. In fact, since training on different data sets will\\ngenerally increase the ambiguity, it is possible that training on subsets of the data\\nwill improve generalization. An additional advantage is that, by holding out for\\neach student a different part of the total data set for the purpose of testing, one\\ncan use the whole data set for training the ensemble while still getting an unbiased\\nestimate of the ensemble generalization error. Denoting this estimate by f, one has\\n(6)\\nwhere Ctest = L,k WkCtest,k is the average of the students\\' test errors. As already\\npointed out, the estimate ft of the ensemble ambiguity can be found from unlabeled\\ndata.\\n\\n\\x0cP. SOLLICH, A. KROGH\\n\\n192\\n\\nSo far, we have not mentioned how to find the weights Wk. Often uniform weights\\nare used, but optimization of the weights in some way is tempting. In [5, 6] the\\ntraining set was used to perform the optimization, i.e., the weights were chosen to\\nminimize the ensemble training error. This can easily lead to over-fitting, and in [7]\\nit was suggested to minimize the estimated generalization error (6) instead. If this\\nis done, the estimate (6) acquires a bias; intuitively, however, we expect this effect\\nto be small for large ensembles.\\n\\n3\\n\\nENSEMBLES OF LINEAR STUDENTS\\n\\nIn preparation for our analysis of learning with ensembles of linear students we now\\nbriefly review the case of a single linear student, sometimes referred to as \\'linear\\nperceptron learning\\'. A linear student implements the input-output mapping\\n1 T\\nJ(x) = ..JNw x\\nparameterized in terms of an N-dimensional parameter vector w with real components; the scaling factor 1/..JN is introduced here for convenience, and . ..T denotes\\nthe transpose of a vector. The student parameter vector w should not be confused with the ensemble weights Wk. The most common method for training such\\na linear student (or parametric inference models in general) is minimization of the\\nsum-of-squares training error\\nE = L:(y/J - J(x/J))2 + Aw2\\n/J\\nwhere J.L = 1, ... ,p numbers the training examples. To prevent the student from\\nfitting noise in the training data, a weight decay term Aw2 has been added. The size\\nof the weight decay parameter A determines how strongly large parameter vectors\\nare penalized; large A corresponds to a stronger regularization of the student.\\nFor a linear student, the global minimum of E can easily be found. However,\\nin practical applications using non-linear networks, this is generally not true, and\\ntraining can be thought of as a stochastic process yielding a different solution each\\ntime. We crudely model this by considering white noise added to gradient descent\\nupdates of the parameter vector w. This yields a limiting distribution of parameter\\nvectors P(w) ex: exp(-E/2T), where the \\'temperature\\' T measures the amount of\\nnoise in the training process.\\nWe focus our analysis on the \\'thermodynamic limit\\' N - t 00 at constant normalized\\nnumber of training examples, ex = p/N. In this limit, quantities such as the training\\nor generalization error become self-averaging, i.e., their averages over all training\\nsets become identical to their typical values for a particular training set. Assume\\nnow that the training inputs x/J are chosen randomly and independently from a\\nGaussian distribution P(x) ex: exp( - ~x2), and that training outputs are generated\\nby a linear target function corrupted by additive noise, i.e., y/J = w\\'f x/J /..IN + 1]/J,\\nwhere the 1]/J are zero mean noise variables with variance u 2 ? Fixing the length of the\\nparameter vector of the target function to w~ = N for simplicity, the generalization\\nerror of a linear student with weight decay A and learning noise T becomes [9]\\n(; = (u 2 + T)G + A(U 2\\n\\n-\\n\\n8G\\n\\nA) 8A .\\n\\n(7)\\n\\nOn the r.h.s. of this equation we have dropped the term arising from the noise on\\nthe target function alone, which is simply u 2 , and we shall follow this convention\\nthroughout . The \\'response function\\' Gis [10, 11]\\n\\nG = G(ex, A) = (1 - ex - A+ )(1 - ex - A)2 + 4A)/2A.\\n\\n(8)\\n\\n\\x0c193\\n\\nLearning with Ensembles: How Overfitting Can Be Useful\\n\\nFor zero training noise, T = 0, and for any a, the generalization error (7} is minimized when the weight decay is set to A = (T2j its value is then (T2G(a, (T2), which\\nis the minimum achievable generalization error [9].\\n\\n3.1\\n\\nENSEMBLE GENERALIZATION ERROR\\n\\nWe now consider an ensemble of K linear students with weight decays Ak and\\nlearning noises Tk (k = 1 . . . K). Each ,student has an ensemble weight Wk and\\nis trained on N ak training examples, with students k and I sharing N akl training\\nexamples (of course, akk = ak). As above, we consider noisy training data generated\\nby a linear target function. The resulting ensemble generalization error can be\\ncalculated by diagrammatic [10] or response function [11] methods. We refer the\\nreader to a forthcoming publication for details and only state the result:\\n\\n(9)\\nwhere\\n(10)\\nHere Pk is defined as Pk = AkG(ak, Ak). The Kronecker delta in the last term\\nof (10) arises because the training noises of different students are uncorrelated. The\\ngeneralization errors and ambiguities of the individual students are\\n\\nak = ckk - 2 LWlckl\\nI\\n\\n+ LWIWmclm;\\n1m\\n\\nthe result for the Ck can be shown to agree with the single student result (7). In\\nthe following sections, we shall explore the consequences of the general result (9) .\\nWe will concentrate on the case where the training set of each student is sampled\\nrandomly from the total available data set of size NO\\', For the overlap of the training\\nsets of students k and I (k \\'II) one then has akl/a = (ak/a)(al/a) and hence\\n\\nak/ = akal/a\\n\\n(11)\\nup to fluctuations which vanish in the thermodynamic limit. For finite ensembles\\none can construct training sets for which akl < akal/a. This is an advantage,\\nbecause it results in a smaller generalization error, but for simplicity we use (11).\\n\\n4\\n\\nLARGE ENSEMBLE LIMIT\\n\\nWe now use our main result (9) to analyse the generalization performance of an ensemble with a large number K of students, in particular when the size of the training\\nsets for the individual students are chosen optimally. If the ensemble weights Wk\\nare approximately uniform (Wk ~ 1/ K) the off-diagonal elements of the matrix\\n(ckl) dominate the generalization error for large K, and the contributions from the\\ntraining noises\\nare suppressed. For the special case where all students are identical and are trained on training sets of identical size, ak = (1 - c)a, the ensemble\\ngeneralization error is shown in Figure 1(left). The minimum at a nonzero value\\nof c, which is the fraction of the total data set held out for testing each student,\\ncan clearly be seen. This confirms our intuition: when the students are trained\\non smaller, less overlapping training sets, the increase in error of the individual\\nstudents can be more than offset by the corresponding increase in ambiguity.\\n\\nn\\n\\nThe optimal training set sizes ak can be calculated analytically:\\n_\\n\\nCk\\n\\n=1-\\n\\nak/ a\\n\\n1 - Ak/(T2\\n\\n= 1 + G(a, (T2) \\'\\n\\n(12)\\n\\n\\x0cP. SOLLICH, A. KROGH\\n\\n194\\n1.0 r - - - , - - - - - , r - - - . , - - - - , . - - - - : .\\n\\n1.0 r - - - , - - - - - , - - - . - - - - r - - - - \"\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n\\n0.6\\n\\nw\\n\\n.\\'\\n\\nw\\n0.4\\n\\n,...-------\\n\\n0.2\\n/\\n\\n/\\n\\n0.0 /\\n\\n0.0\\n\\n/\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n,,\\n,\\n0.8\\n\\n0.2\\n\\n------,\\n\\n1.0\\n\\n0.0\\n\\n0.0\\n\\nC\\n\\n...\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nC\\n\\nFigure 1: Generalization error and ambiguity for an infinite ensemble of identical\\nstudents. Solid line: ensemble generalization error, fj dotted line: average generalization error of the individual students, l; dashed line: ensemble ambiguity, a.\\nFor both plots a = 1 and (72 = 0.2 . The left plot corresponds to under-regularized\\nstudents with A = 0.05 < (72. Here the generalization error of the ensemble has\\na minimum at a nonzero value of c. This minimum exists whenever>\\' < (72. The\\nright plot shows the case of over-regularized students (A = 0.3 > (72), where the\\ngeneralization error is minimal at c = O.\\nThe resulting generalization error is f = (72G(a, (72) + 0(1/ K), which is the globally\\nminimal generalization error that can be obtained using all available training data,\\nas explained in Section 3. Thus, a large ensemble with optimally chosen training\\nset sizes can achieve globally optimal generalization performance. However, we see\\nfrom (12) that a valid solution Ck > 0 exists only for Ak < (72, i.e., if the ensemble\\nis under-regularized. This is exemplified, again for an ensemble of identical students, in Figure 1(right) , which shows that for an over-regularized ensemble the\\ngeneralization error is a: monotonic function of c and thus minimal at c = o.\\nWe conclude this section by discussing how the adaptation of the training set sizes\\ncould be performed in practice, for simplicity confining ourselves to an ensemble of\\nidentical students, where only one parameter c = Ck = 1- ak/a has to be adapted.\\nIf the ensemble is under-regularized one expects a minimum of the generalization\\nerror for some nonzero c as in Figure 1. One could, therefore, start by training\\nall students on a large fraction of the total data set (corresponding to c ~ 0), and\\nthen gradually and randomly remove training examples from the students\\' training\\nsets. Using (6), the generalization error of each student could be estimated by\\ntheir performance on the examples on which they were not trained, and one would\\nstop removing training examples when the estimate stops decreasing. The resulting\\nestimate of the generalization error will be slightly biased; however, for a large\\nenough ensemble the risk of a strongly biased estimate from systematically testing\\nall students on too \\'easy\\' training examples seems small, due to the random selection\\nof examples.\\n\\n5\\n\\nREALISTIC ENSEMBLE SIZES\\n\\nWe now discuss some effects that occur in learning with ensembles of \\'realistic\\' sizes.\\nIn an over-regularized ensemble nothing can be gained by making the students more\\ndiverse by training them on smaller, less overlapping training sets. One would also\\n\\n\\x0c195\\n\\nLearning with Ensembles: How Overfitting Can Be Useful\\n\\nFigure 2: The generalization error of\\nan ensemble with 10 identical students as a function of the test set\\nfraction c. From bottom to top the\\ncurves correspond to training noise\\nT = 0,0.1,0.2, ... ,1.0. The star on\\neach curve shows the error of the optimal single perceptron (i. e., with optimal weight decay for the given T)\\ntrained on all examples, which is independent of c. The parameters for\\nthis example are: a = 1, A = 0.05,\\n0\\'2 = 0.2.\\n\\n0.2\\n0.0 L - _ - - \\' - _ - - - \\'_ _-\\'--_--\\'-_~\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nC\\n\\nexpect this kind of \\'diversification\\' to be unnecessary or even counterproductive\\nwhen the training noise is high enough to provide sufficient \\'inherent\\' diversity of\\nstudents. In the large ensemble limit, we saw that this effect is suppressed, but\\nit does indeed occur in finite ensembles. Figure 2 shows the dependence of the\\ngeneralization error on c for an ensemble of 10 identical, under-regularized students\\nwith identical training noises Tk = T. For small T, the minimum of f. at nonzero c\\npersists. For larger T, f. is monotonically increasing with c, implying that further\\ndiversification of students beyond that caused by the learning noise is wasteful. The\\nplot also shows the performance of the optimal single student (with A chosen to\\nminimize the generalization error at the given T), demonstrating that the ensemble\\ncan perform significantly better by effectively averaging out learning noise.\\nFor realistic ensemble sizes the presence of learning noise generally reduces the\\npotential for performance improvement by choosing optimal training set sizes. In\\nsuch cases one can still adapt the ensemble weights to optimize performance, again\\non the basis of the estimate of the ensemble generalization error (6). An example is\\n1.0\\n\\n1.0\\n\\n,,\\n\\n0.8\\n\\nI\\nI\\n\\n0.8\\n,-\\n\\n,-\\n\\n/\\n\\nI\\n\\n0.6\\n\\n0.6\\n\\nI\\n\\ntV\\n\\ntV\\n\\n0.4 ..... -_ .................\\n\\n0.4\\n0.2\\n... ....\\n0.0\\n0.001\\n\\n---0.010\\n\\n0.2\\n\\n0.100\\n\\n02\\n\\n1.000\\n\\n0.0\\n0.001\\n\\n0.010\\n\\n02\\n\\n0.100\\n\\n1.000\\n\\nFigure 3: The generalization error of an ensemble of 10 students with different\\nweight decays (marked by stars on the 0\\'2-axis) as a function of the noise level\\n0\\'2. Left: training noise T = 0; right: T = 0.1. The dashed lines are for the\\nensemble with uniform weights, and the solid line is for optimized ensemble weights.\\nThe dotted lines are for the optimal single perceptron trained on all data. The\\nparameters for this example are: a = 1, c = 0.2.\\n\\n\\x0c196\\n\\nP. SOu...ICH, A. KROGH\\n\\nshown in Figure 3 for an ensemble of size 1< = 10 with the weight decays >\\'k equally\\nspaced on a logarithmic axis between 10- 3 and 1. For both of the temperatures T\\nshown, the ensemble with uniform weights performs worse than the optimal single\\nstudent. With weight optimization, the generalization performance approaches that\\nof the optimal single student for T = 0, and is actually better at T = 0.1 over\\nthe whole range of noise levels rr2 shown. Even the best single student from the\\nensemble can never perform better than the optimal single student, so combining the\\nstudent outputs in a weighted ensemble average is superior to simply choosing the\\nbest member of the ensemble by cross-validation, i.e., on the basis of its estimated\\ngeneralization error. The reason is that the ensemble average suppresses the learning\\nnoise on the individual students.\\n\\n6\\n\\nCONCLUSIONS\\n\\nWe have studied ensemble learning in the simple, analytically solvable scenario of\\nan ensemble of linear students. Our main findings are: In large ensembles, one\\nshould use under-regularized students in order to maximize the benefits of the\\nvariance-reducing effects of ensemble learning. In this way, the globally optimal\\ngeneralization error on the basis of all the available data can be reached by optimizing the training set sizes of the individual students. At the same time an estimate\\nof the generalization error can be obtained. For ensembles of more realistic size, we\\nfound that for students subjected to a large amount of noise in the training process\\nit is unnecessary to increase the diversity of students by training them on smaller,\\nless overlapping training sets. In this case, optimizing the ensemble weights can\\nstill yield substantially better generalization performance than an optimally chosen\\nsingle student trained on all data with the same amount of training noise. This\\nimprovement is most insensitive to changes in the unknown noise levels rr2 if the\\nweight decays of the individual students cover a wide range. We expect most of these\\nconclusions to carryover, at least qualitatively, to ensemble learning with nonlinear\\nmodels, and this correlates well with experimental results presented in [7].\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n\\nC. Granger, Journal of Forecasting 8, 231 (1989).\\nD. Wolpert, Neural Networks 5, 241 (1992) .\\nL. Breimann, Tutorial at NIPS 7 and personal communication.\\nL. Hansen and P. Salamon, IEEE Trans. Pattern Anal. and Mach. Intell. 12,\\n993 (1990).\\nM. P. Perrone and L. N. Cooper, in Neural Networks for Speech and Image\\nprocessing, ed. R. J. Mammone (Chapman-Hall, 1993).\\nS. Hashem: Optimal Linear Combinations of Neural Networks. Tech. Rep .\\nPNL-SA-25166, submitted to Neural Networks (1995) .\\nA. Krogh and J. Vedelsby, in NIPS 7, ed. G. Tesauro et al., p. 231 (MIT Press,\\n1995).\\nR. Meir, in NIPS 7, ed. G. Tesauro et al., p. 295 (MIT Press, 1995).\\nA. Krogh and J. A. Hertz, J. Phys. A 25,1135 (1992).\\nJ. A. Hertz, A. Krogh, and G. I. Thorbergsson, J. Phys. A 22, 2133 (1989).\\nP. Sollich, J. Phys. A 27, 7771 (1994).\\n\\n\\x0c',\n",
       "     'pdf_name': '1044-learning-with-ensembles-how-overfitting-can-be-useful.pdf',\n",
       "     'title': 'Learning with ensembles: How overfitting can be useful',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': 7241},\n",
       " 'timed_out': False,\n",
       " 'took': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index=\"nips_papers\") #default \"from\" 0, \"size\" 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hits': {'hits': [{'_source': {'title': 'Neural Network Ensembles, Cross Validation, and Active Learning'}},\n",
       "   {'_source': {'title': 'ICEG Morphology Classification using an Analogue VLSI Neural Network'}},\n",
       "   {'_source': {'title': 'Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories'}},\n",
       "   {'_source': {'title': 'Learning to Play the Game of Chess'}},\n",
       "   {'_source': {'title': 'Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex'}},\n",
       "   {'_source': {'title': 'VLSI Model of Primate Visual Smooth Pursuit'}},\n",
       "   {'_source': {'title': 'Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks'}},\n",
       "   {'_source': {'title': 'A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex'}},\n",
       "   {'_source': {'title': 'Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging'}},\n",
       "   {'_source': {'title': 'Learning with ensembles: How overfitting can be useful'}}]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index=\"nips_papers\", filter_path=['hits.hits._source.title']) #filter_path usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airline_tweets': {'aliases': {}},\n",
       " 'business-reviews-study-ela': {'aliases': {}},\n",
       " 'corpora-index': {'aliases': {}},\n",
       " 'criminal-forum': {'aliases': {}},\n",
       " 'debates': {'aliases': {}},\n",
       " 'emails-beagle1': {'aliases': {}},\n",
       " 'enron': {'aliases': {}},\n",
       " 'enron-nlp': {'aliases': {}},\n",
       " 'enron-nlp2': {'aliases': {}},\n",
       " 'fake_news': {'aliases': {}},\n",
       " 'hacking-forum': {'aliases': {}},\n",
       " 'happiness': {'aliases': {}},\n",
       " 'hate_speech': {'aliases': {}},\n",
       " 'health-reviews': {'aliases': {}},\n",
       " 'ieeevis': {'aliases': {}},\n",
       " 'ieeevis-test': {'aliases': {}},\n",
       " 'ieeevis2': {'aliases': {}},\n",
       " 'ieeevis3': {'aliases': {}},\n",
       " 'nips_papers': {'aliases': {}},\n",
       " 'nsf': {'aliases': {}},\n",
       " 'sms_spam': {'aliases': {}},\n",
       " 'texas.store': {'aliases': {}},\n",
       " 'texas.store.exploratory-labeling': {'aliases': {}},\n",
       " 'texas.store.exploratory-labeling.interaction-log': {'aliases': {}},\n",
       " 'texas.store.exploratory-labeling.interpretation-study': {'aliases': {}},\n",
       " 'texas.store.exploratory-labeling.label-sets': {'aliases': {}},\n",
       " 'united-nations-topics': {'aliases': {}},\n",
       " 'united-nations-topics-2': {'aliases': {}},\n",
       " 'visualization-survey': {'aliases': {}},\n",
       " 'vox-nes-nlp': {'aliases': {}},\n",
       " 'vox-news': {'aliases': {}},\n",
       " 'vox-news-nlp': {'aliases': {}},\n",
       " 'yelp-2017-09-28': {'aliases': {}},\n",
       " 'yelp-challange': {'aliases': {}},\n",
       " 'yelp-challange-jun-2018': {'aliases': {}},\n",
       " 'yelp-challange-v2': {'aliases': {}},\n",
       " 'yelp-challange-v3': {'aliases': {}},\n",
       " 'yelp-challange-v4': {'aliases': {}},\n",
       " 'yelp-health-20170604': {'aliases': {'yelp': {}}},\n",
       " 'yelp-lasvegas-negative': {'aliases': {}},\n",
       " 'yelp-lasvegas-positive': {'aliases': {}},\n",
       " 'yelp-nlp': {'aliases': {}},\n",
       " 'yelp-nlp2': {'aliases': {}},\n",
       " 'yelp-nlp2-npl': {'aliases': {}},\n",
       " 'yelp-propublica-nlp': {'aliases': {}},\n",
       " 'yelp-propublica-nlp-flat': {'aliases': {}},\n",
       " 'yelp-restaurants-lasvegas': {'aliases': {}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.get_alias('*') #list of all indices on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'count': 7241}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.count(index=\"nips_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '1001',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1001',\n",
       "     'paper_text': 'Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders Krogh\"\\nNordita\\nBlegdamsvej 17\\n2100 Copenhagen, Denmark\\n\\nJesper Vedelsby\\nElectronics Institute, Building 349\\nTechnical University of Denmark\\n2800 Lyngby, Denmark\\n\\nAbstract\\nLearning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity\\nis defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among\\nthe networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble\\ngeneralization error, and how this type of ensemble cross-validation\\ncan sometimes improve performance. It is shown how to estimate\\nthe optimal weights of the ensemble members using unlabeled data.\\nBy a generalization of query by committee, it is finally shown how\\nthe ambiguity can be used to select new training data to be labeled\\nin an active learning scheme.\\n\\n1\\n\\nINTRODUCTION\\n\\nIt is well known that a combination of many different predictors can improve predictions. In the neural networks community \"ensembles\" of neural networks has been\\ninvestigated by several authors, see for instance [1, 2, 3]. Most often the networks\\nin the ensemble are trained individually and then their predictions are combined.\\nThis combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .\\n.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk\\n\\n\\x0c232\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nAt the workshop after the last NIPS conference (December, 1993) an entire session\\nwas devoted to ensembles of neural networks ( \"Putting it all together\", chaired by\\nMichael Perrone) . Many interesting papers were given, and it showed that this area\\nis getting a lot of attention .\\nA combination of the output of several networks (or other predictors) is only useful\\nif they disagree on some inputs. Clearly, there is no more information to be gained\\nfrom a million identical networks than there is from just one of them (see also\\n[2]). By quantifying the disagreement in the ensemble it turns out to be possible\\nto state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the\\ndisagreement (called the ensemble ambiguity) and the generalization error is the\\nbasis for this paper, so we will derive it with no further delay.\\n\\n2\\n\\nTHE BIAS-VARIANCE TRADEOFF\\n\\nAssume the task is to learn a function J from RN to R for which you have a sample\\nof p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples\\nare assumed to be drawn randomly from the distribution p(x) . Anything in the\\nfollowing is easy to generalize to several output variables.\\nThe ensemble consists of N networks and the output of network a on input x is\\ncalled va (x). A weighted ensemble average is denoted by a bar , like\\n\\nV(x) =\\n\\nL Wa Va(x).\\n\\n(1)\\n\\na\\n\\nThis is the final output of the ensemble. We think of the weight Wa as our belief in\\nnetwork a and therefore constrain the weights to be positive and sum to one. The\\nconstraint on the sum is crucial for some of the following results.\\nThe ambiguity on input x of a single member of the ensemble is defined as aa (x)\\n(V a(x) - V(x))2 . The ensemble ambiguity on input x is\\n\\na(x)\\n\\n= Lwaaa(x) = LWa(va(x) a\\n\\nV(x))2 .\\n\\n=\\n\\n(2)\\n\\na\\n\\nIt is simply the variance of the weighted ensemble around the weighed mean, and\\nit measures the disagreement among the networks on input x. The quadratic error\\nof network a and of the ensemble are\\n\\n(J(x) - V a(x))2\\n(J(x) - V(X))2\\n\\n(3)\\n(4)\\n\\nrespectively. Adding and subtracting J( x) in (2) yields\\n\\na(x)\\n\\n=L\\n\\nWafa(X) - e(x)\\n\\n(5)\\n\\na\\n\\n(after a little algebra using that the weights sum to one) . Calling the weighted\\naverage of the individual errors ?( x) = La Wa fa (x) this becomes\\n\\ne(x)\\n\\n= ?(x) -\\n\\na(x).\\n\\n(6)\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\n233\\n\\nAll these formulas can be averaged over the input distribution . Averages over the\\ninput distribution will be denoted by capital letter, so\\n\\nJ dxp(xVl! (x)\\nJ dxp(x)aa(x)\\nJ dxp(x)e(x).\\n\\nE\\n\\n(7)\\n(8)\\n(9)\\n\\nThe first two of these are the generalization error and the ambiguity respectively\\nfor network n , and E is the generalization error for the ensemble. From (6) we then\\nfind for the ensemble generalization error\\n(10)\\nThe first term on the right is the weighted average of the generalization errors of\\nthe individual networks (E = La waEa), and the second is the weighted average\\nof the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.\\nThe beauty of this equation is that it separates the generalization error into a term\\nthat depends on the generalization errors of the individual networks and another\\nterm that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is\\nrequired of the real function to be approximated. The term \"unlabeled example\" is\\nborrowed from classification problems, and in this context it means an input x for\\nwhich the value of the target function f( x) is unknown.\\nEquation (10) expresses the tradeoff between bias and variance in the ensemble ,\\nbut in a different way than the the common bias-variance relation [4] in which the\\naverages are over possible training sets instead of ensemble averages. If the ensemble\\nis strongly biased the ambiguity will be small , because the networks implement very\\nsimilar functions and thus agree on inputs even outside the training set. Therefore\\nthe generalization error will be essentially equal to the weighted average of the\\ngeneralization errors of the individual networks. If, on the other hand , there is a\\nlarge variance , the ambiguity is high and in this case the generalization error will\\nbe smaller than the average generalization error . See also [5].\\nFrom this equation one can immediately see that the generalization error of the\\nensemble is always smaller than the (weighted) average of the ensemble errors,\\nE < E. In particular for uniform weights:\\n\\nE\\n\\n~ ~ \\'fEcx\\n\\n(11)\\n\\nwhich has been noted by several authors , see e.g. [3] .\\n\\n3\\n\\nTHE CROSS-VALIDATION ENSEMBLE\\n\\nFrom (10) it is obvious that increasing the ambiguity (while not increasing individual\\ngeneralization errors) will improve the overall generalization. We want the networks\\nto disagree! How can we increase the ambiguity of the ensemble? One way is to\\nuse different types of approximators like a mixture of neural networks of different\\ntopologies or a mixture of completely different types of approximators. Another\\n\\n\\x0c234\\n\\nAnders Krogh, Jesper Vedelsby\\n\\n.\\n\\n:~\\n\\n1. -\\n\\nt\\n\\n-\\n\\n,\\',\\n\\n.. ,\\n\\nE o...... -\\' \\'.- .. \\' ........ ....,.\\n\\n.\\'\\n\\n..... , ...\\n\\nv \\'. --:\\n\\n,\\n\\n.~.--c??\\n\\n__ .. -.tI\"\\n\\n.\\n\\n. -- - -\\\\\\\\\\n\\n\\'1\\n\\n-\\n\\n.~\\n\\n~.\\n\\n, . _ ? .\" ?\\n\\n.. - .....\\n\\n_._ ..... .\\'-._._.1\\n\\n,\\n\\n-\\n\\n>\\n\\n-\\n\\n-1.k!\\n~\\n\\n-4\\n\\n.t.\\n\\nf.\\n\\n1\\\\.1\\n\\n:\\\\,\\'. - ?-.l\\n\\n:--,____\\n..\\n\\n~~\\n.\\n\\n~.\\n\\n,\\n\\n,\\'\\n\\n-2\\n\\n.~\\n\\nIf\\n\\no\\n\\n2\\n\\n\\\\.\\n~\\n:\\n?\\n\\n\\' 0\\'\\n\\n~:\\n\\n4\\n\\nx\\n\\nFigure 1: An ensemble of five networks were trained to approximate the square\\nwave target function f(x). The final ensemble output (solid smooth curve) and\\nthe outputs of the individual networks (dotted curves) are shown. Also the square\\nroot of the ambiguity is shown (dash-dot line) _ For training 200 random examples\\nwere used, but each network had a cross-validation set of size 40, so they were each\\ntrained on 160 examples.\\n\\nobvious way is to train the networks on different training sets. Furthermore, to be\\nable to estimate the first term in (10) it would be desirable to have some kind of\\ncross-validation. This suggests the following strategy.\\nChose a number K :::; p. For each network in the ensemble hold out K examples for\\ntesting, where the N test sets should have minimal overlap, i. e., the N training sets\\nshould be as different as possible. If, for instance, K :::; piN it is possible to choose\\nthe K test sets with no overlap. This enables us to estimate the generalization error\\nE(X of the individual members of the ensemble, and at the same time make sure\\nthat the ambiguity increases . When holding out examples the generalization errors\\nfor the individual members of the ensemble, E(X, will increase, but the conjecture\\nis that for a good choice of the size of the ensemble (N) and the test set size\\n(K), the ambiguity will increase more and thus one will get a decrease in overall\\ngeneralization error.\\nThis conjecture has been tested experimentally on a simple square wave function\\nof one variable shown in Figure 1. Five identical feed-forward networks with one\\nhidden layer of 20 units were trained independently by back-propagation using 200\\nrandom examples. For each network a cross-validation set of K examples was held\\nout for testing as described above. The \"true\" generalization and the ambiguity were\\nestimated from a set of 1000 random inputs. The weights were uniform, w(X\\n1/5\\n(non-uniform weights are addressed later).\\n\\n=\\n\\nIn Figure 2 average results over 12 independent runs are shown for some values of\\n\\n\\x0cNeural Network Ensembles, Cross Validation, and Active Learning\\n\\nFigure 2: The solid line shows the generalization error for uniform weights as\\na function of K, where K is the size\\nof the cross-validation sets. The dotted\\nline is the error estimated from equation (10) . The dashed line is for the\\noptimal weights estimated by the use of\\nthe generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.\\nThe bottom solid line is the generalization error one would obtain if the individual generalization errors were known\\nexactly (the best possible weights).\\n\\n0.08\\n\\n235\\n\\n,-----r----,--~---r-----,\\n\\no\\n\\nt=\\nw\\n0.06\\n\\nc\\n\\no\\n~\\n\\n.!::!\\n\\nco...\\n\\n~ 0.04\\n\\nQ)\\n\\n(!)\\n\\n0 .02 \\'---_---1_ _---\\'-_ _--\\'-_ _-----\\'\\no\\n20\\n40\\n60\\n80\\nSize of CV set\\n\\nK (top solid line) . First, one should note that the generalization error is the same\\nfor a cross-validation set of size 40 as for size 0, although not lower, so it supports\\nthe conjecture in a weaker form. However, we have done many experiments, and\\ndepending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,\\nonly ensembles with at least four converging networks out of five were used . If all\\nthe ensembles were kept, the error would have been significantly higher at ]{ = a\\nthan for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set\\nwas used. Thus it is still unclear under which circumstances one can expect a drop\\nin generalization error when using cross-validation in this fashion.\\n\\nThe dotted line in Figure 2 is the error estimated from equation (10) using the\\ncross-validation sets for each of the networks to estimate Ea, and one notices a\\ngood agreement.\\n\\n4\\n\\nOPTIMAL WEIGHTS\\n\\nThe weights Wa can be estimated as described in e.g. [3]. We suggest instead\\nto use unlabeled data and estimate them in such a way that they minimize the\\ngeneralization error given in (10) .\\nThere is no analytical solution for the weights , but something can be said about\\nthe minimum point of the generalization error. Calculating the derivative of E as\\ngiven in (10) subject to the constraints on the weights and setting it equal to zero\\nshows that\\nEa - Aa\\nE or Wa = O.\\n(12)\\n\\n=\\n\\n(The calculation is not shown because of space limitations, but it is easy to do.)\\nThat is, Ea - Aa has to be the same for all the networks. Notice that Aa depends\\non the weights through the ensemble average of the outputs. It shows that the\\noptimal weights have to be chosen such that each network contributes exactly waE\\n\\n\\x0c236\\n\\nAnders Krogh, Jesper Vedelsby\\n\\nto the generalization error. Note, however, that a member of the ensemble can have\\nsuch a poor generalization or be so correlated with the rest of the ensemble that it\\nis optimal to set its weight to zero.\\nThe weights can be \"learned\" from unlabeled examples, e.g. by gradient descent\\nminimization of the estimate of the generalization error (10). A more efficient\\napproach to finding the optimal weights is to turn it into a quadratic optimization\\nproblem. That problem is non-trivial only because of the constraints on the weights\\n(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,\\nC af3\\n\\n=\\n\\nf\\n\\ndxp(x)V a (x)V f3 (x) .\\n\\n(13)\\n\\nThen, using that the weights sum to one, equation (10) can be rewritten as\\nE\\n\\n=\\n\\nL\\na\\n\\nwa Ea\\n\\n+ L w a C af3 w f3 - L\\naf3\\n\\nwaCaa .\\n\\n(14)\\n\\na\\n\\nHaving estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation\\nmatrix can be estimated from unlabeled data to any accuracy needed (provided that\\nthe input distribution p is known).\\nIn Figure 2 the results from an experiment with weight optimization are shown.\\nThe dashed curve shows the generalization error when the weights are optimized as\\ndescribed above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the\\nerrors Ea are known exactly, so it shows the lowest possible error. The performance\\nimprovement is quite convincing when the cross-validation estimates are used.\\nIt is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual\\nnetworks do not overfit, one might even use the training errors as estimates for\\nEa (see [3]). It is also possible to use some kind of regularization in (14), if the\\ncross-validation sets are small.\\n\\n5\\n\\nACTIVE LEARNING\\n\\nIn some neural network applications it is very time consuming and/or expensive\\nto acquire training data, e.g., if a complicated measurement is required to find the\\nvalue of the target function for a certain input. Therefore it is desirable to only use\\nexamples with maximal information about the function. Methods where the learner\\npoints out good examples are often called active learning.\\nWe propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by\\ncommittee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those\\npoints we would benefit the most from including in the training set.\\nSince the generalization error is always non-negative, we see from (6) that the\\nweighted average of the individual network errors is always larger than or equal to\\nthe ensemble ambiguity,\\nf(X) 2:: a(x),\\n(15)\\n\\n\\x0cNeural Network Ensembles. Cross Validation. and Active Learning\\n\\n237\\n\\n2.5 r\"\\':\":\\'T---r--\"T\"\"--.-----r---,\\n\\n.\\n\\n.\\n\\n.\\n\\n:\\n\\n0.5\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\nTraining set size\\n\\n40\\n\\n50\\n\\no\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\nTraining set size\\n\\nFigure 3: In both plots the full line shows the average generalization for active\\nlearning, and the dashed line for passive learning as a function of the number of\\ntraining examples. The dots in the left plot show the results of the individual\\nexperiments contributing to the mean for the active learning. The dots in right plot\\nshow the same for passive learning.\\n\\nwhich tells us that the ambiguity is a lower bound for the weighted average of the\\nsquared error. An input pattern that yields a large ambiguity will always have a\\nlarge average error. On the other hand, a low ambiguity does not necessarily imply\\na low error. If the individual networks are trained to a low training error on the\\nsame set of examples then both the error and the ambiguity are low on the training\\npoints. This ensures that a pattern yielding a large ambiguity cannot be in the close\\nneighborhood of a training example. The ambiguity will to some extent follow the\\nfluctuations in the error. Since the ambiguity is calculated from unlabeled examples\\nthe input-space can be scanned for these areas to any detail. These ideas are well\\nillustrated in Figure 1, where the correlation between error and ambiguity is quite\\nstrong, although not perfect.\\nThe results of an experiment with the active learning scheme is shown in Figure 3.\\nAn ensemble of 5 networks was trained to approximate the square-wave function\\nshown in Figure 1, but in this experiments the function was restricted to the interval\\nfrom - 2 to 2. The curves show the final generalization error of the ensemble in a\\npassive (dashed line) and an active learning test (solid line). For each training set\\nsize 2x40 independent tests were made, all starting with the same initial training\\nset of a single example. Examples were generated and added one at a time. In the\\npassive test examples were generated at random, and in the active one each example\\nwas selected as the input that gave the largest ambiguity out of 800 random ones.\\nFigure 3 also shows the distribution of the individual results of the active and\\npassive learning tests. Not only do we obtain significantly better generalization by\\nactive learning, there is also less scatter in the results. It seems to be easier for the\\nensemble to learn from the actively generated set.\\n\\n\\x0c238\\n\\n6\\n\\nAnders Krogh. Jesper Vedelsby\\n\\nCONCLUSION\\n\\nThe central idea in this paper was to show that there is a lot to be gained from\\nusing unlabeled data when training in ensembles. Although we dealt with neural\\nnetworks, all the theory holds for any other type of method used as the individual\\nmembers of the ensemble.\\nIt was shown that apart from getting the individual members of the ensemble to\\ngeneralize well, it is important for generalization that the individuals disagrees as\\nmuch as possible, and we discussed one method to make even identical networks\\ndisagree. This was done by training the individuals on different training sets by\\nholding out some examples for each individual during training. This had the added\\nadvantage that these examples could be used for testing, and thereby one could\\nobtain good estimates of the generalization error.\\nIt was discussed how to find the optimal weights for the individuals of the ensemble.\\nFor our simple test problem the weights found improved the performance of the\\nensemble significantly.\\n\\nFinally a method for active learning was described, which was based on the method\\nof query by committee developed for classification problems. The idea is that if the\\nensemble disagrees strongly on an input, it would be good to find the label for that\\ninput and include it in the training set for the ensemble. It was shown how active\\nlearning improves the learning curve a lot for a simple test problem.\\nAcknowledgements\\n\\nWe would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank\\nLars Kai Hansen for many discussions and great insights, and David Wolpert for\\nvaluable comments.\\n\\nReferences\\n[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.\\n[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.\\n[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method\\nfor neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image\\nprocessing. Chapman-Hall, 1993.\\n[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance\\ndilemma. Neural Computation, 4(1):1-58, Jan. 1992.\\n[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least\\nsquares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.\\n[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of\\nthe Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,\\nCA, 1992. Morgan Kaufmann.\\n[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query\\nby committee. In Advances in Neural Information Processing Systems, volume 5, San\\nMateo, California, 1993. Morgan Kaufmann.\\n\\n\\x0c',\n",
       "     'pdf_name': '1001-neural-network-ensembles-cross-validation-and-active-learning.pdf',\n",
       "     'title': 'Neural Network Ensembles, Cross Validation, and Active Learning',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1004',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1004',\n",
       "     'paper_text': 'ICEG Morphology Classification using an\\nAnalogue VLSI Neural Network\\n\\nRichard Coggins, Marwan Jabri, Barry Flower and Stephen Pickard\\nSystems Engineering and Design Automation Laboratory\\nDepartment of Electrical Engineering J03,\\nUniversity of Sydney, 2006, Australia.\\nEmail: richardc@sedal.su.oz.au\\n\\nAbstract\\nAn analogue VLSI neural network has been designed and tested\\nto perform cardiac morphology classification tasks. Analogue techniques were chosen to meet the strict power and area requirements\\nof an Implantable Cardioverter Defibrillator (ICD) system. The robustness of the neural network architecture reduces the impact of\\nnoise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer percept ron with on chip digital weight\\nstorage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit\\nat the output. The network was trained in loop and included a\\ncommercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better\\nthan 90% true positive and true negative detections for dangerous\\nrhythms which cannot be detected by present ICDs. The chip was\\nimplemented in 1.2um CMOS and consumes less than 200nW maximum average power in an area of 2.2 x 2.2mm2.\\n\\n1\\n\\nINTRODUCTION\\n\\nTo the present time, most ICDs have used timing information from ventricular\\nleads only to classify rhythms which has meant some dangerous rhythms can not\\nbe distinguished from safe ones, limiting the use of the device. Even two lead\\n\\n\\x0c732\\n\\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\n4.00\\n\\nHO\\n3.00\\n\\n2.00\\n\\nI.SO\\n\\n_ _ _:::::::!\\n\\nQ\\n1.00\\n\\nO.SO\\n\\nFigure 1: The Morphology of ST and VT retrograde 1:1.\\n\\natrial/ventricular systems fail to distinguish some rhythms when timing information alone is used [Leong and Jabri, 1992]. A case in point is the separation of Sinus Tachycardia (ST) from Ventricular Tachycardia with 1:1 retrograde conduction.\\nST is a safe arrhythmia which may occur during vigorous exercise and is characterised by a heart rate of approximately 120 beats/minute. VT retrograde 1:1 also\\noccurs at the same low rate but can be a potentially fatal condition. False negative\\ndetections can cause serious heart muscle injury while false positive detections deplete the batteries, cause patient suffering and may lead to costly transplantation\\nof the device. Figure 1 shows however, the way in which the morphology changes\\non the ventricular lead for these rhythms. Note, that the morphology change is\\npredominantly in the \"QRS complex\" where the letters QRS are the conventional\\nlabels for the different points in the conduction cycle during which the heart is\\nactually pumping blood.\\nFor a number of years, researchers have studied template matching schemes in order\\nto try and detect such morphology changes. However, techniques such as correlation\\nwaveform analysis [Lin et. al., 1988], though quite successful are too computationally intensive to meet power requirements. In this paper, we demonstrate that\\nan analogue VLSI neural network can detect such morphology changes while still\\nmeeting the strict power and area requirements of an implantable system. The\\nadvantages of an analogue approach are born out when one considers that an energy efficient analogue to digital converter such as [Kusumoto et. al., 1993] uses\\n1.5nJ per conversion implying 375nW power consumption for analogue to digital\\nconversion of the ICEG alone. Hence, the integration of a bucket brigade device and\\nanalogue neural network provides a very efficient way of interfacing to the analogue\\ndomain. Further, since the network is trained in loop with the ICD in real time,\\nthe effects of device offsets, noise, QRS detection jitter and signal distortion in the\\nanalogue circuits are largely alleviated.\\nThe next section discusses the chip circuit designs. Section 3 describes the method\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\n733\\n\\nAowAcId. . .\\n\\n1axl Syna.... AIRy\\n\\n\"-\\n\\nColumn\\nAoIcIr.-\\n\\nI\\n\\no.ta Reglsl...\\n\\nIClkcMmux\\n\\nI\\n\\nBu1I...\\n\\nI WTAI\\n\\n10 DOD DO\\n\\nFigure 2: Floor Plan and Photomicrograph of the chip\\nused to train the network for the morphology classification task. Section 4 describes\\nthe classifier performance on seven patients with arrhythmia which can not be\\ndistinguished using the heart rate only. Section 5 summarises the results, remaining\\nproblems and future directions for the work .\\n\\n2\\n\\nARCHITECTURE\\n\\nThe neural network chip consists of a 10:6:3 multilayer perceptron, an input bucket\\nbrigade device (BBD) and a winner take all (WTA) circuit at the output. A floor\\nplan and photomicrograph of the chip appears in figure 2. The BBD samples the\\nincoming ICEG at a rate of 250Hz. For three class problems, the winner take all\\ncircuit converts the winning class to a digital signal. For the two class problem\\nconsidered in this paper , a simple thresholding function suffices. The following\\nsubsections briefly describe the functional elements of the chip . The circuit diagrams\\nfor the chip building blocks appear in figure 3.\\n\\n2.1\\n\\nBUCKET BRIGADE DEVICE\\n\\nOne stage of the bucket brigade circuit is shown in figure 3. The BBD uses a\\ntwo phase clock to shift charge from cell to cell and is based on a design by\\nLeong [Leong, 1992] . The BBD operates by transferring charge deficits from S\\nto D in each of the cells. PHIl and PHI2 are two phase non-overlapping clocks.\\nThe cell is buffered from the synapse array to maintain high charge transfer efficiency. A sample and hold facility is provided to store the input on the gates of the\\nsynapses. The BBD clocks are generated off chip and are controlled by the QRS\\ncomplex detector in the lCD.\\n\\n2.2\\n\\nSYNAPSE\\n\\nThis synapse has been used on a number of neural network chips previously.\\ne.g . [Coggins et. al., 1994] . The synapse has five bits plus sign weight storage which\\n\\n\\x0c734\\n\\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\nNEURON\\n\\n.-----------------------------------------------------------,,,\\n,,\\n~ !\\nBUJIOIII\\'\\n\\n00\\n\\nBUCKET BRIGADE ClLL\\n\\n\"\\n\\nFigure 3: Neuron, Bucket Brigade and Synapse Circuit Diagrams.\\nsets the bias to a differential pair which performs the multiplication. The bias references for the weights are derived from a weighted current source in the corner of\\nthe chip. A four quadrant multiplication is achieved by the four switches at the top\\nof the differential pair.\\n\\n2.3\\n\\nNEURON\\n\\nDue to the low power requirements, the bias currents of the synapse arrays are of\\nthe order of hundreds of nano amps, hence the neurons must provide an effective\\nresistance of many mega ohms to feed the next synapse layer while also providing\\ngain control. Without special high resistance polysilicon, simple resistive neurons\\nuse prohibitive area, However, for larger networks with fan-in much greater than\\nten, an additional problem of common mode cancellation is encountered, That is,\\nas the fan-in increases, a larger common mode range is required or a cancellation\\nscheme using common mode feedback is needed.\\nThe neuron of figure 3 implements such a cancellation scheme, The mirrors MO/M2\\nand Ml/M3 divide the input current and facilitate the sum at the drain of M7.\\nM7/M8 mirrors the sum so that it may be split into two equal currents by the\\nmirrors formed by M4, M5 and M6 which are then subtracted from the input\\ncurrents. Thus, the differential voltage vp - Vm is a function of the transistor\\ntransconductances, the common mode input current and the feedback factor , The\\ngain of the neuron can be controlled by varying the width to length ratio of the\\nmirror transistors MO and Ml. The implementation in this case allows seven gain\\ncombinations, using a three bit RAM cell to store the gain,\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\n735\\n\\nImplantable\\nC.cio?erlor\\n\\nDefibrillalOr\\n\\nRunnngMUME\\n\\nNe .....1\\nNelwa\\'1<\\nChip\\n\\nFigure 4: Block Diagram of the Training and Testing System.\\nThe importance of a common mode cancellation scheme for large networks can\\nbe seen when compared to the straight forward approach of resistive or switched\\ncapacitor neurons. This may be illustrated by considering the energy usage of\\nthe two approaches. Firstly, we need to define the required gain of the neuron\\nas a function of its fan-in . If we assume that useful inputs to the network are\\nmostly sparse, i.e. with a small fraction of non-zero values, then the gain is largely\\nindependent of the fan-in, yet the common mode signal increases linearly with fanin. For the case of a neuron which does not cancel the common mode, the power\\nsupply voltage must be increased to accommodate the common mode signal, thus\\nleading to a quadratic increase in energy use with fan-in. A common mode cancelling\\nneuron on the other hand , suffers only a linear increase in energy use with fan-in\\nsince extra voltage range is not required and the increased energy use arises only\\ndue to the linear increase in common mode current.\\n\\n3\\n\\nTRAINING SYSTEM\\n\\nThe system used to train and test the neural network is shown in figure 4. Control\\nof training and testing takes place on the PC. The PC uses a PC-LAB card to\\nprovide analogue and digital I/O . The PC plays the ICEG signal to the input of\\nthe commercial ICD in real time. Note, that the PC is only required for initially\\ntraining the network and in this case as a source of the heart signal. The commercial\\nICD performs the function of QRS complex detection using analogue circuits. The\\nQRS complex detection signal is then used to freeze the BBD clocks of the chip, so\\nthat a classification can take place.\\nWhen training, a number of examples of the arrhythmia to be classified are selected\\nfrom a single patient data base recorded during an electrophysiological study and\\npreviously classified by a cardiologist. Since most of the morphological information\\nis in the QRS complex, only these segments of the data are repeatedly presented to\\n\\n\\x0c736\\n\\nRichard Coggins. Marwan Jabri. Barry Flower. Stephen Pickard\\n\\nPatient\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n% Training Attempts Converged\\nRun ~\\nRun 1\\n\\nH=3\\n80\\n80\\n0\\n60\\n100\\n100\\n80\\n\\nH= 6\\n10\\n100\\n0\\n10\\n80\\n40\\n100\\n\\nH=3\\n60\\n0\\n0\\n40\\n0\\n60\\n40\\n\\nH=6\\n60\\n10\\n10\\n40\\n60\\n60\\n100\\n\\nAverage\\nIterations\\n62\\n86\\n101\\n77\\n44\\n46\\n17\\n\\nTable 1: Training Performance of the system on seven patients.\\nthe network. The weights are adjusted according to the training algorithm running\\non the PC using the analogue outputs of the network to reduce the output error .\\nThe PC writes weights to the chip via the digital I/Os of the PC-LAB card and the\\nserial weight bus of network. The software package implementing the training and\\ntesting, called MUME [Jabri et. al ., 1992], provides a suite of training algorithms\\nand control options. Online training was used due to its success in training small\\nnetworks and because the presentation of the QRS complexes to the network was\\nthe slowest part of the training procedure. The algorithm used for weight updates\\nin this paper was summed weight node perturbation [Flower and Jabri, 1993].\\nThe system was trained on seven different patients separately all of whom had\\nVT with 1: 1 retrograde conduction. Note, that patient independent training has\\nbeen tried but with mixed results [Tinker, 1992] . Table 1 summarises the training\\nstatistics for the seven patients. For each patient and each architecture, five training\\nruns were performed starting from a different random initial weight set. Each\\nof the patients was trained with eight of each class of arrhythmia. The network\\narchitecture used was 10:H:1, where H is the number of hidden layer neurons and\\nthe unused neurons being disabled by setting their input weights to zero. Two sets\\nof data were collected denoted Run 1 and Run 2. Run 1 corresponded to output\\ntarget values of ?0.6V within margin 0.45V and Run 2 to output target values of\\n?0.2V within margin 0.05V. A training attempt was considered to have converged\\nwhen the training set was correctly classified within two hundred training iterations.\\nOnce the morphologies to be distinguished have been learned for a given patient,\\nthe remainder of the patient data base is played back in a continuous stream and\\nthe outputs of the classifier at each QRS complex are logged and may be compared\\nto the classifications of a cardiologist. The resulting generalisation performance is\\ndiscussed in the next section.\\n\\n4\\n\\nMORPHOLOGY CLASSIFIER GENERALISATION\\nPERFORMANCE\\n\\nTable 2 summarises the generalisation performance of the system on the seven\\npatients for the training attempts which converged. Most of the patients show a\\ncorrect classification rate better than 90% for at least one architecture on one of the\\n\\n\\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\\n\\nPatient\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\nNo. of\\nComplexes\\nST\\nVT\\n440\\n61\\n57\\n94\\n67\\n146\\n166\\n65\\n61\\n96\\n61\\n99\\n28\\n80\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n\\n440\\n94\\n67\\n166\\n61\\n61\\n28\\n\\n61\\n57\\n146\\n65\\n96\\n99\\n80\\n\\n737\\n\\n% Correct Classifications Run 1\\nH = 6\\nH - i3\\nVT\\nST\\nST\\nVT\\n89?10 89?3\\n58?0\\n99?0\\n99?1\\n99?1\\n100?0 99?1\\n66?44 76?37\\n99?1\\n50?3\\n82?1 75?13\\n89?9\\n94?6\\n84?8\\n97?1\\n90?5\\n99?1\\n97?3\\n98?5\\n99?1\\n99?1\\n% Correct Classifications Run 2\\n86?14 99?1\\n88?2\\n99?1\\n94?6\\n94?3\\n84?2\\n99?1\\n76?18 59?2\\n87?7 100?0\\n88?2\\n49?5\\n84?1\\n82?5\\n92?6 90?10\\n99?1\\n99?1\\n94?3\\n99?0\\n94?3\\n92?3\\n\\nTable 2: Generalisation Performance of the system on seven patients.\\nruns, whereas, a timing based classifier can not separate these arrhythmia at all.\\nFor each convergent weight set the network classified the test set five times. Thus,\\nthe \"% Correct\" columns denote the mean and standard deviation of the classifier\\nperformance with respect to both training and testing variations. By duty cycling\\nthe bias to the network and buffers, the chip dissipates less than 200n W power for\\na nominal heart rate of 120 beats/minute during generalisation.\\n\\n5\\n\\nDISCUSSION\\n\\nReferring to table 1 we see that the patient 3 data was relatively difficult to train.\\nHowever, for the one occasion when training converged generalisation performance\\nwas quite acceptable. Inspection of this patients data showed that typically, the\\nmorphologies of the two rhythms were very similar. The choice of output targets,\\nmargins and architecture appear to be patient dependent and possibly interacting\\nfactors. Although larger margins make training easier for some patients they appear\\nto also introduce more variability in generalisation performance. This may be due\\nto the non-linearity of the neuron circuit. Further experiments are required to\\noptimise the architecture for a given patient and to clarify the effect of varying\\ntargets, margins and neuron gain. Penalty terms could also be added to the error\\nfunction to minimise the possibility of missed detections of the dangerous rhythm.\\nThe relatively slow rate of the heart results in the best power consumption being\\nobtained by duty cycling the bias currents to the synapses and the buffers. Hence,\\nthe bias settling time of the weighted current source is the limiting factor for reducing power consumption further for this design. By modifying the connection of the\\ncurrent source to the synapses using a bypassing technique to reduce transients in\\n\\n\\x0cRiclulrd Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\\n\\n738\\n\\nthe weighted currents, still lower power consumption could be achieved.\\n\\n6\\n\\nCONCLUSION\\n\\nThe successful classification of a difficult cardiac arrhythmia problem has been\\ndemonstrated using. an analogue VLSI neural network approach. Furthermore, the\\nchip developed has shown very low power consumption of less than 200n W, meeting the requirements of an implantable system. The chip has performed well, with\\nover 90% classification performance for most patients studied and has proved to be\\nrobust when the real world influence of analogue QRS detection jitter is introduced\\nby a commercial implantable cardioverter defibrillator placed in the signal path to\\nthe classifier.\\nAcknowledgements\\n\\nThe authors acknowledge the funding for the work in this paper provided under\\nAustralian Generic Technology Grant Agreement No. 16029 and thank Dr. Phillip\\nLeong of the University of Sydney and Dr. Peter Nickolls of Telectronics Pacing\\nSystems Ltd., Australia for their helpful suggestions and advice.\\nReferences\\n\\n[Castro et. al., 1993] H.A. Castro, S.M. Tam, M.A. Holler, \"Implementation and\\nPerformance of an analogue Nonvolatile Neural Network,\" Analogue Integrated\\nCircuits and Signal Processing, vol. 4(2), pp. 97-113, September 1993.\\n[Lin et. al., 1988] D. Lin, L.A. Dicarlo, and J .M. Jenkins, \"Identification of Ventricular Tachycardia using Intracavitary Electrograms: analysis of time and frequency domain patterns,\" Pacing (3 Clinical Electrophysiology, pp. 1592-1606,\\nNovember 1988.\\n[Leong, 1992] P.H.W. Leong, Arrhythmia Classification Using Low Power VLSI,\\nPhD Thesis, University of Sydney, Appendix B, 1992.\\n[ Kusumoto et. al., 1993] K. Kusumoto et. al., \"A lObit 20Mhz 30mW Pipelined\\nInterpolating ADC,\" ISSCC, Digest of Technical Papers, pp. 62-63, 1993.\\n[Leong and Jabri, 1992] P.H.W. Leong and M. Jabri, \"MATIC - An Intracardiac Tachycardia Classification System\", Pacing (3 Clinical Electrophysiology,\\nSeptember 1992.\\n[Coggins et. al., 1994] R.J. Coggins and M.A. Jabri, \"WATTLE: A Trainable Gain\\nAnalogue VLSI Neural Network\", NIPS6, Morgan Kauffmann Publishers, 1994.\\n[Jabri et. al., 1992] M.A. Jabri, E.A. Tinker and L. Leerink, \"MUME- A MultiNet-Multi-Architecture Neural Simulation Environment\", Neural Network Simulation Environments, Kluwer Academic Publications, January, 1994.\\n[Flower and Jabri, 1993] B. Flower and M. Jabri, \"Summed Weight Neuron Perturbation: an O(N) improvement over Weight Perturbation,\" NIPS5, Morgan\\nKauffmann Publishers, pp. 212-219, 1993.\\n[Tinker, 1992] E.A. Tinker, \"The SPASM Algorithm for Ventricular Lead Timing and Morphology Classification,\" SEDAL ICEG-RPT-016-92, Department of\\nElectrical Engineering, University of Sydney, 1992.\\n\\n\\x0c',\n",
       "     'pdf_name': '1004-iceg-morphology-classification-using-an-analogue-vlsi-neural-network.pdf',\n",
       "     'title': 'ICEG Morphology Classification using an Analogue VLSI Neural Network',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1006',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1006',\n",
       "     'paper_text': 'Real-Time Control of a Tokamak Plasma\\nUsing Neural Networks\\n\\nChris M Bishop\\nNeural Computing Research Group\\nDepartment of Computer Science\\nAston University\\nBirmingham, B4 7ET, U.K.\\nc.m .bishop@aston .ac .uk\\n\\nPaul S Haynes, Mike E U Smith, Tom N Todd,\\nDavid L Trotman and Colin G Windsor\\nAEA Technology, Culham Laboratory,\\nOxfordshire OX14 3DB\\n(Euratom/UKAEA Fusion Association)\\n\\nAbstract\\nThis paper presents results from the first use of neural networks\\nfor the real-time feedback control of high temperature plasmas in\\na tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen\\nplasmas, at temperatures of up to 100 Million K, are confined\\nby strong magnetic fields. Accurate control of the position and\\nshape of the plasma boundary requires real-time feedback control\\nof the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural\\nnetwork approach can give significantly better performance than\\nthe linear technique currently used on most tokamak experiments.\\nThe practical application of the neural network approach requires\\nhigh-speed hardware, for which a fully parallel implementation of\\nthe multilayer perceptron, using a hybrid of digital and analogue\\ntechnology, has been developed.\\n\\n\\x0c1008\\n\\n1\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nINTRODUCTION\\n\\nFusion of the nuclei of hydrogen provides the energy source which powers the sun.\\n\\nIt also offers the possibility of a practically limitless terrestrial source of energy.\\nHowever, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a\\nhigh temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the\\nRussian for \\'toroidal magnetic chamber\\') as illustrated schematically in Figure 1.\\nAt these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks\\nhad plasmas with circular cross-sections, for which feedback control of the plasma\\nposition and shape is relatively straightforward. However, recent tokamaks, such as\\nthe COMPASS experiment at Culham Laboratory, as well as most next-generation\\ntokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy\\nconfinement properties and thereby significantly enhance the performance of the\\ntokamak.\\n\\nz\\n\\nR\\n\\nFigure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma\\n(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded\\nas axisymmetric about the Z-axis, and so the plasma boundary can\\nbe described by its cross-sectional shape at one particular toroidal\\nlocation.\\nUnlike circular cross-section plasmas, highly non-circular shapes are more difficult to\\nproduce and to control accurately, since currents through several control coils must\\nbe adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape\\nmust evolve, usually from some initial near-circular shape. Due to uncertainties\\nin the current and pressure distributions within the plasma, the desired accuracy\\nfor plasma control can only be achieved by making real-time measurements of the\\nposition and shape of the boundary, and using error feedback to adjust the currents\\nin the control coils.\\nThe physics of the plasma equilibrium is determined by force balance between the\\n\\n\\x0c1009\\n\\nReal-Time Control of Tokamak Plasma Using Neural Networks\\n\\ncircle\\n\\nellipse\\n\\nO-shape\\n\\nbean\\n\\nFigure 2: Cross-sections of the COMPASS vacuum vessel showing\\nsome examples of potential plasma shapes. The solid curve is the\\nboundary of the vacuum vessel, and the plasma is shown by the\\nshaded regions.\\n\\nthermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms\\nof solutions of a non-linear partial differential equation called the Grad-Shafranov\\n(GS) equation. Due to the non-linear nature of this equation, a general analytic\\nsolution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the\\nexternal control coils which surround the vacuum vessel. On the tokamak itself it\\nis changes in these currents which are used to alter the position and cross-sectional\\nshape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used\\nto generate the training dataset for the neural network, as described in the next\\nsection. However , this approach is computationally very intensive and is therefore\\nunsuitable for feedback control purposes.\\nFor real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a\\nvariety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.\\nMost tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these\\nmagnetic signals collectively as a vector m .\\nFor a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,\\ngiven by\\n\\nR(B)\\nZ(B)\\n\\nRo + a cos(B + 8 sinB)\\nZo + a/\\\\,sinB\\n\\nwhere we have defined the following parameters\\n\\n(1)\\n\\n\\x0c1010\\n\\nRo\\nZo\\na\\nK\\n\\n6\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nradial distance of the plasma center from the major axis of the torus,\\nvertical distance of the plasma center from the torus midplane,\\nminor radius measured in the plane Z = Zo,\\nelongation,\\ntriangularity.\\n\\nWe denote these parameters collectively by Yk. The basic problem which has to be\\naddressed, therefore, is to find a representation for the (non-linear) mapping from\\nthe magnetic signals m to the values of the geometrical parameters Yk, which can\\nbe implemented in suitable hardware for real-time control.\\nThe conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical\\nparameters by a single linear transformation. However, the intrinsic non-linearity\\nof the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;\\nBishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the\\ncontrol loop for the neural network approach to tokamak equilibrium control.\\nNeural\\n\\nNetwork\\n\\nFigure 3: Block diagram of the control loop used for real-time\\nfeedback control of plasma position and shape.\\n\\n2\\n\\nSOFTWARE SIMULATION RESULTS\\n\\nThe dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base\\ncurrently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes\\nseveral minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled\\nwith the appropriate values of the shape parameters. Of the 120 magnetic signals\\navailable on COMPASS which could be used to provide inputs to the network, a\\n\\n\\x0c1011\\n\\nReal-Time Control o/Tokamak PLasma Using Neural Networks\\n\\nsubset of 16 has been chosen using sequential forward selection based on a linear\\nrepresentation for the mapping (discussed below) .\\nIt is important to note that the transformation from magnetic signals to flux surface\\nparameters involves an exact linear invariance. This follows from the fact that, if all\\nof the currents are scaled by a constant factor, then the magnetic fields will be scaled\\nby this factor, and the geometry of the plasma boundary will be unchanged . It is\\nimportant to take advantage of this prior knowledge and to build it into the network\\nstructure, rather than force the network to learn it by example. We therefore\\nnormalize the vector m of input signals to the network by dividing by a quantity\\nproportional to the total plasma current. Note that this normalization has to be\\nincorporated into the hardware implementation of the network, as will be discussed\\nin Section 3.\\n1.2\\n\\n4\\n01\\n\\n2\\n\\n2\\n\\n01\\n\\nc\\n\\nc\\n.5.\\n\\n0-\\n\\n~\\n:E\\n\\n.5.\\nCIS\\n\\n:E\\n\\n1iI\\n\\n~\\n::J\\n\\n?\\n\\n1iI\\nCD\\n\\n-2\\n\\ngo.8\\n\\n.5.\\n0-\\n\\n?\\n\\nCIS\\n\\n:E\\n\\n1iI 0 .4\\nCD\\n\\nc\\n::J\\n\\nc\\n\\n::J\\n\\n-2\\n\\n-4\\nDatabase\\n\\n?\\nDatabase\\n\\n1.2\\n\\n4\\n~\\n\\n~CD\\n\\nZ\\n\\n~\\n:::I\\n\\nCD\\n\\nz\\n\\n.2\\nDatabase\\n\\n2\\n\\n~O.8\\n~\\n\\n?\\n\\nCD\\n\\nz\\n\\n~O.4\\n\\n-2\\n\\n:::I\\n\\nCD\\n\\nZ\\n\\n?\\n\\n-4\\nDatabase\\n\\nDatabase\\n\\n.2\\nDatabase\\n\\nFigure 4: Plots of the values from the test set versus the values\\npredicted by the linear mapping for the 3 equilibrium parameters,\\ntogether with the corresponding plots for a neural network with 4\\nhidden units.\\n\\nThe results presented in this paper are based on a multilayer perceptron architecture\\nhaving a single layer of hidden units with \\'tanh\\' activation functions , and linear\\noutput units. Networks are trained by minimization of a sum-of-squares error using\\na standard conjugate gradients optimization algorithm, and the number of hidden\\n\\n\\x0cJ012\\n\\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\nunits is optimized by measuring performance with respect to an independent test\\nset. Results from the neural network mapping are compared with those from the\\noptimal linear mapping, that is the single linear transformation which minimizes\\nthe same sum-of-squares error as is used in the neural network training algorithm,\\nas this represents the method currently used on a number of present day tokamaks .\\nInitial results were obtained on networks having 3 output units, corresponding to\\nthe values of vertical position ZQ, major radius RQ, and elongation K; these being\\nparameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.\\nBy comparison, the optimal linear mapping gave a normalized test set error of 18.3.\\nThis represents a reduction in error of about 30% in going from the linear mapping\\nto the neural network. Such an improvement, in the context of this application , is\\nvery significant.\\nFor the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we\\nconsider the results from this network in more detail. Figure 4 shows plots of the\\nnetwork predictions for various parameters versus the corresponding values from\\nthe test set portion of the database. Analogous plots for the optimal linear map\\npredictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,\\neven for this sub-optimal network topology.\\n\\n3\\n\\nHARDWARE IMPLEMENTATION\\n\\nThe hardware implementation of the neural network must have a bandwidth of 2:\\n20 kHz in order to cope with the fast timescales of the plasma evolution. It must\\nalso have an output precision of at least (the the analogue equivalent of) 8 bits in\\norder to ensure that the final accuracy which is attainable will not be limited by the\\nhardware system. We have chosen to develop a fully parallel custom implementation\\nof the multilayer perceptron, based on analogue signal paths with digitally stored\\nsynaptic weights (Bishop et al., 1993). A VME-based modular construction has\\nbeen chosen as this allows flexibility in changing the network architecture, ease of\\nloading network weights, and simplicity of data acquisition. Three separate types\\nof card have been developed as follows:\\n? Combined 16-input buffer and signal normalizer.\\nThis provides an analogue hardware implementation of the input normalization described earlier.\\n? 16 x 4 matrix multiplier\\nThe synaptic weights are produced using 12 bit frequency-compensated\\nmultiplying DACs (digital to analogue converters) which can be configured\\nto allow 4-quadrant multiplication of analogue signals by a digitally stored\\nnumber.\\n? 4-channel sigmoid module\\nThere are many ways to produce a sigmoidal non-linearity, and we have\\nopted for a solution using two transistors configured as along-tailed-pair,\\n\\n\\x0cReal-Time Control of Tokamak Plasma Using Neural Networks\\n\\n1013\\n\\nto generate a \\'tanh \\' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the\\nappearance of temperature in the denominator of the exponential transistor\\ntransfer characteristic. An elegant solution to this problem has been found\\nby exploiting a chip containing 5 transistors in close thermal contact. Two\\nof the transistors form the long-tailed pair, one of the transistors is used\\nas a heat source, and the remaining two transistors are used to measure\\ntemperature. External circuitry provides active thermal feedback control,\\nand stability to changes in ambient temperature over the range O?C to 50?C\\nis found to be well within the acceptable range.\\nThe complete network is constructed by mounting the appropriate combination\\nof cards in a VME rack and configuring the network topology using front panel\\ninterconnections. The system includes extensive diagnostics, allowing voltages at\\nall key points within the network to be monitored as a function of time via a series\\nof multiplexed output channels.\\n\\n4\\n\\nRESULTS FROM REAL-TIME FEEDBACK CONTROL\\n\\nFigure 5 shows the first results obtained from real-time control of the plasma in\\nthe COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time\\nduring a plasma pulse. Here the desired elongation has been preprogrammed to\\nfollow a series of steps as a function of time. The remaining 2 network outputs\\n(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,\\nbut were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the\\npost-shot reconstruction of the elongation obtained from a simple \\'filament\\' code,\\nwhich gives relatively rapid post-shot plasma shape reconstruction but with limited\\naccuracy. The circles denote the elongation values given by the much more accurate\\nreconstructions obtained from the full equilibrium code. The graph clearly shows\\nthe network generating the required elongation signal in close agreement with the\\nreconstructed values. The typical residual error is of order 0.07 on elongation values\\nup to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is\\ncurrently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units\\navailable with the initial hardware configuration. While these results represent the\\nfirst obtained using closed loop control, it is clear from earlier software modelling of\\nlarger network architectures (such as 32- 16-4) that residual errors of order a few %\\nshould be attainable. The implementation of such larger networks is being persued,\\nfollowing the successes with the smaller system.\\nAcknowledgements\\nWe would like to thank Peter Cox, Jo Lister and Colin Roach for many useful\\ndiscussions and technical contributions. This work was partially supported by the\\nUK Department of Trade and Industry.\\n\\n\\x0cC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\\n\\n1014\\n\\n1.8\\nshot 9576\\n\\nc:\\n\\no\\n\\n~\\n14\\nC)\\n?\\nc:\\n\\no\\n\\nas\\n1.0\\n0.0\\n\\n0.1\\n\\n0.2\\n\\ntime (sec.)\\nFigure 5: Plot of the plasma elongation K. as a function of time\\nduring shot no. 9576 on the COMPASS tokamak, during which the\\nelongation was being controlled in real-time by the neural network.\\n\\nReferences\\n\\nBishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman\\nD L, 1992. A neural network approach to tokamak equilibrium control. In Neural\\nNetwork Applications, Ed. J G Taylor, Springer Verlag, 114-128.\\nBishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.\\n1993. Hardware implementation of a neural network for plasma position control in\\nCOMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,\\nItaly. 2 997-1001.\\nLagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi\\nM, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time\\ncalculations of plasma equilibrium parameters for PBX-M, In Proceedings of the\\n17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.\\nLister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma\\nparameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.\\n\\n\\x0cPulsestream Synapses with Non-Volatile\\nAnalogue Amorphous-Silicon Memories.\\n\\nA.J. Holmes, A.F. Murray, S. Churcher and J. Hajto\\nDepartment of Electrical Engineering\\nUniversity of Edinburgh\\nEdinburgh, EH9 3JL\\nM. J. Rose\\nDept. of Applied Physics and Electronics,\\nDundee University\\nDundee DD14HN\\n\\nAbstract\\nA novel two-terminal device, consisting of a thin lOooA layer of p+\\na-Si:H sandwiched between Vanadium and Chromium electrodes,\\nexhibits a non-volatile, analogue memory action. This device stores\\nsynaptic weights in an ANN chip, replacing the capacitor previously\\nused for dynamic weight storage. Two different synapse designs are\\ndiscussed and results are presented.\\n\\n1\\n\\nINTRODUCTION\\n\\nAnalogue hardware implementations of neural networks have hitherto been hampered by the lack of a straightforward (local) analogue memory capability. The\\nideal storage mechanism would be compact, non-volatile, easily reprogrammable,\\nand would not interfere with the normal silicon chip fabrication process.\\nTechniques which have been used to date include resistors (these are not generally\\nreprogrammable, and suffer from being large and difficult to fabricate with any accuracy), dynamic capacitive storage [4] (this is compact, reprogrammable and simple,\\nbut implies an increase in system complexity, arising from off-chip refresh circuitry),\\n\\n\\x0c764\\n\\nA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\nEEPROM (\"floating gate\") memory [5] (which is compact, reprogrammable, and\\nnon-volatile, but is slow, and cannot be reprogrammed in situ), and local digital\\nstorage (which is non-volatile, easily programmable and simple, but consumes area\\nhorribly).\\nAmorphous silicon has been used for synaptic weight storage [1, 2], but only as\\neither a high-resistance fixed weight medium or a binary memory.\\nIn this paper, we demonstrate that novel amorphous silicon memory devices can be\\nincorporated into standard CMOS synapse circuits, to provide an analogue weight\\nstorage mechanism which is compact, non-volatile, easily reprogrammable, and simple to implement.\\n\\n2\\n\\na-Si:H MEMORY DEVICES\\n\\nThe a-Si:H analogue memory device [3] comprises a lOooA thick layer of amorphous\\nsilicon (p+ a-Si:H) sandwiched between Vanadium and Chromium electrodes.\\nThe a-Si device takes the form of a two-terminal, programmable resistor. It is an\\n\"add-on\" to a conventional CMOS process, and does not demand that the normal\\nCMOS fabrication cycle be disrupted. The a-Si device sits on top of the completed\\nchip circuitry, making contact with the CMOS arithmetic elements via holes cut in\\nthe protective passivation layer, as shown in Figure 1.\\n\\nCMOS Passivation\\nFigure 1: The construction of a-Si:H Devices on a CMOS chip\\nAfter fabrication a number of electronic procedures must be performed in order to\\nprogram the device to a given resistance state.\\nProgramming, and Pre-Programming Procedures\\n\\nBefore the a-Si device is usable, the following steps must be carried out:\\n? Forming: This is a once-only process, applied to the a-Si device in its\\n\"virgin\" state, where it has a resistance of several MO. A series of 300ns\\npulses, increasing in amplitude from 5v to 14v, is applied to the device\\nelectrodes. This creates a vertical conducting channel or filament whose\\napproximate resistance is 1KO. This filament can then be programmed to\\na value in the range lKO to 1 MO . The details of the physical mechanisms\\nare not yet fully established, but it is clear that conduction occurs through\\na narrow (sub-micron) conducting channel.\\n\\n\\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\\n\\n765\\n\\n? Write: To decrease the device\\'s resistance, negative \"Write\", pulses are\\napplied.\\n? Erase: To increase the device\\'s resistance, positive\" Erase\" , pulses are applied.\\n? Usage: Pulses below O.5v do not change the device resistance. The resistance can therefore be utilised as a weight storage medium using a voltage\\nof less than O.5v without causing reprogramming.\\nProgramming pulses, which range between 2v and 5v, are typically 120ns in duration. Programming is therefore much faster than for other EEPROM (floating\\ngate) devices used in the same context, which use a series of 100jls pulses to set the\\nthreshold voltage [5].\\nThe following sections describe synapse circuits using the a-Si:H devices. These\\nsynapses use the reprogrammable a-Si:H resistor in the place of a storage capacitor\\nor EEPROM cell. These new synapses were implemented on a chip referred to as\\nASiTEST2, consisting of five main test blocks, each comprising of four synapses\\nconnected to a single neuron.\\n\\n3\\n\\nThe EPSILON based synapse\\n\\nThe first synapse to be designed used the a-Si:H resistor as a direct replacement for\\nthe storage capacitor used in the EPSILON [4] synapse.\\n\\n+Sv\\n\\nNeuron\\n\\n1\\nI\\n\\nV\\n\\n..\\n\\nt.\\n\\n~:l:\\n\\n><!:\\n\\n~\\n\\nMirror Set\\n\\nE\\n\\n30\\n\\na-Si => Vw\\n\\nCircuitry\\n\\nOriginal\\nStorage\\nCapacitor\\n\\nO.5v\\n\\n<0-----------.,...\\n__\\n\\n...\\n\\nEPSILON Synapse\\n\\nFigure 2: The EPSILON Synapse with a-Si:H weight storage\\n\\nIn the original EPSILON chip the weight voltage was stored as a voltage on a\\ncapacitor. In this new synapse design, shown in Figure 2, the a-Si:H resistance is\\nset such that the voltage drop produced by Iset is equivalent to the original weight\\nvoltage, Vw, that was stored dynamically on the capacitor.\\nA new, simpler, synapse, which can be operated from a single +5v supply, was also\\nbe included on the ASiTEST2 chip.\\n\\n\\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\n766\\n\\n4\\n\\nThe MkII synapse\\n\\nThe circuit is shown in Figure 3. The a-Si:H memory is used to store a current,\\nIasi. This current is subtracted from a zero current, Isy...:z\" to give a weight current\\n, +/-Iw, which adds or subtracts charge from the activity capacitor, Cact, thus\\nimplementing excitation or inhibition respectively.\\nFor the circuit to function correctly we must limit the voltage on the activity capacitor to the range [1.5v,3.5v], to ensure that the transistors mirroring Isy_z and\\nIasi remain in saturation. As Figure 3 shows, there are few reference signals and\\nthe circuit operates from a single +5v power supply rail, in sharp contrast to many\\nearlier analogue neural circuits, including our own.\\n\\n1v\"\"\\n\\n+5vPWm\\n\\nII .\\n\\n~\\n\\n881\\n\\nVsel\\n\\n.r--\\\\.\\n\\n-.L\\n\\n....L\\n\\n*\\n\\nComparator\\nPWout\\n..rL\\n\\nCact\\n\\nVramp\\n\\n~\\n\\nOv\\nE\\n;.\\nMirror Set\\n\\n\"\\'E~-----------:~~\\n\\nE\\n\\nSynapse\\n\\nPower Supplies\\nV5_0=5.Ov\\n\\nReferences\\nVrstv?2.5v\\nIsy_z=5uA\\n\\n;.\\n\\nNeuron\\n\\nTail Currents\\n\\nIneu=4uA\\n\\nFigure 3: The MkII synapse\\n\\nOn first inspection the main drawback of this design would appear to be a reliance\\non the accuracy with which the zero current Isy...:z, is mirrored across an entire chip.\\nThe variation in this current means that two cells with the same synapse resistance\\ncould produce widely differing values of Iw. However, during programming we\\ndo not use the resistance of the a-Si:H device as a target value. We monitor the\\nvoltage on Cact for a given PWin signal, increasing or decreasing the resistance\\nof the a-Si:H device until the desired voltage level is achieved.\\nExample: To set a weight to be the maximum positive value, we adjust the a-Si\\nresistance until a PWin signal of 5us, the maximum input signal, gives a voltage of\\n3.5v on the integration capacitor.\\nWe are able to set the synapse weight using the whole integration range of [1.5v,3.5v]\\nby only closing Vsel for the desired synapse during programming. In normal operating mode all four Vsel switches will be closed so that the integration charge is\\nsummed over all four local capacitors.\\n\\n\\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\\n\\n4.1\\n\\n767\\n\\nExample - Stability Test\\n\\nAs an example of the use of integration voltage as means of monitoring the resistance\\nof a particular synapse we have included a stability test. This was carried out on\\none of the test chips which contained the MkII synapse.\\nThe four synapses on the test chip were programmed to give different levels of\\nactivation. The chip was then powered up for 30mins each day during a 7-day\\nperiod, and the activation levels for each synapse were measured three times.\\n3.5\\n\\nStability Test - PWin = 3us\\n\\n,----~-_._--;--__..___r...:....,.-_._,.__-,.--.,..,.-__..__:___.,\\n\\ntestl\\n\\ntest2\\n\\ntest4\\n\\ntest3\\n\\ntestS\\n\\ntest7\\n\\ntest6\\n\\n3\\n\\n?\\n,.\\nI\\n\\nt:\\'\"\\n-~\\n\\n1:1\\n?\\n\\n\\'t\\n\\n,\\n\\n?\\n\\n~\\n\\n?\\n\\n- ~ -:- - ~ -:- -\\n\\n25\\n?\\n\\n2\\n\\n?\\n\\n~.\\n\\n?\\n\\n-:- - - . of - ~-:- -\\n\\n..\\n\\n.\\n\\nI\\n\\nI\\n\\n?\\n\\n?\\n\\n{,o-:- - .\\n- ~-s4\\n.\\n\\n,\\n.\\n.\\n\\'.\\n.?\\n.?\\n.?\\n.?\\n- - ~ - - ~ - - --:- - - ~ - - -~- - - w. -- --r s2\\n- - - .;. \\'\" - -: -011>- - :.. ~ - ~ -oGii - -:- - - - i-- ~ - ..; -sl\\n?\\n.\\n.\\n.\\n.\\n.\\n?\\n\\n?\\n\\n~ - - ~ - - -~- - - ~ - ?\\n\\nI\\n\\n?\\n\\nI\\nI\\n\\n,\\nI\\n\\n?\\n?\\n\\n?\\n\\n,\\n\\n?\\n?\\nI\\n\\nI\\n?\\n?\\n\\n10\\n\\n20\\n\\n30\\n\\n-.\\nL---L- -- -~\\n\\n~ -s3\\n\\n,\\n\\n?\\n\\n,\\n,\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n,\\n\\nI\\n\\n,\\n\\n?\\n\\n?\\nI\\n:\\n\\nI\\n?\\n:\\n\\n?\\nI\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n\\n40\\n\\n50\\n\\n60\\n\\n,\\n\\n70\\n\\n80\\n\\n?\\n\\n90\\n\\nMeasurement Index\\n\\nFigure 4: ASiTEST2- Stability Test\\nAs figure 4 shows, the memories remain in the same resistance state (i.e retain their\\nprogrammed weight value) over the whole 7-day period. Separate experiments on\\nisolated devices indicate much longer hold times - of the order of months at least.\\n\\n5\\n\\nASiTEST3\\n\\nRecently we have received our latest, overtly neural, a-Si:H based test chip. This\\ncontains an 8x8 array of the MkII synapses.\\nThe circuit board for this device has been constructed and partially tested while\\nthe ASiTEST3 chips are awaiting the deposition of the a-Si:H layers. We have been\\nable to use an ASiTEST2 chip containing two of the MkII synapse test blocks i.e.\\n8 synapses and 2 neurons to exercise much of the board\\'s functionality.\\nThe test board contains a simple state machine which has four different states:\\n? State 0: Load Input Pulsewidths into SRAM from PC.\\n? State 1: Apply Input Pulsewidth signals to chipl.\\n? State 2: Use Vramp to generate threshold function for chipl. The resulting\\nPulsewidth outputs are used as the inputs to chip2, as well as being stored\\n\\n\\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\\n\\n768\\n\\nin SRAM .\\n? State 3: Use Vramp to generate threshold function for chip2. Read resulting\\nPulsewidth Outputs into SRAM .\\n? State 0: Read Output Pulsewidths from SRAM into PC.\\nThe results obtained during a typical test cycle are shown in Figure 5.\\nIE-- Statel --;,,;,,*1E E - - State2\\n\\n-----;>~i~\\n\\nState3 ---;!>~I\\n\\n~r-IF~~~--r-------~---------l\\n4v\\n3v\\n\\nPWin_O\\n\\n2v\\nIv\\n\\nOv~ . . . . . . . . .\\n\\n3.5v r;;;;;\"\"-~,\"\",,,or;:::::;;:;::;;;;:;;;::;;t---~----t--------,\\n\\n~:~\\n\\n....... ~;a~;\"\"\"\\'\"\\n\\n2.Ov\\n\\n~\\'Sig~;,id\"\"\"\"\"\\n... ~\"Li~\"\"\"\"\\n......\\n\\n. . . . . . . . . . . . . . . . . . . . . . . ,.\\n\\n....... .\\n\\n.... .....\\n\\n..........\\n\\n.. ..\\n\\nl.~\\ne?_e\\n\\n_____ ...... ___\\n\\n?\\n\\n_____\\n\\n?\\n\\n____________ . . . . . . . .\\n\\n_____\\n\\n????\\n\\n____\\n\\n..........\\n\\n.\\n\\n5v\\n4v\\n3v\\n\\n2v\\n\\n:;; ~,...,................-..--t;?.~...:...~__--! ..........~~.n-.-~~~~~~......J\\nIS.\\n\\n10.\\n\\nFigure 5: ASiTEST3 Board Scope Waveforms\\nAs this figure shows different ramp signals, corresponding to different threshold\\nfunctions, can be applied to chipl and chip2 neurons.\\n10.0\\n\\nSingle Buffer PulscWidth Sweeps\\n\\n.----.,..----r------.----r----.------,\\n\\n9.0\\n8.0\\n\\n!\\n\\n7.0 -\\n\\n~\\n\\n5\\'O~~~~~~~-~~-+++~~~~N~~~\\n\\n~\\n\\ni6.o~\\n\\n~----\\n\\nJ::\\n2D\\n\\nNeal-Syal\\nN~~JIII\\n\\n1.0\\n\\nN~~ya2\\n\\no.oL---~--~- --~--?\\n\\no\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n2.5\\n\\n3.0\\n\\nPulscwldlh Input [WI)\\n\\nFigure 6: ASiTEST3 Board - MkII Synapse Characteristic\\nWhile the signals shown in Figure 5 appear noisy the multiplier characteristic that\\nthe chip produces is still admirably linear, as shown in Figure 6. In this experiment\\nall eight synapses on a test chip were programmed into different resistance states\\nand PWin was swept from 0 to 3us.\\n\\n\\x0cPulsestream Synapses with Non- Volatile Analogue Amorphous-Silicon Memories\\n\\n6\\n\\n769\\n\\nConclusions\\n\\nWe have demonstrated the use of novel a-Si:H analogue memory devices as a means\\nof storing synaptic weights in a Pulsewidth ANN. We have also demonstrated the\\noperation of an interface board which allows two 8x8 ANN chips, operating as a\\ntwo layer network, to be controlled by a simple PC interface card.\\nThis technology is most suitable for small networks in, for example, remote control and other embedded-system applications where cost and power considerations\\nfavour a single all-inclusive ANN chip with non-volatile, but programmable weights.\\nAnother possible application of this technology is in large networks constructed\\nusing Thin Film Technology(TFT). If TFT\\'s were used in place of the CMOS transistors then the area constraint imposed by crystalline silicon would be removed,\\nallowing truly massively parallel networks to be integrated.\\nIn summary - the a-Si:H analogue memory devices described in this paper provide a\\nroute to an analogue, non-volatile and fast synaptic weight storage medium. At the\\npresent time neither the programming nor storage mechanisms are fully understood\\nmaking it difficult to compare this new device with more established technologies\\nsuch as the ubiquitous Floating-Gate EEPROM technique. Current research is\\nfocused on firstly, improving the yield on the a-Si:H device which is unacceptably\\nlow at present, a demerit that we attribute to imperfections in the a-Si fabrication\\nprocess and secondly, improving understanding of the device physics and hence the\\nprogramming and storage mechanisms.\\nAcknowledgements\\nThis research has been jointly funded by BT, and EPSRC (formerly SERC), the\\nEngineering and Physical Sciences Research Council.\\n\\nReferences\\n[1] W. Hubbard et al.(1986) Electronic Neural Networks AlP Conference Proceedings - Snowbird 1986 :227-234\\n[2] H.P. Graf (1986) VLSI Implementation of a NN memory with several hundreds\\nof neurons AlP Conference Proceedings - Snowbird 1986 :182-187.\\n[3] M.J. Rose et al (1989) Amorphous Silicon Analogue Memory Devices Journal\\nof Non-Crystalline Solids 1(115):168-170\\n[4] A.Hamilton et al. (1992) Integrated Pulse-Stream Neural Networks - Results,\\nIssues and Pointers IEEE Transactions on N.N.s 3(3):385-393\\n[5] M.Holler, S.Tam, H.Castro and R.Benson (1989) An Electrically Trainable ANN\\nwith 10240 Floating Gate Synapses. Int Conf on N.N.s Proc :191-196\\n[6] A.F.Murray and A.V.W.Smith.(1987) Asynchronous Arithmetic for VLSI Neural Systems. Electronics Letters 23(12):642-643\\n[7] A.J. Holmes et al. (1993) Use of a-Si:H Memory Devices for Non-volatile Weight\\nStorage in ANNs. Proc lCAS 15 :817-820\\n\\n\\x0c\\x0c',\n",
       "     'pdf_name': '1006-pulsestream-synapses-with-non-volatile-analogue-amorphous-silicon-memories.pdf',\n",
       "     'title': 'Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1007',\n",
       "     'paper_text': 'Learning To Play the Game of Chess\\n\\nSebastian Thrun\\nUniversity of Bonn\\nDepartment of Computer Science III\\nRomerstr. 164, 0-53117 Bonn, Germany\\nE-mail: thrun@carbon.informatik.uni-bonn.de\\n\\nAbstract\\nThis paper presents NeuroChess, a program which learns to play chess from the final\\noutcome of games. NeuroChess learns chess board evaluation functions, represented\\nby artificial neural networks. It integrates inductive neural network learning, temporal\\ndifferencing, and a variant of explanation-based learning. Performance results illustrate\\nsome of the strengths and weaknesses of this approach.\\n\\n1 Introduction\\nThroughout the last decades, the game of chess has been a major testbed for research on\\nartificial intelligence and computer science. Most oftoday\\'s chess programs rely on intensive\\nsearch to generate moves. To evaluate boards, fast evaluation functions are employed which\\nare usually carefully designed by hand, sometimes augmented by automatic parameter tuning\\nmethods [1]. Building a chess machine that learns to play solely from the final outcome of\\ngames (win/loss/draw) is a challenging open problem in AI.\\nIn this paper, we are interested in learning to play chess from the final outcome of games.\\nOne of the earliest approaches, which learned solely by playing itself, is Samuel\\'s famous\\nchecker player program [10]. His approach employed temporal difference learning (in short:\\nTO) [14], which is a technique for recursively learning an evaluation function . Recently,\\nTesauro reported the successful application of TO to the game of Backgammon, using\\nartificial neural network representations [16]. While his TO-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go\\n[12] and chess have been less successful. For example, Schafer [11] reports a system just\\nlike Tesauro\\'s TO-Gammon, applied to learning to play certain chess endgames. Gherrity [6]\\npresented a similar system which he applied to entire chess games. Both approaches learn\\npurely inductively from the final outcome of games. Tadepalli [15] applied a lazy version\\nof explanation-based learning [5, 7] to endgames in chess. His approach learns from the\\nfinal outcome, too, but unlike the inductive neural network approaches listed above it learns\\nanalytically, by analyzing and generalizing experiences in terms of chess-specific knowledge.\\n\\n\\x0c1070\\n\\nSebastian Thrun\\n\\nThe level of play reported for all these approaches is still below the level of GNU-Chess, a\\npublicly available chess tool which has frequently been used as a benchmark. This illustrates\\nthe hardness of the problem of learning to play chess from the final outcome of games.\\nThis paper presents NeuroChess, a program that learns to play chess from the final outcome\\nof games. The central learning mechanisms is the explanation-based neural network (EBNN)\\nalgorithm [9, 8]. Like Tesauro\\'s TD-Gammon approach, NeuroChess constructs a neural\\nnetwork evaluation function for chess boards using TO. In addition, a neural network version\\nof explanation-based learning is employed, which analyzes games in terms of a previously\\nlearned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate\\nsome of its strengths and weaknesses.\\n\\n2\\n\\nTemporal Difference Learning in the Domain of Chess\\n\\nTemporal difference learning (TO) [14] comprises a family of approaches to prediction in\\ncases where the event to be predicted may be delayed by an unknown number of time steps.\\nIn the context of game playing, TD methods have frequently been applied to learn functions\\nwhich predict the final outcome of games. Such functions are used as board evaluation\\nfunctions.\\nThe goal of TO(O), a basic variant of TO which is currently employed in the NeuroChess\\napproach, is to find an evaluation function, V, which ranks chess boards according to their\\ngoodness: If the board S is more likely to be a winning board than the board Sf, then\\nV(s) > V(Sf). To learn such a function, TO transforms entire chess games, denoted by\\na sequence of chess boards So, SI, s2, . . . , StunaJ\\' into training patterns for V. The TO(O)\\nlearning rule works in the following way. Assume without loss of generality we are learning\\nwhite\\'s evaluation function. Then the target values for the final board is given by\\n{\\n\\nI,\\n0,\\n-1,\\n\\nif Stu.?tI is a win for white\\nif StUnaJ is a draw\\nif StonaJ is a loss for white\\n\\nand the targets for the intermediate chess boards So, SI , S2, . .. , Stu.?tI-2 are given by\\nVt.1fget( St)\\nI? V (St+2)\\n\\n=\\n\\n(1)\\n\\n(2)\\n\\nThis update rule constructs V recursively. At the end of the game, V evaluates the final\\noutcome of the game (Eq. (l In between, when the assignment of V -values is less obvious,\\nV is trained based on the evaluation two half-moves later (Eq. (2?. The constant I (with\\no ~ I ~ 1) is a so-called discount factor. It decays V exponentially in time and hence\\nfavors early over late success. Notice that in NeuroChess V is represented by an artificial\\nneural network, which is trained to fit the target values vtarget obtained via Eqs. (l) and (2)\\n(cj [6, 11, 12, 16]).\\n\\n?.\\n\\n3\\n\\nExplanation-Based Neural Network Learning\\n\\nIn a domain as complex as chess, pure inductive learning techniques. such as neural network Back-Propagation, suffer from enormous training times. To illustrate why, consider\\nthe situation of a knight fork. in which the opponent\\'s knight attacks our queen and king\\nsimultaneously. Suppose in order to save our king we have to move it, and hence sacrifice\\nour queen. To learn the badness of a knight fork, NeuroChess has to discover that certain\\nboard features (like the position of the queen relative to the knight) are important, whereas\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n1071\\n\\nFigure 1: Fitting values and slopes in EBNN: Let V be the target function for which three\\nexamples (s\\\\, V(S\\\\)), (S2\\' V(S2)), and (S3, V(S3)) are known. Based on these points the\\nS2)OS2, and a~;:3) are\\nlearner might generate the hypothesis V\\'. If the slopes a~;:I),\\nalso known, the learner can do much better: V\".\\n\\nar\\n\\nothers (like the number of weak pawns) are not. Purely inductive learning algorithms such\\nas Back-propagation figure out the relevance of individual features by observing statistical\\ncorrelations in the training data. Hence, quite a few versions of a knight fork have to be\\nexperienced in order to generalize accurately. In a domain as complex as chess, such an\\napproach might require unreasonably large amounts of training data.\\nExplanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training\\ndata. They rely instead on the availability of domain knowledge, which they use for explaining\\nand generalizing training examples. For example, in the explanation of a knight fork, EBL\\nmethods employ knowledge about the game of chess to figure out that the position of the\\nqueen is relevant, whereas the number of weak pawns is not. Most current approaches to\\nEBL require that the domain knowledge be represented by a set of symbolic rules. Since\\nNeuroChess relies on neural network representations, it employs a neural network version\\nof EBL, called explanation-based neural network learning (EBNN) [9]. In the context of\\nchess, EBNN works in the following way: The domain-specific knowledge is represented\\nby a separate neural network, called the chess model M. M maps arbitrary chess boards St\\nto the corresponding expected board St+2 two half-moves later. It is trained prior to learning\\nV, using a large database of grand-master chess games. Once trained, M captures important\\nknowledge about temporal dependencies of chess board features in high-quality chess play.\\nEBNN exploits M to bias the board evaluation function V. It does this by extracting slope\\nconstraints for the evaluation function V at all non-final boards, i.e., all boards for which V\\nis updated by Eq. (2). Let\\nwith\\n\\nt E\\n\\n{a, 1,2, ... , tlioa\\\\ - 2}\\n\\ndenote the target slope of V at St, which, because\\nEq. (2), can be rewritten as\\n\\noV target ( St)\\n\\n=\\n\\n\\'Y.\\n\\noV( St+2) OSt+2\\n._OSt+2\\nOSt\\n\\nvtarget ( St)\\n\\n(3)\\n\\nis set to \\'Y V (St+2) according\\n(4)\\n\\nusing the chain rule of differentiation. The rightmost term in Eq. (4) measures how infinitesimal small changes of the chess board St influence the chess board St+2. It can be\\napproximated by the chess model M:\\n\\novtarget(St)\\nOSt\\n\\n~\\n\\n\\'Y.\\n\\nOV(St+2) oM(st)\\n.\\nOSt+2\\nOSt\\n\\n(5)\\n\\nThe right expression is only an approximation to the left side, because M is a trained neural\\n\\n\\x0cSebastian Thrun\\n\\n1072\\n\\n~\\n\\nbmrd at time\\n\\nf\\n\\n(W\"T\"~)\\n\\n~\\n\\nboard attime 1+ I\\n(black to move)\\n\\n~\\n\\nboard at time 1+2\\n\\n(w\"\\'?ro~)\\n\\npredictive model network M\\n\\n165 hidden unit,\\n\\nV(1+2)\\n\\nFigure 2: Learning an evaluation function in NeuroChess. Boards are mapped into a\\nhigh-dimensionalJeature vector, which forms the input for both the evaluation network V\\nand the chess model M. The evaluation network is trained by Back-propagation and the\\nTD(O) procedure. Both networks are employed for analyzing training example in order to\\nderive target slopes for V.\\nnetwork and thus its first derivative might be erroneous. Notice that both expressions on\\nthe right hand side of Eq. (5) are derivatives of neural network functions, which are easy to\\ncompute since neural networks are differentiable.\\nThe result of Eq . (5) is an estimate of the slope of the target function V at 8t . This slope\\nadds important shape information to the target values constructed via Eq. (2). As depicted in\\nFig. 1, functions can be fit more accurately if in addition to target values the slopes of these\\nvalues are known. Hence, instead of just fitting the target values vtarget ( 8t), NeuroChess also\\nfits these target slopes. This is done using the Tangent-Prop algorithm [13].\\nThe complete NeuroChess learning architecture is depicted in Fig. 2. The target slopes\\nprovide a first-order approximation to the relevance of each chess board feature in the\\ngoodness of a board position. They can be interpreted as biasing the network V based on\\nchess-specific domain knowledge, embodied in M . For the relation ofEBNN and EBL and\\nthe accommodation of inaccurate slopes in EBNN see [8].\\n\\n4\\n\\nTraining Issues\\n\\nIn this section we will briefly discuss some training issues that are essential for learning good\\nevaluation functions in the domain of chess. This list of points has mainly been produced\\nthrough practical experience with the NeuroChess and related TD approaches. It illustrates\\nthe importance of a careful design of the input representation, the sampling rule and the\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n1073\\n\\nparameter setting in a domain as complex as chess.\\nSampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for\\nexample, the opponent leads by more than a queen and a rook, one is most likely to loose.\\nWithout an appropriate sampling method there is the danger that the learner spends most\\nof its time learning from uninteresting examples. Therefore, NeuroChess interleaves selfplay and expert play for guiding the sampling process. More specifically, after presenting\\na random number of expert moves generated from a large database of grand-master games,\\nNeuroChess completes the game by playing itself. This sampling mechanism has been found\\nto be of major importance to learn a good evaluation function in a reasonable amount of time.\\nQuiescence. In the domain of chess certain boards are harder to evaluate than others. For\\nexample, in the middle of an ongoing material exchange, evaluation functions often fail to\\nproduce a good assessment. Thus, most chess programs search selectively. A common\\ncriterion for determining the depth of search is called quiescence. This criterion basically\\ndetects material threats and deepens the search correspondingly. NeuroChess\\' search engine\\ndoes the same. Consequently, the evaluation function V is only trained using quiescent\\nboards.\\nSmoothness. Obviously, using the raw, canonical board description as input representation is\\na poor choice. This is because small changes on the board can cause a huge difference in value,\\ncontrasting the smooth nature of neural network representations. Therefore, NeuroChess\\nmaps chess board descriptions into a set of board features . These features were carefully\\ndesigned by hand.\\nDiscounting. The variable \\'Y in Eq. (2) allows to discount values in time. Discounting has\\nfrequently been used to bound otherwise infinite sums of pay-off. One might be inclined to\\nthink that in the game of chess no discounting is needed, as values are bounded by definition.\\nIndeed, without discounting the evaluation function predicts the probability for winning-in\\nthe ideal case. In practice, however, random disturbations of the evaluation function can\\nseriously hurt learning, for reasons given in [4, 17]. Empirically we found that learning\\nfailed completely when no discount factor was used. Currently, NeuroChess uses \\'Y = 0.98.\\nLearning rate. TO approaches minimize a Bellman equation [2]. In the NeuroChess\\ndomain, a close-to-optimal approximation of the Bellman equation is the constant function\\nV(s) == O. This function violates the Bellman equation only at the end of games (Eq. (1?,\\nwhich is rare if complete games are considered. To prevent this, we amplified the learning\\nrate for final values by a factor of20, which was experimentally found to produce sufficiently\\nnon-constant evaluation functions.\\nSoftware architecture. Training is performed completely asynchronously on up to 20\\nworkstations simultaneously. One of the workstations acts as a weight server, keeping track\\nof the most recent weights and biases of the evaluation network. The other workstations\\ncan dynamically establish links to the weight server and contribute to the process of weight\\nrefinement. The main process also monitors the state of all other workstations and restarts\\nprocesses when necessary. Training examples are stored in local ring buffers (1000 items\\nper workstation).\\n\\n5\\n\\nResults\\n\\nIn this section we will present results obtained with the NeuroChess architecture. Prior to\\nlearning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)\\nis trained using a database of 120,000 expert games. NeuroChess then learns an evaluation\\n\\n\\x0c1074\\n\\nI. e2e3 b8c6\\n2. dlf3 c6e5\\n3. f3d5 d7d6\\n4. flb5 c7c6\\n5. b5a4 g8f6\\n6. d5d4 c8f5\\n7. f2f4 e5d7\\n8. ele2d8a5\\n9. a4b3 d7c5\\n10. b I a3 c5b3\\n11 . a2b3 e7e5\\n12. f4e5 f6e4\\n13. e5d6 e8c8\\n14. b3b4 a5a6\\n15. b4b5 a6a5\\n\\nSebastian Thrun\\n\\n16. b2b4 a5a4\\n17. b5c6 a4c6\\n18. gl f3 d8d6\\n19. d4a7 f5g4\\n20. c2c4 c8d7\\n21. b4b5 c6c7\\n22. d2d3 d6d3\\n23. b5b6 c7c6\\n24. e2d3 e4f2\\n25. d3c3 g4f3\\n26. g2f3 f2h 1\\n27. clb2 c6f3\\n28. a7a4 d7e7\\n29. a3c2 hi f2\\n30. b2a3 e7f6\\n\\n31 . a3f8 f2e4\\n32. c3b2 h8f8\\n33. a4d7 f3f5\\n34. d7b7 f5e5\\n35. b2cl f8e8\\n36. b7d5 e5h2\\n37. ala7 e8e6\\n38. d5d8 f6g6\\n39. b6b7 e6d6\\n40. d8a5 d6c6\\n41 . a5b4 h2b8\\n42. a7a8 e4c3\\n43. c2d4 c6f6\\n44. b4e7 c3a2\\n45. cldl a2c3\\n\\n46. d I c2 b8h2\\n47. c2c3 f6b6\\n48. e7e4 g6h6\\n49. d4f5 h6g5\\n50. e4e7 g5g4\\n51. f5h6 g7h6\\n52. e7d7 g4h5\\n53. d7d I h5h4\\n54. d I d4 h4h3\\n55. d4b6 h2e5\\n56. b6d4 e5e6\\n57. c3d2 e6f5\\n58. e3e4 f5 g5\\n59. d4e3 g5e3\\n60. d2e3 f7f5\\n\\n61 . e4f5 h3g4 65. a8e8 e6d7\\n62. f5f6 h6h5\\n66. e8e7 d7d8\\n63. b7b8q g4f5 67. f4c7\\n64. b8f4 f5e6\\nfinal board\\n\\nFigure 3: NeuroChess against GNU-Chess. NeuroChess plays white. Parameters: Both\\nplayers searched to depth 3, which could be extended by quiescence search to at most 11.\\nThe evaluation network had no hidden units. Approximately 90% of the training boards\\nwere sampled from expert play.\\n\\nnetwork V (175 input units, 0 to 80 hidden units, and one output units). To evaluate the level\\nof play, NeuroChess plays against GNU-Chess in regular time intervals. Both players employ\\nthe same search mechanism which is adopted from GNU-Chess. Thus far, experiments lasted\\nfor 2 days to 2 weeks on I to 20 SUN Sparc Stations.\\nA typical game is depicted in Fig. 3. This game has been chosen because it illustrates both\\nthe strengths and the shortcomings of the NeuroChess approach. The opening of NeuroChess\\nis rather weak. In the first three moves NeuroChess moves its queen to the center of the\\nboard.\\' NeuroChess then escapes an attack on its queen in move 4, gets an early pawn\\nadvantage in move 12, attacks black\\'s queen pertinaciously through moves 15 to 23, and\\nsuccessfully exchanges a rook. In move 33, it captures a strategically important pawn, which,\\nafter chasing black\\'s king for a while and sacrificing a knight for no apparent reason, finally\\nleads to a new queen (move 63). Four moves later black is mate. This game is prototypical.\\nAs can be seen from this and various other games, NeuroChess has learned successfully to\\nprotect its material, to trade material, and to protect its king. It has not learned, however, to\\nopen a game in a coordinated way, and it also frequently fails to play short.endgames even\\nif it has a material advantage (this is due to the short planning horizon). Most importantly, it\\nstill plays incredibly poor openings, which are often responsible for a draw or a loss. Poor\\nopenings do not surprise, however, as TD propagates values from the end of a game to the\\nbeginning.\\nTable I shows a performance comparison of NeuroChess versus GNU-Chess, with and\\nwithout the explanation-based learning strategy. This table illustrates that NeuroChess wins\\napproximately 13% of all games against GNU-Chess, if both use the same search engine. It\\n\\'This is because in the current version NeuroChess still heavily uses expert games for sampling.\\nWhenever a grand-master moves its queen to the center of the board, the queen is usually safe, and there\\nis indeed a positive correlation between having the queen in the center and winning in the database.\\nNeuroChess falsely deduces that having the queen in the center is good. This effect disappears when\\nthe level of self-play is increased, but this comes at the expense of drastically increased training time,\\nsince self-play requires search.\\n\\n\\x0cLearning to Play the Game of Chess\\n\\n# of games\\n100\\n200\\n500\\n1000\\n1500\\n2000\\n2400\\n\\nGNU depth 2, NeuroChess depth 2\\nBack-propagation\\nEBNN\\n1\\n0\\n6\\n2\\n35\\n13\\n73\\n85\\n130\\n135\\n190\\n215\\n239\\n316\\n\\n1075\\n\\nGNU depth 4, NeuroChess depth 2\\nBack-propagation\\nEBNN\\n0\\n0\\n0\\n0\\nI\\n0\\n2\\n1\\n3\\n3\\n3\\n8\\nII\\n3\\n\\nTable 1: Performance ofNeuroChess vs. GNU-Chess during training. The numbers show the\\ntotal number of games won against GNU-Chess using the same number of games for testing\\nas for training. This table also shows the importance of the explanation-based learning\\nstrategy in EBNN. Parameters: both learners used the original GNU-Chess features, the\\nevaluation network had 80 hidden units and search was cut at depth 2, or 4, respectively (no\\nquiescence extensions).\\nalso illustrates the utility of explanation-based learning in chess.\\n\\n6 Discussion\\nThis paper presents NeuroChess, an approach for learning to play chess from the final\\noutcomes of games. NeuroChess integrates TD, inductive neural network learning and\\na neural network version of explanation-based learning. The latter component analyzes\\ngames using knowledge that was previously learned from expert play. Particular care has\\nbeen taken in the design of an appropriate feature representation, sampling methods, and\\nparameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in\\nseveral hundreds of games. However, the level of play still compares poorly to GNU-Chess\\nand human chess players.\\nDespite the initial success, NeuroChess faces two fundamental problems which both might\\nweB be in the way of excellent chess play. Firstly, training time is limited, and it is to\\nbe expected that excellent chess skills develop only with excessive training time. This is\\nparticularly the case if only the final outcomes are considered. Secondly, with each step of\\nTO-learning NeuroChess loses information. This is partially because the features used for\\ndescribing chess boards are incomplete, i.e., knowledge about the feature values alone does\\nnot suffice to determine the actual board exactly. But, more importantly, neural networks have\\nnot the discriminative power to assign arbitrary values to all possible feature combinations.\\nIt is therefore unclear that a TD-like approach will ever, for example, develop good chess\\nopenmgs.\\nAnother problem of the present implementation is related to the trade-off between knowledge\\nand search. It has been well recognized that the ul timate cost in chess is determi ned by the ti me\\nit takes to generate a move. Chess programs can generally invest their time in search, or in the\\nevaluation of chess boards (search-knowledge trade-off) [3] . Currently, NeuroChess does a\\npoor job, because it spends most of its time computing board evaluations. Computing a large\\nneural network function takes two orders of magnitude longer than evaluating an optimized\\nlinear evaluation function (like that of GNU-Chess). VLSI neural network technology offers\\na promising perspective to overcome this critical shortcoming of sequential neural network\\nsimulations.\\n\\n\\x0c1076\\n\\nSebastian Thrun\\n\\nAcknowledgment\\nThe author gratefully acknowledges the guidance and advise by Hans Berliner, who provided\\nthe features for representing chess boards, and without whom the current level of play would\\nbe much worse. He also thanks Tom Mitchell for his suggestion on the learning methods,\\nand Horst Aurisch for his help with GNU-Chess and the database.\\n\\nReferences\\n[I] Thomas S. Anantharaman. A Statistical Study of Selective Min-Max Search in Computer Chess.\\nPhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA, 1990.\\nTechnical Report CMU-CS-90-173.\\n[2] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.\\n[3] Hans J. Berliner, Gordon Goetsch, Murray S. Campbell, and Carl Ebeling. Measuring the\\nperformance potential of chess programs. Artificial Intelligence, 43:7-20, 1990.\\n[4] Justin A. Boyan. Generalization in reinforcement learning: Safely approximating the value\\nfunction. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information\\nProcessing Systems 7, San Mateo, CA, 1995. Morgan Kaufmann. (to appear).\\n[5] Gerald Dejong and Raymond Mooney. Explanation-based learning: An alternative view. Machine Learning, 1(2): 145-176, 1986.\\n[6] Michael Gherrity. A Game-Learning Machine. PhD thesis, University of California, San Diego,\\n1993.\\n[7] Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. Explanation-based generalization: A\\nunifying view. Machine Learning, 1(1 ):47-80, 1986.\\n[8] Tom M. Mitchell and Sebastian Thrun. Explanation based learning: A comparison of symbolic\\nand neural network approaches. In Paul E. Utgoff, editor, Proceedings of the Tenth International\\nConference on Machine Learning, pages 197-204, San Mateo, CA, 1993. Morgan Kaufmann.\\n[9] Tom M. Mitchell and Sebastian Thrun. Explanation-based neural network learning for robot\\ncontrol. In S. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information\\nProcessing Systems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.\\n[10] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal on\\nresearch and development, 3:210-229, 1959.\\n[11] Johannes Schafer. Erfolgsorientiertes Lemen mit Tiefensuche in Bauemendspielen. Technical\\nreport, UniversiUit Karlsruhe, 1993. (in German).\\n[12] Nikolaus Schraudolph, Pater Dayan, and Terrence J. Sejnowski. Using the TD(lambda) algorithm\\nto learn an evaluation function for the game of go. In Advances in Neural Information Processing\\nSystems 6, San Mateo, CA, 1994. Morgan Kaufmann.\\n[13] Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. Tangent prop -a formalism for\\nspecifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P.\\nLippmann, editors, Advances in Neural Information Processing Systems 4, pages 895-903, San\\nMateo, CA, 1992. Morgan Kaufmann.\\n[14] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\\n3,1988.\\n[15] Prasad Tadepalli. Planning in games using approximately learned macros. In Proceedings of the\\nSixth International Workshop on Machine Learning, pages 221-223, Ithaca, NY, 1989. Morgan\\nKaufmann.\\n[16] Gerald J. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8, 1992.\\n[17] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors,\\nProceedings of the 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Erlbaum\\nAssociates.\\n\\n\\x0c',\n",
       "     'pdf_name': '1007-learning-to-play-the-game-of-chess.pdf',\n",
       "     'title': 'Learning to Play the Game of Chess',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1013',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1013',\n",
       "     'paper_text': 'Ocular Dominance and Patterned Lateral\\nConnections in a Self-Organizing Model of the\\nPrimary Visual Cortex\\nJoseph Sirosh and Risto Miikkulainen\\n\\nDepartment of Computer Sciences\\nUniversity of Texas at Austin, Austin, \\'IX 78712\\nemail:\\n\\nsirosh.risto~cs.utexas.edu\\n\\nAbstract\\nA neural network model for the self-organization of ocular dominance and\\nlateral connections from binocular input is presented. The self-organizing\\nprocess results in a network where (1) afferent weights of each neuron organize into smooth hill-shaped receptive fields primarily on one of the retinas, (2) neurons with common eye preference form connected, intertwined\\npatches, and (3) lateral connections primarily link regions of the same eye\\npreference. Similar self-organization of cortical structures has been observed experimentally in strabismic kittens. The model shows how patterned lateral connections in the cortex may develop based on correlated\\nactivity and explains why lateral connection patterns follow receptive field\\nproperties such as ocular dominance.\\n\\n1 Introduction\\nLateral connections in the primary visual cortex have a patterned structure that closely\\nmatches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.1993).\\nFor example, in the normal visual cortex, long-range lateral connections link areas with similar orientation preference (Gilbert and Wiesel 1989). Like cortical response properties, the\\nconnectivity pattern is highly plastic in early development and can be altered by experience\\n(Katz and Callaway 1992). In a cat that is brought up squint-eyed from birth, the lateral connections link areas with the same ocular dominance instead of orientation (Lowel and Singer\\n1992). Such patterned lateral connections develop at the same time as the orientation selectivity and ocular dominance itself (Burkhalter et al.1993; Katz and Callaway 1992). Together,\\n\\n\\x0c110\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\nthese observations suggest that the same experience-dependent process drives the development of both cortical response properties and lateral connectivity.\\nSeveral computational models have been built to demonstrate how orientation preference,\\nocular dominance, and retinotopy can emerge from simple self-organizing processes (e.g.\\nGoodhill1993; Miller 1994; Obermayer et al.1992; von der Malsburg 1973). These models\\nassume that the neuronal response properties are primarily determined by the afferent connections, and concentrate only on the self-organization of the afferent synapses to the cortex. Lateral interactions between neurons are abstracted into simple mathematical functions\\n(e.g. Gaussians) and assumed to be uniform throughout the network; lateral connectivity is not\\nexplicitly taken into account. Such models do not explicitly replicate the activity dynamics\\nof the visual cortex, and therefore can make only limited predictions about cortical function.\\nWe have previously shown how Kohonen\\'s self-organizing feature maps (Kohonen 1982)\\ncan be generalized to include self-organizing lateral connections and recurrent activity dynamics (the Laterally Interconnected Synergetically Self-Organizing Map (LISSOM); Sirosh\\nand Miikkulainen 1993, 1994a), and how the algorithm can model the development of ocular dominance columns and patterned lateral connectivity with abstractions of visual input.\\nLISSOM is a low-dimensional abstraction of cortical self-organizing processes and models a\\nsmall region of the cortex where all neurons receive the same input vector. This paper shows\\nhow realistic, high-dimensional receptive fields develop as part of the self-organization, and\\nscales up the LISSOM approach to large areas of the cortex where different parts of the cortical network receive inputs from different parts of the receptor surface. The new model shows\\nhow (1) afferent receptive fields and ocular dominance columns develop from simple retinal images, (2) input correlations affect the wavelength of the ocular dominance columns and\\n(3) lateral connections self-organize cooperatively and simultaneously with ocular dominance\\nproperties. The model suggests new computational roles for lateral connections in the cortex,\\nand suggests that the visual cortex maybe maintained in a continuously adapting equilibrium\\nwith the visual input by co adapting lateral and afferent connections.\\n\\n2\\n\\nThe LISSOM Model of Receptive Fields and Ocular Dominance\\n\\nThe LISSOM network is a sheet of interconnected neurons (figure 1). Through afferent connections, each neuron receives input from two \"retinas\". In addition, each neuron has reciprocal excitatory and inhibitory lateral connections with other neurons. Lateral excitatory connections are short-range, connecting only close neighbors. Lateral inhibitory connections run\\nfor long distances, and may even implement full connectivity between neurons in the network.\\nNeurons receive afferent connections from broad overlapping patches on the retina called\\nanatomical receptive fields, or RFs. The N x N network is projected on to each retina of\\nR x R receptors, and each neuron is connected to receptors in a square area of side s around\\nthe projections. Thus, neurons receive afferents from corresponding regions of each retina.\\nDepending on the location of the projection, the number of afferents to a neuron from each\\nretina could vary from\\nx ~s (at the comers) to s x s (at the center).\\n\\nts\\n\\nThe external and lateral weights are organized through an unsupervised learning process. At\\neach training step, neurons start out with zero activity. The initial response TJij of neuron (i, j)\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\nLoft _ . .\\n\\n111\\n\\nfllgIIl Roll . .\\n\\nFigure 1: The Receptive-Field LISSOM architecture. The afferent and lateral connectionsof a single\\nneuron in the liSSOM network are shown. All connection weights are positive.\\n\\nis based on the scalar product\\nTJij\\n\\n=\\n\\n(T\\n\\n(L\\n\\neabJJij ,ab\\n\\n+\\n\\na,b\\n\\nL\\n\\n(1)\\n\\neCdJJij,Cd) ,\\n\\nc,d\\n\\nwhere eab and ecd are the activations of retinal receptors (a, b) and (c, d) within the receptive\\nfields of the neuron in each retina, JJij,ab and JJij,cd are the corresponding afferent weights,\\nand (T is a piecewise linear approximation of the familiar sigmoid activation function. The\\nresponse evolves over time through lateral interaction. At each time step, the neuron combines the above afferent activation I:: eJJ with lateral excitation and inhibition:\\nTJij(t)\\n\\n=\\n\\n(T\\n\\n(L eJJ + L\\n\"Ie\\n\\nEij,kITJkl(t -\\n\\n1) - L\\n\"Ii\\n\\nk,1\\n\\nIij,klTJkl(t -\\n\\n1)) ,\\n\\n(2)\\n\\nk,1\\n\\nwhere Eij,kl is the excitatory lateral connection weight on the connection from neuron (k, l)\\nto neuron (i, j), Iij,kl is the inhibitory connection weight, and TJkl (t - 1) is the activity of\\nneuron (k, I) during the previous time step. The constants \"Ie and \"Ii determine the relative\\nstrengths of excitatory and inhibitory lateral interactions. The activity pattern starts out diffuse and spread over a substantial part of the map, and converges iteratively into stable focused\\npatches of activity, or activity bubbles. After the-activity has settled, typically in a few iterations of equation 2, the connection weights of each neuron are modified. Both afferent and\\nlateral weights adapt according to the same mechanism: the Hebb rule, normalized so that the\\nsum of the weights is constant:\\n(\\n\\nWij,mn t\\n\\nr ) _\\n\\n+ vt\\n\\n-\\n\\n+\\n\\nWij,mn(t)\\nCtTJijXmn\\n\\'\"\"\\n( )\\nwmn [Wij ,mn t\\nCtTJijXmn\\n\\n+\\n\\n1\\'\\n\\n(3)\\n\\nwhere TJij stands for the activity of neuron (i, j) in the final activity bubble, Wij,mn is the afferent or lateral connection weight (JJ, E or I), Ct is the learning rate for each type of connection\\n(Ct a for afferent weights, Ct E for excitatory, and Ct I for inhibitory) and X mn is the presynaptic\\nactivity for afferent, TJ for lateral).\\n\\n(e\\n\\n\\x0cJoseph Sirosh, Risto Miikkulainen\\n\\n112\\n\\n\"\\n(a) Random Initial Weights\\n\\n(b) Monocular RF\\n\\n(c) Binocular RF\\n\\nFigure 2: Self-organization of the afferent input weights into receptive fields. The afferent weights\\nof a neuron at position (42,39) in a 60 x 60 network are shown before (a) and after self-organization\\n(b). This particular neuron becomes monocular with strong connections to the right eye, and weak connections to the left. A neuron at position (38, 23) becomes binocular with appoximately equal weights\\nto both eyes (c).\\nBoth excitatory and inhibitory lateral connections follow the same Hebbian learning process and strengthen by correlated activity. The short-range excitation keeps the activity of\\nneighboring neurons correlated, and as self-organization progresses, excitation and inhibition strengthen in the vicinity of each neuron. At longer distances, very few neurons have\\ncorrelated activity and therefore most long-range connections become weak. Such weak connections are eliminated, and through weight normalization, inhibition concentrates in a closer\\nneighborhood of each neuron. As a result, activity bubbles become more focused and local,\\nweights change in smaller neighborhoods, and receptive fields become better tuned to local\\nareas of each retina.\\nThe input to the model consists of gaussian spots of \"light\" on each retina:\\nt\\n_\\n((x\\n<\"x,y - exp -\\n\\n- xd 2 + (y - Yi)2)\\nu2\\n\\n(4)\\n\\nwhere ex,y is the activation of receptor (x, V), u 2 is a constant determining the width of the\\nspot, and (Xi,Yi): 0 ~ xi, Yi < R its center. At each input presentation, one spot is randomly\\nplaced at (Xi ,Yi) in the left retina, and a second spot within a radius of p x RN of (Xi, yd\\nin the right retina. The parameter p E [0, 1] specifies the spatial correlations between spots\\nin the two retinas, and can be adjusted to simulate different degrees of correlations between\\nimages in the two eyes.\\n\\n3\\n\\nSimulation results\\n\\nTo see how correlation between the input from the two eyes affects the columnar structures\\nthat develop, several simulations were run with different values of p. The afferent weights of\\nall neurons were initially random (as shown in figure 2a), with the total strength to both eyes\\nbeing equal.\\nFigures 2b,c show the final afferent receptive fields of two typical neurons in a simulation\\nwith p = 1. In this case, the inputs were uncorrelated, simulating perfect strabismus. In\\nthe early stages of such simulation, some of the neurons randomly develop a preference for\\none eye or the other. Nearby neurons will tend to share the same preference because lateral\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\n(a) Connections of a Monocular Neuron\\n\\n113\\n\\n(b) Connections of a Binocular Neuron\\n\\nFigure 3: Ocular dominance and lateral connection patterns. The ocular dominance of a neuron is\\nmeasured as the difference in total afferent synaptic weight from each eye to the neuron. Each neuron\\nis labeled with a grey-scale value (black ~ white) that represents continuously changing eye preference from exclusive left through binocular to exclusive right. Small white dots indicate the lateral input\\nconnections to the neuron marked with a big white dot. (a) The surviving lateral connections of a left\\nmonocular neuron predominantly link areas of the same ocular dominance. (b) The lateral connections\\nof a binocular neuron come from both eye regions.\\n\\nexcitation keeps neural activity partially correlated over short distances. As self-organization\\nprogresses, such preferences are amplified, and groups of neurons develop strong weights to\\none eye. Figure 2b shows the afferent weights of a typical monocular neuron.\\nThe extent of activity correlations on the network detennines the size of the monocular neuronal groups. Farther on the map, where the activations are anticorrelated due to lateral inhibition, neurons will develop eye preferences to the opposite eye. As a result, alternating\\nocular dominance patches develop over the map, as shown in figure 3. 1 In areas between ocular dominance patches, neurons will develop approximately equal strengths to both eyes and\\nbecome binocular, like the one shown in figure 2e.\\nThe width and number of ocular dominance columns in the network (and therefore, the wavelength of ocular dominance) depends on the input correlations (figure 4). When inputs in the\\ntwo eyes become more correlated (p < 1), the activations produced by the two inputs in the\\nnetwork overlap closely and activity correlations become shorter range. By Hebbian adaptation, lateral inhibition concentrates in the neighborhood of each neuron, and the distance at\\nwhich activations becomes anticorrelated decreases. Therefore, smaller monocular patches\\ndevelop, and the ocular dominance wavelength decreases. Similar dependence was very recently observed in the cat primary visual cortex (LoweI1994). The LISSOM model demonstrates that the adapting lateral interactions and recurrent activity dynamics regulate the wavelength, and suggests how these processes help the cortex develop feature detectors at a scale\\n1 For a thorough treatment of the mathematical principles underlying the development of ocular dominance columns, see (GoodhillI993; Miller et al.1989; von der Malsburg and Singer 1988).\\n\\n\\x0c114\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\n-0\\n-0\\n\\n(a) Strabismic case\\n\\n(b ) Normal case\\n\\nFigure 4: Ocular dominance wavelength in strabismic and normal models. In the strabismic case,\\nthere are no between-eye correlations (p = 1), and broad ocular dominance columns are produced (a) .\\nWith normal, partial between-eye correlations (p = 0.45 in this example), narrower stripes are formed\\n(b). As a result, there are more ocular dominance columns in the normal case and the ocular dominance\\nwavelength is smaller.\\n\\nthat matches the input correlations.\\nAs eye preferences develop, left or right eye input tends to cause activity only in the left or\\nright ocular dominance patches. Activity patterns in areas of the network with the same ocular dominance tend to be highly correlated because they are caused by the same input spot.\\nTherefore, the long-range lateral connections between similar eye preference areas become\\nstronger, and those between opposite areas weaker. After the weak lateral connections are\\neliminated, the initially wide-ranging connections are pruned, and eventually only connect\\nareas of similar ocular dominance as shown in figure 3. Binocular neurons between ocular\\ndominance patches will see some correlated activity in both the neigbboring areas, and maintain connections to both ocular dominance columns (figure 3b).\\nThe lateral connection patterns shown above closely match observations in the primary visual cortex. Lowel and Singer (1992) observed that when between-eye correlations are abolished in kittens by surgically induced strabismus, long-range lateral connections primarily\\nlink areas of the same ocular dominance. However, binocular neurons, located between ocular dominance columns, retained connections to both eye regions. The receptive field model\\nconfinns that such patterned lateral connections develop based on correlated neuronal activity,\\nand demonstrates that they can self-organize simultaneously with ocular dominance columns.\\nThe model also predicts that the long-range connections have an inhibitory function.\\n\\n4 Discussion\\nIn LISSOM, evolving lateral interactions and dynamic activity patterns are explicitly modeled. Therefore, LISSOM has several novel properties that set it apart from other selforganizing models of the cortex.\\nPrevious models (e.g. Goodhill1993; Milleret al.1989; Obermayer et al.1992; von der Malsburg 1973) have concentrated only on forming ordered topographic maps where clusters of\\nadjacent neurons assume similar response properties such as ocular dominance or orientation\\npreference. The lateral connections in LISSOM, in addition, adapt to encode correlations be-\\n\\n\\x0cOcular Dominance and Patterned Lateral Connections\\n\\n115\\n\\ntween the responses. 2 This property can be potentially very useful in models of cortical function. While afferent connections learn to detect the significant features in the input space (such\\nas ocularity or orientation), the lateral connections can learn correlations between these features (such as Gestalt principles), and thereby form a basis for feature grouping.\\nAs an illustration, consider a single spot of light presented to the left eye. The spot causes disjoint activity patterns in the left-eye-dominant patches. How can these multiple activity patterns be recognized as representing the same spatially coherent entity? As proposed by Singer\\net al. (1990), the long-range lateral connections between similar ocular dominance columns\\ncould synchronize cortical activity, and form a coherently firing assembly of neurons. The\\nspatial coherence of the spot will then be represented by temporal coherence of neural activity. LISSOM can be potentially extended to model such feature binding.\\nEven after the network has self-organized, the lateral and afferent connections remain plastic\\nand in a continuously-adapting dynamic equilibrium with the input. Therefore, the receptive\\nfield properties of neurons can dynamically readapt when the activity correlations in the network are forced to change. For example, when a small area of the cortex is set inactive (or\\nlesioned), the sharply-tuned afferent weight profiles of the neurons surrounding that region\\nexpand in size, and neurons begin to respond to the stimuli that previously activated only the\\nlesioned area (Sirosh and Miikkulainen 1994b, 1994c). This expansion of receptive fields is\\nreversible, and when the lesion is repaired, neurons return to their original tuning. Similar\\nchanges occur in response to retinal lesions as well. Such dynamic expansions of receptive\\nfields have been observed in the visual cortex (Pettet and Gilbert 1992). The LISSOM model\\ndemonstrates that such plasticity is a consequence of the same self-organizing mechanisms\\nthat drive the development of cortical maps.\\n\\n5\\n\\nConclusion\\n\\nThe LISSOM model shows how a single local and unsupervised self-organizing process can\\nbe responsible for the development of both afferent and lateral connection structures in the primary visual cortex. It suggests that this same developmental mechanism also encodes higherorder visual information such as feature correlations into the lateral connections. The model\\nforms a framework for future computational study of cortical reorganization and plasticity, as\\nwell as dynamic perceptual processes such as feature grouping and binding.\\nAcknowledgments\\n\\nThis research was supported in part by National Science Foundation under grant #IRI9309273. Computer time for the simulations was provided by the Pittsburgh Supercomputing\\nCenter under grants IRI930005P and TRA940029P.\\n\\nReferences\\nBurkhalter, A., Bernardo, K. L., and Charles, V. (1993). Development of local circuits in\\nhuman visual cortex. Journalo/Neuroscience, 13:1916-1931.\\nGilbert, C. D., and Wiesel, T. N. (1989). Columnar specificity of intrinsic horizontal and\\ncorticocortical connections in cat visual cortex. Journal 0/ Neuroscience, 9:2432-2442.\\n2Tbe idea was conceived by von der Malsburg and Singer (1988), but not modeled.\\n\\n\\x0c116\\n\\nJoseph Sirosh, Risto Miikkulainen\\n\\nGoodhill, G. (1993). Topography and ocular dominance: a model exploring positive correlations. Biological Cybernetics, 69:109-118.\\nKatz, L. C., and Callaway, E. M. (1992). Development of local circuits in mammalian visual\\ncortex. Annual Review o/Neuroscience, 15:31-56.\\nKohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biolog-\\n\\nical Cybernetics, 43:59-69.\\nLowel, S. (1994). Ocular dominance column development: Strabismus changes the spacing\\nof adjacent columns in cat visual cortex. Journal 0/ Neuroscience, 14(12):7451-7468.\\nLowel, S., and Singer, W. (1992). Selection of intrinsic horizontal connections in the visual\\ncortex by correlated neuronal activity. Science, 255:209-212.\\nMalach, R., Amir, Y., Harel, M., and Grinvald, A (1993). Relationship between intrinsic\\nconnections and functional architecture revealed by optical imaging and in vivo targeted\\nbiocytin injections in the primate striate cortex. Proceedings o/the National Academy\\n\\no/Sciences, USA,90:10469-10473.\\nMiller, K. D. (1994). A model for the development of simple cell receptive fields and the\\nordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs. Journalo/Neuroscience, 14:409-441.\\nMiller, K. D., Keller, 1. B., and Stryker, M. P. (1989). Ocular dominance column development:\\nAnalysis and simulation. Science, 245:605-615.\\nObermayer, K., Blasdel, G. G., and Schulten, K. J. (1992). Statistical-mechanical analysis of\\nself-organization and pattern formation during the development of visual maps. Physical\\n\\nReview A, 45:7568-7589.\\nPettet, M. W., and Gilbert, C. D. (1992). Dynamic changes in receptive-field size in cat primary visual cortex. Proceedings o/the NationalAcademy 0/ Sciences, USA,89:83668370.\\nSinger, W., Gray, C., Engel, A, Konig, P., Artola, A, and Bracher, S. (1990). Formation of\\ncortical cell assemblies. In Cold Spring Harbor Symposia on Quantitative Biology, Vol.\\nLV, 939-952. Cold Spring Harbor, NY: Cold Spring Harbor Laboratory.\\nSirosh, J., and Miikkulainen, R. (1993). How lateral interaction develops in a self-organizing\\nfeature map. In Proceedings o/the IEEE International Conference on Neural Networks\\n(San Francisco, CA), 1360--1365. Piscataway, NJ: IEEE.\\nSirosh, J., and Miikkulainen, R. (1994a). Cooperative self-organization of afferent and lateral\\nconnections in cortical maps. Biological Cybernetics, 71(1):66--78.\\nSirosh, 1., and Miikkulainen, R. (1994b). Modeling cortical plasticity based on adapting lateral interaction. In The Neurobiologyo/Computation: Proceedings o/the Annual ComputationalNeuroscience Meeting. Dordrecht; Boston: Kluwer. In Press.\\nSirosh, J., and Miikkulainen, R. (1994c). A neural network model oftopographic reorganization following cortical lesions. In Proceedings o/the World Congress on Computational\\nMediCine, Public Health and BioteChnology (Austin, TX). World Scientific. In Press.\\nvon der Malsburg, C. (1973). Self-organization of orientation-sensitive cells in the striate\\ncortex. Kybernetik, 15:85-100.\\nvon der Malsburg, C., and Singer, W. (1988). Principles of cortical network organization. In\\nRakic, P., and Singer, W., editors, Neurobiology 0/Neocortex, 69-99. New York: Wiley.\\n\\n\\x0c',\n",
       "     'pdf_name': '1013-ocular-dominance-and-patterned-lateral-connections-in-a-self-organizing-model-of-the-primary-visual-cortex.pdf',\n",
       "     'title': 'Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1032',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1032',\n",
       "     'paper_text': 'VLSI Model of Primate Visual Smooth Pursuit\\n\\nRalph Etienne-Cummings\\n\\nJan Van der Spiegel\\n\\nDepartment of Electrical Engineering,\\nSouthern Illinois University, Carbondale,\\nIL 62901\\n\\nMoore School of Electrical Engineering,\\nUniversity of Pennsylvania, Philadelphia,\\nPA 19104\\n\\nPaul Mueller\\nCorticon, Incorporated,\\n3624 Market Str, Philadelphia,\\nPA 19104\\n\\nAbstract\\nA one dimensional model of primate smooth pursuit mechanism has\\nbeen implemented in 2 11m CMOS VLSI. The model consolidates\\nRobinson\\'s negative feedback model with Wyatt and Pola\\'s positive\\nfeedback scheme, to produce a smooth pursuit system which zero\\'s the\\nvelocity of a target on the retina. Furthermore, the system uses the\\ncurrent eye motion as a predictor for future target motion. Analysis,\\nstability and biological correspondence of the system are discussed. For\\nimplementation at the focal plane, a local correlation based visual\\nmotion detection technique is used. Velocity measurements, ranging\\nover 4 orders of magnitude with < 15% variation, provides the input to\\nthe smooth pursuit system. The system performed successful velocity\\ntracking for high contrast scenes. Circuit design and performance of the\\ncomplete smooth pursuit system is presented.\\n\\n1 INTRODUCTION\\nThe smooth pursuit mechanism of primate visual systems is vital for stabilizing a region\\nof the visual field on the retina. The ability to stabilize the image of the world on the\\nretina has profound architectural and computational consequences on the retina and visual\\ncortex, such as reducing the required size, computational speed and communication\\nhardware and bandwidth of the visual system (Bandera, 1990; Eckert and Buchsbaum,\\n1993). To obtain similar benefits in active machine vision, primate smooth pursuit can\\nbe a powerful model for gaze control. The mechanism for smooth pursuit in primates\\nwas initially believed to be composed of a simple negative feedback system which\\nattempts to zero the motion of targets on the fovea, figure I (a) (Robinson, 1965).\\nHowever, this scheme does not account for many psychophysical properties of smooth\\n\\n\\x0c707\\n\\nVLSI Model of Primate Visual Smooth Pursuit\\n\\npursuit, which led Wyatt and Pola (1979) to proposed figure l(b), where the eye\\nmovement signal is added to the target motion in a positive feed back loop. This\\nmechanism results from their observation that eye motion or apparent target motion\\nincreases the magnitude of pursuit motion even when retinal motion is zero or constant.\\nTheir scheme also exhibited predictive qualities, as reported by Steinbach (1976). The\\nsmooth pursuit model presented in this paper attempts the consolidate the two models\\ninto a single system which explains the findings of both approaches.\\nTarget\\nMoticn\\n\\nEye\\nMotion\\n\\nRetinal\\nMotion\\n\\ne~\\n\\nlee\\n\\nG\\n\\nee = e t G+l\\n~;\\n\\n>\\n\\nI\\nG ~ co G r\\n\\nTarget\\nMotion\\n\\nEye\\nMotion\\n\\ne~~\\n\\n>\\n\\n=0\\n(b)\\n\\n(a)\\n\\nFigure I: System Diagrams of Primate Smooth Pursuit Mechanism.\\n(a) Negative feedback model by Robinson (1965). (b) Positive\\nfeedback model by Wyatt and Pola (1979).\\nThe velocity based smooth pursuit implemented here attempts to zero the relative velocity\\nof the retina and target. The measured retinal velocity, is zeroed by using positive\\nfeedback to accumulate relative velocity error between the target and the retina, where the\\naccumulated value is the current eye velocity. Hence, this model uses the Robinson\\napproach to match target motion, and the Wyatt and Pola positive feed back loop to\\nachieve matching and to predict the future velocity of the target. Figure 2 shows the\\nsystem diagram of the velocity based smooth pursuit system. This system is analyzed\\nand the stability criterion is derived. Possible computational blocks for the elements in\\nfigure I (b) are also discussed. Furthermore, since this entire scheme is implemented on a\\nsingle 2 /lm CMOS chip, the method for motion detection, the complete tracking circuits\\nand the measured results are presented.\\nRetinal\\nMotion\\n\\nEye\\nMotion\\n\\ner\\n\\nFigure 2: System Diagram of VLSI Smooth Pursuit Mechanism.\\nis target velocity in space, Bt is projected target velocity, Be is the eye\\nvelocity and Br is the measured retinal velocity.\\n\\n2 VELOCITY BASED SMOOTH PURSUIT\\nAlthough figure I (b) does not indicate how retinal motion is used in smooth pursuit, it\\nprovides the only measurement of the projected target motion. The very process of\\ncalculating retinal motion realizes negative feed back between the eye movement and the\\ntarget motion, since retinal motion is the difference between project target and eye\\nmotion. If Robinson\\'s model is followed, then the eye movement is simply the\\namplified version of the retinal motion. If the target disappears from the retina, the eye\\nmotion would be zero. However, Steinbach showed that eye movement does not cea~\\nwhen the target fades off and on, indicating that memory is used to predict target motion.\\nWyatt and Palo showed a direct additive influence of eye movement on pursuit. However,\\nthe computational blocks G\\' and a of their model are left unfilled.\\n\\n\\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\n708\\n\\nIn figure 2, the gain G models the internal gain of the motion detection system , and the\\ninternal representation of retinal velocity is then Vr. Under zero-slip tracking, the retinal\\nvelocity is zero. This is obtained by using positive feed back to correct the velocity error\\nand eye,\\nThe delay element represents a memory of the last eye\\nbetween target,\\nvelocity while the current retinal motion is measured. If the target disappears, the eye\\nmotion continues with the last value, as recorded by Steinbach, thus anticipating the\\nposition of the target in space. The memory also stores the current eye velocity during\\nperfect pursuit. The internal representation of eye velocity, Ve , is subsequently amplified\\nby H and used to drive the eye muscles. The impulse response of the system is given in\\nequations (I). Hence, the relationship between eye velocity and target velocity is recursive\\nand given by equations (2). To prove the stability of this system, the retinal velocity can\\nbe expressed in terms of the target motion as given in equations (3a). The ideal condition\\nfor accurate performance is for GH = 1. However, in practice, gains of different amplifiers\\n\\ner,\\n\\n()\\n\\nz-)\\n\\n=GH--_-)\\n\\n-.f..(z)\\n\\n1- Z\\n\\n(}r\\n\\nee.\\n\\n()\\n\\n(a); ~(I1)\\n(}r\\n\\n=GH[-8(11) + u(n)]\\n\\n(I)\\n\\n(b)\\nn-)\\n\\n(}e(n)\\n\\n= (},(n) -\\n\\n(}r(n)\\n\\n=GH[-8(n) + u(n)] * (}r(n) = GHL(},.(k)\\n\\n(2)\\n\\nk=O\\n() r ( 11)\\n\\n() r (n)\\n\\n= (),( n ) (1\\n11\\n\\n~\\n\\n00\\n\\n)\\n\\n11\\n\\n- GH)\\n0\\n\\nif 11 -\\n\\n=> () r( 1l )\\n\\nI\\n\\nGH < 1\\n\\n= 0 if\\n\\nGH\\n\\n= 1 =>\\n\\n() in)\\n\\n= (),( 11 )\\n\\n=> 0 < GH < 2 for stability\\n\\n(\\n\\na)\\n\\n(3)\\n\\n( b)\\n\\nare rarely perfectly matched. Equations (3b) shows that stability is assured for O<GH< 2.\\nFigure 3 shows a plot of eye motion versus updates for various choices of GH. At each\\nupdate, the retinal motion is computed. Figure 3(a) shows the eye\\'s motion at the on-set\\nof smooth pursuit. For GH = 1, the eye movement tracks the target\\'s motion exactly,\\nand lags slightly only when the target accelerates. On the other hand, if GH? I, the\\neye\\'s motion always lags the target\\'s. If GH -> 2, the system becomes increasing\\nunstable, but converges for GH < 2. The three cases presented correspond to the smooth\\npursuit system being critically, over and under damped, respectively.\\n\\n3 HARDWARE IMPLEMENTATION\\nUsing the smooth pursuit mechanism described, a single chip one dimensional tracking\\nsystem has been implemented. The chip has a multi-layered computational architecture,\\nsimilar to the primate\\'s visual system. Phototransduction, logarithmic compression,\\nedge detection, motion detection and smooth pursuit control has been integrated at the\\nfocal-plane. The computational layers can be partitioned into three blocks, where each\\nblock is based on a segment of biological oculomotor systems.\\n\\n3.1\\n\\nIMAGING AND PREPROCESSING\\n\\nThe first three layers of the system mimics the photoreceptors, horizontal cells arx:l\\nbipolar cells of biological retinas. Similar to previous implementations of silicon\\nretinas, the chip uses parasitic bipolar transistors as the photoreceptors. The dynamic\\nrange of photoreceptor current is compressed with a logarithmic response in low light arx:l\\nsquare root response in bright light. The range compress circuit represents 5-6 orders of\\nmagnitude of light intensity with 3 orders of magnitude of output current dynamic range.\\nSubsequently, a passive resistive network is used to realize a discrete implementation of a\\nLaplacian edge detector. Similar to the rods and cones system in primate retinas, the\\nresponse time, hence the maximum detectable target speed, is ambient intensity dependent\\n(160 (12.5) Ils in 2.5 (250) IlW/cm2). However, this does prevent the system from\\nhandling fast targets even in dim ambient lighting.\\n\\n\\x0cVLSI Model of Primate Visual Smooth Pursuit\\n\\n~\\n\\ng\\n\\nu\\n>\\n\\n709\\n\\n20\\n\\n20\\n\\n15\\n\\n15\\n\\n10\\n\\n10\\n\\n5\\n\\n~\\n\\n5\\n\\n0\\n\\n]\\n\\n-5\\n\\n>\" -5\\n\\n- 10\\n\\n?\\n\\n? 10\\n\\nTarget\\n\\n- -Eye: GH=I 99\\n- E ye GH=IOO\\n__ . Eye: GH=O_IO\\n\\n-15\\n\\n0\\n\\n? 15\\n-20\\n\\n-20\\n100\\n\\n50\\n\\n0\\n\\n150\\n\\n500\\n\\n600\\n\\nUpdates\\n\\n(a)\\n\\n700\\n800\\nUpdates\\n\\n900\\n\\n1000\\n\\n(b)\\n\\nFigure 3: (a) The On-Set of Smooth Pursuit for Various GH Values.\\n(b) Steady-State Smooth Pursuit.\\n\\n3.2\\n\\nMOTION MEASUREMENT\\n\\nThis computational layer measures retinal motion. The motion detection technique\\nimplemented here differs from those believed to exist in areas V 1 and MT of the primate\\nvisual cortex. Alternatively, it resembles the fly\\'s and rabbit\\'s retinal motion detection\\nsystem (Reichardt, 1961; Barlow and Levick, 1965; Delbruck, 1993). This is not\\ncoincidental, since efficient motion detection at the focal plane must be performed in a\\nsmall areas and using simple computational elements in both systems.\\nThe motion detection scheme is a combination of local correlation for direction\\ndetermination, and pixel transfer time measurement for speed. In this framework, motion\\nis defined as the disappearance of an object, represented as the zero-crossings of its edges,\\nat a pixel , followed by its re-appearance at a neighboring pixel. The (dis)appearance of\\nthe zero-crossing is determined using the (negative) positive temporal derivative at the\\npixel. Hence, motion is detected by AND gating the positive derivative of the zerocrossing of the edge at one pixel with the negative derivative at a neighboring pixel. The\\ndirection of motion is given by the neighboring pixel from which the edge disappeared.\\nProvided that motion has been detected at a pixel, the transfer time of the edge over the\\npixel\\'s finite geometry is inversely proportional to its speed.\\nEquation (4) gives the mathematical representation of the motion detection process for an\\nobject moving in +x direction. In the equation. f,(l.\\'k ,y.t) is the temporal response of\\npixel k as the zero crossing of an edge of an object passes over its 2a aperture. Equation\\n(4) gives the direction of motion, while equation (5) gives the speed. The schematic of\\n\\nmotion _ x = [\\n\\nf f,( l: k, y, t) > 0] [ f f t(l.\\' k + J, y, t) < 0] =0\\n\\nmotion+x=[~f,(l.\\'k-J,y,t)<O][~f/l.\\'k , y,t?O]\\n\\n= 8[t\\nMotion.\\' t m =\\n\\nSpeed + x\\n\\n=\\n\\nt\\n\\n-\\n\\n(b)\\n\\n(4)\\n\\n2a(k-n)-a\\nv\\n]8[x - 2ak]\\nx\\n\\n2a(k -n) -a\\nvx\\n\\nJ\\n- t\\n\\n( a)\\n\\nvx\\n\\n2a\\n\\nDisappear .\\' t d\\n\\n2a(k -n) +a\\n\\n= --~--?\\nvx\\n\\n(5)\\n\\nd\\nm\\nthe VLSI circuit of the motion detection model is shown in figure 4(a). Figure 4(b)\\nshows reciprocal of the measured motion pulse-width for 1 D motion. The on-chip speed,\\net, is the projected target speed. The measured pulse-widths span 3-4 orders magnitude,\\n\\n\\x0c710\\n\\nR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\nOne-Over Pulse-Width vs On-Chip Speed\\n\\n?\\n\\nO.R\\n\\n~\\n\\n0.4\\n\\n\"\\n\\n~ -0.0 +--------::II~-----__+\\nM\\n~ -0.4\\n\\n---e-- \\\\IPW_Lefi\\n\\n-0 .8\\n\\n- - . - - IIPW_ Rlght\\n\\n- 1.2 +-\\'----\\'--\\'\\'-+--\\'--\\'--\\'--t---\\'--\\'\\'--\\'-+-\\'--\\'--\\'-t-\\'--\\'--\\'-t---\\'--\\'--\\'-+\\n-40\\n00\\n4.0\\n8.0\\n12.0\\n-12.0\\n-R.O\\nOn-Chip Speed rcml~J\\n\\nRight\\n\\nLeft\\n\\n(b)\\n\\n(a)\\n\\nFigure 4: (a) Schematic of the Motion Detection Circuit.\\nMeasured Output of the Motion Detection Circuit.\\n\\n(b)\\n\\ndepending on the ambient lighting, and show less than 15% variation between chips,\\npixels, and directions (Etienne-Cummings, 1993).\\n\\n3.3\\n\\nTHE SMOOTH PURSUIT CONTROL SYSTEM\\n\\nThe one dimensional smooth pursuit system is implemented using a 9 x I array of\\nmotion detectors. Figure 5 shows the organization of the smooth pursuit chip. In this\\nsystem, only diverging motion is computed to reduce the size of each pixel. The outputs\\nof the motion detectors are grouped into one global motion signal per direction. This\\ngrouping is performed with a simple, but delayed, OR, which prevents pulses from\\nneighboring motion cells from overlapping. The motion pulse trains for each direction\\nare XOR gated, which allows a single integrator to be used for both directions, thus\\nlimiting mis-match_ The final value of the integrator is inversely proportional to the\\ntarget\\'s speed. The OR gates conserve the direction of motion. The reciprocal of the\\nintegrator voltage is next computed using the linear mode operation of a MOS transistor\\n(Etienne-Cummings, 1993). The unipolar integrated pulse allows a single inversion\\ncircuit to be used for both directions of motion, again limiting mis-match. The output of\\nthe \"one-over\" circuit is amplified, and the polarity of the measured speed is restored.\\nThis analog voltage is proportional to retinal speed.\\nThe measured retinal speed is subsequently ailed to the stored velocity. Figure 6 shows\\nthe schematic for the retinal velocity accumulation (positive feedback) and storage (analog\\nWave Forms\\n\\nMotion Pulse Integration\\nand \"One-Over\"\\nV = GIRetinal Velocityl\\n\\nPolarity\\nRestoration\\n\\nRetinal Velocity\\nAccumulation\\nand Sample/Hold\\n\\nFigure 5: Architecture of the VLSI Smooth Pursuit System. Sketches\\nof the wave forms for a fast leftward followed by a slow rightward\\nretinal motion are shown.\\n\\n\\x0c711\\n\\nVLSI Model of Primate Visual Smooth Pursuit\\n\\nmemory). The output of the XOR gate in figure 5 is used by the sample-and-hold circuit\\nto control sampling switches S I and S2. During accumulation, the old stored velocity\\nvalue, which is the current eye velocity, is isolated from the summed value. At the\\nfalling edge of the XOR output, the stored value on C2 is replaced by the new value on\\nCl. This stored value is amplified using an off chip motor driver circuit, and used to\\nmove the chip. The gain of the motor driver can be finely controlled for optimal\\noperation.\\n\\nMotor\\n\\nRetinal\\nVelocity\\n\\nSystem\\n\\nAccumulatiun\\n\\nTarget\\nVelocity\\n\\nTwo Phase Sample/Hold\\n\\nFigure 6: Schematic Retinal Velocity Error Accumulation, Storage and\\nMotor Driver Systems.\\nFigure 7(a) shows a plot of one-over the measured integrated voltage as a function of on\\nchip target speed. Due to noise in the integrator circuit, the dynamic range of the motion\\ndetection system is reduced to 2 orders of magnitude. However, the matching between left\\nand right motion is unaffected by the integrator. The MaS \"one-over\" circuit, used to\\ncompute the analog reciprocal of the integrated voltage, exhibits only 0.06% deviation\\nfrom a fitted line (Etienne-Cummings, 1993b). Figure 7(b) shows the measured\\nincrements in stored target velocity as a function of retinal (on-chip) speed. This is a test\\nof all the circuit components of the tracking system. Linearity between retinal velocity\\nincrements and target velocity is observed, however matching between opposite motion\\nhas degraded. This is caused by the polarity restoration circuit since it is the only\\nlocation where different circuits are used for opposite motion. On average, positive\\nincrements are a factor of 1.2 times larger than negative increments. The error bars shows\\nthe variation in velocity increments for different motion cells and different Chips. The\\ndeviation is less than 15 %. The analog memory has a leakage of 10 mV/min and an\\nasymmetric swing of 2 to -1 V, caused by the buffers. The dynamic range of the\\ncomplete smooth pursuit system is measured to be 1.5 orders magnitude. The maximum\\nspeed of the system is adjustable by varying the integrator charging time. The maximum\\nspeed is ambient intensity dependent and ranges from 93 cmls to 7 cm/s on-chip speed in\\nVelocity Error Increment vs On-Chip Speed\\n\\nIntegrated Pulse vs On-Chip Speed\\n1.4\\n24\\n\\n~\\n\\n16\\n\\n~\\n\\n8\\n\\n~\\n\\n0\\n\\nil\\n?\\noS\\n\\n.\\'\\n._\\n\\n1.2\\n\\n~\\n\\nl\\'! 1.0\\n\\n\"e~\\nu\\n\\n-t--------\",/II!...------+\\n\\n-8\\n\\n.s\\n\\nO.R\\n\\ng 0 .6\\n\\nLLl\\n\\n.::;.\\n\\ng 04\\n\\n:: -16\\n-e--lnlPuI~_l..xft\\n\\n-24\\n\\n_ _? _\\n\\nJntPlllo;e_Rl~hl\\n\\n-32 -t-\\'---\\'---\\'-\\'--+-\\'--~~-t--\\'\"-\\'-~_t_--\"--\\'\\'---\\'---\"-t\\n10.0\\n-100\\n-5.0\\n0.0\\n5.0\\nOn-Chip Speed lemlsl\\n\\n(a)\\n\\nOJ\\n\\n>\\n\\n- - - - . Nc~_ Jn c rt~nl\\n\\n02\\n\\n__ ? _ _Po,,_Incremclll\\n\\n0.0\\n0\\n\\n4\\n6\\nOn-Chip Speed lem/s)\\n\\n(b)\\n\\nFigure 7. (a) Measured integrated motion pulse voltage. (b) Measured\\noutput for the complete smooth pursuit system.\\n\\n10\\n\\n\\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\\n\\n712\\n\\nbright (250 JlW/cm 2) and dim (2.5 JlW/cm 2) lighting, respectively. However, for any\\nmaximum speed chosen, the minimum speed is a factor of 0.03 slower. The minimum\\nspeed is limited by the discharge time of the temporal differentiators in the motion\\ndetection circuit to 0.004 cmls on chip. The contrast sensitivity of this system proved to\\nbe the stumbling block, and it can not track objects in normal indoor lighting. However,\\nall circuits components tested successfully when a light source is used as the target.\\nAdditional measured data can be found in (Etienne-Cummings, 1995). Further work will\\nimprove the contrast sensitivity, combat noise and also consider two dimensional\\nimplementations with target acquisition (saccades) capabilities.\\n\\n4\\n\\nCONCLUSION\\n\\nA model for biological and silicon smooth pursuit has been presented. It combines the\\nnegative feed back and positive feedback models of Robinson and Wyatt and Pola. The\\nsmooth pursuit system is stable if the gain product of the retinal velocity detection\\nsystem and the eye movement system is less than 2. VLSI implementation of this\\nsystem has been performed and tested. The performance of the system suggests that wide\\nrange (92.9 - 0.004 cmls retinal speed) target tracking is possible with a single chip focal\\nplane system. To improve this chip\\'s performance, care must be taken to limit noise,\\nimprove matching and increase contrast sensitivity. Future design should also include a\\nsaccadic component to re-capture escaped targets, similar to biological systems.\\n\\nReferences\\nC. Bandera, \"Foveal Machine Vision Systems\", Ph.D. Thesis, SUNY Buffalo, New\\nYork, ]990\\nH. Barlow and W. Levick, \\'The Mechanism for Directional Selective Units in Rabbit\\' s\\nRetina\", Journal of Physiology, Vol. 178, pp. 477-504, ]965\\nT. Delbruck, \"Silicon Retina with Correlation-Based, Velocity-Tuned Pixels \", IEEE\\nTransactions on Neural Networks, Vol. 4:3, pp. 529-41, 1993\\n\\nM. Eckert and G. Buchsbaum, \"Effect of Tracking Strategies on the Velocity Structure of\\nTwo-Dimensional Image Sequences\", J. Opt. Soc. Am., Vol. AIO:7, pp. 1582-85, 1993\\nR. Etienne-Cummings et at., \"A New Temporal Domain Optical Flow Measurement\\nTechnique for Focal Plane VLSI Implementation\", Proceedings of CAMP 93, M.\\nBayoumi, L. Davis and K. Valavanis (Eds.), pp. 24]-25] , 1993\\nR. Etienne-Cummings, R. Hathaway and J. Van der Spiegel, \"An Accurate and Simple\\nCMOS \\'One-Over\\' Circuit\", Electronic Letters, Vol. 29-18, pp. ]618-]620, 1993b\\nR. Etienne-Cummings et aI., \"Real-Time Visual Target Tracking: Two Implementations\\nof Velocity Based Smooth Pursuit\", Visual Information Processing IV, SPIE Vol. 2488,\\nOrlando, 17-18 April 1995\\n\\nW. Reichardt, \"Autocorrelation, A Principle for the Evaluation of Sensory Information by\\nthe Central Nervous System\", Sensory Communication, Wiley, New York, 1961\\nD. Robinson, \"The Mechanism of Human Smooth Pursuit Eye Movement\", Journal of\\nPhysiology ( London) Vol. 180, pp. 569-591 , 1965\\nM. Steinbach, \"Pursuing the Perceptual Rather than the Retinal Stimuli\", Vision\\nResearch, Vol. 16, pp. 1371-1376,1976\\nH. Wyatt and J. Pola, \"The Role of Perceived Motion in Smooth Pursuit Eye\\nMovements\", Vision Research, Vol. 19, pp. 613-618, 1979\\n\\n\\x0c',\n",
       "     'pdf_name': '1032-vlsi-model-of-primate-visual-smooth-pursuit.pdf',\n",
       "     'title': 'VLSI Model of Primate Visual Smooth Pursuit',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1033',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1033',\n",
       "     'paper_text': 'Gradient and Hamiltonian Dynamics\\nApplied to Learning in Neural Networks\\nJames W. Howse\\n\\nChaouki T. Abdallah\\n\\nGregory L. Heileman\\n\\nDepartment of Electrical and Computer Engineering\\nUniversity of New Mexico\\nAlbuquerque, NM 87131\\n\\nAbstract\\nThe process of machine learning can be considered in two stages: model\\nselection and parameter estimation. In this paper a technique is presented\\nfor constructing dynamical systems with desired qualitative properties. The\\napproach is based on the fact that an n-dimensional nonlinear dynamical\\nsystem can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and\\nHamiltonian portions appropriately so that a certain behavior is obtainable.\\nTo estimate the parameters, a stably convergent learning rule is presented.\\nThis algorithm has been proven to converge to the desired system trajectory\\nfor all initial conditions and system inputs. This technique can be used to\\ndesign neural network models which are guaranteed to solve the trajectory\\nlearning problem.\\n\\n1\\n\\nIntroduction\\n\\nA fundamental problem in mathematical systems theory is the identification of dynamical systems. System identification is a dynamic analogue of the functional approximation problem. A set of input-output pairs {u(t), y(t)} is given over some time\\ninterval t E [7i, 1j]. The problem is to find a model which for the given input sequence\\nreturns an approximation of the given output sequence. Broadly speaking, solving an\\nidentification problem involves two steps. The first is choosing a class of identification models which are capable of emulating the behavior of the actual system. The\\nsecond is selecting a method to determine which member of this class of models best\\nemulates the actual system. In this paper we present a class of nonlinear models and\\na learning algorithm for these models which are guaranteed to learn the trajectories\\nof an example system. Algorithms to learn given trajectories of a continuous time\\nsystem have been proposed in [6], [8], and [7] to name only a few. To our knowledge,\\nno one has ever proven that the error between the learned and desired trajectories\\nvanishes for any of these algorithms. In our trajectory learning system this error is\\nguaranteed to vanish. Our models extend the work in [1] by showing that Cohen\\'s\\nsystems are one instance of the class of models generated by decomposing the dynamics into a component normal to some surface and a set of components tangent to the\\nsame surface. Conceptually this formalism can be used to design dynamical systems\\nwith a variety of desired qualitative properties. Furthermore, we propose a provably\\nconvergent learning algorithm which allows the parameters of Cohen\\'s models to be\\nlearned from examples rather than being programmed in advance. The algorithm is\\n\\n\\x0c275\\n\\nGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\nconvergent in the sense that the error between the model trajectories and the desired trajectories is guaranteed to vanish. This learning procedure is related to one\\ndiscussed in [5] for use in linear system identification.\\n\\n2\\n\\nConstructing the Model\\n\\nFirst some terminology will be defined. For a system of n first order ordinary differential equations, the phase space of the system is the n-dimensional space of all state\\ncomponents. A solution trajectory is a curve in phase space described by the differential equations for one specific starting point. At every point on a trajectory there\\nexists a tangent vector. The space of all such tangent vectors for all possible solution\\ntrajectories constitutes the vector field for this system of differential equations.\\nThe trajectory learning models in this paper are systems of first order ordinary differential equations. The form of these equations will be obtained by considering the\\nsystem dynamics as motion relative to some surface. At each point in the state space\\nan arbitrary system trajectory will be decomposed into a component normal to this\\nsurface and a set of components tangent to this surface. This approach was suggested\\nto us by the results in [4], where it is shown that an arbitrary n-dimensional vector\\nfield can be decomposed locally into the sum of one gradient vector field and (n - 1)\\nHamiltonian vector fields. The concept of a potential function will be used to define these surfaces. A potential function V(:z:) is any scalar valued function of the\\nsystem states :z: = [Xl, X2, ??? , Xn.] t which is at least twice continuously differentiable\\n(Le. V(:z:) E or : r ~ 2). The operation [.]t denotes the transpose of the vector. If\\nthere are n components in the system state, the function V{:z:), when plotted with\\nrespect all of the state components, defines a surface in an (n + 1)-dimensional space.\\nThere are two curves passing through every point on this potential surface which are\\nof interest in this discussion, they are illustrated in Figure 1(a). The dashed curve is\\n(z - zo)t \\\\7 ... v (z)l ...o = 0\\n\\n(a)\\n\\n(b)\\n\\nV(z) = K-\\n\\nFigure 1: (a) The potential function V(z) = X~ (Xl _1)2 +x~ plotted versus its two dependent variables Xl and X2. The dashed curve is called a level surface and is given\\nby V(z) = 0.5. The solid curve follows the path of steepest descent through Zo.\\n(b) The partitioning of a 3-dimensional vector field at the point Zo into a 1dimensional portion which is normal to the surface V(z) = K- and a 2-dimensional\\nportion which is tangent to V(z) = K-. The vector -\\\\7 ... V(z) 1\"\\'0 is the normal vector to the surface V(z) = K- at the point Zo. The plane (z - zo)t \\\\7 ... V (z) 1\"\\'0 = 0\\ncontains all of the vectors which are tangent to V(z) = K- at Zo. Two linearly\\nindependent vectors are needed to form a basis for this tangent space, the pair\\nQ2(z) \\\\7 ... V (z)l ... o and Q3(Z) \\\\7 ... V (z)l ... o that are shown are just one possibility.\\nreferred to as a level surface, it is a surface along which V(:z:) = K for some constant\\nK. Note that in general this level surface is an n-dimensional object. The solid curve\\n\\n\\x0c276\\n\\nJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\nmoves downhill along V (X) following the path of steepest descent through the point\\nXo. The vector which is tangent to this curve at Xo is normal to the level surface\\nat Xo. The system dynamics will be designed as motion relative to the level surfaces\\nof V(x). The results in [4] require n different local potential functions to achieve\\narbitrary dynamics. However, the results in [1] suggest that a considerable number\\nof dynamical systems can be achieved using only a single global potential function.\\nA system which is capable of traversing any downhill path along a given potential\\nsurface V(x), can be constructed by decomposing each element of the vector field\\ninto a vector normal to the level surface of V(x) which passes through each point\\nand a set of vectors tangent to the level surface of V(x) which passes through the\\nsame point. So the potential function V(x) is used to partition the n-dimensional\\nphase space into two subspaces. The first contains a vector field normal to some\\nlevel surface V(x) = }( for }( E IR, while the second subspace holds a vector field\\ntangent to V(x) = IC. The subspace containing all possible normal vectors to the\\nn-dimensional level surface at a given point, has dimension one. This is equivalent\\nto the statement that every point on a smooth surface has a unique normal vector.\\nSimilarly, the subspace containing all possible tangent vectors to the level surface at\\na given point has dimension (n - 1). An example of this partition in the case of a\\n3-dimensional system is shown in Figure 1(b). Since the space of all tangent vectors\\nat each point on a level surface is (n - I)-dimensional, (n - 1) linearly independent\\nvectors are required to form a basis for this space.\\nMathematically, there is a straightforward way to construct dynamical systems which\\neither move downhill along V(x) or remain at a constant height on V(x). In this\\npaper, dynamical systems which always move downhill along some potential surface\\nare called gradient-like systems. These systems are defined by differential equations\\nof the form\\nx = -P(x) VII:V(x),\\n(1)\\nwhere P(x) is a matrix function which is symmetric (Le. pt = P) and positive\\n:z~]f. These systems\\ndefinite at every point x, and where VIII V(x) =\\nare similar to the gradient flows discussed in [2]. The trajectories of the system\\nformed by Equation (1) always move downhill along the potential surface defined by\\nV(x). This can be shown by taking the time derivative of V(x) which is V(x) =\\n-[VII: V (x)]t P(x) [VII: V(x)] :5 O. Because P(x) is positive definite, V(x) can only be\\nzero where V II: V (x) = 0, elsewhere V(x) is negative. This means that the trajectories\\nof Equation (1) always move toward a level surface of V(x) formed by \"slicing\" V(x)\\nat a lower height, as pointed out in [2]. It is also easy to design systems which remain\\nat a constant height on V(x). Such systems will be denoted Hamiltonian-like systems.\\nThey are specified by the equation\\nx = Q(x) VII: V(x),\\n(2)\\nwhere Q(x) is a matrix function which is skew-symmetric (Le. Qt = -Q) at every\\npoint x. These systems are similar to the Hamiltonian systems defined in [2]. The\\nelements of the vector field defined by Equation (2) are always tangent to some level\\nsurface of V (x). Hence the trajectories ofthis system remain at a constant height on\\nthe potential surface given by V(x). Again this is indicated by the time derivative\\nof V(x), which in this case is V(x) = [VII: V(x)]f Q(x)[VII: V(x)] = o. This indicates\\nthat the trajectories of Equation (2) always remain on the level surface on which the\\nsystem starts. So a model which can follow an arbitrary downhill path along the\\npotential surface V(x) can be designed by combining the dynamics of Equations (1)\\nand (2) . The dynamics in the subspace normal to the level surfaces of V(x) can be\\n\\n[g;: , g;: ,... ,\\n\\n\\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\n277\\n\\ndefined using one equation of the form in Equation (1). Similarly the dynamics in the\\nsubspace tangent to the level surfaces of Vex) can be defined using (n - 1) equations\\nof the form in Equation (2). Hence the total dynamics for the model are\\nn\\n\\nz= -P(x)VIDV(x) + LQi(X)VIDV(x).\\n\\n(3)\\n\\ni=2\\n\\nFor this model the number and location of equilibria is determined by the function\\nVex), while the manner in which the equilibria are approached is determined by the\\nmatrices P(x) and Qi(x).\\nIf the potential function Vex) is bounded below (i.e. Vex) > Bl V x E IRn , where\\nBl is a constant), eventually increasing (i.e. limlllDlI-+oo Vex) ~ 00) , and has only\\na finite number of isolated local maxima and minima (i.e. in some neighborhood\\nof every point where V III V (x) = 0 there are no other points where the gradient\\nvanishes), then the system in Equation (3) satisfies the conditions of Theorem 10\\nin [1]. Therefore the system will converge to one of the points where V ID Vex) = 0,\\ncalled the critical points of Vex), for all initial conditions. Note that this system\\nis capable of all downhill trajectories along the potential surface only if the (n - 1)\\nvectors Qi(X) V ID Vex) V i = 2, ... , n are linearly independent at every point x. It\\nis shown in [1] that the potential function\\n\\nV(z) = C (\\n\\n1:., (-y) d-y +\\n\\nt, [~\\n\\n(XI - I:.,(xd)\\'\\n\\n+~\\n\\nJ:\\'\\n\\n1:., h )II:.: (-y)]\\' d-y\\n\\n1\\n\\n(4)\\n\\nsatisfies these three criteria. In this equation ?.i(Xt} Vi = 1, ... , n are interpolation\\npolynomials, C is a real positive constant, Xi Vi = 1, ... , n are real constants chosen\\nso that the integrals are positive valued, and ?.Hxt} ==\\n\\nf:-.\\n\\n3\\n\\nThe Learning Rule\\n\\nIn Equation (3) the number and location of equilibria can be controlled using the\\npotential function Vex), while the manner in which the equilibria are approached can\\nbe controlled with the matrices P(x) and Qi(X). If it is assumed that the locations\\nof the equilibria are known, then a potential function which has local minima and\\nmaxima at these points can be constructed using Equation (4). The problem of\\ntrajectory learning is thereby reduced to the problem of parameterizing the matrices\\nP(x) and Qi(x) and finding the parameter values which cause this model to best\\nemulate the actual system. If the elements P(x) and Qi(x) are correctly chosen,\\nthen a learning rule can be designed which makes the model dynamics converge to\\nthat of the actual system. Assume that the dynamics given by Equation (3) are a\\nparameterized model of the actual dynamics. Using this model and samples of the\\nactual system states, an estimator for states of the actual system can be designed. The\\nbehavior of the model is altered by changing its parameters, so a parameter estimator\\nmust also be constructed. The following theorem provides a form for both the state\\nand parameter estimators which guarantees convergence to a set of parameters for\\nwhich the error between the estimated and target trajectories vanishes.\\nTheorem 3.1. Given the model system\\nk\\n\\nZ = LAili(x) +Bg(u)\\n\\n(5)\\n\\ni=l\\n\\nwhere Ai E IRnxn and BE IRnxm are unknown, and li(\\') and g(.) are known smooth\\nfunctions such that the system has bounded solutions for bounded inputs u(t). Choose\\n\\n\\x0cJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\n278\\n\\na state estimator of the form\\nk\\n\\n~ = \\'R. B (x - x) +\\n\\nL Ai fi(x) + iJ g(u)\\n\\n(6)\\n\\ni=1\\n\\nwhere\\'R. B is an (n x n) matrix of real constants whose eigenvalues must all be in the\\nleft half plane, and Ai and iJ are the estimates of the actual parameters. Choose\\nparameter estimators of the form\\n~\\nt\\nAi = -\\'R.p (x - x) [fi(x)] V i = 1, ... , k\\n(7)\\n= -\\'R.p (x - x) [g(u)]t\\n\\nB\\n\\nwhere \\'R. p is an (n x n) matrix of real constants which is symmetric and positive\\ndefinite, and (x - x) [.]t denotes an outer product. For these choices of state and\\nparameter estimators limt~oo(x(t) -x(t? = 0 for all initial conditions. Furthermore,\\nthis remains true if any of the elements of Ai or iJ are set to 0, or if any of these\\nmatrices are restricted to being symmetric or skew-symmetric.\\nThe proof of this theorem appears in [3]. Note that convergence of the parameter\\nestimates to the actual parameter values is not guaranteed by this theorem. The\\nmodel dynamics in Equation (3) can be cast in the form of Equation (5) by choosing\\neach element of P(x) and Qi(X) to have the form\\nI-I\\n\\nn\\n\\nn\\n\\nI-I\\n\\n= LL~rBjkt?k(Xj)\\n\\nand\\nQrB = LLArBjk ek(Xj),\\n(8)\\nj=1 k=O\\nj=1 k=O\\nwhere {t?o(Xj), t?1 (Xj), ... ,t?I-1 (Xj)} and {eo(Xj), el (Xj), ... ,el-l (Xj)} are a set of 1\\northogonal polynomials which depend on the state Xj\\' There is a set of such polynomials for every state Xj, j = 1,2, ... , n. The constants ~rBjk and ArBjk determine\\nthe contribution of the kth polynomial which depends on the jth state to the value\\nof Prs and Qrs respectively. In this case the dynamics in Equation (3) become\\nPrB\\n\\n:i:\\n\\n=\\n\\nt. ~ {\\n\\nS;. [11.(x;) V. V (z)j\\n\\n+\\n\\nt,\\n\\nA;;. [e;.(x;)\\n\\nv. V(z)j } + T g(u(t))\\n\\n(9)\\n\\nwhere 8 jk is the (n x n) matrix of all values ~rsjk which have the same value of j and\\nk. Likewise A ijk is the (n x n) matrix of all values Arsjk, having the same value of\\nj and k, which are associated with the ith matrix Qi(X). This system has m inputs,\\nwhich may explicitly depend on time, that are represented by the m-element vector\\nfunction u(t). The m-element vector function g(.) is a smooth, possibly nonlinear,\\ntransformation of the input function. The matrix Y is an (n x m) parameter matrix\\nwhich determines how much of input S E {I, ... , m} effects state r E {I, ... , n}.\\nAppropriate state and parameter estimators can be designed based on Equations (6)\\nand (7) respectively.\\n\\n4\\n\\nSimulation Results\\n\\nNow an example is presented in which the parameters of the model in Equation (9)\\nare trained, using the learning rule in Equations (6) and (7), on one input signal and\\nthen are tested on a different input signal. The actual system has three equilibrium\\npoints, two stable points located at (1,3) and (3,5), and a saddle point located at\\n(2 - ~,4 + ~). In this example the dynamics of both the actual system and the\\nmodel are given by\\n\\n(~1) =\\nZ2\\n\\nZ~\\n\\nZ~\\n\\nO\\n\\n(1\\'1 + 1\\'2\\n+:3\\n2)\\n0 1\\'4 + 1\\'5 Z1 + 1\\'6 Z2\\n\\n(:~)\\n+ (0 - {1\\'7 + 1\\'8 Z1 + 1\\'9 Z2}) (:~ ) + (1\\'10) u(t)\\n8Y\\n\\'P7 + \\'P8 ZI + 1\\'9 Z2\\n8Y\\n0\\n\\n8Z2\\n\\n0\\n\\n8Z2\\n\\n(10)\\n\\n\\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\\n\\n279\\n\\nwhere V(x) is defined in Equation (4) and u(t) is a time varying input. For the actual\\nsystem the parameter values were \\'PI = \\'P4 = -4, \\'P2 = \\'Ps = -2, \\'P3 = \\'P6 = -1,\\n\\'P7 = 1, \\'Ps = 3, \\'P9 = 5, and \\'PIO = 1. In the model the 10 elements \\'Pi are\\ntreated as the unknown parameters which must be learned. Note that the first matrix\\nfunction is positive definite if the parameters \\'PI-\\'P6 are all negative valued. The\\nsecond matrix function is skew-symmetric for all values of \\'P7-\\'P9. The two input\\nsignals used for training and testing were Ul = 10000 (sin! 1000t + sin ~ 1000t) and\\nU2 = 5000 sin 1000 t. The phase space responses of the actual system to the inputs UI\\nand U2 are shown by the solid curves in Figures 3(b) and 3(a) respectively. Notice that\\nboth of these inputs produce a periodic attractor in the phase space of Equation (10).\\nIn order to evaluate the effectiveness of the learning algorithm the Euclidean distance\\nbetween the actual and learned state and parameter values was computed and plotted\\nversus time. The results are shown in Figure 2. Figure 2(a) shows these statistics when\\n{1I~zll, II~\\'PII}\\n\\n{1I~zll, II~\\'PII}\\n\\n17.5\\n15\\n15\\n12.5\\n12.5\\n10\\n\\n7.5\\n\\ni\\n\\n----\\n\\n,., ~--.----... ... .......\\n\\n- --\\n\\n2.5\\n\\n150\\n200\\n250\\n300 t\\n50\\n100\\n150\\n200\\n250\\n300 t\\n(a)\\n(b)\\nFigure 2: (a) The state and parameter errors for training using input signal Ut. The solid\\ncurve is the Euclidean distance between the state estimates and the actual states\\nas a function of time. The dashed curve shows the distance between the estimated\\nand actual parameter values versus time.\\n(b) The state and parameter errors for training using input signal U2.\\n50\\n\\n100\\n\\ntraining with input UI, while Figure 2(b) shows the same statistics for input U2. The\\nsolid curves are the Euclidean distance between the learned and actual system states,\\nand the dashed curves are the distance between the learned and actual parameter\\nvalues. These statistics have two noteworthy features. First, the error between the\\nlearned and desired states quickly converges to very small values, regardless of how\\nwell the actual parameters are learned. This result was guaranteed by Theorem 3.1.\\nSecond, the final error between the learned and desired parameters is much lower when\\nthe system is trained with input UI. Intuitively this is because input Ul excites more\\nfrequency modes of the system than input U2. Recall that in a nonlinear system the\\nfrequency modes excited by a given input do not depend solely on the input because\\nthe system can generate frequencies not present in the input. The quality of the\\nlearned parameters can be qualitatively judged by comparing the phase plots using\\nthe learned and actual parameters for each input, as shown in Figure 3. In Figure 3(a)\\nthe system was trained using input Ul and tested with input U2, while in Figure 3(b)\\nthe situation was reversed. The solid curves are the system response using the actual\\nparameter values, and the dashed curves are the response for the learned parameters.\\nThe Euclidean distance between the target and test trajectories in Figure 3(a) is in\\nthe range (0,0.64) with a mean distance of 0.21 and a standard deviation of 0.14. The\\ndistance between the the target and test trajectories in Figure 3(b) is in the range\\n(0,4.53) with a mean distance of 0.98 and a standard deviation of 1.35. Qualitatively,\\nboth sets of learned parameters give an accurate response for non-training inputs.\\n\\n\\x0c280\\n\\n1. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\\n\\n5\\nI\\n\\no\\n\\n{i\\n\\n-------r-- -- ----- --- -- I\\n\\n-5\\n\\n-10\\n\\n-15\\n\\n-l\\n\\n-1\\n\\n1\\n\\n-2\\n\\n-1\\n\\n4\\n\\nXl\\n\\n(a)\\n(b)\\nFigure 3: (a) A phase plot of the system response when trained with input UI and tested\\nwith input U2. The solid line is the response to the test input using the actual\\nparameters. The dotted line is the system response using the learned parameters.\\n(b) A phase plot of the system response when trained with input U2 and tested\\nwith input UI.\\n\\nNote that even when the error between the learned and actual parameters is large,\\nthe periodic attractor resulting from the learned parameters appears to have the same\\n\"shape\" as that for the actual parameters.\\n\\n5\\n\\nConclusion\\n\\nWe have presented a conceptual framework for designing dynamical systems with\\nspecific qualitative properties by decomposing the dynamics into a component normal\\nto some surface and a set of components tangent to the same surface. We have\\npresented a specific instance of this class of systems which converges to one of a finite\\nnumber of equilibrium points. By parameterizing these systems, the manner in which\\nthese equilibrium points are approached can be fitted to an arbitrary data set. We\\npresent a learning algorithm to estimate these parameters which is guaranteed to\\nconverge to a set of parameter values for which the error between the learned and\\ndesired trajectories vanishes.\\n\\nAcknowledgments\\nThis research was supported by a grant from Boeing Computer Services under Contract\\nW-300445. The authors would like to thank Vangelis Coutsias, Tom Caudell, and Bill\\nHome for stimulating discussions and insightful suggestions.\\n\\nReferences\\n[1] M.A. Cohen. The construction of arbitrary stable dynamics in nonlinear neural networks.\\nNeural Networks, 5(1):83-103, 1992.\\n[2] M.W. Hirsch and S. Smale. Differential equations, dynamical systems, and linear algebra,\\nvolume 60 of Pure and Applied Mathematics. Academic Press, Inc., San Diego, CA, 1974.\\n[3] J.W. Howse, C.T. Abdallah, and G.L. Heileman. A gradient-hamiltonian decomposition\\nfor designing and learning dynamical systems. Submitted to Neural Computation, 1995.\\n[4] R.V. Mendes and J .T. Duarte. Decomposition of vector fields and mixed dynamics.\\nJournal of Mathematical Physics, 22(7):1420-1422, 1981.\\n[5] K.S. Narendra and A.M. Annaswamy. Stable adaptitJe systems. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1989.\\n[6] B.A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural\\nComputation, 1(2):263-269, 1989.\\n[7] D. Saad. Training recurrent neural networks via trajectory modification. Complex Systems, 6(2) :213-236, 1992.\\n[8] M.-A. Sato. A real time learning algorithm for recurrent analog neural networks. Biological Cybernetics, 62(2):237-241, 1990.\\n\\n\\x0c',\n",
       "     'pdf_name': '1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf',\n",
       "     'title': 'Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1035',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1035',\n",
       "     'paper_text': 'A Dynamical Model of Context Dependencies for the\\nVestibulo-Ocular Reflex\\nTerrence J. Sejnowskit\\n\\nOlivier J.M.D. Coenen*\\n\\nComputational Neurobiology Laboratory\\nHoward Hughes Medical Institute\\nThe Salk Institute for Biological Studies\\n10010 North Torrey Pines Road\\nLa Jolla, CA 92037, U.S.A.\\nDepartments oftBiology and *tPhysics\\nUniversity of California, San Diego\\nLa Jolla, CA 92093, U.S.A\\n\\n{olivier,terry}@salk.edu\\n\\nAbstract\\nThe vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid\\nhead motions. The gain of the VOR (the ratio of eye to head rotation velocity)\\nis typically around -1 when the eyes are focused on a distant target. However, to\\nstabilize images accurately, the VOR gain must vary with context (eye position,\\neye vergence and head translation). We first describe a kinematic model of the\\nVOR which relies solely on sensory information available from the semicircular\\ncanals (head rotation), the otoliths (head translation), and neural correlates of eye\\nposition and vergence angle. We then propose a dynamical model and compare it\\nto the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and\\nsuggests one way to combine the required neural signals within the cerebellum and\\nthe brain stem. It also makes predictions for the responses of neurons to multiple\\ninputs (head rotation and translation, eye position, etc.) in the oculomotor system.\\n\\n1 Introduction\\nThe VOR stabilizes images on the retina during rapid head motions: Rotations and translations of\\nthe head in three dimensions must be compensated by appropriate rotations of the eye. Because the\\nhead\\'s rotation axis is not the same as the eye\\'s rotation axis, the calculations for proper image stabilization of an object must take into account diverse variables such as object distance from each eye,\\n\\n\\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\\n\\n90\\n\\ngaze direction, and head translation (Viire et al., 1986). The stabilization is achieved by integrating\\ninfonnation from different sources: head rotations from the semicircular canals of the inner ear, head\\ntranslations from the otolith organs, eye positions, viewing distance, as well as other context infonnation, such as posture (head tilts) or activity (walking, running) (Snyder and King, 1992; Shelhamer\\net al.,1992; Grossman et al., 1989). In this paper we concentrate on the context modulation of the\\nVOR which can be described by the kinematics of the reflex, i.e. eye position, eye vergence and\\nhead translation.\\n\\n2\\n\\nThe Vestibulo-Ocular Reflex: Kinematic Model\\nDefinition of Vectors\\n\\nTarget Object\\n\\nCoordinate System\\n\\nGaze Vector\\n\\nGaze Angle\\nInterocular\\nDistance\\nEye position\\nVector\\n\\nRotation Axis\\n\\nSemicircular\\nCanals and\\nOtoliths\\nHead\\nTop View\\n\\n?\\n\\n~_--+_\\n\\nOrigin of coordinate\\nsyste,,-, (arbitrary)\\n\\nFigure 1: Diagram showing the definition of the vectors used in the equation of the kinematic model of the\\nvestibulo-ocular reflex.\\n\\nThe ideal VOR response is a compensatory eye movement which keeps the image fixed on the retina\\nfor any head rotations and translations. We therefore derived an equation for the eye rotation velocity\\nby requiring that a target remains stationary on the retina. The velocity of the resulting compensatory\\neye rotation can be written as (see fig. 1):\\n\\nw= -Oe + 1:1\\n\\nx [Dej x\\n\\nOe - To;]\\n\\n(1)\\n\\nwhere Oe is the head rotation velocity sensed by the semicircular canals, TOj is the head translation\\nvelocity sensed by the otoliths, Dej == (e - OJ), eis a constant vector specifying the location of an\\neye in the head, OJ is the position of either the left or right otolith, fJ and Igl are the unit vector and\\namplitude of the gaze vector: fJ gives the eye position (orientation of the eye relative to the head),\\nand Igl gives the distance from the eye to the object, and the symbol x indicates the cross-product\\nbetween two vectors. wand Oe are rotation vectors which describe the instantaneous angUlar velocity\\nof the eye and head, respectively. A rotation vector lies along the instantaneous axis of rotation;\\nits magnitude indicates the speed of rotation around the axis, and its direction is given by the righthand screw rule. A motion of the head combining rotation (0) and translation (T) is sensed as the\\ncombination of a rotation velocity Oe measured by the semicircular canals and a translation velocity\\nTo sensed by the otoliths. The rotation vectors are equal (0 = Oe), and the translation velocity vector\\nas measured by the otoliths is given by: TOj = OOj x 0 + T, where OOj == (a - OJ), and a is the\\nposition vector of the axis of rotation.\\n\\n\\x0c91\\n\\nA Dynarnical Model of Context Dependencies for the Vestibula-Ocular Reflex\\n\\nThe special case where the gaze is horizontal and the rotation vector is vertical (horizontal head rotation) has been studied extensively in the literature. We used this special case in the sirnulations.\\nIn that case rnay be sirnplify by writing its equation with dot products. Since 9 and\\nare then\\nperpendicular (9 . fie = 0). the first term of the following expression in brackets is zero:\\n\\nw\\n\\nslc\\n\\n(2)\\n\\nThe sernicircular canals decornpose and report acceleration and velocity of head rotation fi by its\\ncornponents along the three canals on each side of the head fie : horizontal. anterior and posterior.\\nThe two otolith organs on each side report the dynamical inertial forces generated during linear rnotion (translation) in two perpendicular plane. one vertical and the other horizontal relative to the head.\\nHere we assurne that a translation velocity signal (To) derived frorn or reported by the otolith afferents is available. The otoliths encode as well the head orientation relative to the gravity vector force.\\nbut was not included in this study.\\nTo cornplete the correspondence between the equation and a neural correlate. we need to determine\\nThe eye position 9 is assurned to be given by the output of the\\na physiological source for 9 and\\nvelocity-to-position transformation or so-called \"neural integrator\" which provides eye position information and which is necessary for the activation of the rnotoneuron to sustain the eye in a fixed\\nposition. The integrator for horizontal eye position appears to be located in the nucleus prepositus\\nhypoglossi in the pons. and the vertical integrator in the rnidbrain interstitial nucleus of Cajal. (Crawford. Cadera and Vilis. 1991; Cannon and Robinson. 1987). We assurne that the eye position is given\\nas the coordinates of the unit vector 9 along the ~ and 1; of fig. 1. The eye position depends on the\\neye velocity according to\\n= 9 x w. For the special case w(t) = w(t)z. i.e. for horizontal head\\nrotation. the eye position coordinates are given by:\\n\\nI!I.\\n\\n\\'*\\n\\n91 (t) =\\n\\n91 (0) + f~ iJ2( r )w( r) dr\\n\\n92(t) =\\n\\n92(0) - f~ 91(r)w(r)dr\\n\\n(3)\\n\\nThis is a set of two negatively coupled integrators. The \"neural integrator\" therefore does not integrate the eye velocity directly but a product of eye position and eye velocity. The distance frorn eye\\nto target\\ncan be written using the gaze angles in the horizontal plane of the head:\\n\\nI!I\\n\\n1\\n\\n(4)\\n\\n1\\n\\n(5)\\n\\nRight eye:\\n\\n19RT\\n\\nLeft eye:\\n\\n19LT\\n\\nwhere ?()R - () L) is the vergence angle. and I is the interocular distance; the angles are rneasured frorn\\na straight ahead gaze. and take on negative values when the eyes are turned towards the right. Within\\nthe oculornotor systern. the vergence angle and speed are encoded by the rnesencephalic reticular\\nformation neurons (Judge and Curnrning. 1986; Mays. 1984). The nucleus reticularis tegrnenti pontis\\nwith reciprocal connections to the flocculus. oculornotor vermis. paravermis of the cerebellurn also\\ncontains neurons which activity varies linearly with vergence angle (Gamlin and Clarke. 1995).\\nWe conclude that it is possible to perform the cornputations needed to obtain an ideal VOR with signals known to be available physiologically.\\n\\n\\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\\n\\n92\\nDynamical Model Overview\\n\\nNod_\\nPftpoIItao\\n\\nIIyposIoooI\\n\\nFigure 2: Anatomical connections considered in the dynamical model. Only the left side is shown, the right\\nside is identical and connected to the left side only for the calculation of vergence angle. The nucleus prepositus\\nhypoglossi and the nucleus reticularis tegmenti pontis are meant to be representative of a class of nuclei in the\\nbrain stem carrying eye position or vergence signal. All connections are known to exist except the connection\\nbetween the prepositus nucleus to the reticularis nucleus which has not been verified. Details of the cerebellum\\nare in fig. 3 and of the vestibular nucleus in fig. 4.\\n\\n3 Dynamical Model\\nSnyder & King (1992) studied the effect of viewing distance and location of the axis of rotation on\\nthe VOR in monkeys; their main results are reproduced in fig. 5. In an attempt to reproduce their\\ndata and to understand how the signals that we have described in section 2 may be combined in time,\\nwe constructed a dynamical model based on the kinematic model. Its basic anatomical structure is\\nshown in fig. 2. Details of the model are shown in fig. 3, and fig . 4 where all constants are written\\nusing a millisecond time scale. The results are presented in fig. 5. The dynamical variables represent\\nthe change of average firing rate from resting level of activity. The firing rate of the afferents has a\\ntonic component proportional to the velocity and a phasic component proportional to the acceleration\\nof movement. Physiologically, the afferents have a wide range of phasic and tonic amplitudes. This\\nis reflected by a wide selection of parameters in the numerators in the boxes of fig. 3 and fig. 4. The\\nLaplace transform of the integration operator in equation (3) of the eye position coordinates is ~.\\nFollowing Robinson (1981), we modeled the neural integrator with a gain and a time constant of\\n20 seconds. We therefore replaced the pure integrator ~ with 20~~~~1 in the calculations of eye\\nposition. The term 1 in fig. 3 is calculated by using equations (4) and (5), and by using the integrator\\n9\\n\\n20~o:!~1 on the eye velocity motor command to find the angles (h and (JR.\\n\\nThe dynamical model is based on the assumption that the cerebellum is required for context modulation, and that because of its architecture, the cerebellum is more likely to implement complex functions of multiple signals than other relevant nuclei. The major contributions of vergence and eye\\nposition modulation on the VOR are therefore mediated by the cerebellum. Smaller and more transient contributions from eye position are assumed to be mediated through the vestibular nucleus as\\nshown in fig. 4. The motivation for combining eye position as in fig . 4 are, first, the evidence for eye\\nresponse oscillations; second, the theoretical consideration that linear movement information (To) is\\nuseless without eye position information for proper VOR.\\nThe parameters in the dynamical model were adjusted by hand after observing the behavior of the different components of the model and noting how these combine to produce the oscillations observed\\n\\n\\x0c93\\n\\nA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\\nVestibular\\nSemicirtular\\n\\nCerebellum\\n\\nc..l\\n\\nO-\\n\\n- - - - t 401+1 r-----?--f--..j\\n\\nx\\n\\n300+1\\n\\nOIolith\\n0Igan\\n\\nVHlibabr\\nNuc1tul\\n\\nFigure 3: Contribution of the cerebellum to the dynamical model. Filtered velocity inputs from the canals and\\notoliths are combined with eye position according to equation (2). These calculations could be performed either\\noutside the cerebellum in one or multiple brain stem nuclei (as shown) or possibly inside the cerebellum. The\\nonly output is to the vestibular nucleus. The Laplace notation is used in each boxes to represent a leaky integrator\\nwith a time constant. input derivative and input gain. The term oe are the coordinates of the vector oe shown\\nin fig. 1. The x indicates a multiplication. The term! multiplies each inputs individually. The open arrows\\nindicate inhibitory (negative) connections.\\nCere... lIum\\n\\nVHlibalu\\n\\nSemicimtlu\\n\\nc.w\\n\\nO--\\'----t~l---+--?----t~~\\n\\nX\\nFigure 4: Contribution of the vestibular nucleus to the dynamical model. Three pathways in the vestibular nucleus process the canal and otolith inputs to drive the eye. The first pathway is modulated by the output of the\\ncerebellum through a FIN (Flocculus Target Neuron). The second and third pathways report transient information from the inputs which are combined with eye position in a manner identical to fig. 3. The location of these\\ncalculations is hypothetical.\\n\\nin the data. Even though the number of parameters in the model is not small. it was not possible to\\nfit any single response in fig. 5 without affecting most of the other eye responses. This puts severe\\nlimits on the set of parameters allowed in the model.\\nThe dynamical model suggests that the oscillations present in the data reflect: 1) important acceleration components in the neural signals. both rotational and linear, 2) different time delays between the\\ncanal and otolith signal processing. and 3) antagonistic or synergistic action of the canal and otolith\\nsignals with different axes of rotation, as described by the two terms in the bracket of equation (2).\\n\\n4 Discussion\\nBy fitting the dynamical model to the data, we tested the hypothesis that the VOR has a response\\nclose to ideal taking into account the time constraints imposed by the sensory inputs and the neural\\nnetworks performing the computations. The vector computations that we used in the model may not\\n\\n\\x0c94\\n\\nO. J. M. D. COENEN, T. J. SEJNOWSKI\\nDynamical Model Responses vs Experimental Data\\n80\\n\\n80\\nLOMtIOftof\\n.... 01 rotMIon\\n\\n-,a.-om\\n\\n.-\\n\\nT..........~\\n\\n60\\n\\n40\\n20\\n\\n~\\nw\\n\\n-20\\n\\n-20\\n\\n-400~----~5~0------~\\n10\\n=0\\n~\\nTime (m.)\\n\\n-40oL-----~\\n5~\\n0 ----~1~\\n0~\\n0-?\\n\\nTime (m.)\\n\\nFigure 5: Comparison between the dynamical model and monkey data. The dotted lines show the effect of\\nviewing distance and location of the axis of rotation on the VOR as recorded by Snyder & King (1992) from\\nmonkeys in the dark. The average eye velocity response (of left and right eye) to a sudden change in head velocity is shown for different target distances (left) and rotational axes (right). On the left, the location of the axis\\nof rotation was in the midsagittal plane 12.5 cm behind the eyes (-12.5 cm), and the target distance was varied\\nbetween 220 cm and 9 cm. On the right, the target di stance was kept constant at 9 cm in front of the eye, and the\\nlocation of the axis of rotation was varied from 14 cm behind t04cm in front of the eyes (-14cm to 4cm) in the\\nmidsagittal plane. The solid lines show the model responses. The model replicates many characteristics of the\\ndata. On the left the model captures the eye velocity fluctuations between 20-50 ms, followed by a decrease and\\nan increase which are both modulated with target distance (50-80 ms). The later phase of the response (80-100\\nms) is almost exact for 220 cm, and one peak is seen at the appropriate location for the other distances. On the\\nright the closest fits were obtained for the 4 cm and 0 cm locations. The mean values are in good agreement and\\nthe waveforms are close, but could be shifted in time for the other locations of the axis of rotations. Finally, the\\nlatest peak (..... lOOms) in the data appears in the model for -14 cm and 9 cm location.\\n\\nbe the representation used in the oculomotor system. Mathematically, the vector representation is\\nonly one way to describe the computations involved. Other representations exist such as the quaternion representation which has been studied in the context of the saccadic system (Tweed and Vilis,\\n1987; see also Handzel and Flash, 1996 for a very general representation). Detailed comparisons\\nbetween the model and recordings from neurons will be require to settle this issue.\\nDirect comparison between Purkinje cell recordings (L.H. Snyder & W.M. King, unpublished data)\\nand predictions of the model could be used to determine more precisely the different inputs to some\\nPurkinje cells. The model can therefore be an important tool to gain insights difficult to obtain directly with experiments.\\nThe question of how the central nervous system learns the transformations that we described still\\nremains. The cerebellum may be one site of learning for these transformations, and its output may\\nmodulate the VOR in real time depending on the context. This view is compatible with the results\\nof Angelaki and Hess (1995) which indicate that the cerebellum is required to correctly perform an\\notolith transformation. It is also consistent with adaptation results in the VOR. To test this hypothesis,\\nwe have been working on a model of the cerebellum which learns to anticipate sensory inputs and\\nfeedbacks, and use these signals to modulate the VOR. The learning in the cerebellum and vestibular\\nnuclei is mediated by the climbing fibers which report a reinforcement signal of the prediction error\\n(Coenen and Sejnowski. in preparation).\\n\\n\\x0cA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\\n\\n95\\n\\n5 Conclusion\\nMost research on the VOR has assumed forward gaze focussed at infinity. The kinematics of offcenter gaze and fixation at finite distance necessitates nonlinear corrections that require the integration of a variety of sensory inputs. The dynamical model studied here is a working hypothesis for\\nhow these corrections could be computed and is generally consistent with what is known about the\\ncerebellum and brain stem nuclei. We are, however, far from knowing the mechanisms underlying\\nthese computations, or how they are learned through experience.\\n\\n6 Acknowledgments\\nThe first author was supported by a McDonnell-Pew Graduate Fellowship during this research. We\\nwould like to thank Paul Viola for helpful discussions.\\nReferences\\nAngelaki, D. E. and Hess, B. J. (1995). Inertial representation of angular motion in the vestibular system of rhesus monkeyus. II. Otolith-controlled transformation that depends on an intact cerebellar nodulus. Journal\\nof Neurophysiology, 73(5): 1729-1751.\\nCannon, S. C. and Robinson, D. A. (1987). Loss of the neural integrator of the oculomotor system from brain\\nstem lesions in monkey. Journal of Neurophysiology, 57(5):1383-1409.\\nCrawford, J. D., Cadera, W., and Vilis, T. (1991). Generation of torsional and vertical eye position signals by\\nthe interstitial nucleus of Cajal. Science, 252:1551-1553.\\nGamlin, P. D. R. and Clarke, R. J. (1995). Single-unit activity in the primate nucleus reticularis tegmenti pontis\\nrelated to vergence and ocular accomodation. Journal of Neurophysiology, 73(5):2115-2119.\\nGrossman, G. E., Leigh, R. J., Bruce, E. N., Huebner, W. P.,and Lanska, D.J. (1989). Performanceofthe human\\nvestibu1oocu1ar reflex during locomotion. Journal of Neurophysiology, 62(1 ):264-272.\\nHandzel, A. A. and Flash, T. (1996). The geometry of eye rotations and listing\\'s law. In Touretzky, D., Mozer,\\nM., and Hasselmo, M., editors, Advances in Neural Information Processing Systems 8, Cambridge, MA.\\nMIT Press.\\nJudge, S. J. and Cumming, B. G. (1986). Neurons in the monkey midbrain with activity related to vergence eye\\nmovement and accomodation. Journal of Neurophysiology, 55:915-930.\\nMays, L. E. (1984). Neural control of vergence eye movements: Convergence and divergence neurons in midbrain. Journal of Neurophysiology, 51:1091-1108.\\nRobinson, D. A. (1981). The use of control systems analysis in the neurophysiology of eye movements. Ann.\\nRev. Neurosci., 4:463-503.\\nShelhamer, M., Robinson, D. A., and Tan, H. S. (1992). Context-specific adaptation of the gain of the vestibuloocular reflex in humans. Journal of Vestibular Research, 2:89-96.\\nSnyder, L. H. and King, W. M. (1992). Effect of viewing distance and location ofthe axis of head rotation on the\\nmonkey\\'s vestibuloocular reflex I. eye movement response. Journal of Neurophysiology, 67(4):861-874.\\nTweed, D. and Vilis, T. (1987). Implications of rotational kinematics for the oculomotor system in three dimensions. Journal of Neurophysiology, 58(4):832-849.\\nViire, E., Tweed, D., Milner, K., and Vilis, T. (1986). A reexamination of the gain ofthe vestibuloocular reflex.\\nJournal of Neurophysiology, 56(2):439-450.\\n\\n\\x0c',\n",
       "     'pdf_name': '1035-a-dynamical-model-of-context-dependencies-for-the-vestibulo-ocular-reflex.pdf',\n",
       "     'title': 'A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1036',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1036',\n",
       "     'paper_text': 'Improved Gaussian Mixture Density\\nEstimates Using Bayesian Penalty Terms\\nand Network Averaging\\n\\nDirk Ormoneit\\nInstitut fur Informatik (H2)\\nTechnische Universitat Munchen\\n80290 Munchen, Germany\\normoneit@inJormatik.tu-muenchen.de\\n\\nVolker Tresp\\nSiemens AG\\nCentral Research\\n81730 Munchen, Germany\\nVolker. Tresp@zJe.siemens.de\\n\\nAbstract\\n\\nWe compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density\\nestimates. The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules\\nwhich maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation.\\nThis includes Breiman\\'s \"bagging\" , which recently has been found\\nto produce impressive results for classification networks.\\n\\n1\\n\\nIntroduction\\n\\nGaussian mixture models have recently attracted wide attention in the neural network community. Important examples of their application include the training of\\nradial basis function classifiers, learning from patterns with missing features, and\\nactive learning. The appeal of Gaussian mixtures is based to a high degree on the\\napplicability of the EM (Expectation Maximization) learning algorithm, which may\\nbe implemented as a fast neural network learning rule ([Now91], [Orm93]). Severe\\nproblems arise, however, due to singularities and local maxima in the log-likelihood\\nfunction. Particularly in high-dimensional spaces these problems frequently cause\\nthe computed density estimates to possess only relatively limited generalization capabilities in terms of predicting the densities of new data points. As shown in this\\npaper, considerably better generalization can be achieved using regularization.\\n\\n\\x0c543\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nWe will compare two regularization methods. The first one uses a Bayesian prior\\non the parameters. By using conjugate priors we can derive EM learning rules\\nfor finding the MAP (maximum a posteriori probability) parameter estimate. The\\nsecond approach consists of averaging the outputs of ensembles of Gaussian mixture\\ndensity estimators trained on identical or resampled data sets. The latter is a form\\nof \"bagging\" which was introduced by Breiman ([Bre94]) and which has recently\\nbeen found to produce impressive results for classification networks. By using the\\nregularized density estimators in a Bayes classifier ([THA93], [HT94], [KL95]) , we\\ndemonstrate that both methods lead to density estimates which are superior to the\\nunregularized Gaussian mixture estimate.\\n\\n2\\n\\nGaussian Mixtures and the EM Algorithm\\n\\nConsider the lroblem of estimating the probability density of a continuous random\\nvector x E \\'R based on a set x* = {x k 11 S k S m} of iid. realizations of x. As a density model we choose the class of Gaussian mixtures p(xle) = L:7=1 Kip(xli, pi, E i ),\\nwhere the restrictions Ki ~ 0 and L:7=1 Kj = 1 apply. e denotes the parameter\\nvector (Ki\\' Iti, E i )i=1. The p(xli, Pi, E i ) are multivariate normal densities:\\np( xli , Pi , Ei) = (271\")- 41Ei 1- 1 / 2 exp [-1/2(x - Pi)tEi 1 (x - Iti)] .\\nThe Gaussian mixture model is well suited to approximate a wide class of continuous\\nprobability densities. Based on the model and given the data x*, we may formulate\\nthe log-likelihood as\\n\\nlee)\\n\\n= log [rr mk=l p(xkle)] = \",m\\nlog \"\\'~ Kip(xkli, Pi, Ei) .\\n.L...\".k=1\\n.L...\".J=l\\n\\ne\\n\\nMaximum likelihood parameter estimates may efficiently be computed with the\\nEM (Expectation Maximization) algorithm ([DLR77]) . It consists of the iterative\\napplication of the following two steps:\\n1. In the E-step, based on the current parameter estimates, the posterior\\nprobability that unit i is responsible for the generation of pattern xk is\\n\\nestimated as\\n(1)\\n\\n2. In the M-step, we obtain new parameter estimates (denoted by the prime):\\n,\\n\\nK ?\\nJ\\n\\n= -m1 L mk=1 h?k\\nJ\\n\\n~m\\n\\n,\\n\\n(2)\\n\\n=\\n\\nPi\\n\\nwk-l\\n\\n~m\\n\\nhki X k\\nhi\\n\\nwl=l\\n\\n~.\\' _ L:~1 hf(x k - pD(x k - pDt\\nL.J J\\n\\n-\\n\\nm\\n\\nI\\n\\n(3)\\n\\ni\\n\\n(4)\\n\\nL:l=l hi\\nNote that K~ is a scalar , whereas p~ denotes a d-dimensional vector and E/\\nis a d x d matrix.\\nIt is well known that training neural networks as predictors using the maximum\\nlikelihood parameter estimate leads to overfitting. The problem of overfitting is\\neven more severe in density estimation due to singularities in the log-likelihood\\nfunction. Obviously, the model likelihood becomes infinite in a trivial way if we\\nconcentrate all the probability mass on one or several samples of the training set.\\n\\n\\x0c544\\n\\nD. ORMONEIT, V. TRESP\\n\\nIn a Gaussian mixture this is just the case if the center of a unit coincides with\\none of the data points and E approaches the zero matrix. Figure 1 compares the\\ntrue and the estimated probability density in a toy problem. As may be seen,\\nthe contraction of the Gaussians results in (possibly infinitely) high peaks in the\\nGaussian mixture density estimate. A simple way to achieve numerical stability\\nis to artificially enforce a lower bound on the diagonal elements of E. This is a\\nvery rude way of regularization, however, and usually results in low generalization\\ncapabilities. The problem becomes even more severe in high-dimensional spaces.\\nTo yield reasonable approximations, we will apply two methods of regularization,\\nwhich will be discussed in the following two sections.\\n\\nFigure 1: True density (left) and unregularized density estimation (right).\\n\\n3\\n\\nBayesian Regularization\\n\\nIn this section we propose a Bayesian prior distribution on the Gaussian mixture\\nparameters, which leads to a numerically stable version of the EM algorithm. We\\nfirst select a family of prior distributions on the parameters which is conjugate*.\\nSelecting a conjugate prior has a number of advantages. In particular, we obtain\\nanalytic solutions for the posterior density and the predictive density. In our case,\\nthe posterior density is a complex mixture of densities t . It is possible, however, to\\nderive EM-update rules to obtain the MAP parameter estimates.\\nA conjugate prior of a single multivariate normal density is a product of a normal\\ndensity N(JLilft,1]-lE i ) and a Wishart density Wi(E;lla,,8) ([Bun94]). A proper\\nconjugate prior for the the mixture weightings \\'\" = (\"\\'1, ... , \"\\'n) is a Dirichlet density\\nD(\"\\'hV. Consequently, the prior of the overall Gaussian mixture is the product\\nD(\",lr)\\nN(JLilil, 71- 1Ei)Wi(E;1I a , ,8). Our goal is to find the MAP parameter\\nestimate, that is parameters which assume the maximum of the log-posterior\\n\\nil7=1\\n\\nIp(S)\\n\\n2:=~=1 log 2:=;=1 \"\\'iP(X k Ii, JLi, Ei ) + log D(\"\\'lr)\\n\\n+ 2:=;=1 [logN(JLilft, 71- 1Ei) + log Wi(E;lla, ,8)].\\nAs in the unregularized case, we may use the EM-algorithm to find a local maximum\\n? A family F of probability distributions on 0 is said to be conjugate if, for every 1r E F,\\nthe posterior 1r(0Ix) also belongs to F ([Rob94]).\\ntThe posterior distribution can be written as a sum of nm simple terms.\\ntThose densities are defined as follows (b and c are normalizing constants):\\n\\nbII n\\n\\nD(1I:17)\\n\\n.=1\\n\\n~.=l\\n\\n(21r)-i 11,-IE;I-l/2 exp [-~(Il\\' -\\n\\nN(Il.lp,1,-IE.)\\nW i(Ei l la,,8)\\n\\n11:7,-1, with 11:, ~ 0 and \",n\\n\\n=\\n\\ncIEillo-Cd+l)/2 exp [-tr(,8Ei 1 )]\\n\\n11:.\\n\\n=1\\n\\nMt Ei 1 (1l\\' - M]\\n?\\n\\n\\x0c545\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nof Ip(8). The E-step is identical to (1). The M-step becomes\\n,\\nL..\"k-l hki + ri - 1\\n(5)\\n,L..\"k=l hki x k + \\'1J1.\\n\"\\'i\\nJ1.i\\nhi\\nm + L..\"i=l ri - n\\nL..,,1=1 i + 11\\n\\n=\\n\\n\"m\\n\\nE~ =\\n\\n\"m\\n\\n\"n\\n\\n2:;-1 hf(x k -\\n\\nA\\n\\n= \"m\\n\\n+ 11(J1.i 2:~1 h~ + 20: - d\\n\\nJ1.D(xk - J1.D t\\n\\nI\\n\\njJ.)(J1.i - jJ.)t\\n\\n(6)\\n\\n+ 2f3\\n\\n(7)\\n\\nAs typical for conjugate priors, prior knowledge corresponds to a set of artificial\\ntraining data which is also reflected in the EM-update equations. In our experiments, we focus on a prior on the variances which is implemented by f3 =F 0, where\\no denotes the d x d zero matrix. All other parameters we set to \"neutral\" values:\\n\\nri=l\\'v\\'i : l::;i::;n,\\n\\n0:= (d+I)/2,\\n\\n11=0,\\n\\nf3=iJl d\\n\\nld is the d x d unity matrix. The choice of 0: introdu~es a bias which favors large\\nvariances?. The effect of various values of the scalar f3 on the density estimate is\\nillustrated in figure 2. Note that if iJ is chosen too small, overfitting still occurs. If\\nit is chosen to large , on the other hand, the model is too constraint to recognize the\\nunderlying structure.\\n\\nFigure 2: Regularized density estimates (left:\\n\\niJ =\\n\\n0.05, right: \\'iJ = 0.1).\\n\\nTypically, the optimal value for iJ is not known a priori. The simplest procedure\\nconsists of using that iJ which leads to the best performance on a validation set,\\nanalogous to the determination of the optimal weight decay parameter in neural\\nnetwork training. Alternatively, iJ might be determined according to appropriate\\nBayesian methods ([Mac9I]). Either way, only few additional computations are\\nrequired for this method if compared with standard EM.\\n\\n4\\n\\nAveraging Gaussian Mixtures\\n\\nIn this section we discuss the averaging of several Gaussian mixtures to yield improved probability density estimation. The averaging over neural network ensembles\\nhas been applied previously to regression and classification tasks ([PC93]) .\\nThere are several different variants on the simple averaging idea. First, one may\\ntrain all networks on the complete set of training data. The only source of disagreement between the individual predictions consists in different local solutions\\nfound by the likelihood maximization procedure due to different starting points.\\nDisagreement is essential to yield an improvement by averaging, however, so that\\nthis proceeding only seems advantageous in cases where the relation between training data and weights is extremely non-deterministic in the sense that in training,\\n?If A is distributed according to Wi(AIO\\', (3), then E[A- 1 ] = (0\\' - (d + 1)/2)-1 {3. In our\\ncase A is B;-I, so that E[Bi] -+ 00 ? {3 for 0\\' -+ (d + 1)/2.\\n\\n\\x0c546\\n\\nD. ORMONEIT, V. TRESP\\n\\ndifferent solutions are found from different random starting points. A straightforward way to increase the disagreement is to train each network on a resampled\\nversion of the original data set. If we resample the data without replacement, the\\nsize of each training set is reduced, in our experiments to 70% of the original. The\\naveraging of neural network predictions based on resampling with replacement has\\nrecently been proposed under the notation \"bagging\" by Breiman ([Bre94]), who\\nhas achieved dramatic.ally improved results in several classification tasks. He also\\nnotes, however, that an actual improvement of the prediction can only result if the\\nestimation procedure is relatively unstable. As discussed, this is particularly the\\ncase for Gaussian mixture training. We therefore expect bagging to be well suited\\nfor our task.\\n\\n5\\n\\nExperiments and Results\\n\\nTo assess the practical advantage resulting from regularization, we used the density\\nestimates to construct classifiers and compared the resulting prediction accuracies\\nusing a toy problem and a real-world problem. The reason is that the generalization error of density estimates in terms of the likelihood based on the test data\\nis rather unintuitive whereas performance on a classification problem provides a\\ngood impression of the degree of improvement. Assume we have a set of N labeled\\ndata z* = {(xk, lk)lk = 1, ... , N}, where lk E Y = {I, ... , C} denotes the class label\\nof each input xk . A classifier of new inputs x is yielded by choosing the class I\\nwith the maximum posterior class-probability p(llx). The posterior probabilities\\nmay be derived from the class-conditional data likelihood p(xll) via Bayes theorem:\\np(llx) = p(xll)p(l)/p(x) ex p(xll)p(l) . The resulting partitions ofthe input space are\\noptimal for the true p(llx). A viable way to approximate the posterior p(llx) is to\\nestimate p(xll) and p(l) from the sample data.\\n5.1\\n\\nToy Problem\\n\\nIn the toy classification problem the task is to discriminate the two classes of circulatory arranged data shown in figure 3. We generated 200 data points for each class\\nand subdivided them into two sets of 100 data points. The first was used for training, the second to test the generalization performance. As a network architecture\\nwe chose a Gaussian mixture with 20 units. Table 1 summarizes the results, beginning with the unregularized Gaussian mixture which is followed by the averaging\\nand the Bayesian penalty approaches. The three rows for averaging correspond to\\nthe results yielded without applying resampling (local max.), with resampling with-\\n\\nFigure 3: Toy Classification Task.\\n\\n\\x0c547\\n\\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\\n\\nout replacement (70% subsets), and with resampling with replacement (bagging).\\nThe performances on training and test set are measured in terms of the model loglikelihood. Larger values indicate a better performance. We report separate results\\nfor dass A and B, since the densities of both were estimated separately. The final\\ncolumn shows the prediction accuracy in terms of the percentage of correctly classified data in the test set. We report the average results from 20 experiments. The\\nnumbers in brackets denote the standard deviations u of the results. Multiplying u\\nwith T19;95%/v\\'20 = 0.4680 yields 95% confidence intervals. The best result in each\\ncategory is underlined.\\nAlgorithm\\n\\nLog- Likelihood\\n\\nunreg.\\nAveraging:\\nlocal max.\\n70% subset\\nbagging\\nPenalty:\\n[3 = 0.01\\n[3 = 0.02\\n[3 = 0.05\\n[3 = 0.1\\n\\nAccuracy\\n\\nA\\n-120.8 (13.3)\\n\\n-120.4 (10.8)\\n\\nTest\\nA\\nB\\n-224.9 (32.6) -241.9 (34.1)\\n\\n-115.6 (6.0)\\n-106.8 (5.8)\\n-83.8 (4.9)\\n\\n-112.6 (6.6)\\n-105.1 (6.7)\\n-83.1 (7.1)\\n\\n-200.9 (13.9)\\n-188.8 (9.5)\\n-194.2 (7.3)\\n\\n-209.1 (16.3)\\n-196.4 (11.3)\\n-200.1 (11.3)\\n\\n81.8% (3.1)\\n83.2% (2.9)\\n82.6% (3.4)\\n\\n-149.3\\n-156.0\\n-173.9\\n-183.0\\n\\n-146.5 (5.9)\\n-153.0 (4.8)\\n-167.0 (15.8)\\n-181.9 (21.1)\\n\\n-186.2\\n-177.1\\n-182.0\\n-184.6\\n\\n-182.9 (11.6)\\n-174.9 (7.0)\\n-173.9 (14.3)\\n-182.5 (21.1)\\n\\n83.1%\\n84.4%\\n81.5%\\n78.5%\\n\\nTraining\\n\\n(18.5)\\n(16.5)\\n(24.3)\\n(21.9)\\n\\nB\\n\\n(13.9)\\n(11.8)\\n(20.1)\\n(21.0)\\n\\nI\\n80.6\\'70 (2.8)\\n\\n(2.9)\\n(6.3)\\n(5.9)\\n(5.1)\\n\\nTable 1: Performances in the toy classification problem .\\nAs expected, all regularization methods outperform the maximum likelihood approach in terms of correct classification. The performance of the Bayesian regularization is hereby very sensitive to the appropriate choice of the regularization\\nparameter (3. Optimality of (3 with respect to the density prediction and oytimality\\nwith respect to prediction accuracy on the test set roughly coincide (for (3 = 0.02).\\nA veraging is inferior to the Bayesian approach if an optimal {3 is chosen.\\n5.2\\n\\nBUPA Liver Disorder Classification\\n\\nAs a second task we applied our methods to a real-world decision problem from\\nthe medical environment. The problem is to detect liver disorders which might\\narise from excessive alcohol consumption. Available information consists of five\\nblood tests as well as a measure of the patients\\' daily alcohol consumption. We\\nsubdivided the 345 available samples into a training set of 200 and a test set of 145\\nsamples. Due to the relatively few data we did not try to determine the optimal\\nregularization parameter using a validation process and will report results on the\\ntest set for different parameter values.\\nAlgorithm\\nunregularized\\nBayesian penalty ({3 = 0.05)\\nBayesian penalty ?(3 = 0.10)\\nBayesian penal ty (3 = 0.20\\naveraging local maxima\\naveraging (70 % subset)\\naveraging (bagging)\\n\\nAccuracy\\n64.8 %\\n65.5 %\\n66.9 %\\n61.4 %\\n65 .5 0\\n72.4 %\\n71.0 %\\n\\nTable 2: Performances in the liver disorder classification problem.\\n\\n\\x0c548\\n\\nD. ORMONEIT. V. TRESP\\n\\nThe results of our experiments are shown in table 2. Again, both regularization\\nmethods led to an improvement in prediction accuracy. In contrast to the toy problem, the averaged predictor was superior to the Bayesian approach here. Note that\\nthe resampling led to an improvement of more than five percent points compared\\nto unresampled averaging.\\n\\n6\\n\\nConclusion\\n\\nWe proposed a Bayesian and an averaging approach to regularize Gaussian mixture\\ndensity estimates. In comparison with the maximum likelihood solution both approaches led to considerably improved results as demonstrated using a toy problem\\nand a real-world classification task. Interestingly, none of the methods outperformed\\nthe other in both tasks. This might be explained with the fact that Gaussian mixture density estimates are particularly unstable in high-dimensional spaces with\\nrelatively few data. The benefit of averaging might thus be greater in this case.\\nA veraging proved to be particularly effective if applied in connection with resampIing of the training data, which agrees with results in regression and classification\\ntasks. If compared to Bayesian regularization, averaging is computationally expensive. On the other hand, Baysian approaches typically require the determination of\\nhyper parameters (in our case 13), which is not the case for averaging approaches.\\n\\nReferences\\n[Bre94]\\n\\nL. Breiman. Bagging predictors. Technical report , UC Berkeley, 1994.\\n\\n[Bun94]\\n\\nW . Buntine. Operations for learning with graphical models. Journal of Artificial\\nIntelligence Research, 2:159-225, 1994.\\n\\n[DLR77] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from\\nincomplete data via the EM algorithm. J. Royal Statistical Society B, 1977.\\n[HT94]\\n\\nT. Hastie and R. Tibshirani. Discriminant analysis by gaussian mixtures. Technical report , AT&T Bell Labs and University of Toronto, 1994.\\n\\n[KL95]\\n\\nN. Kambhatla and T. K. Leen. Classifying with gaussian mixtures and clusters.\\nIn Advances in Neural Information Processing Systems 7. Morgan Kaufman,\\n1995.\\n\\n[Mac91]\\n\\nD. MacKay. Bayesian Modelling and Neural Networks. PhD thesis, California\\nInstitute of Technology, Pasadena, 1991.\\n\\n[Now91] S. J. Nowlan. Soft Competitive Adaption: Neural Network Learning Algorithms\\nbased on Fitting Statistical Mixtures. PhD thesis, School of Computer Science,\\nCarnegie Mellon University, Pittsburgh, 1991.\\n[Orm93] D. Ormoneit. Estimation of probability densities using neural networks. Master\\'s\\nthesis, Technische Universitiit Munchen, 1993.\\n[PC93]\\n\\nM. P. Perrone and L. N. Cooper. When networks disagree: Ensemble methods for\\nhybrid Neural networks. In Neural Networks for Speech and Image Processing.\\nChapman Hall, 1993.\\n\\n[Rob94]\\n\\nC. P. Robert. The Bayesian Choice. Springer-Verlag, 1994.\\n\\n[THA93] V. Tresp, J. Hollatz, and S. Ahmad. Network structuring and training using\\nrule-based knowledge. In Advances in Neural Information Processing Systems 5.\\nMorgan Kaufman, 1993.\\n\\n\\x0c',\n",
       "     'pdf_name': '1036-improved-gaussian-mixture-density-estimates-using-bayesian-penalty-terms-and-network-averaging.pdf',\n",
       "     'title': 'Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1044',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1044',\n",
       "     'paper_text': 'Learning with ensembles: How\\nover-fitting can be useful\\n\\nPeter Sollich\\nDepartment of Physics\\nUniversity of Edinburgh, U.K.\\nP.SollichGed.ac.uk\\n\\nAnders Krogh\\'\"\\nNORDITA, Blegdamsvej 17\\n2100 Copenhagen, Denmark\\nkroghGsanger.ac.uk\\n\\nAbstract\\nWe study the characteristics of learning with ensembles. Solving\\nexactly the simple model of an ensemble of linear students, we\\nfind surprisingly rich behaviour. For learning in large ensembles,\\nit is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can\\nbe obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble\\nweights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide\\nrange of regularization parameters makes this improvement robust\\nagainst changes in the unknown level of noise in the training data.\\n\\n1\\n\\nINTRODUCTION\\n\\nAn ensemble is a collection of a (finite) number of neural networks or other types\\nof predictors that are trained for the same task. A combination of many different predictors can often improve predictions, and in statistics this idea has been\\ninvestigated extensively, see e.g. [1, 2, 3]. In the neural networks community, ensembles of neural networks have been investigated by several groups, see for instance\\n[4, 5, 6, 7]. Usually the networks in the ensemble are trained independently and\\nthen their predictions are combined.\\nIn this paper we study an ensemble of linear networks trained on different but\\noverlapping training sets. The limit in which all the networks are trained on the\\nfull data set and the one where all the data sets are different has been treated in\\n[8] . In this paper we treat the case of intermediate training set sizes and overlaps\\n?Present address: The Sanger Centre, Hinxton, Cambs CBIO IRQ, UK.\\n\\n\\x0cLearning with Ensembles: How Overfitting Can Be Useful\\n\\n191\\n\\nexactly, yielding novel insights into ensemble learning. Our analysis also allows us to\\nstudy the effect of regularization and of having different predictors in an ensemble.\\n\\n2\\n\\nGENERAL FEATURES OF ENSEMBLE LEARNING\\n\\nWe consider the task of approximating a target function fo from RN to R. It\\nwill be assumed that we can only obtain noisy samples of the function, and the\\n(now stochastic) target function will be denoted y(x) . The inputs x are taken\\nto be drawn from some distribution P(x). Assume now that an ensemble of K\\nindependent predictors fk(X) of y(x) is available. A weighted ensemble average is\\ndenoted by a bar, like\\n(1)\\nlex) = L,wkfk(X),\\nk\\n\\nwhich is the final output of the ensemble. One can think of the weight Wk as the\\nbelief in predictor k and we therefore constrain the weights to be positive and sum\\nto one. For an input x we define the error of the ensemble c(x), the error of the\\nkth predictor ck(X), and its ambiguity ak(x)\\nc(x)\\nck(X)\\n\\n(y(x) -lex)?\\n(y(x) - fk(X)?\\n(fk(X) -1(x?2.\\n\\n=\\n\\n(2)\\n(3)\\n(4)\\n\\n=\\n\\nThe ensemble error can be written as c(x)\\nlex) - a(x) [7], where lex)\\nL,k Wkck(X) is the average error over the individual predictors and a(x) =\\nL,k Wkak(X) is the average of their ambiguities, which is the variance of the output\\nover the ensemble. By averaging over the input distribution P(x) (and implicitly\\nover the target outputs y(x?, one obtains the ensemble generalization error\\n(5)\\nwhere c(x) averaged over P(x) is simply denoted c, and similarly for land a. The\\nfirst term on the right is the weighted average of the generalization errors of the individual predictors, and the second is the weighted average of the ambiguities, which\\nwe refer to as the ensemble ambiguity. An important feature of equation (5) is that\\nit separates the generalization error into a term that depends on the generalization\\nerrors of the individual students and another term that contains all correlations between the students. The latter can be estimated entirely from unlabeled data, i. e.,\\nwithout any knowledge of the target function to be approximated. The relation (5)\\nalso shows that the more the predictors differ, the lower the error will be, provided\\nthe individual errors remain constant.\\n\\nIn this paper we assume that the predictors are trained on a sample of p examples\\nof the target function, (xt\\',yt\\'), where yt\\' = fo(xt\\') + TJt\\' and TJt\\' is some additive\\nnoise (Jl. = 1, ... ,p). The predictors, to which we refer as students in this context\\nbecause they learn the target function from the training examples, need not be\\ntrained on all the available data. In fact, since training on different data sets will\\ngenerally increase the ambiguity, it is possible that training on subsets of the data\\nwill improve generalization. An additional advantage is that, by holding out for\\neach student a different part of the total data set for the purpose of testing, one\\ncan use the whole data set for training the ensemble while still getting an unbiased\\nestimate of the ensemble generalization error. Denoting this estimate by f, one has\\n(6)\\nwhere Ctest = L,k WkCtest,k is the average of the students\\' test errors. As already\\npointed out, the estimate ft of the ensemble ambiguity can be found from unlabeled\\ndata.\\n\\n\\x0cP. SOLLICH, A. KROGH\\n\\n192\\n\\nSo far, we have not mentioned how to find the weights Wk. Often uniform weights\\nare used, but optimization of the weights in some way is tempting. In [5, 6] the\\ntraining set was used to perform the optimization, i.e., the weights were chosen to\\nminimize the ensemble training error. This can easily lead to over-fitting, and in [7]\\nit was suggested to minimize the estimated generalization error (6) instead. If this\\nis done, the estimate (6) acquires a bias; intuitively, however, we expect this effect\\nto be small for large ensembles.\\n\\n3\\n\\nENSEMBLES OF LINEAR STUDENTS\\n\\nIn preparation for our analysis of learning with ensembles of linear students we now\\nbriefly review the case of a single linear student, sometimes referred to as \\'linear\\nperceptron learning\\'. A linear student implements the input-output mapping\\n1 T\\nJ(x) = ..JNw x\\nparameterized in terms of an N-dimensional parameter vector w with real components; the scaling factor 1/..JN is introduced here for convenience, and . ..T denotes\\nthe transpose of a vector. The student parameter vector w should not be confused with the ensemble weights Wk. The most common method for training such\\na linear student (or parametric inference models in general) is minimization of the\\nsum-of-squares training error\\nE = L:(y/J - J(x/J))2 + Aw2\\n/J\\nwhere J.L = 1, ... ,p numbers the training examples. To prevent the student from\\nfitting noise in the training data, a weight decay term Aw2 has been added. The size\\nof the weight decay parameter A determines how strongly large parameter vectors\\nare penalized; large A corresponds to a stronger regularization of the student.\\nFor a linear student, the global minimum of E can easily be found. However,\\nin practical applications using non-linear networks, this is generally not true, and\\ntraining can be thought of as a stochastic process yielding a different solution each\\ntime. We crudely model this by considering white noise added to gradient descent\\nupdates of the parameter vector w. This yields a limiting distribution of parameter\\nvectors P(w) ex: exp(-E/2T), where the \\'temperature\\' T measures the amount of\\nnoise in the training process.\\nWe focus our analysis on the \\'thermodynamic limit\\' N - t 00 at constant normalized\\nnumber of training examples, ex = p/N. In this limit, quantities such as the training\\nor generalization error become self-averaging, i.e., their averages over all training\\nsets become identical to their typical values for a particular training set. Assume\\nnow that the training inputs x/J are chosen randomly and independently from a\\nGaussian distribution P(x) ex: exp( - ~x2), and that training outputs are generated\\nby a linear target function corrupted by additive noise, i.e., y/J = w\\'f x/J /..IN + 1]/J,\\nwhere the 1]/J are zero mean noise variables with variance u 2 ? Fixing the length of the\\nparameter vector of the target function to w~ = N for simplicity, the generalization\\nerror of a linear student with weight decay A and learning noise T becomes [9]\\n(; = (u 2 + T)G + A(U 2\\n\\n-\\n\\n8G\\n\\nA) 8A .\\n\\n(7)\\n\\nOn the r.h.s. of this equation we have dropped the term arising from the noise on\\nthe target function alone, which is simply u 2 , and we shall follow this convention\\nthroughout . The \\'response function\\' Gis [10, 11]\\n\\nG = G(ex, A) = (1 - ex - A+ )(1 - ex - A)2 + 4A)/2A.\\n\\n(8)\\n\\n\\x0c193\\n\\nLearning with Ensembles: How Overfitting Can Be Useful\\n\\nFor zero training noise, T = 0, and for any a, the generalization error (7} is minimized when the weight decay is set to A = (T2j its value is then (T2G(a, (T2), which\\nis the minimum achievable generalization error [9].\\n\\n3.1\\n\\nENSEMBLE GENERALIZATION ERROR\\n\\nWe now consider an ensemble of K linear students with weight decays Ak and\\nlearning noises Tk (k = 1 . . . K). Each ,student has an ensemble weight Wk and\\nis trained on N ak training examples, with students k and I sharing N akl training\\nexamples (of course, akk = ak). As above, we consider noisy training data generated\\nby a linear target function. The resulting ensemble generalization error can be\\ncalculated by diagrammatic [10] or response function [11] methods. We refer the\\nreader to a forthcoming publication for details and only state the result:\\n\\n(9)\\nwhere\\n(10)\\nHere Pk is defined as Pk = AkG(ak, Ak). The Kronecker delta in the last term\\nof (10) arises because the training noises of different students are uncorrelated. The\\ngeneralization errors and ambiguities of the individual students are\\n\\nak = ckk - 2 LWlckl\\nI\\n\\n+ LWIWmclm;\\n1m\\n\\nthe result for the Ck can be shown to agree with the single student result (7). In\\nthe following sections, we shall explore the consequences of the general result (9) .\\nWe will concentrate on the case where the training set of each student is sampled\\nrandomly from the total available data set of size NO\\', For the overlap of the training\\nsets of students k and I (k \\'II) one then has akl/a = (ak/a)(al/a) and hence\\n\\nak/ = akal/a\\n\\n(11)\\nup to fluctuations which vanish in the thermodynamic limit. For finite ensembles\\none can construct training sets for which akl < akal/a. This is an advantage,\\nbecause it results in a smaller generalization error, but for simplicity we use (11).\\n\\n4\\n\\nLARGE ENSEMBLE LIMIT\\n\\nWe now use our main result (9) to analyse the generalization performance of an ensemble with a large number K of students, in particular when the size of the training\\nsets for the individual students are chosen optimally. If the ensemble weights Wk\\nare approximately uniform (Wk ~ 1/ K) the off-diagonal elements of the matrix\\n(ckl) dominate the generalization error for large K, and the contributions from the\\ntraining noises\\nare suppressed. For the special case where all students are identical and are trained on training sets of identical size, ak = (1 - c)a, the ensemble\\ngeneralization error is shown in Figure 1(left). The minimum at a nonzero value\\nof c, which is the fraction of the total data set held out for testing each student,\\ncan clearly be seen. This confirms our intuition: when the students are trained\\non smaller, less overlapping training sets, the increase in error of the individual\\nstudents can be more than offset by the corresponding increase in ambiguity.\\n\\nn\\n\\nThe optimal training set sizes ak can be calculated analytically:\\n_\\n\\nCk\\n\\n=1-\\n\\nak/ a\\n\\n1 - Ak/(T2\\n\\n= 1 + G(a, (T2) \\'\\n\\n(12)\\n\\n\\x0cP. SOLLICH, A. KROGH\\n\\n194\\n1.0 r - - - , - - - - - , r - - - . , - - - - , . - - - - : .\\n\\n1.0 r - - - , - - - - - , - - - . - - - - r - - - - \"\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n\\n0.6\\n\\nw\\n\\n.\\'\\n\\nw\\n0.4\\n\\n,...-------\\n\\n0.2\\n/\\n\\n/\\n\\n0.0 /\\n\\n0.0\\n\\n/\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n,,\\n,\\n0.8\\n\\n0.2\\n\\n------,\\n\\n1.0\\n\\n0.0\\n\\n0.0\\n\\nC\\n\\n...\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nC\\n\\nFigure 1: Generalization error and ambiguity for an infinite ensemble of identical\\nstudents. Solid line: ensemble generalization error, fj dotted line: average generalization error of the individual students, l; dashed line: ensemble ambiguity, a.\\nFor both plots a = 1 and (72 = 0.2 . The left plot corresponds to under-regularized\\nstudents with A = 0.05 < (72. Here the generalization error of the ensemble has\\na minimum at a nonzero value of c. This minimum exists whenever>\\' < (72. The\\nright plot shows the case of over-regularized students (A = 0.3 > (72), where the\\ngeneralization error is minimal at c = O.\\nThe resulting generalization error is f = (72G(a, (72) + 0(1/ K), which is the globally\\nminimal generalization error that can be obtained using all available training data,\\nas explained in Section 3. Thus, a large ensemble with optimally chosen training\\nset sizes can achieve globally optimal generalization performance. However, we see\\nfrom (12) that a valid solution Ck > 0 exists only for Ak < (72, i.e., if the ensemble\\nis under-regularized. This is exemplified, again for an ensemble of identical students, in Figure 1(right) , which shows that for an over-regularized ensemble the\\ngeneralization error is a: monotonic function of c and thus minimal at c = o.\\nWe conclude this section by discussing how the adaptation of the training set sizes\\ncould be performed in practice, for simplicity confining ourselves to an ensemble of\\nidentical students, where only one parameter c = Ck = 1- ak/a has to be adapted.\\nIf the ensemble is under-regularized one expects a minimum of the generalization\\nerror for some nonzero c as in Figure 1. One could, therefore, start by training\\nall students on a large fraction of the total data set (corresponding to c ~ 0), and\\nthen gradually and randomly remove training examples from the students\\' training\\nsets. Using (6), the generalization error of each student could be estimated by\\ntheir performance on the examples on which they were not trained, and one would\\nstop removing training examples when the estimate stops decreasing. The resulting\\nestimate of the generalization error will be slightly biased; however, for a large\\nenough ensemble the risk of a strongly biased estimate from systematically testing\\nall students on too \\'easy\\' training examples seems small, due to the random selection\\nof examples.\\n\\n5\\n\\nREALISTIC ENSEMBLE SIZES\\n\\nWe now discuss some effects that occur in learning with ensembles of \\'realistic\\' sizes.\\nIn an over-regularized ensemble nothing can be gained by making the students more\\ndiverse by training them on smaller, less overlapping training sets. One would also\\n\\n\\x0c195\\n\\nLearning with Ensembles: How Overfitting Can Be Useful\\n\\nFigure 2: The generalization error of\\nan ensemble with 10 identical students as a function of the test set\\nfraction c. From bottom to top the\\ncurves correspond to training noise\\nT = 0,0.1,0.2, ... ,1.0. The star on\\neach curve shows the error of the optimal single perceptron (i. e., with optimal weight decay for the given T)\\ntrained on all examples, which is independent of c. The parameters for\\nthis example are: a = 1, A = 0.05,\\n0\\'2 = 0.2.\\n\\n0.2\\n0.0 L - _ - - \\' - _ - - - \\'_ _-\\'--_--\\'-_~\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nC\\n\\nexpect this kind of \\'diversification\\' to be unnecessary or even counterproductive\\nwhen the training noise is high enough to provide sufficient \\'inherent\\' diversity of\\nstudents. In the large ensemble limit, we saw that this effect is suppressed, but\\nit does indeed occur in finite ensembles. Figure 2 shows the dependence of the\\ngeneralization error on c for an ensemble of 10 identical, under-regularized students\\nwith identical training noises Tk = T. For small T, the minimum of f. at nonzero c\\npersists. For larger T, f. is monotonically increasing with c, implying that further\\ndiversification of students beyond that caused by the learning noise is wasteful. The\\nplot also shows the performance of the optimal single student (with A chosen to\\nminimize the generalization error at the given T), demonstrating that the ensemble\\ncan perform significantly better by effectively averaging out learning noise.\\nFor realistic ensemble sizes the presence of learning noise generally reduces the\\npotential for performance improvement by choosing optimal training set sizes. In\\nsuch cases one can still adapt the ensemble weights to optimize performance, again\\non the basis of the estimate of the ensemble generalization error (6). An example is\\n1.0\\n\\n1.0\\n\\n,,\\n\\n0.8\\n\\nI\\nI\\n\\n0.8\\n,-\\n\\n,-\\n\\n/\\n\\nI\\n\\n0.6\\n\\n0.6\\n\\nI\\n\\ntV\\n\\ntV\\n\\n0.4 ..... -_ .................\\n\\n0.4\\n0.2\\n... ....\\n0.0\\n0.001\\n\\n---0.010\\n\\n0.2\\n\\n0.100\\n\\n02\\n\\n1.000\\n\\n0.0\\n0.001\\n\\n0.010\\n\\n02\\n\\n0.100\\n\\n1.000\\n\\nFigure 3: The generalization error of an ensemble of 10 students with different\\nweight decays (marked by stars on the 0\\'2-axis) as a function of the noise level\\n0\\'2. Left: training noise T = 0; right: T = 0.1. The dashed lines are for the\\nensemble with uniform weights, and the solid line is for optimized ensemble weights.\\nThe dotted lines are for the optimal single perceptron trained on all data. The\\nparameters for this example are: a = 1, c = 0.2.\\n\\n\\x0c196\\n\\nP. SOu...ICH, A. KROGH\\n\\nshown in Figure 3 for an ensemble of size 1< = 10 with the weight decays >\\'k equally\\nspaced on a logarithmic axis between 10- 3 and 1. For both of the temperatures T\\nshown, the ensemble with uniform weights performs worse than the optimal single\\nstudent. With weight optimization, the generalization performance approaches that\\nof the optimal single student for T = 0, and is actually better at T = 0.1 over\\nthe whole range of noise levels rr2 shown. Even the best single student from the\\nensemble can never perform better than the optimal single student, so combining the\\nstudent outputs in a weighted ensemble average is superior to simply choosing the\\nbest member of the ensemble by cross-validation, i.e., on the basis of its estimated\\ngeneralization error. The reason is that the ensemble average suppresses the learning\\nnoise on the individual students.\\n\\n6\\n\\nCONCLUSIONS\\n\\nWe have studied ensemble learning in the simple, analytically solvable scenario of\\nan ensemble of linear students. Our main findings are: In large ensembles, one\\nshould use under-regularized students in order to maximize the benefits of the\\nvariance-reducing effects of ensemble learning. In this way, the globally optimal\\ngeneralization error on the basis of all the available data can be reached by optimizing the training set sizes of the individual students. At the same time an estimate\\nof the generalization error can be obtained. For ensembles of more realistic size, we\\nfound that for students subjected to a large amount of noise in the training process\\nit is unnecessary to increase the diversity of students by training them on smaller,\\nless overlapping training sets. In this case, optimizing the ensemble weights can\\nstill yield substantially better generalization performance than an optimally chosen\\nsingle student trained on all data with the same amount of training noise. This\\nimprovement is most insensitive to changes in the unknown noise levels rr2 if the\\nweight decays of the individual students cover a wide range. We expect most of these\\nconclusions to carryover, at least qualitatively, to ensemble learning with nonlinear\\nmodels, and this correlates well with experimental results presented in [7].\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n\\nC. Granger, Journal of Forecasting 8, 231 (1989).\\nD. Wolpert, Neural Networks 5, 241 (1992) .\\nL. Breimann, Tutorial at NIPS 7 and personal communication.\\nL. Hansen and P. Salamon, IEEE Trans. Pattern Anal. and Mach. Intell. 12,\\n993 (1990).\\nM. P. Perrone and L. N. Cooper, in Neural Networks for Speech and Image\\nprocessing, ed. R. J. Mammone (Chapman-Hall, 1993).\\nS. Hashem: Optimal Linear Combinations of Neural Networks. Tech. Rep .\\nPNL-SA-25166, submitted to Neural Networks (1995) .\\nA. Krogh and J. Vedelsby, in NIPS 7, ed. G. Tesauro et al., p. 231 (MIT Press,\\n1995).\\nR. Meir, in NIPS 7, ed. G. Tesauro et al., p. 295 (MIT Press, 1995).\\nA. Krogh and J. A. Hertz, J. Phys. A 25,1135 (1992).\\nJ. A. Hertz, A. Krogh, and G. I. Thorbergsson, J. Phys. A 22, 2133 (1989).\\nP. Sollich, J. Phys. A 27, 7771 (1994).\\n\\n\\x0c',\n",
       "     'pdf_name': '1044-learning-with-ensembles-how-overfitting-can-be-useful.pdf',\n",
       "     'title': 'Learning with ensembles: How overfitting can be useful',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': 7241},\n",
       " 'timed_out': False,\n",
       " 'took': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index='nips_papers', body=\n",
    "         {\n",
    "             \"query\": {\n",
    "                 \"match_all\": {}\n",
    "             }\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '83',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2901017,\n",
       "    '_source': {'id': '83',\n",
       "     'title': 'Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1630',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28948608,\n",
       "    '_source': {'id': '1630',\n",
       "     'title': 'Mechanisms of Generalization in Perceptual Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5101',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28939492,\n",
       "    '_source': {'id': '5101',\n",
       "     'title': 'Statistical Active Learning Algorithms'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7225',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28939492,\n",
       "    '_source': {'id': '7225',\n",
       "     'title': 'Gradient Episodic Memory for Continual Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5948',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2889576,\n",
       "    '_source': {'id': '5948',\n",
       "     'title': 'Enforcing balance allows local supervised learning in spiking recurrent networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '614',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2889482,\n",
       "    '_source': {'id': '614',\n",
       "     'title': 'Explanation-Based Neural Network Learning for Robot Control'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1944',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28892428,\n",
       "    '_source': {'id': '1944',\n",
       "     'title': 'Convergence of Optimistic and Incremental Q-Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6884',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28890923,\n",
       "    '_source': {'id': '6884', 'title': 'State Aware Imitation Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5054',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28887862,\n",
       "    '_source': {'id': '5054',\n",
       "     'title': 'Perfect Associative Learning with Spike-Timing-Dependent Plasticity'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3545',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2888599,\n",
       "    '_source': {'id': '3545',\n",
       "     'title': 'Policy Search for Motor Primitives in Robotics'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '887',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28880703,\n",
       "    '_source': {'id': '887',\n",
       "     'title': 'Finding Structure in Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3991',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28870603,\n",
       "    '_source': {'id': '3991',\n",
       "     'title': 'Multi-View Active Learning in the Non-Realizable Case'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1183',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28842846,\n",
       "    '_source': {'id': '1183',\n",
       "     'title': 'Adaptive On-line Learning in Changing Environments'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2944',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2883316,\n",
       "    '_source': {'id': '2944',\n",
       "     'title': 'Active Learning for Misspecified Models'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4905',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2883316,\n",
       "    '_source': {'id': '4905',\n",
       "     'title': 'More data speeds up training time in learning halfspaces over sparse vectors'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4251',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28811893,\n",
       "    '_source': {'id': '4251', 'title': 'Speedy Q-Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6654',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2878458,\n",
       "    '_source': {'id': '6654',\n",
       "     'title': 'Learning multiple visual domains with residual adapters'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '388',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28782478,\n",
       "    '_source': {'id': '388',\n",
       "     'title': 'Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '797',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28778103,\n",
       "    '_source': {'id': '797',\n",
       "     'title': 'Learning in Computer Vision and Image Understanding'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3492',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28777346,\n",
       "    '_source': {'id': '3492',\n",
       "     'title': 'Translated Learning: Transfer Learning across Different Feature Spaces'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1986',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2877599,\n",
       "    '_source': {'id': '1986',\n",
       "     'title': 'The Steering Approach for Multi-Criteria Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1476',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28768793,\n",
       "    '_source': {'id': '1476',\n",
       "     'title': 'Nonparametric Model-Based Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4180',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28765315,\n",
       "    '_source': {'id': '4180',\n",
       "     'title': 'A Reduction from Apprenticeship Learning to Classification'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3656',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28762358,\n",
       "    '_source': {'id': '3656',\n",
       "     'title': 'Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4035',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28762358,\n",
       "    '_source': {'id': '4035',\n",
       "     'title': 'Semi-Supervised Learning with Adversarially Missing Label Information'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5687',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2875459,\n",
       "    '_source': {'id': '5687', 'title': 'Regressive Virtual Metric Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1383',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28733233,\n",
       "    '_source': {'id': '1383',\n",
       "     'title': 'The Asymptotic Convergence-Rate of Q-learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2759',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28730977,\n",
       "    '_source': {'id': '2759',\n",
       "     'title': 'Analysis of Spectral Kernel Design based Semi-supervised Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1086',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28723827,\n",
       "    '_source': {'id': '1086',\n",
       "     'title': 'Examples of learning curves from a modified VC-formalism'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3787',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28721514,\n",
       "    '_source': {'id': '3787',\n",
       "     'title': 'DUOL: A Double Updating Approach for Online Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5765',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28721514,\n",
       "    '_source': {'id': '5765', 'title': 'The Human Kernel'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6833',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28721514,\n",
       "    '_source': {'id': '6833', 'title': 'Collaborative PAC Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '587',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28707376,\n",
       "    '_source': {'id': '587',\n",
       "     'title': 'Using Prior Knowledge in a NNPDA to Learn Context-Free Languages'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '637',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28707376,\n",
       "    '_source': {'id': '637',\n",
       "     'title': 'Weight Space Probability Densities in Stochastic Learning: II. Transients and Basin Hopping Times'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6434',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2870367,\n",
       "    '_source': {'id': '6434',\n",
       "     'title': 'Adaptive Smoothed Online Multi-Task Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6618',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2870367,\n",
       "    '_source': {'id': '6618',\n",
       "     'title': 'Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '816',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2867894,\n",
       "    '_source': {'id': '816',\n",
       "     'title': 'Optimal Stopping and Effective Machine Complexity in Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2124',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28671935,\n",
       "    '_source': {'id': '2124',\n",
       "     'title': 'Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6946',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28654647,\n",
       "    '_source': {'id': '6946',\n",
       "     'title': 'Unsupervised Sequence Classification using Sequential Output Statistics'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '606',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28634247,\n",
       "    '_source': {'id': '606',\n",
       "     'title': 'Using Aperiodic Reinforcement for Directed Self-Organization During Development'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '918',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28632665,\n",
       "    '_source': {'id': '918',\n",
       "     'title': 'Limits on Learning Machine Accuracy Imposed by Data Quality'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1141',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2862784,\n",
       "    '_source': {'id': '1141',\n",
       "     'title': 'Investment Learning with Hierarchical PSOMs'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3705',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2862176,\n",
       "    '_source': {'id': '3705',\n",
       "     'title': 'An Online Algorithm for Large Scale Image Similarity Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1821',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2861809,\n",
       "    '_source': {'id': '1821',\n",
       "     'title': 'Automated State Abstraction for Options using the U-Tree Algorithm'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1362',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2861139,\n",
       "    '_source': {'id': '1362',\n",
       "     'title': 'Multi-time Models for Temporally Abstract Planning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '141',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2861139,\n",
       "    '_source': {'id': '141',\n",
       "     'title': 'Speech Production Using A Neural Network with a Cooperative Learning Mechanism'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '928',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2861139,\n",
       "    '_source': {'id': '928',\n",
       "     'title': 'Reinforcement Learning Predicts the Site of Plasticity for Auditory Remapping in the Barn Owl'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6846',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2861014,\n",
       "    '_source': {'id': '6846',\n",
       "     'title': 'Information-theoretic analysis of generalization capability of learning algorithms'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2635',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28601232,\n",
       "    '_source': {'id': '2635',\n",
       "     'title': 'Heuristics for Ordering Cue Search in Decision Making'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3814',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28598168,\n",
       "    '_source': {'id': '3814',\n",
       "     'title': 'Learning to Explore and Exploit in POMDPs'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3832',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28598168,\n",
       "    '_source': {'id': '3832',\n",
       "     'title': 'Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '205',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28588897,\n",
       "    '_source': {'id': '205',\n",
       "     'title': 'The CHIR Algorithm for Feed Forward Networks with Binary Weights'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4571',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28585827,\n",
       "    '_source': {'id': '4571',\n",
       "     'title': 'Online L1-Dictionary Learning with Application to Novel Document Detection'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4854',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28585827,\n",
       "    '_source': {'id': '4854',\n",
       "     'title': 'Transferring Expectations in Model-based Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1382',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285731,\n",
       "    '_source': {'id': '1382',\n",
       "     'title': 'Reinforcement Learning for Call Admission Control and Routing in Integrated Service Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3391',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285731,\n",
       "    '_source': {'id': '3391',\n",
       "     'title': 'Hebbian Learning of Bayes Optimal Decisions'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4801',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285731,\n",
       "    '_source': {'id': '4801',\n",
       "     'title': 'Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1984',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28565237,\n",
       "    '_source': {'id': '1984',\n",
       "     'title': 'Playing is believing: The role of beliefs in multi-agent learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3960',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28565237,\n",
       "    '_source': {'id': '3960',\n",
       "     'title': 'On the Theory of Learnining with Privileged Information'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1585',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2856508,\n",
       "    '_source': {'id': '1585',\n",
       "     'title': 'Multiple Paired Forward-Inverse Models for Human Motor Learning and Control'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '622',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2856508,\n",
       "    '_source': {'id': '622',\n",
       "     'title': 'Information, Prediction, and Query by Committee'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6621',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285648,\n",
       "    '_source': {'id': '6621',\n",
       "     'title': 'Label Efficient Learning of Transferable Representations acrosss Domains and Tasks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3935',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28546405,\n",
       "    '_source': {'id': '3935',\n",
       "     'title': 'Large Margin Multi-Task Metric Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4591',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28546405,\n",
       "    '_source': {'id': '4591',\n",
       "     'title': 'Multilabel Classification using Bayesian Compressed Sensing'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1065',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28545988,\n",
       "    '_source': {'id': '1065',\n",
       "     'title': 'A Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4545',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28545988,\n",
       "    '_source': {'id': '4545', 'title': 'Imitation Learning by Coaching'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6670',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2854323,\n",
       "    '_source': {'id': '6670',\n",
       "     'title': 'Recycling Privileged Learning and Distribution Matching for Fairness'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '837',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28539816,\n",
       "    '_source': {'id': '837',\n",
       "     'title': 'Agnostic PAC-Learning of Functions on Analog Neural Nets'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '308',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28534764,\n",
       "    '_source': {'id': '308',\n",
       "     'title': 'A Framework for the Cooperation of Learning Algorithms'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3903',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285324,\n",
       "    '_source': {'id': '3903',\n",
       "     'title': 'Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3921',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285324,\n",
       "    '_source': {'id': '3921',\n",
       "     'title': 'More data means less inference: A pseudo-max approach to structured learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3981',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.285324,\n",
       "    '_source': {'id': '3981',\n",
       "     'title': 'Exact learning curves for Gaussian process regression on large random graphs'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1058',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2852583,\n",
       "    '_source': {'id': '1058',\n",
       "     'title': 'From Isolation to Cooperation: An Alternative View of a System of Experts'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1208',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2852583,\n",
       "    '_source': {'id': '1208',\n",
       "     'title': 'A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1780',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2852583,\n",
       "    '_source': {'id': '1780',\n",
       "     'title': 'Learning Statistically Neutral Tasks without Expert Guidance'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7123',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28517926,\n",
       "    '_source': {'id': '7123',\n",
       "     'title': 'Hybrid Reward Architecture for Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7137',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28517926,\n",
       "    '_source': {'id': '7137',\n",
       "     'title': 'Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '428',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28512976,\n",
       "    '_source': {'id': '428',\n",
       "     'title': 'Navigating through Temporal Difference'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2158',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28504694,\n",
       "    '_source': {'id': '2158', 'title': 'Dynamical Causal Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3760',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28502962,\n",
       "    '_source': {'id': '3760',\n",
       "     'title': 'Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6660',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28487477,\n",
       "    '_source': {'id': '6660',\n",
       "     'title': 'Hypothesis Transfer Learning via Transformation Functions'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1284',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28484398,\n",
       "    '_source': {'id': '1284',\n",
       "     'title': 'Analytical Mean Squared Error Curves in Temporal Difference Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '537',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28484398,\n",
       "    '_source': {'id': '537',\n",
       "     'title': 'Adaptive Synchronization of Neural and Physical Oscillators'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '843',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28484398,\n",
       "    '_source': {'id': '843',\n",
       "     'title': 'Robust Reinforcement Learning in Motion Planning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2824',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28482506,\n",
       "    '_source': {'id': '2824',\n",
       "     'title': 'A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4593',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28471452,\n",
       "    '_source': {'id': '4593',\n",
       "     'title': 'Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '194',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28459188,\n",
       "    '_source': {'id': '194',\n",
       "     'title': 'Sequential Decision Problems and Neural Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2907',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28459188,\n",
       "    '_source': {'id': '2907',\n",
       "     'title': 'Radial Basis Function Network for Multi-task Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3115',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28459188,\n",
       "    '_source': {'id': '3115',\n",
       "     'title': 'A Local Learning Approach for Clustering'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5870',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28459188,\n",
       "    '_source': {'id': '5870',\n",
       "     'title': 'Equilibrated adaptive learning rates for non-convex optimization'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6814',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28454846,\n",
       "    '_source': {'id': '6814',\n",
       "     'title': 'Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1394',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28453913,\n",
       "    '_source': {'id': '1394',\n",
       "     'title': 'Combining Classifiers Using Correspondence Analysis'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '573',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28453913,\n",
       "    '_source': {'id': '573',\n",
       "     'title': 'Active Exploration in Dynamic Environments'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '604',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28453913,\n",
       "    '_source': {'id': '604',\n",
       "     'title': 'Metamorphosis Networks: An Alternative to Constructive Models'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '901',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28453913,\n",
       "    '_source': {'id': '901',\n",
       "     'title': 'Higher Order Statistical Decorrelation without Information Loss'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2048',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28434652,\n",
       "    '_source': {'id': '2048', 'title': 'Motivated Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7002',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28434652,\n",
       "    '_source': {'id': '7002', 'title': 'Learning Low-Dimensional Metrics'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1626',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28421324,\n",
       "    '_source': {'id': '1626',\n",
       "     'title': 'A Phase Space Approach to Minimax Entropy Learning and the Minutemax Approximations'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2576',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28408802,\n",
       "    '_source': {'id': '2576',\n",
       "     'title': 'Object Classification from a Single Example Utilizing Class Relevance Metrics'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3621',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28408802,\n",
       "    '_source': {'id': '3621',\n",
       "     'title': 'Analyzing human feature learning as nonparametric Bayesian inference'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1133',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '1133', 'title': 'Stable Fitted Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1228',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '1228',\n",
       "     'title': 'Efficient Nonlinear Control with Actor-Tutor Architecture'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1242',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '1242',\n",
       "     'title': 'Unsupervised Learning by Convex and Conic Coding'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1841',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '1841', 'title': 'Robust Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2596',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '2596',\n",
       "     'title': 'Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '297',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '297',\n",
       "     'title': 'Spherical Units as Dynamic Consequential Regions: Implications for Attention, Competition and Categorization'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '951',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28386405,\n",
       "    '_source': {'id': '951',\n",
       "     'title': 'Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3706',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28382018,\n",
       "    '_source': {'id': '3706',\n",
       "     'title': 'Heterogeneous multitask learning with joint sparsity constraints'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3872',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28382018,\n",
       "    '_source': {'id': '3872',\n",
       "     'title': '3D Object Recognition with Deep Belief Nets'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4807',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28382018,\n",
       "    '_source': {'id': '4807', 'title': 'Multi-task Vector Field Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5939',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28382018,\n",
       "    '_source': {'id': '5939',\n",
       "     'title': 'Efficient and Parsimonious Agnostic Active Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6420',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28382018,\n",
       "    '_source': {'id': '6420',\n",
       "     'title': 'Cooperative Inverse Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1431',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28381523,\n",
       "    '_source': {'id': '1431', 'title': 'Learning to Order Things'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2740',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28381523,\n",
       "    '_source': {'id': '2740',\n",
       "     'title': 'Semi-supervised Learning by Entropy Minimization'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2868',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28381523,\n",
       "    '_source': {'id': '2868',\n",
       "     'title': 'Bayesian model learning in human visual perception'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4497',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2836202,\n",
       "    '_source': {'id': '4497',\n",
       "     'title': 'Emergence of Object-Selective Features in Unsupervised Feature Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4729',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2836202,\n",
       "    '_source': {'id': '4729',\n",
       "     'title': 'Multi-Stage Multi-Task Feature Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5385',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2836202,\n",
       "    '_source': {'id': '5385',\n",
       "     'title': 'Discriminative Metric Learning by Neighborhood Gerrymandering'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6200',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2836202,\n",
       "    '_source': {'id': '6200',\n",
       "     'title': 'Improved Deep Metric Learning with Multi-class N-pair Loss Objective'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1747',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28352696,\n",
       "    '_source': {'id': '1747', 'title': 'A SNoW-Based Face Detector'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1096',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28348896,\n",
       "    '_source': {'id': '1096',\n",
       "     'title': 'Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1355',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28348896,\n",
       "    '_source': {'id': '1355',\n",
       "     'title': 'A Neural Network Model of Naive Preference and Filial Imprinting in the Domestic Chick'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1418',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28348896,\n",
       "    '_source': {'id': '1418',\n",
       "     'title': 'An Incremental Nearest Neighbor Algorithm with Queries'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '504',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28348896,\n",
       "    '_source': {'id': '504',\n",
       "     'title': \"Green's Function Method for Fast On-Line Learning Algorithm of Recurrent Neural Networks\"},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5029',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28341216,\n",
       "    '_source': {'id': '5029',\n",
       "     'title': 'Discriminative Transfer Learning with Tree-based Priors'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5064',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28341216,\n",
       "    '_source': {'id': '5064',\n",
       "     'title': 'Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5247',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28341216,\n",
       "    '_source': {'id': '5247',\n",
       "     'title': 'Sparse Multi-Task Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2883',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28322187,\n",
       "    '_source': {'id': '2883',\n",
       "     'title': 'Online Discovery and Learning of Predictive State Representations'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3887',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2831956,\n",
       "    '_source': {'id': '3887',\n",
       "     'title': 'Maximin affinity learning of image segmentation'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4292',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2831956,\n",
       "    '_source': {'id': '4292',\n",
       "     'title': 'Clustered Multi-Task Learning Via Alternating Structure Optimization'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5026',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2831956,\n",
       "    '_source': {'id': '5026',\n",
       "     'title': 'Learning Stochastic Feedforward Neural Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1346',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28308496,\n",
       "    '_source': {'id': '1346',\n",
       "     'title': 'A Framework for Multiple-Instance Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '184',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28308496,\n",
       "    '_source': {'id': '184',\n",
       "     'title': 'Does the Neuron \"Learn\" like the Synapse?'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2138',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28308496,\n",
       "    '_source': {'id': '2138',\n",
       "     'title': 'Scaling of Probability-Based Optimization Algorithms'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '871',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28308496,\n",
       "    '_source': {'id': '871',\n",
       "     'title': 'Comparison Training for a Rescheduling Problem in Neural Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5340',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28297,\n",
       "    '_source': {'id': '5340',\n",
       "     'title': 'Design Principles of the Hippocampal Cognitive Map'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5386',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28297,\n",
       "    '_source': {'id': '5386',\n",
       "     'title': 'Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5474',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28297,\n",
       "    '_source': {'id': '5474',\n",
       "     'title': 'Active Learning and Best-Response Dynamics'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5530',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28297,\n",
       "    '_source': {'id': '5530',\n",
       "     'title': 'Advances in Learning Bayesian Networks of Bounded Treewidth'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6114',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28297,\n",
       "    '_source': {'id': '6114',\n",
       "     'title': 'Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4216',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2828984,\n",
       "    '_source': {'id': '4216',\n",
       "     'title': 'Optimal learning rates for least squares SVMs using Gaussian kernels'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '585',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2828984,\n",
       "    '_source': {'id': '585',\n",
       "     'title': 'Fast, Robust Adaptive Control by Learning only Forward Models'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1225',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28285655,\n",
       "    '_source': {'id': '1225', 'title': 'Dynamics of Training'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5142',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28273472,\n",
       "    '_source': {'id': '5142',\n",
       "     'title': 'Learning Adaptive Value of Information for Structured Prediction'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5952',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28273472,\n",
       "    '_source': {'id': '5952',\n",
       "     'title': 'Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6350',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28273472,\n",
       "    '_source': {'id': '6350',\n",
       "     'title': 'Adaptive Skills Adaptive Partitions (ASAP)'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6697',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28273472,\n",
       "    '_source': {'id': '6697',\n",
       "     'title': 'Decoupling \"when to update\" from \"how to update\"'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7055',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28273472,\n",
       "    '_source': {'id': '7055',\n",
       "     'title': 'QMDP-Net: Deep Learning for Planning under Partial Observability'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1011',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28264856,\n",
       "    '_source': {'id': '1011',\n",
       "     'title': 'Active Learning with Statistical Models'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1018',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28264856,\n",
       "    '_source': {'id': '1018',\n",
       "     'title': 'Generalization in Reinforcement Learning: Safely Approximating the Value Function'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1062',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28264856,\n",
       "    '_source': {'id': '1062',\n",
       "     'title': 'Universal Approximation and Learning of Trajectories Using Oscillators'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1243',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28264856,\n",
       "    '_source': {'id': '1243',\n",
       "     'title': 'Learning with Noise and Regularizers in Multilayer Neural Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2345',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2825549,\n",
       "    '_source': {'id': '2345',\n",
       "     'title': 'Error Bounds for Transductive Learning via Compression and Clustering'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5766',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2824892,\n",
       "    '_source': {'id': '5766',\n",
       "     'title': 'On the Pseudo-Dimension of Nearly Optimal Auctions'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2824892,\n",
       "    '_source': {'id': '6007',\n",
       "     'title': 'Lifelong Learning with Non-i.i.d. Tasks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3843',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28223273,\n",
       "    '_source': {'id': '3843',\n",
       "     'title': 'Adaptive Regularization for Transductive Support Vector Machine'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4032',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28223273,\n",
       "    '_source': {'id': '4032',\n",
       "     'title': 'Tight Sample Complexity of Large-Margin Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4194',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28223273,\n",
       "    '_source': {'id': '4194',\n",
       "     'title': 'Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4306',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28223273,\n",
       "    '_source': {'id': '4306',\n",
       "     'title': 'Similarity-based Learning via Data Driven Embeddings'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5469',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28223273,\n",
       "    '_source': {'id': '5469',\n",
       "     'title': 'A Drifting-Games Analysis for Online Learning and Applications to Boosting'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6516',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28223273,\n",
       "    '_source': {'id': '6516', 'title': 'Tensor Switching Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2266',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28218937,\n",
       "    '_source': {'id': '2266',\n",
       "     'title': 'FloatBoost Learning for Classification'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3482',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28218937,\n",
       "    '_source': {'id': '3482',\n",
       "     'title': 'Using matrices to model symbolic relationship'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1854',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28217578,\n",
       "    '_source': {'id': '1854',\n",
       "     'title': 'Algorithmic Stability and Generalization Performance'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6171',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28217578,\n",
       "    '_source': {'id': '6171',\n",
       "     'title': 'The Limits of Learning with Missing Data'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3840',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2819645,\n",
       "    '_source': {'id': '3840',\n",
       "     'title': 'Kernels and learning curves for Gaussian process regression on random graphs'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5722',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2819645,\n",
       "    '_source': {'id': '5722', 'title': 'Combinatorial Cascading Bandits'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1872',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28179973,\n",
       "    '_source': {'id': '1872', 'title': 'Dopamine Bonuses'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '2826',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28179973,\n",
       "    '_source': {'id': '2826',\n",
       "     'title': 'Temporal Abstraction in Temporal-difference Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3490',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28179973,\n",
       "    '_source': {'id': '3490',\n",
       "     'title': 'Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4937',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28179973,\n",
       "    '_source': {'id': '4937',\n",
       "     'title': 'Accelerating Stochastic Gradient Descent using Predictive Variance Reduction'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3070',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28168374,\n",
       "    '_source': {'id': '3070',\n",
       "     'title': 'Information Bottleneck Optimization and Independent Component Extraction with Spiking Neurons'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4999',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28168374,\n",
       "    '_source': {'id': '4999',\n",
       "     'title': 'Speedup Matrix Completion with Side Information: Application to Multi-Label Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6082',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28168374,\n",
       "    '_source': {'id': '6082',\n",
       "     'title': 'Sampling for Bayesian Program Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7172',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28168374,\n",
       "    '_source': {'id': '7172',\n",
       "     'title': 'Collaborative Deep Learning in Fixed Topology Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1424',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28167263,\n",
       "    '_source': {'id': '1424',\n",
       "     'title': 'Learning Nonlinear Overcomplete Representations for Efficient Coding'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1826',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2816618,\n",
       "    '_source': {'id': '1826',\n",
       "     'title': 'Algebraic Information Geometry for Learning Machines with Singularities'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '230',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2816618,\n",
       "    '_source': {'id': '230',\n",
       "     'title': 'Learning Aspect Graph Representations from View Sequences'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '634',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2816618,\n",
       "    '_source': {'id': '634',\n",
       "     'title': 'Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '889',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.2816618,\n",
       "    '_source': {'id': '889',\n",
       "     'title': 'Reinforcement Learning Methods for Continuous-Time Markov Decision Problems'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4040',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28138956,\n",
       "    '_source': {'id': '4040',\n",
       "     'title': 'Generative Local Metric Learning for Nearest Neighbor Classification'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5547',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28138956,\n",
       "    '_source': {'id': '5547',\n",
       "     'title': 'Deep Joint Task Learning for Generic Object Extraction'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5942',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28138956,\n",
       "    '_source': {'id': '5942',\n",
       "     'title': 'Scalable Semi-Supervised Aggregation of Classifiers'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6788',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28138956,\n",
       "    '_source': {'id': '6788',\n",
       "     'title': 'Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3611',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28138342,\n",
       "    '_source': {'id': '3611',\n",
       "     'title': 'PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1221',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28110102,\n",
       "    '_source': {'id': '1221',\n",
       "     'title': 'Multi-Task Learning for Stock Selection'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '325',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28110102,\n",
       "    '_source': {'id': '325',\n",
       "     'title': 'Leaning by Combining Memorization and Gradient Descent'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '506',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28110102,\n",
       "    '_source': {'id': '506',\n",
       "     'title': 'Principles of Risk Minimization for Learning Theory'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '612',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28110102,\n",
       "    '_source': {'id': '612',\n",
       "     'title': 'Combining Neural and Symbolic Learning to Revise Probabilistic Rule Bases'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '772',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28110102,\n",
       "    '_source': {'id': '772',\n",
       "     'title': 'Optimal Stochastic Search and Adaptive Momentum'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '894',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28110102,\n",
       "    '_source': {'id': '894',\n",
       "     'title': 'JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3284',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28108096,\n",
       "    '_source': {'id': '3284',\n",
       "     'title': 'Gaussian Process Models for Link Analysis and Transfer Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3448',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28108096,\n",
       "    '_source': {'id': '3448', 'title': 'Supervised Dictionary Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3551',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28108096,\n",
       "    '_source': {'id': '3551',\n",
       "     'title': \"Unlabeled data: Now it helps, now it doesn't\"},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4820',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28108096,\n",
       "    '_source': {'id': '4820',\n",
       "     'title': 'Active Learning of Multi-Index Function Models'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5107',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28108096,\n",
       "    '_source': {'id': '5107',\n",
       "     'title': 'Sequential Transfer in Multi-armed Bandit with Finite Set of Models'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3381',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28093767,\n",
       "    '_source': {'id': '3381',\n",
       "     'title': 'Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '578',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28093767,\n",
       "    '_source': {'id': '578',\n",
       "     'title': 'A Comparison of Projection Pursuit and Neural Network Regression Modeling'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3477',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28075683,\n",
       "    '_source': {'id': '3477',\n",
       "     'title': 'Linear Classification and Selective Sampling Under Low Noise Conditions'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4019',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 0.28075683,\n",
       "    '_source': {'id': '4019',\n",
       "     'title': 'Two-Layer Generalization Analysis for Ranking Using Rademacher Average'},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 0.2901017,\n",
       "  'total': 6431},\n",
       " 'timed_out': False,\n",
       " 'took': 77}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieves first 200 documents whose content contains \"learning\"\n",
    "#takes total score of the entire index\n",
    "es.search(index='nips_papers', body=\n",
    "          {\n",
    "              \"_source\": [\"id\", \"title\"],\n",
    "              \"size\": 200,\n",
    "              \"query\": {\n",
    "                  \"match\": {\n",
    "                      \"paper_text\": \"learning\"\n",
    "                  }\n",
    "              }\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'aggregations': {'years': {'buckets': [{'doc_count': 679,\n",
       "     'key': '2017',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 184,\n",
       "        'doc_count': 61,\n",
       "        'key': 'deep',\n",
       "        'score': 0.22777688686180225},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 25,\n",
       "        'key': 'adversarial',\n",
       "        'score': 0.19146256628641664},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 14,\n",
       "        'key': 'gan',\n",
       "        'score': 0.17177702200021255},\n",
       "       {'bg_count': 1547,\n",
       "        'doc_count': 180,\n",
       "        'key': 'learning',\n",
       "        'score': 0.06384200431763128},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 19,\n",
       "        'key': 'generative',\n",
       "        'score': 0.06346573613108793},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 6,\n",
       "        'key': 'fairness',\n",
       "        'score': 0.06183940256940272},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 5,\n",
       "        'key': 'gated',\n",
       "        'score': 0.058076902777265654},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 5,\n",
       "        'key': 'safe',\n",
       "        'score': 0.058076902777265654},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 4,\n",
       "        'key': 'disentangled',\n",
       "        'score': 0.05693202990623394},\n",
       "       {'bg_count': 141,\n",
       "        'doc_count': 28,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.046091376221197605},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 17,\n",
       "        'key': 'descent',\n",
       "        'score': 0.045884510542446334},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 13,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.04405119015858626},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'start',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'assumption',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cold',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 5,\n",
       "        'key': 'imitation',\n",
       "        'score': 0.036263345101387706},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'wasserstein',\n",
       "        'score': 0.035991014537391106},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'acceleration',\n",
       "        'score': 0.035991014537391106},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'convolution',\n",
       "        'score': 0.035991014537391106},\n",
       "       {'bg_count': 154,\n",
       "        'doc_count': 27,\n",
       "        'key': 'online',\n",
       "        'score': 0.03458304423836773},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'frank',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recommendation',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'wolfe',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'deeper',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hawke',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'inner',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'decentralized',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'revenue',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 121,\n",
       "        'doc_count': 22,\n",
       "        'key': 'variational',\n",
       "        'score': 0.030422457004908466},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'shot',\n",
       "        'score': 0.030007867289150298},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 9,\n",
       "        'key': 'agent',\n",
       "        'score': 0.029150769671243994},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'update',\n",
       "        'score': 0.028331142310159364},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'compression',\n",
       "        'score': 0.028331142310159364},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'predicting',\n",
       "        'score': 0.028331142310159364},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'translation',\n",
       "        'score': 0.028331142310159364},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'invariance',\n",
       "        'score': 0.028331142310159364},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 8,\n",
       "        'key': 'regret',\n",
       "        'score': 0.025446439366242593},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 5,\n",
       "        'key': 'general',\n",
       "        'score': 0.025356566263448732},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 6,\n",
       "        'key': 'streaming',\n",
       "        'score': 0.024422735403014425},\n",
       "       {'bg_count': 92,\n",
       "        'doc_count': 17,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.024299758118277327},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 9,\n",
       "        'key': 'variance',\n",
       "        'score': 0.02416188071572555},\n",
       "       {'bg_count': 226,\n",
       "        'doc_count': 32,\n",
       "        'key': 'multi',\n",
       "        'score': 0.02403425890769356},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multitask',\n",
       "        'score': 0.023852108597716904},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'riemannian',\n",
       "        'score': 0.023852108597716904},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'trust',\n",
       "        'score': 0.023852108597716904},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'counterfactual',\n",
       "        'score': 0.023852108597716904},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'stein',\n",
       "        'score': 0.023852108597716904},\n",
       "       {'bg_count': 111,\n",
       "        'doc_count': 19,\n",
       "        'key': 'recurrent',\n",
       "        'score': 0.02309677134692698},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 5,\n",
       "        'key': 'collaborative',\n",
       "        'score': 0.022839617300847426},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'interpretable',\n",
       "        'score': 0.022030337624829224},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'alternating',\n",
       "        'score': 0.022030337624829224},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'accuracy',\n",
       "        'score': 0.022030337624829224},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'practical',\n",
       "        'score': 0.022030337624829224},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'hypothesi',\n",
       "        'score': 0.022030337624829224},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'stable',\n",
       "        'score': 0.022030337624829224},\n",
       "       {'bg_count': 158,\n",
       "        'doc_count': 24,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.021910349882663716},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 8,\n",
       "        'key': 'attention',\n",
       "        'score': 0.021723592189559424},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 6,\n",
       "        'key': 'empirical',\n",
       "        'score': 0.020921760697387458},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 5,\n",
       "        'key': 'guarantees',\n",
       "        'score': 0.0206822324757606},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 5,\n",
       "        'key': 'sum',\n",
       "        'score': 0.0206822324757606},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 5,\n",
       "        'key': 'objective',\n",
       "        'score': 0.0206822324757606},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 5,\n",
       "        'key': 'accelerated',\n",
       "        'score': 0.0206822324757606},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 6,\n",
       "        'key': 'batch',\n",
       "        'score': 0.01943384644749599},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 7,\n",
       "        'key': 'coordinate',\n",
       "        'score': 0.019290041449675843},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'moment',\n",
       "        'score': 0.019238202242316843},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'influence',\n",
       "        'score': 0.019238202242316843},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'differentiable',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'activation',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'box',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'edge',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'worst',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'thresholding',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'perspective',\n",
       "        'score': 0.019140380139727266},\n",
       "       {'bg_count': 216,\n",
       "        'doc_count': 29,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.018440805899242355},\n",
       "       {'bg_count': 96,\n",
       "        'doc_count': 16,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.018317965936507457},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 6,\n",
       "        'key': 'language',\n",
       "        'score': 0.018087638316641813},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 5,\n",
       "        'key': 'reasoning',\n",
       "        'score': 0.017176482134994498},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 7,\n",
       "        'key': 'polynomial',\n",
       "        'score': 0.017175804321090747},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 7,\n",
       "        'key': 'domain',\n",
       "        'score': 0.017175804321090747},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'residual',\n",
       "        'score': 0.01695372783844308},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'testing',\n",
       "        'score': 0.01695372783844308},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'quadratic',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'screening',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'interactive',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'autoencoder',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'calcium',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'black',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'arm',\n",
       "        'score': 0.015774859812591815},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 9,\n",
       "        'key': 'convolutional',\n",
       "        'score': 0.015658092723364575},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'size',\n",
       "        'score': 0.01504999916854828},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'partial',\n",
       "        'score': 0.01504999916854828},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'hard',\n",
       "        'score': 0.01504999916854828},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 11,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.014953757002017643},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 5,\n",
       "        'key': 'understanding',\n",
       "        'score': 0.014449787425509758},\n",
       "       {'bg_count': 125,\n",
       "        'doc_count': 18,\n",
       "        'key': 'representation',\n",
       "        'score': 0.014199760975704979},\n",
       "       {'bg_count': 193,\n",
       "        'doc_count': 25,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.014041775453573963},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 9,\n",
       "        'key': 'brain',\n",
       "        'score': 0.013812589797362834},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 6,\n",
       "        'key': 'inverse',\n",
       "        'score': 0.013779772297908428},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 6,\n",
       "        'key': 'transfer',\n",
       "        'score': 0.013779772297908428},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'geometric',\n",
       "        'score': 0.013439151832483445},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 5,\n",
       "        'key': 'theoretic',\n",
       "        'score': 0.013301705442568814},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 5,\n",
       "        'key': 'case',\n",
       "        'score': 0.013301705442568814},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'program',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'estimate',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reducing',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'quantization',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subset',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'robustness',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'train',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'aware',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'semidefinite',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'accurate',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'universal',\n",
       "        'score': 0.013250719567240223},\n",
       "       {'bg_count': 112,\n",
       "        'doc_count': 16,\n",
       "        'key': 'training',\n",
       "        'score': 0.012334818688266646},\n",
       "       {'bg_count': 281,\n",
       "        'doc_count': 33,\n",
       "        'key': 'data',\n",
       "        'score': 0.012265928811991239},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'knowledge',\n",
       "        'score': 0.012058425544427874},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'evaluation',\n",
       "        'score': 0.012058425544427874},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 11,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.011746722283344203},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 7,\n",
       "        'key': 'maximization',\n",
       "        'score': 0.011678787786769507},\n",
       "       {'bg_count': 223,\n",
       "        'doc_count': 27,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.011578690661082705},\n",
       "       {'bg_count': 134,\n",
       "        'doc_count': 18,\n",
       "        'key': 'processe',\n",
       "        'score': 0.011465551983964125},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 5,\n",
       "        'key': 'design',\n",
       "        'score': 0.011333564900384339},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 5,\n",
       "        'key': 'consistency',\n",
       "        'score': 0.011333564900384339},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'question',\n",
       "        'score': 0.011287499376411209},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'contextual',\n",
       "        'score': 0.011287499376411209},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'side',\n",
       "        'score': 0.011287499376411209},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cost',\n",
       "        'score': 0.011287499376411209},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'fixed',\n",
       "        'score': 0.011287499376411209},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'limited',\n",
       "        'score': 0.011287499376411209},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 8,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.01106271163814853},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 15,\n",
       "        'key': 'structured',\n",
       "        'score': 0.010934823300224668},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'generation',\n",
       "        'score': 0.010861796094779712},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'encoding',\n",
       "        'score': 0.010861796094779712},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'text',\n",
       "        'score': 0.010861796094779712},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'partition',\n",
       "        'score': 0.010861796094779712},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'flow',\n",
       "        'score': 0.010861796094779712},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 6,\n",
       "        'key': 'causal',\n",
       "        'score': 0.010660283111929085},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 10,\n",
       "        'key': 'net',\n",
       "        'score': 0.010604332929315431},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 5,\n",
       "        'key': 'correlation',\n",
       "        'score': 0.010483686029895587},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 22,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.01030496471236844},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 11,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.010194110237002306},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'faster',\n",
       "        'score': 0.00981474532633757},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'property',\n",
       "        'score': 0.00981474532633757},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'log',\n",
       "        'score': 0.009716923223747995},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'implicit',\n",
       "        'score': 0.009716923223747995},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'baye',\n",
       "        'score': 0.009716923223747995},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'imaging',\n",
       "        'score': 0.009716923223747995},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'flexible',\n",
       "        'score': 0.009716923223747995},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'best',\n",
       "        'score': 0.009716923223747995},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'improved',\n",
       "        'score': 0.009707709669884119},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'output',\n",
       "        'score': 0.009707709669884119},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 17,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.00961176955482971},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 8,\n",
       "        'key': 'game',\n",
       "        'score': 0.009604536486739746},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 5,\n",
       "        'key': 'simple',\n",
       "        'score': 0.008996398006540273},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 5,\n",
       "        'key': 'factorization',\n",
       "        'score': 0.008996398006540273},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 5,\n",
       "        'key': 'constrained',\n",
       "        'score': 0.008996398006540273},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 7,\n",
       "        'key': 'risk',\n",
       "        'score': 0.008930279519608883},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 6,\n",
       "        'key': 'dual',\n",
       "        'score': 0.00883245741701931},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 8,\n",
       "        'key': 'low',\n",
       "        'score': 0.008731615307665098},\n",
       "       {'bg_count': 135,\n",
       "        'doc_count': 17,\n",
       "        'key': 'regression',\n",
       "        'score': 0.008585144713168034},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'communication',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'private',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'experimental',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multiplicative',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'symmetric',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'additive',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'expectation',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pruning',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'locally',\n",
       "        'score': 0.008431906371569006},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'predictive',\n",
       "        'score': 0.008341991276263934},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 7,\n",
       "        'key': 'filtering',\n",
       "        'score': 0.008014110097222011},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 7,\n",
       "        'key': 'label',\n",
       "        'score': 0.008014110097222011},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 9,\n",
       "        'key': 'complexity',\n",
       "        'score': 0.007947991610290625},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'reduction',\n",
       "        'score': 0.00773792352523962},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'exploration',\n",
       "        'score': 0.00773792352523962},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'identification',\n",
       "        'score': 0.00773792352523962},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unified',\n",
       "        'score': 0.007361058994753178},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cross',\n",
       "        'score': 0.007361058994753178},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pac',\n",
       "        'score': 0.007361058994753178},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'transformation',\n",
       "        'score': 0.007361058994753178},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reward',\n",
       "        'score': 0.007361058994753178},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 6,\n",
       "        'key': 'parallel',\n",
       "        'score': 0.007317973269808357},\n",
       "       {'bg_count': 860,\n",
       "        'doc_count': 85,\n",
       "        'key': 'network',\n",
       "        'score': 0.006762565080853684},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'multivariate',\n",
       "        'score': 0.006673593021011146},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'study',\n",
       "        'score': 0.006673593021011146},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'decoding',\n",
       "        'score': 0.006673593021011146},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'parametric',\n",
       "        'score': 0.006673593021011146},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 5,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.0066592311126962055},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 8,\n",
       "        'key': 'sequence',\n",
       "        'score': 0.006493762830401003},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'consistent',\n",
       "        'score': 0.006454957368216709},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connectivity',\n",
       "        'score': 0.006454957368216709},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'strategy',\n",
       "        'score': 0.006454957368216709},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 7,\n",
       "        'key': 'metric',\n",
       "        'score': 0.006420771971331795},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 13,\n",
       "        'key': 'bound',\n",
       "        'score': 0.00637605982981991},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 10,\n",
       "        'key': 'sample',\n",
       "        'score': 0.006213474868106451},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 5,\n",
       "        'key': 'minimax',\n",
       "        'score': 0.006175679341556055},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'video',\n",
       "        'score': 0.006075278296187065},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'discovery',\n",
       "        'score': 0.006075278296187065},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'interaction',\n",
       "        'score': 0.006075278296187065},\n",
       "       {'bg_count': 146,\n",
       "        'doc_count': 17,\n",
       "        'key': 'graph',\n",
       "        'score': 0.0060519830747528406}],\n",
       "      'doc_count': 679}},\n",
       "    {'doc_count': 569,\n",
       "     'key': '2016',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 184,\n",
       "        'doc_count': 43,\n",
       "        'key': 'deep',\n",
       "        'score': 0.14917540824196268},\n",
       "       {'bg_count': 193,\n",
       "        'doc_count': 38,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.10055005553015162},\n",
       "       {'bg_count': 216,\n",
       "        'doc_count': 39,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.0889474282984465},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 19,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.06628536878731724},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 15,\n",
       "        'key': 'convolutional',\n",
       "        'score': 0.06513222014780264},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'maximizing',\n",
       "        'score': 0.06453896547144343},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'posterior',\n",
       "        'score': 0.061823382062694394},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'composition',\n",
       "        'score': 0.061823382062694394},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 5,\n",
       "        'key': 'oracle',\n",
       "        'score': 0.06110410148226625},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 5,\n",
       "        'key': 'budget',\n",
       "        'score': 0.06110410148226625},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 5,\n",
       "        'key': 'interpretable',\n",
       "        'score': 0.06110410148226625},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 11,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.05380829385573625},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'strategic',\n",
       "        'score': 0.052610825063344045},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'safe',\n",
       "        'score': 0.052610825063344045},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 5,\n",
       "        'key': 'chain',\n",
       "        'score': 0.04712581194152478},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'supervision',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reparameterization',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'path',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'fidelity',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'crowdsourcing',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'aggregation',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 11,\n",
       "        'key': 'adversarial',\n",
       "        'score': 0.0436026488962609},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 11,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.042172312292092},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'robustness',\n",
       "        'score': 0.04204279756307333},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 4,\n",
       "        'key': 'asynchronous',\n",
       "        'score': 0.037700649553219806},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 18,\n",
       "        'key': 'structured',\n",
       "        'score': 0.03608840685440429},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 8,\n",
       "        'key': 'joint',\n",
       "        'score': 0.0352980684240365},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exploiting',\n",
       "        'score': 0.034985066144470764},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'guided',\n",
       "        'score': 0.034985066144470764},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'score',\n",
       "        'score': 0.034985066144470764},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'flexible',\n",
       "        'score': 0.034985066144470764},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 5,\n",
       "        'key': 'combinatorial',\n",
       "        'score': 0.0342227754423788},\n",
       "       {'bg_count': 169,\n",
       "        'doc_count': 24,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.034047907608960364},\n",
       "       {'bg_count': 98,\n",
       "        'doc_count': 16,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.030304036947385413},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 6,\n",
       "        'key': 'ensemble',\n",
       "        'score': 0.02971265841160609},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 8,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.02931530207514652},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 10,\n",
       "        'key': 'constraint',\n",
       "        'score': 0.029019606026256004},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'contextual',\n",
       "        'score': 0.02875454424714527},\n",
       "       {'bg_count': 141,\n",
       "        'doc_count': 20,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.028298170476466312},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'augmented',\n",
       "        'score': 0.02827548716491486},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'autoencoder',\n",
       "        'score': 0.02827548716491486},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'automated',\n",
       "        'score': 0.02827548716491486},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tradeoff',\n",
       "        'score': 0.02827548716491486},\n",
       "       {'bg_count': 121,\n",
       "        'doc_count': 18,\n",
       "        'key': 'variational',\n",
       "        'score': 0.02825270482529443},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 6,\n",
       "        'key': 'block',\n",
       "        'score': 0.027795635846018695},\n",
       "       {'bg_count': 206,\n",
       "        'doc_count': 26,\n",
       "        'key': 'method',\n",
       "        'score': 0.02769860531484585},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 5,\n",
       "        'key': 'understanding',\n",
       "        'score': 0.02615837763041256},\n",
       "       {'bg_count': 147,\n",
       "        'doc_count': 20,\n",
       "        'key': 'fast',\n",
       "        'score': 0.025708474339232398},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'nonconvex',\n",
       "        'score': 0.025501415044936346},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'generating',\n",
       "        'score': 0.025501415044936346},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'residual',\n",
       "        'score': 0.025501415044936346},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'divergence',\n",
       "        'score': 0.025501415044936346},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 6,\n",
       "        'key': 'program',\n",
       "        'score': 0.0244616835580406},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 11,\n",
       "        'key': 'generative',\n",
       "        'score': 0.02431617468520935},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 7,\n",
       "        'key': 'completion',\n",
       "        'score': 0.02422764528978681},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 7,\n",
       "        'key': 'conditional',\n",
       "        'score': 0.02422764528978681},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 8,\n",
       "        'key': 'finite',\n",
       "        'score': 0.023608057860744904},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classe',\n",
       "        'score': 0.023482930750946356},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'smoothing',\n",
       "        'score': 0.023482930750946356},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pass',\n",
       "        'score': 0.023482930750946356},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 15,\n",
       "        'key': 'convex',\n",
       "        'score': 0.023461567619507618},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 6,\n",
       "        'key': 'interaction',\n",
       "        'score': 0.023003079432050186},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 6,\n",
       "        'key': 'dual',\n",
       "        'score': 0.023003079432050186},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'threshold',\n",
       "        'score': 0.022790474043095574},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'structural',\n",
       "        'score': 0.022790474043095574},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'social',\n",
       "        'score': 0.022790474043095574},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 5,\n",
       "        'key': 'observation',\n",
       "        'score': 0.022275519424651044},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 13,\n",
       "        'key': 'unsupervised',\n",
       "        'score': 0.02214967948873516},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 6,\n",
       "        'key': '3d',\n",
       "        'score': 0.021661163636139007},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 8,\n",
       "        'key': 'free',\n",
       "        'score': 0.020851876508424124},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 5,\n",
       "        'key': 'sum',\n",
       "        'score': 0.020640631759067243},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'losse',\n",
       "        'score': 0.020496600887691844},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mixed',\n",
       "        'score': 0.020496600887691844},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'long',\n",
       "        'score': 0.020496600887691844},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'flow',\n",
       "        'score': 0.020496600887691844},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 6,\n",
       "        'key': 'exploration',\n",
       "        'score': 0.02042247213222099},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'smooth',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'batch',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'code',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'update',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'count',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'incomplete',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'diverse',\n",
       "        'score': 0.019888513440469976},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 10,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.019700746332428354},\n",
       "       {'bg_count': 154,\n",
       "        'doc_count': 19,\n",
       "        'key': 'online',\n",
       "        'score': 0.019035746947694342},\n",
       "       {'bg_count': 112,\n",
       "        'doc_count': 15,\n",
       "        'key': 'training',\n",
       "        'score': 0.01856817771663136},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mcmc',\n",
       "        'score': 0.018530423897345792},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'estimating',\n",
       "        'score': 0.018530423897345792},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'video',\n",
       "        'score': 0.018530423897345792},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'partial',\n",
       "        'score': 0.018530423897345792},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'objective',\n",
       "        'score': 0.018530423897345792},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 10,\n",
       "        'key': 'metric',\n",
       "        'score': 0.01849831282354736},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 6,\n",
       "        'key': 'general',\n",
       "        'score': 0.01821052301808168},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 7,\n",
       "        'key': 'improved',\n",
       "        'score': 0.018139323623152744},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 9,\n",
       "        'key': 'descent',\n",
       "        'score': 0.017730671699185508},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 7,\n",
       "        'key': 'generalized',\n",
       "        'score': 0.01731657745198598},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 7,\n",
       "        'key': 'domain',\n",
       "        'score': 0.01731657745198598},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 6,\n",
       "        'key': 'factorization',\n",
       "        'score': 0.0172189596220882},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'alternating',\n",
       "        'score': 0.017092855532321684},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unlabeled',\n",
       "        'score': 0.017092855532321684},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'influence',\n",
       "        'score': 0.017092855532321684},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'minimizing',\n",
       "        'score': 0.017092855532321684},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hashing',\n",
       "        'score': 0.017092855532321684},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'correspondence',\n",
       "        'score': 0.017092855532321684},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 6,\n",
       "        'key': 'sparsity',\n",
       "        'score': 0.01629350045249428},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 9,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.015964993020355013},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 16,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.015586655418775474},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'making',\n",
       "        'score': 0.015522722545065789},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 6,\n",
       "        'key': 'power',\n",
       "        'score': 0.01542774832609997},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'unified',\n",
       "        'score': 0.015335386288033452},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'pac',\n",
       "        'score': 0.015335386288033452},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 7,\n",
       "        'key': 'decomposition',\n",
       "        'score': 0.01509516278983571},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'consistent',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'collaborative',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'forward',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'blind',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'armed',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'resolution',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'moment',\n",
       "        'score': 0.014856329205803044},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 5,\n",
       "        'key': 'cluster',\n",
       "        'score': 0.014509803013128002},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 5,\n",
       "        'key': 'regret',\n",
       "        'score': 0.014509803013128002},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'solution',\n",
       "        'score': 0.014019782566551902},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'scene',\n",
       "        'score': 0.013577917043745235},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'region',\n",
       "        'score': 0.013577917043745235},\n",
       "       {'bg_count': 93,\n",
       "        'doc_count': 12,\n",
       "        'key': 'random',\n",
       "        'score': 0.013540454124313723},\n",
       "       {'bg_count': 83,\n",
       "        'doc_count': 11,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.013272619699426504},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'communication',\n",
       "        'score': 0.013026444029560528},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'faster',\n",
       "        'score': 0.013026444029560528},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'intrinsic',\n",
       "        'score': 0.013026444029560528},\n",
       "       {'bg_count': 63,\n",
       "        'doc_count': 9,\n",
       "        'key': 'sample',\n",
       "        'score': 0.012938115285217005},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'nearest',\n",
       "        'score': 0.012850357036346082},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 7,\n",
       "        'key': 'maximization',\n",
       "        'score': 0.012604485744394503},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 4,\n",
       "        'key': 'neighbor',\n",
       "        'score': 0.011804028930372452},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 26,\n",
       "        'key': 'inference',\n",
       "        'score': 0.011792179752179076},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'adaptation',\n",
       "        'score': 0.011501539716025093},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'lower',\n",
       "        'score': 0.011501539716025093},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'behavior',\n",
       "        'score': 0.011501539716025093},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'step',\n",
       "        'score': 0.011501539716025093},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'view',\n",
       "        'score': 0.011501539716025093},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'shape',\n",
       "        'score': 0.011501539716025093},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 5,\n",
       "        'key': 'minimax',\n",
       "        'score': 0.011181638836760982},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 5,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.011181638836760982},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'causal',\n",
       "        'score': 0.010862333634996182},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'estimator',\n",
       "        'score': 0.010862333634996182},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 10,\n",
       "        'key': 'space',\n",
       "        'score': 0.01073576738646757},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 6,\n",
       "        'key': 'reduction',\n",
       "        'score': 0.010643328680236671},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 8,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.010619157234865352},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 8,\n",
       "        'key': 'order',\n",
       "        'score': 0.010619157234865352},\n",
       "       {'bg_count': 860,\n",
       "        'doc_count': 73,\n",
       "        'key': 'network',\n",
       "        'score': 0.01029135905088759},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'solving',\n",
       "        'score': 0.010211236066110496},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'loss',\n",
       "        'score': 0.010211236066110496},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'square',\n",
       "        'score': 0.010211236066110496},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reward',\n",
       "        'score': 0.010211236066110496},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'class',\n",
       "        'score': 0.010211236066110496},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'testing',\n",
       "        'score': 0.010211236066110496},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'greedy',\n",
       "        'score': 0.010010323605846229},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 5,\n",
       "        'key': 'global',\n",
       "        'score': 0.009850373166214177},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 5,\n",
       "        'key': 'pca',\n",
       "        'score': 0.009850373166214177},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 5,\n",
       "        'key': 'set',\n",
       "        'score': 0.009850373166214177},\n",
       "       {'bg_count': 129,\n",
       "        'doc_count': 14,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.009376760812341019},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 5,\n",
       "        'key': 'single',\n",
       "        'score': 0.00924915641177368},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 5,\n",
       "        'key': 'risk',\n",
       "        'score': 0.00924915641177368},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'response',\n",
       "        'score': 0.009235769033891722},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'projection',\n",
       "        'score': 0.009235769033891722},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'simple',\n",
       "        'score': 0.00910526150904084},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'quadratic',\n",
       "        'score': 0.00910526150904084},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'synthesi',\n",
       "        'score': 0.00910526150904084},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connectivity',\n",
       "        'score': 0.00910526150904084},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'trajectory',\n",
       "        'score': 0.00910526150904084},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'combining',\n",
       "        'score': 0.00910526150904084},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 18,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.009075358647308466},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 10,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.009050620968053893},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 7,\n",
       "        'key': 'matching',\n",
       "        'score': 0.008772674905254184},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'word',\n",
       "        'score': 0.00814675022624714},\n",
       "       {'bg_count': 158,\n",
       "        'doc_count': 16,\n",
       "        'key': 'feature',\n",
       "        'score': 0.008117880672956164},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 7,\n",
       "        'key': 'rank',\n",
       "        'score': 0.007992120845429305},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'relaxation',\n",
       "        'score': 0.007880298532971336},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 6,\n",
       "        'key': 'parameter',\n",
       "        'score': 0.007754036296695852},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 7,\n",
       "        'key': 'nonlinear',\n",
       "        'score': 0.007623131653512088},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 16,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.007442649829821361},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exact',\n",
       "        'score': 0.007308052853802651},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'coordinate',\n",
       "        'score': 0.007308052853802651},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measure',\n",
       "        'score': 0.007308052853802651},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'product',\n",
       "        'score': 0.006733361955269472},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 11,\n",
       "        'key': 'bound',\n",
       "        'score': 0.006688961919440573},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 6,\n",
       "        'key': 'game',\n",
       "        'score': 0.006586024482072968},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'missing',\n",
       "        'score': 0.006568025760469278},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'equation',\n",
       "        'score': 0.006568025760469278},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sensing',\n",
       "        'score': 0.006568025760469278},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'functional',\n",
       "        'score': 0.006568025760469278},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 5,\n",
       "        'key': 'segmentation',\n",
       "        'score': 0.006324318146928037},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'maximum',\n",
       "        'score': 0.006223612365179755},\n",
       "       {'bg_count': 94,\n",
       "        'doc_count': 10,\n",
       "        'key': 'local',\n",
       "        'score': 0.006218140817954305},\n",
       "       {'bg_count': 204,\n",
       "        'doc_count': 19,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.0061858296464270355},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 8,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.006100483355157958},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variance',\n",
       "        'score': 0.0059102238997285035},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'learn',\n",
       "        'score': 0.0059102238997285035},\n",
       "       {'bg_count': 194,\n",
       "        'doc_count': 18,\n",
       "        'key': 'linear',\n",
       "        'score': 0.005717848952917002},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 7,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.005663254634148515},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subspace',\n",
       "        'score': 0.005321664340118336}],\n",
       "      'doc_count': 569}},\n",
       "    {'doc_count': 411,\n",
       "     'key': '2014',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 76,\n",
       "        'doc_count': 16,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.10546194281412768},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exploiting',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'decomposable',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'permutation',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subset',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'tight',\n",
       "        'score': 0.08824750707642712},\n",
       "       {'bg_count': 89,\n",
       "        'doc_count': 16,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.08437106661587503},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 15,\n",
       "        'key': 'convex',\n",
       "        'score': 0.08406481432148753},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 12,\n",
       "        'key': 'rank',\n",
       "        'score': 0.08303440392523455},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 8,\n",
       "        'key': 'factorization',\n",
       "        'score': 0.07513653086966383},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 30,\n",
       "        'key': 'inference',\n",
       "        'score': 0.0736976897630933},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 8,\n",
       "        'key': 'parallel',\n",
       "        'score': 0.0719831558341868},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'strategic',\n",
       "        'score': 0.06985987532633597},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'strategy',\n",
       "        'score': 0.06985987532633597},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hilbert',\n",
       "        'score': 0.06985987532633597},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 9,\n",
       "        'key': 'low',\n",
       "        'score': 0.06713197293409345},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'attribute',\n",
       "        'score': 0.06647420326003807},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'positive',\n",
       "        'score': 0.06647420326003807},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'balanced',\n",
       "        'score': 0.06647420326003807},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 5,\n",
       "        'key': 'coordinate',\n",
       "        'score': 0.05927820302587206},\n",
       "       {'bg_count': 184,\n",
       "        'doc_count': 22,\n",
       "        'key': 'deep',\n",
       "        'score': 0.05922874160625122},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'limit',\n",
       "        'score': 0.057000017759781196},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'deconvolution',\n",
       "        'score': 0.057000017759781196},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'provable',\n",
       "        'score': 0.057000017759781196},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 7,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.05539745267579683},\n",
       "       {'bg_count': 883,\n",
       "        'doc_count': 66,\n",
       "        'key': 'model',\n",
       "        'score': 0.05088279889968199},\n",
       "       {'bg_count': 204,\n",
       "        'doc_count': 22,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.04817416100414034},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'stream',\n",
       "        'score': 0.04781440521224207},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'word',\n",
       "        'score': 0.04781440521224207},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'meet',\n",
       "        'score': 0.04781440521224207},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 6,\n",
       "        'key': 'polynomial',\n",
       "        'score': 0.04712877617347754},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'proximal',\n",
       "        'score': 0.0430260299193114},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'design',\n",
       "        'score': 0.0430260299193114},\n",
       "       {'bg_count': 65,\n",
       "        'doc_count': 10,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.04161708727748475},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 5,\n",
       "        'key': 'estimator',\n",
       "        'score': 0.041417289738990413},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 5,\n",
       "        'key': 'weighted',\n",
       "        'score': 0.041417289738990413},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'spatio',\n",
       "        'score': 0.040925195801587724},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cognitive',\n",
       "        'score': 0.040925195801587724},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 8,\n",
       "        'key': 'convolutional',\n",
       "        'score': 0.04017519895459236},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 10,\n",
       "        'key': 'selection',\n",
       "        'score': 0.03870761723980267},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'feedback',\n",
       "        'score': 0.0359915779170934},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'train',\n",
       "        'score': 0.035566921815523234},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'attention',\n",
       "        'score': 0.035566921815523234},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'passing',\n",
       "        'score': 0.035566921815523234},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'combinatorial',\n",
       "        'score': 0.035566921815523234},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'limited',\n",
       "        'score': 0.035566921815523234},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'communication',\n",
       "        'score': 0.035566921815523234},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'norm',\n",
       "        'score': 0.03442823671368891},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 7,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.03419869672083689},\n",
       "       {'bg_count': 222,\n",
       "        'doc_count': 21,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.03405822040272464},\n",
       "       {'bg_count': 98,\n",
       "        'doc_count': 12,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.033789977176868935},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mdp',\n",
       "        'score': 0.03313383179119233},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 7,\n",
       "        'key': 'parameter',\n",
       "        'score': 0.03297892703295229},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mode',\n",
       "        'score': 0.031280302626671634},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'iterative',\n",
       "        'score': 0.031280302626671634},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partition',\n",
       "        'score': 0.031280302626671634},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'max',\n",
       "        'score': 0.031280302626671634},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'autoencoder',\n",
       "        'score': 0.031280302626671634},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'pairwise',\n",
       "        'score': 0.03061229109186786},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'randomized',\n",
       "        'score': 0.028370921581357234},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'relaxation',\n",
       "        'score': 0.028370921581357234},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'varying',\n",
       "        'score': 0.027773068744883972},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'video',\n",
       "        'score': 0.027773068744883972},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'incremental',\n",
       "        'score': 0.027773068744883972},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'message',\n",
       "        'score': 0.027773068744883972},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'diffusion',\n",
       "        'score': 0.027773068744883972},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'differential',\n",
       "        'score': 0.027773068744883972},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 5,\n",
       "        'key': 'pca',\n",
       "        'score': 0.02752546829363803},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 10,\n",
       "        'key': 'scale',\n",
       "        'score': 0.026700280576352826},\n",
       "       {'bg_count': 153,\n",
       "        'doc_count': 15,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.026542167118148156},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 6,\n",
       "        'key': 'framework',\n",
       "        'score': 0.02601153637997706},\n",
       "       {'bg_count': 86,\n",
       "        'doc_count': 10,\n",
       "        'key': 'structured',\n",
       "        'score': 0.025513508929383946},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 6,\n",
       "        'key': 'monte',\n",
       "        'score': 0.024970252366490843},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 6,\n",
       "        'key': 'carlo',\n",
       "        'score': 0.024970252366490843},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'testing',\n",
       "        'score': 0.02485037384339425},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'diverse',\n",
       "        'score': 0.02485037384339425},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'square',\n",
       "        'score': 0.02485037384339425},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'missing',\n",
       "        'score': 0.02485037384339425},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'discovery',\n",
       "        'score': 0.02485037384339425},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 5,\n",
       "        'key': 'decomposition',\n",
       "        'score': 0.024788163575342},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'subspace',\n",
       "        'score': 0.024560593413489144},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'causal',\n",
       "        'score': 0.024560593413489144},\n",
       "       {'bg_count': 103,\n",
       "        'doc_count': 11,\n",
       "        'key': 'high',\n",
       "        'score': 0.023593380785859392},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 5,\n",
       "        'key': 'generative',\n",
       "        'score': 0.023556376452108785},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 9,\n",
       "        'key': 'map',\n",
       "        'score': 0.02319519683257762},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'direction',\n",
       "        'score': 0.02237732431136448},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pose',\n",
       "        'score': 0.02237732431136448},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'poisson',\n",
       "        'score': 0.02237732431136448},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pursuit',\n",
       "        'score': 0.02237732431136448},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'armed',\n",
       "        'score': 0.02237732431136448},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 8,\n",
       "        'key': 'space',\n",
       "        'score': 0.02148208996751727},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'loss',\n",
       "        'score': 0.021443052185233444},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'completion',\n",
       "        'score': 0.021323762291248573},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 9,\n",
       "        'key': 'large',\n",
       "        'score': 0.02044562323187301},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 5,\n",
       "        'key': 'prior',\n",
       "        'score': 0.02030893767267575},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'agent',\n",
       "        'score': 0.020257567569624685},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'multivariate',\n",
       "        'score': 0.02008759947729618},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'activity',\n",
       "        'score': 0.02008759947729618},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'recursive',\n",
       "        'score': 0.02008759947729618},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'covariance',\n",
       "        'score': 0.02008759947729618},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 6,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.018948914375461846},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'accelerated',\n",
       "        'score': 0.018420445060116857},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'global',\n",
       "        'score': 0.018420445060116857},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimension',\n",
       "        'score': 0.018420445060116857},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partial',\n",
       "        'score': 0.018420445060116857},\n",
       "       {'bg_count': 146,\n",
       "        'doc_count': 13,\n",
       "        'key': 'graph',\n",
       "        'score': 0.017988914814925653},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'margin',\n",
       "        'score': 0.017702002711326596},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'making',\n",
       "        'score': 0.017702002711326596},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 15,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.017688442752542586},\n",
       "       {'bg_count': 180,\n",
       "        'doc_count': 15,\n",
       "        'key': 'classification',\n",
       "        'score': 0.017086389495681406},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'correlation',\n",
       "        'score': 0.01681296286429751},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regret',\n",
       "        'score': 0.01681296286429751},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 5,\n",
       "        'key': 'ranking',\n",
       "        'score': 0.016798193046261675},\n",
       "       {'bg_count': 90,\n",
       "        'doc_count': 9,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.01668176248068624},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.016646834910993898},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dependent',\n",
       "        'score': 0.01566982768846362},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 6,\n",
       "        'key': 'state',\n",
       "        'score': 0.015659948245908197},\n",
       "       {'bg_count': 169,\n",
       "        'doc_count': 14,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.015651376405811913},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'joint',\n",
       "        'score': 0.015394596220927495},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 6,\n",
       "        'key': 'decision',\n",
       "        'score': 0.014518118495270701},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 8,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.014404863519735757},\n",
       "       {'bg_count': 206,\n",
       "        'doc_count': 16,\n",
       "        'key': 'method',\n",
       "        'score': 0.014341167006142873},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 7,\n",
       "        'key': 'spike',\n",
       "        'score': 0.014318271360090873},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'filtering',\n",
       "        'score': 0.013917952668754157},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'minimax',\n",
       "        'score': 0.013917952668754157},\n",
       "       {'bg_count': 177,\n",
       "        'doc_count': 14,\n",
       "        'key': 'multi',\n",
       "        'score': 0.013404387174334193},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 5,\n",
       "        'key': 'pattern',\n",
       "        'score': 0.013350140288176413},\n",
       "       {'bg_count': 130,\n",
       "        'doc_count': 11,\n",
       "        'key': 'process',\n",
       "        'score': 0.013134542182440307},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'population',\n",
       "        'score': 0.0131296089098849},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.0131296089098849},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measure',\n",
       "        'score': 0.01300576818998853},\n",
       "       {'bg_count': 147,\n",
       "        'doc_count': 12,\n",
       "        'key': 'fast',\n",
       "        'score': 0.012794291353922356},\n",
       "       {'bg_count': 280,\n",
       "        'doc_count': 20,\n",
       "        'key': 'based',\n",
       "        'score': 0.012575616496976196},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 7,\n",
       "        'key': 'detection',\n",
       "        'score': 0.012552079724574979},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 6,\n",
       "        'key': 'unsupervised',\n",
       "        'score': 0.012474844204656242},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 5,\n",
       "        'key': 'density',\n",
       "        'score': 0.01219034072409319},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'expectation',\n",
       "        'score': 0.011990516276839466},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'discrete',\n",
       "        'score': 0.011990516276839466},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exponential',\n",
       "        'score': 0.011990516276839466},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'factor',\n",
       "        'score': 0.011990516276839466},\n",
       "       {'bg_count': 216,\n",
       "        'doc_count': 16,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.011874935182280041},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 5,\n",
       "        'key': 'generalized',\n",
       "        'score': 0.011649100927521019},\n",
       "       {'bg_count': 153,\n",
       "        'doc_count': 12,\n",
       "        'key': 'linear',\n",
       "        'score': 0.011147570897220661},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 5,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.011131393296017202},\n",
       "       {'bg_count': 121,\n",
       "        'doc_count': 10,\n",
       "        'key': 'variational',\n",
       "        'score': 0.011095704623233095},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'consistency',\n",
       "        'score': 0.011071955022085557},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 4,\n",
       "        'key': 'boosting',\n",
       "        'score': 0.011051248091047761},\n",
       "       {'bg_count': 91,\n",
       "        'doc_count': 8,\n",
       "        'key': 'structure',\n",
       "        'score': 0.010682931243429937},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 4,\n",
       "        'key': 'variable',\n",
       "        'score': 0.01043996549727213},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sequential',\n",
       "        'score': 0.01043996549727213},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 7,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.010246855576921109},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 4,\n",
       "        'key': 'set',\n",
       "        'score': 0.009863613337426544},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 5,\n",
       "        'key': 'temporal',\n",
       "        'score': 0.009705055943914855},\n",
       "       {'bg_count': 192,\n",
       "        'doc_count': 14,\n",
       "        'key': 'feature',\n",
       "        'score': 0.009695977212227414},\n",
       "       {'bg_count': 177,\n",
       "        'doc_count': 13,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.009298566571603346},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 7,\n",
       "        'key': 'rate',\n",
       "        'score': 0.009223912361399706},\n",
       "       {'bg_count': 65,\n",
       "        'doc_count': 6,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.009142735361500344},\n",
       "       {'bg_count': 281,\n",
       "        'doc_count': 19,\n",
       "        'key': 'data',\n",
       "        'score': 0.008841379472781406},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'tree',\n",
       "        'score': 0.00880437153014275},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'task',\n",
       "        'score': 0.00880437153014275},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 6,\n",
       "        'key': 'training',\n",
       "        'score': 0.008783019065932379},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'brain',\n",
       "        'score': 0.008775551885200773},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'improved',\n",
       "        'score': 0.008775551885200773},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 5,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.008443295978593545},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'programming',\n",
       "        'score': 0.00831656280310416},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'game',\n",
       "        'score': 0.00831656280310416},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 7,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.007973648431317995},\n",
       "       {'bg_count': 100,\n",
       "        'doc_count': 8,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.007969642614002997},\n",
       "       {'bg_count': 117,\n",
       "        'doc_count': 9,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.007778784165379087},\n",
       "       {'bg_count': 201,\n",
       "        'doc_count': 14,\n",
       "        'key': 'time',\n",
       "        'score': 0.007736608366576963},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 5,\n",
       "        'key': 'descent',\n",
       "        'score': 0.0076800090859917665},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 5,\n",
       "        'key': 'metric',\n",
       "        'score': 0.0076800090859917665},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'discriminative',\n",
       "        'score': 0.00753902711918589},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'trees',\n",
       "        'score': 0.00753902711918589},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 6,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.007136430389036762},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 7,\n",
       "        'key': 'random',\n",
       "        'score': 0.0071113974450547415},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'maximization',\n",
       "        'score': 0.006989460556512611},\n",
       "       {'bg_count': 154,\n",
       "        'doc_count': 11,\n",
       "        'key': 'online',\n",
       "        'score': 0.0069165890733369},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 8,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.0069144748136702955},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 8,\n",
       "        'key': 'robust',\n",
       "        'score': 0.0069144748136702955},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 7,\n",
       "        'key': 'search',\n",
       "        'score': 0.006837044858516434},\n",
       "       {'bg_count': 223,\n",
       "        'doc_count': 15,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.006754291675018937},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 6,\n",
       "        'key': 'support',\n",
       "        'score': 0.006540951744241634},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'binary',\n",
       "        'score': 0.006479148748315992},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sequence',\n",
       "        'score': 0.006479148748315992},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'general',\n",
       "        'score': 0.006004030857926038},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'scene',\n",
       "        'score': 0.006004030857926038},\n",
       "       {'bg_count': 142,\n",
       "        'doc_count': 10,\n",
       "        'key': 'representation',\n",
       "        'score': 0.005856558833110673},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 4,\n",
       "        'key': 'signal',\n",
       "        'score': 0.00550895257414873},\n",
       "       {'bg_count': 111,\n",
       "        'doc_count': 8,\n",
       "        'key': 'problem',\n",
       "        'score': 0.005250921975307933},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'visual',\n",
       "        'score': 0.005145753378511924},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'svm',\n",
       "        'score': 0.005145753378511924},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hidden',\n",
       "        'score': 0.004756846395652405},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 6,\n",
       "        'key': 'local',\n",
       "        'score': 0.004453100693355011},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 3,\n",
       "        'key': 'point',\n",
       "        'score': 0.0043915095329661894},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 4,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.004264763784640787},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sample',\n",
       "        'score': 0.004047663073967398},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'adversarial',\n",
       "        'score': 0.004047663073967398},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.0037158569657402213},\n",
       "       {'bg_count': 293,\n",
       "        'doc_count': 18,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.0036059023329624387}],\n",
       "      'doc_count': 411}},\n",
       "    {'doc_count': 403,\n",
       "     'key': '2015',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mixing',\n",
       "        'score': 0.13274633794925156},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sgd',\n",
       "        'score': 0.13274633794925156},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'proposal',\n",
       "        'score': 0.1263107340110462},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'frank',\n",
       "        'score': 0.1263107340110462},\n",
       "       {'bg_count': 216,\n",
       "        'doc_count': 31,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.12143874643874647},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 5,\n",
       "        'key': 'mcmc',\n",
       "        'score': 0.11144018428097516},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 6,\n",
       "        'key': 'path',\n",
       "        'score': 0.09975872202719237},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'wolfe',\n",
       "        'score': 0.0928720083246618},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'augmented',\n",
       "        'score': 0.0928720083246618},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'return',\n",
       "        'score': 0.0928720083246618},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'cover',\n",
       "        'score': 0.09198293901727825},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 12,\n",
       "        'key': 'convolutional',\n",
       "        'score': 0.08695487654904929},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 4,\n",
       "        'key': 'lifted',\n",
       "        'score': 0.0792443768510366},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 5,\n",
       "        'key': 'private',\n",
       "        'score': 0.07333337438196161},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 5,\n",
       "        'key': 'poisson',\n",
       "        'score': 0.07333337438196161},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'black',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'bidirectional',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mirror',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measurement',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'walk',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'primal',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 92,\n",
       "        'doc_count': 15,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.07181847921792724},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 5,\n",
       "        'key': 'guarantees',\n",
       "        'score': 0.061901331412257544},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'infinite',\n",
       "        'score': 0.061410389818298244},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'normalized',\n",
       "        'score': 0.061410389818298244},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'box',\n",
       "        'score': 0.05943328263827744},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'lstm',\n",
       "        'score': 0.05943328263827744},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'market',\n",
       "        'score': 0.05943328263827744},\n",
       "       {'bg_count': 193,\n",
       "        'doc_count': 22,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.0572183635270985},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 12,\n",
       "        'key': 'sample',\n",
       "        'score': 0.055826462819178746},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 5,\n",
       "        'key': 'block',\n",
       "        'score': 0.053159180906013244},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 26,\n",
       "        'key': 'inference',\n",
       "        'score': 0.05008249486632666},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reduced',\n",
       "        'score': 0.049879361013596184},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'smoothness',\n",
       "        'score': 0.049879361013596184},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tractable',\n",
       "        'score': 0.049879361013596184},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'nonconvex',\n",
       "        'score': 0.049879361013596184},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'lower',\n",
       "        'score': 0.04952106512980602},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'max',\n",
       "        'score': 0.04952106512980602},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 5,\n",
       "        'key': 'subspace',\n",
       "        'score': 0.04951661819507814},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 6,\n",
       "        'key': 'measure',\n",
       "        'score': 0.049314015848875375},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 14,\n",
       "        'key': 'robust',\n",
       "        'score': 0.04928606173303204},\n",
       "       {'bg_count': 184,\n",
       "        'doc_count': 20,\n",
       "        'key': 'deep',\n",
       "        'score': 0.047296051005954645},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 6,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.04684469456741929},\n",
       "       {'bg_count': 147,\n",
       "        'doc_count': 17,\n",
       "        'key': 'fast',\n",
       "        'score': 0.04546981683892641},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'unified',\n",
       "        'score': 0.0449482479419244},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'expectation',\n",
       "        'score': 0.0449482479419244},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 10,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.044850116064996406},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'accurate',\n",
       "        'score': 0.042713919795085244},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'stationary',\n",
       "        'score': 0.042713919795085244},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'diverse',\n",
       "        'score': 0.042713919795085244},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'universal',\n",
       "        'score': 0.042713919795085244},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partitioning',\n",
       "        'score': 0.042713919795085244},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 9,\n",
       "        'key': 'descent',\n",
       "        'score': 0.04215646476312449},\n",
       "       {'bg_count': 141,\n",
       "        'doc_count': 16,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.041246360373156495},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'lasso',\n",
       "        'score': 0.041028690352311575},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'marginal',\n",
       "        'score': 0.041028690352311575},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 6,\n",
       "        'key': 'regret',\n",
       "        'score': 0.04045851883951565},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 11,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.04013947810774034},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 9,\n",
       "        'key': 'framework',\n",
       "        'score': 0.03993270714358662},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'randomized',\n",
       "        'score': 0.0376317404413138},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'locally',\n",
       "        'score': 0.037140798847354516},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'question',\n",
       "        'score': 0.037140798847354516},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'resolution',\n",
       "        'score': 0.037140798847354516},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 10,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.03710967030425928},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 6,\n",
       "        'key': 'making',\n",
       "        'score': 0.036887753916386726},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 8,\n",
       "        'key': 'complexity',\n",
       "        'score': 0.03609864661465322},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 8,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.03609864661465322},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'causal',\n",
       "        'score': 0.03605497339379619},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 11,\n",
       "        'key': 'convex',\n",
       "        'score': 0.0347136925413864},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 9,\n",
       "        'key': 'semi',\n",
       "        'score': 0.03409534339229968},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'synthesi',\n",
       "        'score': 0.03268230208916993},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'consistency',\n",
       "        'score': 0.03268230208916993},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'proximal',\n",
       "        'score': 0.03268230208916993},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'small',\n",
       "        'score': 0.03268230208916993},\n",
       "       {'bg_count': 158,\n",
       "        'doc_count': 16,\n",
       "        'key': 'linear',\n",
       "        'score': 0.03253670156553492},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 14,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.03196782564648417},\n",
       "       {'bg_count': 146,\n",
       "        'doc_count': 15,\n",
       "        'key': 'graph',\n",
       "        'score': 0.03148886664203186},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 8,\n",
       "        'key': 'generative',\n",
       "        'score': 0.03110313203965649},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'smooth',\n",
       "        'score': 0.02903444110520073},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cross',\n",
       "        'score': 0.02903444110520073},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unknown',\n",
       "        'score': 0.02903444110520073},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 5,\n",
       "        'key': 'pca',\n",
       "        'score': 0.02887542949977914},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 13,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.028506931623644722},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 6,\n",
       "        'key': 'monte',\n",
       "        'score': 0.028491630989029532},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 6,\n",
       "        'key': 'carlo',\n",
       "        'score': 0.028491630989029532},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 10,\n",
       "        'key': 'structured',\n",
       "        'score': 0.028263446577702567},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 4,\n",
       "        'key': 'energy',\n",
       "        'score': 0.027619677545741406},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 5,\n",
       "        'key': 'free',\n",
       "        'score': 0.02602836899008273},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'empirical',\n",
       "        'score': 0.025994556951893063},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'norm',\n",
       "        'score': 0.025742415752821577},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 6,\n",
       "        'key': 'risk',\n",
       "        'score': 0.025238133354678616},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 5,\n",
       "        'key': 'class',\n",
       "        'score': 0.024747191760719347},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'completion',\n",
       "        'score': 0.024043940797322694},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'asynchronous',\n",
       "        'score': 0.023422347283709646},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'entropy',\n",
       "        'score': 0.023422347283709646},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sery',\n",
       "        'score': 0.023422347283709646},\n",
       "       {'bg_count': 121,\n",
       "        'doc_count': 12,\n",
       "        'key': 'variational',\n",
       "        'score': 0.0232831211924959},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 7,\n",
       "        'key': 'rank',\n",
       "        'score': 0.023087002795639636},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 9,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.022809773473144956},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 8,\n",
       "        'key': 'local',\n",
       "        'score': 0.02273750912451271},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'dual',\n",
       "        'score': 0.022425058032498203},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 12,\n",
       "        'key': 'bound',\n",
       "        'score': 0.021999416447404094},\n",
       "       {'bg_count': 111,\n",
       "        'doc_count': 11,\n",
       "        'key': 'recurrent',\n",
       "        'score': 0.021306345968156584},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'chain',\n",
       "        'score': 0.021217596139552434},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'testing',\n",
       "        'score': 0.021217596139552434},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'likelihood',\n",
       "        'score': 0.021090071309498536},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 8,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.020912282306663164},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'label',\n",
       "        'score': 0.01979775340857547},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 5,\n",
       "        'key': 'discrete',\n",
       "        'score': 0.019439457524785313},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'property',\n",
       "        'score': 0.019306811814616187},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'family',\n",
       "        'score': 0.019306811814616187},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'coordinate',\n",
       "        'score': 0.019306811814616187},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 6,\n",
       "        'key': 'map',\n",
       "        'score': 0.01926185046604822},\n",
       "       {'bg_count': 132,\n",
       "        'doc_count': 12,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.018861471514957463},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sequential',\n",
       "        'score': 0.018608820939726246},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 8,\n",
       "        'key': 'decision',\n",
       "        'score': 0.01769411923308632},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'knowledge',\n",
       "        'score': 0.017634875530296967},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'factor',\n",
       "        'score': 0.017634875530296967},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 6,\n",
       "        'key': 'propagation',\n",
       "        'score': 0.017212839189946372},\n",
       "       {'bg_count': 169,\n",
       "        'doc_count': 14,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.01696855564559941},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 7,\n",
       "        'key': 'memory',\n",
       "        'score': 0.016765638757704315},\n",
       "       {'bg_count': 65,\n",
       "        'doc_count': 7,\n",
       "        'key': 'rate',\n",
       "        'score': 0.016240479283783537},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'constrained',\n",
       "        'score': 0.016159637632368242},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'noisy',\n",
       "        'score': 0.016159637632368242},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': '3d',\n",
       "        'score': 0.016159637632368242},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'output',\n",
       "        'score': 0.016159637632368242},\n",
       "       {'bg_count': 110,\n",
       "        'doc_count': 10,\n",
       "        'key': 'random',\n",
       "        'score': 0.015717892929131227},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'variance',\n",
       "        'score': 0.015551566019828244},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'general',\n",
       "        'score': 0.015551566019828244},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 6,\n",
       "        'key': 'theory',\n",
       "        'score': 0.015395791454535294},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 8,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.015376512081333445},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'iteration',\n",
       "        'score': 0.014848315056431602},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dynamical',\n",
       "        'score': 0.014848315056431602},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 6,\n",
       "        'key': 'parallel',\n",
       "        'score': 0.014834974252247926},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 5,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.01477900795177007},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 14,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.0143541057149307},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 5,\n",
       "        'key': 'probability',\n",
       "        'score': 0.014131723288851284},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 8,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.014118382484667605},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'matching',\n",
       "        'score': 0.01385309106432936},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'exponential',\n",
       "        'score': 0.01385309106432936},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'polynomial',\n",
       "        'score': 0.01367502643585671},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'statistic',\n",
       "        'score': 0.01367502643585671},\n",
       "       {'bg_count': 103,\n",
       "        'doc_count': 9,\n",
       "        'key': 'high',\n",
       "        'score': 0.01272945859389996},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'adversarial',\n",
       "        'score': 0.012619066677339308},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'estimator',\n",
       "        'score': 0.012619066677339308},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 9,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.012392324317002134},\n",
       "       {'bg_count': 154,\n",
       "        'doc_count': 12,\n",
       "        'key': 'online',\n",
       "        'score': 0.011913164878825648},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 5,\n",
       "        'key': 'selection',\n",
       "        'score': 0.011824012751488661},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 4,\n",
       "        'key': 'set',\n",
       "        'score': 0.01169139566642168},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 5,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.011308460397397014},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 5,\n",
       "        'key': 'game',\n",
       "        'score': 0.011308460397397014},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 4,\n",
       "        'key': 'low',\n",
       "        'score': 0.011055602902331187},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 6,\n",
       "        'key': 'hidden',\n",
       "        'score': 0.010999708223702047},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'covariance',\n",
       "        'score': 0.010002122927970097},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'distance',\n",
       "        'score': 0.010002122927970097},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 4,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.00988998283483195},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 5,\n",
       "        'key': 'decomposition',\n",
       "        'score': 0.00988553590010406},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 6,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.009430735757478732},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'point',\n",
       "        'score': 0.00884705961654316},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'manifold',\n",
       "        'score': 0.008606419594973186},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 4,\n",
       "        'key': 'order',\n",
       "        'score': 0.008365710438871408},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reduction',\n",
       "        'score': 0.007989089274609165},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 4,\n",
       "        'key': 'metric',\n",
       "        'score': 0.007908428720083245},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 7,\n",
       "        'key': 'scale',\n",
       "        'score': 0.007741346648375966},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 6,\n",
       "        'key': 'active',\n",
       "        'score': 0.0077181249668828635},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'weight',\n",
       "        'score': 0.007417487126123963},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 6,\n",
       "        'key': 'detection',\n",
       "        'score': 0.007404146321940287},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 6,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.007404146321940287},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 4,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.007059191242333803},\n",
       "       {'bg_count': 90,\n",
       "        'doc_count': 7,\n",
       "        'key': 'large',\n",
       "        'score': 0.00690431085852522},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sequence',\n",
       "        'score': 0.006664197066636387},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 5,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.006170121934950241},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'segmentation',\n",
       "        'score': 0.0059313215400624356},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'population',\n",
       "        'score': 0.0059313215400624356},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 4,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.005252302991803071},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 7,\n",
       "        'key': 'approach',\n",
       "        'score': 0.005152576164301496},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 3,\n",
       "        'key': 'natural',\n",
       "        'score': 0.004715367878739366},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 4,\n",
       "        'key': 'generalization',\n",
       "        'score': 0.004632798448763961},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 4,\n",
       "        'key': 'variable',\n",
       "        'score': 0.004632798448763961},\n",
       "       {'bg_count': 100,\n",
       "        'doc_count': 7,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.004476907067958058},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'constraint',\n",
       "        'score': 0.004357734448938463},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.004357734448938463},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 4,\n",
       "        'key': 'belief',\n",
       "        'score': 0.00434163131353558},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'structure',\n",
       "        'score': 0.004061882497335763},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rule',\n",
       "        'score': 0.004020537215126185},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.004020537215126185},\n",
       "       {'bg_count': 244,\n",
       "        'doc_count': 15,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.0038923436468684736},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 7,\n",
       "        'key': 'process',\n",
       "        'score': 0.0036366519096848082},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 5,\n",
       "        'key': 'object',\n",
       "        'score': 0.0035162548169832275},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 3,\n",
       "        'key': 'programming',\n",
       "        'score': 0.003400823380011726},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 6,\n",
       "        'key': 'search',\n",
       "        'score': 0.0033509674508633925},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 5,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.0032919843563099585},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 5,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.0032919843563099585},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'task',\n",
       "        'score': 0.003115428850682698},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.003115428850682698},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 10,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.002878630666713733},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 3,\n",
       "        'key': 'human',\n",
       "        'score': 0.002587448971423997},\n",
       "       {'bg_count': 201,\n",
       "        'doc_count': 12,\n",
       "        'key': 'time',\n",
       "        'score': 0.002164794374401904},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 5,\n",
       "        'key': 'latent',\n",
       "        'score': 0.002068690934455764},\n",
       "       {'bg_count': 222,\n",
       "        'doc_count': 13,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.0016827441385193723}],\n",
       "      'doc_count': 403}},\n",
       "    {'doc_count': 368,\n",
       "     'key': '2012',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'hashing',\n",
       "        'score': 0.11134553065082366},\n",
       "       {'bg_count': 63,\n",
       "        'doc_count': 13,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.10810690749992498},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 11,\n",
       "        'key': 'prior',\n",
       "        'score': 0.09696676697616666},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 4,\n",
       "        'key': 'collaborative',\n",
       "        'score': 0.0960686436672968},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 4,\n",
       "        'key': 'conjugate',\n",
       "        'score': 0.0960686436672968},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'link',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'geometric',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unknown',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'newton',\n",
       "        'score': 0.07205148275047259},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multilabel',\n",
       "        'score': 0.07205148275047259},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'compressive',\n",
       "        'score': 0.07205148275047259},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 6,\n",
       "        'key': '3d',\n",
       "        'score': 0.0711905503522942},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 5,\n",
       "        'key': 'topic',\n",
       "        'score': 0.06995851916942343},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 8,\n",
       "        'key': 'ranking',\n",
       "        'score': 0.06831409809969156},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'forest',\n",
       "        'score': 0.0669036776078364},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'proximal',\n",
       "        'score': 0.06059381751282744},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'accuracy',\n",
       "        'score': 0.06059381751282744},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 11,\n",
       "        'key': 'task',\n",
       "        'score': 0.05873556739907295},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 12,\n",
       "        'key': 'object',\n",
       "        'score': 0.05797425775603247},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 8,\n",
       "        'key': 'metric',\n",
       "        'score': 0.05784279245614806},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 5,\n",
       "        'key': 'weighted',\n",
       "        'score': 0.05676712827081881},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'symmetric',\n",
       "        'score': 0.05493856332703213},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'semantic',\n",
       "        'score': 0.05493856332703213},\n",
       "       {'bg_count': 78,\n",
       "        'doc_count': 11,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.05305435767170762},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'walk',\n",
       "        'score': 0.052000568584593565},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'differentially',\n",
       "        'score': 0.052000568584593565},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partial',\n",
       "        'score': 0.052000568584593565},\n",
       "       {'bg_count': 94,\n",
       "        'doc_count': 12,\n",
       "        'key': 'latent',\n",
       "        'score': 0.049301421791416965},\n",
       "       {'bg_count': 134,\n",
       "        'doc_count': 15,\n",
       "        'key': 'processe',\n",
       "        'score': 0.04901934311782298},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'choice',\n",
       "        'score': 0.04616414618777567},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'max',\n",
       "        'score': 0.04531693052930057},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'expectation',\n",
       "        'score': 0.04531693052930057},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'exponential',\n",
       "        'score': 0.044531635263417434},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'comparison',\n",
       "        'score': 0.044531635263417434},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'parametric',\n",
       "        'score': 0.042599539224952736},\n",
       "       {'bg_count': 1547,\n",
       "        'doc_count': 92,\n",
       "        'key': 'learning',\n",
       "        'score': 0.04254201680672267},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'view',\n",
       "        'score': 0.03997002008506616},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'gibb',\n",
       "        'score': 0.03997002008506616},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'losse',\n",
       "        'score': 0.03997002008506616},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partition',\n",
       "        'score': 0.03997002008506616},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sensitive',\n",
       "        'score': 0.03997002008506616},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.03988214792060491},\n",
       "       {'bg_count': 212,\n",
       "        'doc_count': 19,\n",
       "        'key': 'classification',\n",
       "        'score': 0.039418370423458284},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.03782564390359168},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'locally',\n",
       "        'score': 0.0355952751761471},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'region',\n",
       "        'score': 0.0355952751761471},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'output',\n",
       "        'score': 0.0355952751761471},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multiclass',\n",
       "        'score': 0.0355952751761471},\n",
       "       {'bg_count': 222,\n",
       "        'doc_count': 19,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.03531707289165347},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 7,\n",
       "        'key': 'coding',\n",
       "        'score': 0.03444736531190927},\n",
       "       {'bg_count': 160,\n",
       "        'doc_count': 15,\n",
       "        'key': 'feature',\n",
       "        'score': 0.034430058556828916},\n",
       "       {'bg_count': 89,\n",
       "        'doc_count': 10,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.03290373239735774},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 5,\n",
       "        'key': 'margin',\n",
       "        'score': 0.03250709903200574},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'relaxation',\n",
       "        'score': 0.03194965441871456},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'social',\n",
       "        'score': 0.03194965441871456},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subspace',\n",
       "        'score': 0.03194965441871456},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'generalized',\n",
       "        'score': 0.03190571833648393},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 9,\n",
       "        'key': 'decision',\n",
       "        'score': 0.031790198518400314},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 6,\n",
       "        'key': 'inverse',\n",
       "        'score': 0.030644134123288303},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 6,\n",
       "        'key': 'discrete',\n",
       "        'score': 0.030644134123288303},\n",
       "       {'bg_count': 93,\n",
       "        'doc_count': 10,\n",
       "        'key': 'random',\n",
       "        'score': 0.030319747647214252},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 9,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.029680946508742912},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 5,\n",
       "        'key': 'measure',\n",
       "        'score': 0.029533288996280264},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'preference',\n",
       "        'score': 0.028864898393194702},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'energy',\n",
       "        'score': 0.028864898393194702},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partitioning',\n",
       "        'score': 0.028864898393194702},\n",
       "       {'bg_count': 293,\n",
       "        'doc_count': 22,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.02854178226030181},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'label',\n",
       "        'score': 0.028185781323842155},\n",
       "       {'bg_count': 226,\n",
       "        'doc_count': 18,\n",
       "        'key': 'multi',\n",
       "        'score': 0.0277417788196129},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 8,\n",
       "        'key': 'scale',\n",
       "        'score': 0.02714690791250338},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 5,\n",
       "        'key': 'entropy',\n",
       "        'score': 0.026919940783066965},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 6,\n",
       "        'key': 'convex',\n",
       "        'score': 0.026470935727788275},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 6,\n",
       "        'key': 'variable',\n",
       "        'score': 0.026470935727788275},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 6,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.026470935727788275},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tracking',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'driven',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dual',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'product',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sum',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'weight',\n",
       "        'score': 0.02622082179989198},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 8,\n",
       "        'key': 'high',\n",
       "        'score': 0.025137892637957376},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'text',\n",
       "        'score': 0.023929288752362946},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 6,\n",
       "        'key': 'monte',\n",
       "        'score': 0.023797480505671077},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 6,\n",
       "        'key': 'carlo',\n",
       "        'score': 0.023797480505671077},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'manifold',\n",
       "        'score': 0.023350661625708882},\n",
       "       {'bg_count': 206,\n",
       "        'doc_count': 16,\n",
       "        'key': 'method',\n",
       "        'score': 0.022968781544221554},\n",
       "       {'bg_count': 883,\n",
       "        'doc_count': 52,\n",
       "        'key': 'model',\n",
       "        'score': 0.02243343067006059},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'action',\n",
       "        'score': 0.022034499054820415},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'factorization',\n",
       "        'score': 0.022034499054820415},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dirichlet',\n",
       "        'score': 0.022034499054820415},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'matrice',\n",
       "        'score': 0.021924197335775043},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'factor',\n",
       "        'score': 0.021924197335775043},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuronal',\n",
       "        'score': 0.021924197335775043},\n",
       "       {'bg_count': 110,\n",
       "        'doc_count': 10,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.021434363722289053},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sensing',\n",
       "        'score': 0.02015499902702101},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dynamical',\n",
       "        'score': 0.019684208749662434},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'covariance',\n",
       "        'score': 0.019684208749662434},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 11,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.019496189068384826},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'trees',\n",
       "        'score': 0.01863063033700541},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'constrained',\n",
       "        'score': 0.018582378308128544},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'computation',\n",
       "        'score': 0.018582378308128544},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'boltzmann',\n",
       "        'score': 0.018582378308128544},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 5,\n",
       "        'key': 'probability',\n",
       "        'score': 0.018239891360608516},\n",
       "       {'bg_count': 204,\n",
       "        'doc_count': 15,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.018212407393250304},\n",
       "       {'bg_count': 119,\n",
       "        'doc_count': 10,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.01775810749630665},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'code',\n",
       "        'score': 0.017647290485192188},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'binary',\n",
       "        'score': 0.017647290485192188},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'family',\n",
       "        'score': 0.01717529661227738},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimension',\n",
       "        'score': 0.01717529661227738},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.01717529661227738},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 5,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.01679321645686544},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 4,\n",
       "        'key': 'risk',\n",
       "        'score': 0.016727391914141104},\n",
       "       {'bg_count': 192,\n",
       "        'doc_count': 14,\n",
       "        'key': 'function',\n",
       "        'score': 0.016539565857356654},\n",
       "       {'bg_count': 141,\n",
       "        'doc_count': 11,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.01599352996085214},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 10,\n",
       "        'key': 'graph',\n",
       "        'score': 0.01594633247454113},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 4,\n",
       "        'key': 'regret',\n",
       "        'score': 0.01586498700378072},\n",
       "       {'bg_count': 177,\n",
       "        'doc_count': 13,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.015726334799162682},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'completion',\n",
       "        'score': 0.014763156562246828},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'making',\n",
       "        'score': 0.014763156562246828},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'svm',\n",
       "        'score': 0.014763156562246828},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'likelihood',\n",
       "        'score': 0.014763156562246828},\n",
       "       {'bg_count': 95,\n",
       "        'doc_count': 8,\n",
       "        'key': 'regression',\n",
       "        'score': 0.014282160979007065},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 5,\n",
       "        'key': 'map',\n",
       "        'score': 0.01426153537531506},\n",
       "       {'bg_count': 63,\n",
       "        'doc_count': 6,\n",
       "        'key': 'detection',\n",
       "        'score': 0.014249426140966782},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 7,\n",
       "        'key': 'space',\n",
       "        'score': 0.01414264210595583},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 7,\n",
       "        'key': 'policy',\n",
       "        'score': 0.01414264210595583},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variance',\n",
       "        'score': 0.013721550631551812},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.013721550631551812},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'plasticity',\n",
       "        'score': 0.013721550631551812},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exploration',\n",
       "        'score': 0.013721550631551812},\n",
       "       {'bg_count': 65,\n",
       "        'doc_count': 6,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.013309310018903588},\n",
       "       {'bg_count': 280,\n",
       "        'doc_count': 18,\n",
       "        'key': 'based',\n",
       "        'score': 0.012958348805022963},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'distance',\n",
       "        'score': 0.012770519129612886},\n",
       "       {'bg_count': 100,\n",
       "        'doc_count': 8,\n",
       "        'key': 'online',\n",
       "        'score': 0.012481096408317578},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.01225220967659531},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'similarity',\n",
       "        'score': 0.011898740252835539},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'diffusion',\n",
       "        'score': 0.011898740252835539},\n",
       "       {'bg_count': 211,\n",
       "        'doc_count': 14,\n",
       "        'key': 'image',\n",
       "        'score': 0.011624505012587464},\n",
       "       {'bg_count': 139,\n",
       "        'doc_count': 10,\n",
       "        'key': 'markov',\n",
       "        'score': 0.011293068569718897},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tree',\n",
       "        'score': 0.011096703686200377},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'adaptation',\n",
       "        'score': 0.011096703686200377},\n",
       "       {'bg_count': 158,\n",
       "        'doc_count': 11,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.011056554117513339},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 7,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.010750830388597699},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 9,\n",
       "        'key': 'bound',\n",
       "        'score': 0.010470877130465276},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'component',\n",
       "        'score': 0.010356362240075613},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 5,\n",
       "        'key': 'field',\n",
       "        'score': 0.010283179390021603},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'projection',\n",
       "        'score': 0.009670860901071204},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 5,\n",
       "        'key': 'search',\n",
       "        'score': 0.009460071255133304},\n",
       "       {'bg_count': 129,\n",
       "        'doc_count': 9,\n",
       "        'key': 'application',\n",
       "        'score': 0.009117101980480942},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'programming',\n",
       "        'score': 0.00903432394342425},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'free',\n",
       "        'score': 0.00903432394342425},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'order',\n",
       "        'score': 0.00903432394342425},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'rank',\n",
       "        'score': 0.009025915505341363},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 6,\n",
       "        'key': 'spike',\n",
       "        'score': 0.008694194510593374},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 5,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.008691836995904222},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'baye',\n",
       "        'score': 0.008441686086304676},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 5,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.008326610872664167},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 4,\n",
       "        'key': 'noise',\n",
       "        'score': 0.008141671917664356},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 5,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.007973166237270565},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 6,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.007756749172967862},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 6,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.007756749172967862},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sample',\n",
       "        'score': 0.007728384153858797},\n",
       "       {'bg_count': 98,\n",
       "        'doc_count': 7,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.007712813090737242},\n",
       "       {'bg_count': 98,\n",
       "        'doc_count': 7,\n",
       "        'key': 'representation',\n",
       "        'score': 0.007712813090737242},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dependent',\n",
       "        'score': 0.007371114473443502},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 4,\n",
       "        'key': 'maximum',\n",
       "        'score': 0.007332683103406669},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 4,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.006953469596723378},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 3,\n",
       "        'key': 'descent',\n",
       "        'score': 0.006886011711365783},\n",
       "       {'bg_count': 84,\n",
       "        'doc_count': 6,\n",
       "        'key': 'selection',\n",
       "        'score': 0.006610982649203349},\n",
       "       {'bg_count': 121,\n",
       "        'doc_count': 8,\n",
       "        'key': 'variational',\n",
       "        'score': 0.006542048774391101},\n",
       "       {'bg_count': 103,\n",
       "        'doc_count': 7,\n",
       "        'key': 'process',\n",
       "        'score': 0.006415019293592969},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 6,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.00634139052596464},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 4,\n",
       "        'key': 'density',\n",
       "        'score': 0.006240548204158789},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 5,\n",
       "        'key': 'unsupervised',\n",
       "        'score': 0.0060708024644167665},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'propagation',\n",
       "        'score': 0.006001412556988767},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'parameter',\n",
       "        'score': 0.006001412556988767},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'state',\n",
       "        'score': 0.0055970243721307055},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.0055970243721307055},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 4,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.005582466918714554},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 5,\n",
       "        'key': 'structured',\n",
       "        'score': 0.005509152207669458},\n",
       "       {'bg_count': 89,\n",
       "        'doc_count': 6,\n",
       "        'key': 'robust',\n",
       "        'score': 0.005323604532614004},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 5,\n",
       "        'key': 'active',\n",
       "        'score': 0.005240192929790464},\n",
       "       {'bg_count': 223,\n",
       "        'doc_count': 13,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.005195341970635853},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 15,\n",
       "        'key': 'inference',\n",
       "        'score': 0.004982660851236643},\n",
       "       {'bg_count': 74,\n",
       "        'doc_count': 5,\n",
       "        'key': 'structure',\n",
       "        'score': 0.004476930114187911},\n",
       "       {'bg_count': 169,\n",
       "        'doc_count': 10,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.004464610295186855},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'human',\n",
       "        'score': 0.0035849465743003368},\n",
       "       {'bg_count': 330,\n",
       "        'doc_count': 18,\n",
       "        'key': 'algorithm',\n",
       "        'score': 0.0035838954287678356},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 3,\n",
       "        'key': 'semi',\n",
       "        'score': 0.003039033993493647},\n",
       "       {'bg_count': 216,\n",
       "        'doc_count': 12,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.003037373976055451},\n",
       "       {'bg_count': 119,\n",
       "        'doc_count': 7,\n",
       "        'key': 'fast',\n",
       "        'score': 0.0029949509340598274},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 10,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.002864909676939741},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 5,\n",
       "        'key': 'large',\n",
       "        'score': 0.0027145997106828353},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'natural',\n",
       "        'score': 0.0025416469754253298},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hidden',\n",
       "        'score': 0.0025416469754253298},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rate',\n",
       "        'score': 0.002309172608284704},\n",
       "       {'bg_count': 201,\n",
       "        'doc_count': 11,\n",
       "        'key': 'time',\n",
       "        'score': 0.002296564495575056},\n",
       "       {'bg_count': 184,\n",
       "        'doc_count': 10,\n",
       "        'key': 'deep',\n",
       "        'score': 0.0018853828491000224},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.001668682004938081}],\n",
       "      'doc_count': 368}},\n",
       "    {'doc_count': 360,\n",
       "     'key': '2013',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'thompson',\n",
       "        'score': 0.1592824074074074},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 5,\n",
       "        'key': 'setting',\n",
       "        'score': 0.11309273288439953},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 12,\n",
       "        'key': 'low',\n",
       "        'score': 0.1053831417624521},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 5,\n",
       "        'key': 'lasso',\n",
       "        'score': 0.09355709876543208},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dropout',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'coupled',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dataset',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'optimality',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'shot',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'critic',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 74,\n",
       "        'doc_count': 13,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.09148825909242574},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 16,\n",
       "        'key': 'process',\n",
       "        'score': 0.08923041421483792},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 9,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.08538109756097563},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 12,\n",
       "        'key': 'rank',\n",
       "        'score': 0.08498366013071894},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 5,\n",
       "        'key': 'message',\n",
       "        'score': 0.0792309670781893},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 15,\n",
       "        'key': 'robust',\n",
       "        'score': 0.07921006944444443},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 12,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.07841049382716049},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'optimistic',\n",
       "        'score': 0.07828395061728395},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'test',\n",
       "        'score': 0.07547453703703705},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'overlapping',\n",
       "        'score': 0.07547453703703705},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'actor',\n",
       "        'score': 0.07547453703703705},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 6,\n",
       "        'key': 'estimator',\n",
       "        'score': 0.07476010101010101},\n",
       "       {'bg_count': 222,\n",
       "        'doc_count': 23,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.06924733761539316},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 6,\n",
       "        'key': 'sery',\n",
       "        'score': 0.0637888888888889},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partitioning',\n",
       "        'score': 0.06350198412698413},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'determinantal',\n",
       "        'score': 0.06350198412698413},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'gibb',\n",
       "        'score': 0.06350198412698413},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 7,\n",
       "        'key': 'completion',\n",
       "        'score': 0.05877623456790124},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'solving',\n",
       "        'score': 0.05765432098765433},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'passing',\n",
       "        'score': 0.05765432098765433},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'agent',\n",
       "        'score': 0.05765432098765433},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 10,\n",
       "        'key': 'high',\n",
       "        'score': 0.05561313801363552},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 6,\n",
       "        'key': 'transfer',\n",
       "        'score': 0.05516865079365079},\n",
       "       {'bg_count': 158,\n",
       "        'doc_count': 17,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.05497387287076105},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'level',\n",
       "        'score': 0.05452256944444444},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'permutation',\n",
       "        'score': 0.05452256944444444},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': '1',\n",
       "        'score': 0.05452256944444444},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 11,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.05290742645938118},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 11,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.05188958521529659},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'armed',\n",
       "        'score': 0.048485596707818934},\n",
       "       {'bg_count': 154,\n",
       "        'doc_count': 16,\n",
       "        'key': 'online',\n",
       "        'score': 0.04843354176687511},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'covariance',\n",
       "        'score': 0.047538580246913584},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'universal',\n",
       "        'score': 0.047538580246913584},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'marginal',\n",
       "        'score': 0.047538580246913584},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 8,\n",
       "        'key': 'binary',\n",
       "        'score': 0.046543209876543215},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'matrice',\n",
       "        'score': 0.04476080246913581},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'topic',\n",
       "        'score': 0.04476080246913581},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 6,\n",
       "        'key': 'inverse',\n",
       "        'score': 0.04249183006535947},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'dirichlet',\n",
       "        'score': 0.04198302469135802},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subspace',\n",
       "        'score': 0.041951388888888885},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'principal',\n",
       "        'score': 0.041951388888888885},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 22,\n",
       "        'key': 'inference',\n",
       "        'score': 0.04171020513542694},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 8,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.040511154429283096},\n",
       "       {'bg_count': 126,\n",
       "        'doc_count': 13,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.03882820154810894},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 5,\n",
       "        'key': 'tree',\n",
       "        'score': 0.03784436442615455},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mdp',\n",
       "        'score': 0.037380050505050506},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'moment',\n",
       "        'score': 0.037380050505050506},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'operator',\n",
       "        'score': 0.037380050505050506},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exploration',\n",
       "        'score': 0.037380050505050506},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'faster',\n",
       "        'score': 0.037380050505050506},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'proximal',\n",
       "        'score': 0.037380050505050506},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 8,\n",
       "        'key': 'policy',\n",
       "        'score': 0.03545201115093589},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 5,\n",
       "        'key': 'planning',\n",
       "        'score': 0.03427655385270327},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 8,\n",
       "        'key': 'variable',\n",
       "        'score': 0.033649691358024696},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'statistic',\n",
       "        'score': 0.03358641975308642},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'iteration',\n",
       "        'score': 0.03358641975308642},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'parametric',\n",
       "        'score': 0.03358641975308642},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'driven',\n",
       "        'score': 0.03357060185185185},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'limited',\n",
       "        'score': 0.03357060185185185},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 8,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.031956603067714175},\n",
       "       {'bg_count': 204,\n",
       "        'doc_count': 17,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.031929655349794236},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'label',\n",
       "        'score': 0.03145796590241035},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'smooth',\n",
       "        'score': 0.03034722222222222},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'noise',\n",
       "        'score': 0.029761043595679007},\n",
       "       {'bg_count': 69,\n",
       "        'doc_count': 8,\n",
       "        'key': 'search',\n",
       "        'score': 0.029601001968151734},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'variance',\n",
       "        'score': 0.029523007856341196},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 6,\n",
       "        'key': 'parameter',\n",
       "        'score': 0.02904671717171717},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 7,\n",
       "        'key': 'component',\n",
       "        'score': 0.028585797054364315},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 9,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.028242647058823536},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 6,\n",
       "        'key': 'generalized',\n",
       "        'score': 0.028030864197530862},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 6,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.028030864197530862},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'minimax',\n",
       "        'score': 0.027756307031669352},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'asymptotic',\n",
       "        'score': 0.027584325396825395},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'streaming',\n",
       "        'score': 0.027584325396825395},\n",
       "       {'bg_count': 216,\n",
       "        'doc_count': 17,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.027532328818015546},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 5,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.02719340050835148},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 7,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.02618428497942387},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'relaxation',\n",
       "        'score': 0.026136831275720168},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dependent',\n",
       "        'score': 0.026136831275720168},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 6,\n",
       "        'key': 'pca',\n",
       "        'score': 0.026128841607565012},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 5,\n",
       "        'key': 'constraint',\n",
       "        'score': 0.02601962081128748},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 8,\n",
       "        'key': 'latent',\n",
       "        'score': 0.025455144032921814},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 6,\n",
       "        'key': 'prior',\n",
       "        'score': 0.02523726851851852},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'randomized',\n",
       "        'score': 0.025189814814814814},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cross',\n",
       "        'score': 0.025189814814814814},\n",
       "       {'bg_count': 226,\n",
       "        'doc_count': 17,\n",
       "        'key': 'multi',\n",
       "        'score': 0.02422460532065989},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'family',\n",
       "        'score': 0.023271604938271608},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'trajectory',\n",
       "        'score': 0.02309461805555555},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'associative',\n",
       "        'score': 0.02309461805555555},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'maximization',\n",
       "        'score': 0.02309461805555555},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 13,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.02253704662219154},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'unsupervised',\n",
       "        'score': 0.021998171010516692},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'exponential',\n",
       "        'score': 0.021998171010516692},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'probability',\n",
       "        'score': 0.021998171010516692},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 5,\n",
       "        'key': 'maximum',\n",
       "        'score': 0.021926440329218103},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'propagation',\n",
       "        'score': 0.021245915032679735},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'loss',\n",
       "        'score': 0.021245915032679735},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'equation',\n",
       "        'score': 0.021245915032679735},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mapping',\n",
       "        'score': 0.021245915032679735},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 5,\n",
       "        'key': 'complexity',\n",
       "        'score': 0.02103105709876543},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.020815696649029984},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'entropy',\n",
       "        'score': 0.020815696649029984},\n",
       "       {'bg_count': 293,\n",
       "        'doc_count': 20,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.02072009438334807},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 6,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.020581275720164608},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 9,\n",
       "        'key': 'convex',\n",
       "        'score': 0.019808168316831687},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'projection',\n",
       "        'score': 0.019714772243507882},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'adaptation',\n",
       "        'score': 0.019714772243507882},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'population',\n",
       "        'score': 0.019602623456790123},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 6,\n",
       "        'key': 'matching',\n",
       "        'score': 0.01925099206349206},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 8,\n",
       "        'key': 'active',\n",
       "        'score': 0.018411896745230083},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'set',\n",
       "        'score': 0.01813230994152047},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'domain',\n",
       "        'score': 0.01813230994152047},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'simple',\n",
       "        'score': 0.01813230994152047},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 7,\n",
       "        'key': 'task',\n",
       "        'score': 0.018058620835447323},\n",
       "       {'bg_count': 106,\n",
       "        'doc_count': 9,\n",
       "        'key': 'markov',\n",
       "        'score': 0.017694575471698115},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'baye',\n",
       "        'score': 0.016809027777777773},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'observation',\n",
       "        'score': 0.016809027777777773},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'belief',\n",
       "        'score': 0.016809027777777773},\n",
       "       {'bg_count': 92,\n",
       "        'doc_count': 8,\n",
       "        'key': 'decision',\n",
       "        'score': 0.01664519592055824},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 5,\n",
       "        'key': 'trees',\n",
       "        'score': 0.015830214079327554},\n",
       "       {'bg_count': 78,\n",
       "        'doc_count': 7,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.01565457818930041},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.015611772486772488},\n",
       "       {'bg_count': 184,\n",
       "        'doc_count': 13,\n",
       "        'key': 'deep',\n",
       "        'score': 0.01520602690552871},\n",
       "       {'bg_count': 96,\n",
       "        'doc_count': 8,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.015025720164609057},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 7,\n",
       "        'key': 'local',\n",
       "        'score': 0.014777102623456788},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 6,\n",
       "        'key': 'space',\n",
       "        'score': 0.01476128472222222},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tracking',\n",
       "        'score': 0.014523358585858586},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'invariant',\n",
       "        'score': 0.014523358585858586},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.014523358585858586},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 4,\n",
       "        'key': 'rate',\n",
       "        'score': 0.014430335097001768},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 7,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.0143546143880506},\n",
       "       {'bg_count': 115,\n",
       "        'doc_count': 9,\n",
       "        'key': 'fast',\n",
       "        'score': 0.014353260869565223},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 5,\n",
       "        'key': 'decomposition',\n",
       "        'score': 0.014047067901234566},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 7,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.013942430743751885},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 4,\n",
       "        'key': 'action',\n",
       "        'score': 0.013720850480109743},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dual',\n",
       "        'score': 0.013529589371980675},\n",
       "       {'bg_count': 230,\n",
       "        'doc_count': 15,\n",
       "        'key': 'data',\n",
       "        'score': 0.012990640096618359},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 6,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.012912581699346403},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regret',\n",
       "        'score': 0.01261863425925926},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'game',\n",
       "        'score': 0.01261863425925926},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 5,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.01246578732820871},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 5,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.01246578732820871},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spike',\n",
       "        'score': 0.01241390513320338},\n",
       "       {'bg_count': 121,\n",
       "        'doc_count': 9,\n",
       "        'key': 'variational',\n",
       "        'score': 0.012401859504132235},\n",
       "       {'bg_count': 86,\n",
       "        'doc_count': 7,\n",
       "        'key': 'large',\n",
       "        'score': 0.012389552828021821},\n",
       "       {'bg_count': 330,\n",
       "        'doc_count': 20,\n",
       "        'key': 'algorithm',\n",
       "        'score': 0.012167976056864947},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 6,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.012067460317460319},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 11,\n",
       "        'key': 'regression',\n",
       "        'score': 0.011435137259412616},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dimension',\n",
       "        'score': 0.011237654320987655},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 8,\n",
       "        'key': 'structured',\n",
       "        'score': 0.011196492442598367},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 5,\n",
       "        'key': 'human',\n",
       "        'score': 0.01105392967372134},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'estimating',\n",
       "        'score': 0.011006944444444444},\n",
       "       {'bg_count': 110,\n",
       "        'doc_count': 8,\n",
       "        'key': 'random',\n",
       "        'score': 0.01028507295173962},\n",
       "       {'bg_count': 223,\n",
       "        'doc_count': 14,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.010218263854287772},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 4,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.01017342739564962},\n",
       "       {'bg_count': 166,\n",
       "        'doc_count': 11,\n",
       "        'key': 'time',\n",
       "        'score': 0.010170357355347315},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'training',\n",
       "        'score': 0.009678438128050533},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.00962549603174603},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'distance',\n",
       "        'score': 0.00962549603174603},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 4,\n",
       "        'key': 'density',\n",
       "        'score': 0.009205948372615042},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sample',\n",
       "        'score': 0.009205948372615042},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 4,\n",
       "        'key': 'point',\n",
       "        'score': 0.009205948372615042},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.009006226053639847},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'conditional',\n",
       "        'score': 0.008428240740740741},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 5,\n",
       "        'key': 'scale',\n",
       "        'score': 0.00793607735339506},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'generative',\n",
       "        'score': 0.007887544802867383},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 3,\n",
       "        'key': 'boosting',\n",
       "        'score': 0.0069044612794612785},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sequential',\n",
       "        'score': 0.0064562908496732015},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 6,\n",
       "        'key': 'selection',\n",
       "        'score': 0.006452745849297573},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'control',\n",
       "        'score': 0.006417332365044784},\n",
       "       {'bg_count': 106,\n",
       "        'doc_count': 7,\n",
       "        'key': 'processe',\n",
       "        'score': 0.006383138248311203},\n",
       "       {'bg_count': 883,\n",
       "        'doc_count': 46,\n",
       "        'key': 'model',\n",
       "        'score': 0.006112334493799186},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'parallel',\n",
       "        'score': 0.006033730158730159},\n",
       "       {'bg_count': 108,\n",
       "        'doc_count': 7,\n",
       "        'key': 'representation',\n",
       "        'score': 0.0059048496799268425},\n",
       "       {'bg_count': 90,\n",
       "        'doc_count': 6,\n",
       "        'key': 'structure',\n",
       "        'score': 0.005682098765432098},\n",
       "       {'bg_count': 109,\n",
       "        'doc_count': 7,\n",
       "        'key': 'graph',\n",
       "        'score': 0.005672287348510591},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 5,\n",
       "        'key': 'problem',\n",
       "        'score': 0.005245328090647724},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 5,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.005245328090647724},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 3,\n",
       "        'key': 'monte',\n",
       "        'score': 0.0045601851851851845},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 3,\n",
       "        'key': 'carlo',\n",
       "        'score': 0.0045601851851851845},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 3,\n",
       "        'key': 'error',\n",
       "        'score': 0.004237847222222221},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 6,\n",
       "        'key': 'bound',\n",
       "        'score': 0.0040693012600229094},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'order',\n",
       "        'score': 0.003931233062330624},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 10,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.003610937716742958},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sequence',\n",
       "        'score': 0.00336078811369509},\n",
       "       {'bg_count': 122,\n",
       "        'doc_count': 7,\n",
       "        'key': 'visual',\n",
       "        'score': 0.0029959142886055467},\n",
       "       {'bg_count': 162,\n",
       "        'doc_count': 9,\n",
       "        'key': 'application',\n",
       "        'score': 0.0029359567901234567},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'object',\n",
       "        'score': 0.002841049382716049},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hidden',\n",
       "        'score': 0.002841049382716049},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 4,\n",
       "        'key': 'theory',\n",
       "        'score': 0.0024335952113729888}],\n",
       "      'doc_count': 360}},\n",
       "    {'doc_count': 306,\n",
       "     'key': '2011',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measurement',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'passing',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'message',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'crowdsourcing',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 5,\n",
       "        'key': 'multiclass',\n",
       "        'score': 0.09738273112654007},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dictionary',\n",
       "        'score': 0.08962212335914758},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'orthogonal',\n",
       "        'score': 0.08962212335914758},\n",
       "       {'bg_count': 222,\n",
       "        'doc_count': 21,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.08499026708369273},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'penalty',\n",
       "        'score': 0.0771938677431757},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'trace',\n",
       "        'score': 0.0771938677431757},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 8,\n",
       "        'key': 'tree',\n",
       "        'score': 0.07284036054508951},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'structural',\n",
       "        'score': 0.06941489740413231},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'group',\n",
       "        'score': 0.0675274467085309},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'intrinsic',\n",
       "        'score': 0.0675274467085309},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 6,\n",
       "        'key': 'segmentation',\n",
       "        'score': 0.06738994617454824},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 8,\n",
       "        'key': 'latent',\n",
       "        'score': 0.06723748405104632},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'test',\n",
       "        'score': 0.05979430988081507},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measure',\n",
       "        'score': 0.05979430988081507},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'knowledge',\n",
       "        'score': 0.05979430988081507},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'labeling',\n",
       "        'score': 0.05979430988081507},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'observation',\n",
       "        'score': 0.059710568836018316},\n",
       "       {'bg_count': 144,\n",
       "        'doc_count': 14,\n",
       "        'key': 'large',\n",
       "        'score': 0.05950495061253743},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 7,\n",
       "        'key': 'maximum',\n",
       "        'score': 0.0560666214561351},\n",
       "       {'bg_count': 1547,\n",
       "        'doc_count': 79,\n",
       "        'key': 'learning',\n",
       "        'score': 0.05380490014793565},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 6,\n",
       "        'key': 'detection',\n",
       "        'score': 0.05365345312531617},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'predicting',\n",
       "        'score': 0.05346719793086575},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 4,\n",
       "        'key': 'manifold',\n",
       "        'score': 0.0520492568085599},\n",
       "       {'bg_count': 226,\n",
       "        'doc_count': 18,\n",
       "        'key': 'multi',\n",
       "        'score': 0.052040910065223385},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 6,\n",
       "        'key': 'distance',\n",
       "        'score': 0.05177495834935281},\n",
       "       {'bg_count': 92,\n",
       "        'doc_count': 10,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.051376096521776285},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'lasso',\n",
       "        'score': 0.04879319919689008},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 15,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.04873071722630449},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 6,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.04829287047195732},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pursuit',\n",
       "        'score': 0.048194604639241315},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 10,\n",
       "        'key': 'object',\n",
       "        'score': 0.047043321517453204},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 8,\n",
       "        'key': 'structure',\n",
       "        'score': 0.046638673411181726},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'noisy',\n",
       "        'score': 0.043733179546328334},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'inverse',\n",
       "        'score': 0.043169099685824025},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 9,\n",
       "        'key': 'active',\n",
       "        'score': 0.041768244731047496},\n",
       "       {'bg_count': 211,\n",
       "        'doc_count': 16,\n",
       "        'key': 'image',\n",
       "        'score': 0.04153625848526279},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 7,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.04134853971129458},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dependent',\n",
       "        'score': 0.04072383902883878},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'iteration',\n",
       "        'score': 0.03990910089526006},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'instance',\n",
       "        'score': 0.03990910089526006},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'channel',\n",
       "        'score': 0.03659489939766756},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'input',\n",
       "        'score': 0.03659489939766756},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'comparison',\n",
       "        'score': 0.03659489939766756},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'global',\n",
       "        'score': 0.03659489939766756},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'greedy',\n",
       "        'score': 0.036420180272544754},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.036420180272544754},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'low',\n",
       "        'score': 0.036420180272544754},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 8,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.03495753717178525},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 5,\n",
       "        'key': 'action',\n",
       "        'score': 0.03453603090129528},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'decoding',\n",
       "        'score': 0.03369497308727412},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'norm',\n",
       "        'score': 0.03369497308727412},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'single',\n",
       "        'score': 0.03369497308727412},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'transfer',\n",
       "        'score': 0.03369497308727412},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'ranking',\n",
       "        'score': 0.03275410059125724},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 6,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.03194640238085067},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'view',\n",
       "        'score': 0.03113621457810344},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 6,\n",
       "        'key': 'noise',\n",
       "        'score': 0.03100905246233966},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 9,\n",
       "        'key': 'processe',\n",
       "        'score': 0.030817474048442907},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 5,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.029690707074405662},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'point',\n",
       "        'score': 0.028861762569951727},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'topic',\n",
       "        'score': 0.028861762569951727},\n",
       "       {'bg_count': 267,\n",
       "        'doc_count': 17,\n",
       "        'key': 'algorithm',\n",
       "        'score': 0.028147685763166407},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 7,\n",
       "        'key': 'scale',\n",
       "        'score': 0.026982565185230144},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 4,\n",
       "        'key': 'adaptation',\n",
       "        'score': 0.026841068847245138},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 4,\n",
       "        'key': 'label',\n",
       "        'score': 0.026841068847245138},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'constrained',\n",
       "        'score': 0.026826726562658085},\n",
       "       {'bg_count': 130,\n",
       "        'doc_count': 10,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.026805929343414928},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 5,\n",
       "        'key': 'rank',\n",
       "        'score': 0.026622001984042232},\n",
       "       {'bg_count': 78,\n",
       "        'doc_count': 7,\n",
       "        'key': 'process',\n",
       "        'score': 0.025704145129366193},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 5,\n",
       "        'key': 'policy',\n",
       "        'score': 0.025688048260888142},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dimension',\n",
       "        'score': 0.02559378871374258},\n",
       "       {'bg_count': 63,\n",
       "        'doc_count': 6,\n",
       "        'key': 'spike',\n",
       "        'score': 0.024581510163978443},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 4,\n",
       "        'key': 'tracking',\n",
       "        'score': 0.024422101315603816},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 4,\n",
       "        'key': 'generalized',\n",
       "        'score': 0.024422101315603816},\n",
       "       {'bg_count': 98,\n",
       "        'doc_count': 8,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.024358327208879198},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 5,\n",
       "        'key': 'signal',\n",
       "        'score': 0.023936885029974233},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.02333809340729756},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 4,\n",
       "        'key': 'svm',\n",
       "        'score': 0.023319336705590863},\n",
       "       {'bg_count': 223,\n",
       "        'doc_count': 14,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.02221674351227834},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'factorization',\n",
       "        'score': 0.021831638181119148},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 4,\n",
       "        'key': 'field',\n",
       "        'score': 0.021297601587233782},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 4,\n",
       "        'key': 'filtering',\n",
       "        'score': 0.021297601587233782},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'covariance',\n",
       "        'score': 0.02045617906156495},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.02045617906156495},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'carlo',\n",
       "        'score': 0.02036869626258323},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'monte',\n",
       "        'score': 0.02036869626258323},\n",
       "       {'bg_count': 89,\n",
       "        'doc_count': 7,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.01969988014789279},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 5,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.019461690106527565},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dirichlet',\n",
       "        'score': 0.019195341535306932},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 6,\n",
       "        'key': 'task',\n",
       "        'score': 0.018528174095316343},\n",
       "       {'bg_count': 153,\n",
       "        'doc_count': 10,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.01786363939683553},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 4,\n",
       "        'key': 'population',\n",
       "        'score': 0.017106199512591055},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 5,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.016427659649953557},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sequence',\n",
       "        'score': 0.015702567189919997},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 5,\n",
       "        'key': 'rate',\n",
       "        'score': 0.014842134056518104},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 7,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.014641381477632574},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 4,\n",
       "        'key': 'matching',\n",
       "        'score': 0.014423702184819703},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 6,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.014342513667351208},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'decomposition',\n",
       "        'score': 0.014195468586352731},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'temporal',\n",
       "        'score': 0.014195468586352731},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'state',\n",
       "        'score': 0.014195468586352731},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'scene',\n",
       "        'score': 0.013395488914520057},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'completion',\n",
       "        'score': 0.013395488914520057},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.013253676754621567},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 6,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.013144265780129814},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 5,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.012952315672423324},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 7,\n",
       "        'key': 'structured',\n",
       "        'score': 0.01253761333906641},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 6,\n",
       "        'key': 'representation',\n",
       "        'score': 0.012391343736052003},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 6,\n",
       "        'key': 'high',\n",
       "        'score': 0.012391343736052003},\n",
       "       {'bg_count': 194,\n",
       "        'doc_count': 11,\n",
       "        'key': 'linear',\n",
       "        'score': 0.012284738929823886},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.012179163604439599},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 14,\n",
       "        'key': 'inference',\n",
       "        'score': 0.011879347694908539},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 3,\n",
       "        'key': 'boosting',\n",
       "        'score': 0.011286451597870282},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 3,\n",
       "        'key': 'brain',\n",
       "        'score': 0.011286451597870282},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 3,\n",
       "        'key': 'convex',\n",
       "        'score': 0.011286451597870282},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'complexity',\n",
       "        'score': 0.011188925995448375},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'problem',\n",
       "        'score': 0.011188925995448375},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.011188925995448375},\n",
       "       {'bg_count': 244,\n",
       "        'doc_count': 13,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.011077820356329256},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.010666146504737996},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'coding',\n",
       "        'score': 0.010666146504737996},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'theory',\n",
       "        'score': 0.010666146504737996},\n",
       "       {'bg_count': 201,\n",
       "        'doc_count': 11,\n",
       "        'key': 'time',\n",
       "        'score': 0.010605001818197657},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 4,\n",
       "        'key': 'nonlinear',\n",
       "        'score': 0.01027342330034328},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'set',\n",
       "        'score': 0.010081287416927558},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 4,\n",
       "        'key': 'metric',\n",
       "        'score': 0.009841102583210321},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 4,\n",
       "        'key': 'margin',\n",
       "        'score': 0.009841102583210321},\n",
       "       {'bg_count': 206,\n",
       "        'doc_count': 11,\n",
       "        'key': 'method',\n",
       "        'score': 0.009475081569738959},\n",
       "       {'bg_count': 280,\n",
       "        'doc_count': 14,\n",
       "        'key': 'based',\n",
       "        'score': 0.008380323807082746},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 3,\n",
       "        'key': 'human',\n",
       "        'score': 0.008041778803024476},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 3,\n",
       "        'key': 'component',\n",
       "        'score': 0.008041778803024476},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 6,\n",
       "        'key': 'variational',\n",
       "        'score': 0.007955812882326295},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 7,\n",
       "        'key': 'fast',\n",
       "        'score': 0.007682546277348523},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 7,\n",
       "        'key': 'graph',\n",
       "        'score': 0.007682546277348523},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 4,\n",
       "        'key': 'search',\n",
       "        'score': 0.0075498027824056286},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'descent',\n",
       "        'score': 0.007171256833675604},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sample',\n",
       "        'score': 0.007171256833675604},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 3,\n",
       "        'key': 'error',\n",
       "        'score': 0.006767085919335054},\n",
       "       {'bg_count': 63,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.006567817153489332},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 6,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.006410187311134827},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 5,\n",
       "        'key': 'training',\n",
       "        'score': 0.0058817882698618245},\n",
       "       {'bg_count': 883,\n",
       "        'doc_count': 39,\n",
       "        'key': 'model',\n",
       "        'score': 0.0057551477500377615},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.00566235208680422},\n",
       "       {'bg_count': 180,\n",
       "        'doc_count': 9,\n",
       "        'key': 'classification',\n",
       "        'score': 0.005387351018838907},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 3,\n",
       "        'key': 'order',\n",
       "        'score': 0.005004212782317767},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 8,\n",
       "        'key': 'regression',\n",
       "        'score': 0.004596628838141299},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 3,\n",
       "        'key': 'robust',\n",
       "        'score': 0.004399799135340411},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.004115724721261053},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 3,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.004115724721261053},\n",
       "       {'bg_count': 281,\n",
       "        'doc_count': 13,\n",
       "        'key': 'data',\n",
       "        'score': 0.0040252410751165},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 5,\n",
       "        'key': 'machine',\n",
       "        'score': 0.0035908957388404252},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 3,\n",
       "        'key': 'markov',\n",
       "        'score': 0.0033278202142862316},\n",
       "       {'bg_count': 102,\n",
       "        'doc_count': 5,\n",
       "        'key': 'approach',\n",
       "        'score': 0.002613897453551886},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.002203436580528012},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 3,\n",
       "        'key': 'system',\n",
       "        'score': 0.001992388846532299},\n",
       "       {'bg_count': 247,\n",
       "        'doc_count': 11,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.0019352655636271185},\n",
       "       {'bg_count': 112,\n",
       "        'doc_count': 5,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.0009215968522485259},\n",
       "       {'bg_count': 65,\n",
       "        'doc_count': 3,\n",
       "        'key': 'deep',\n",
       "        'score': 0.0009034986543637057},\n",
       "       {'bg_count': 114,\n",
       "        'doc_count': 5,\n",
       "        'key': 'recognition',\n",
       "        'score': 0.000618764113067926},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.0001386829241500528}],\n",
       "      'doc_count': 306}},\n",
       "    {'doc_count': 292,\n",
       "     'key': '2010',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 23,\n",
       "        'doc_count': 7,\n",
       "        'key': 'group',\n",
       "        'score': 0.1569534417910204},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'matrice',\n",
       "        'score': 0.14259007318446235},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 12,\n",
       "        'key': 'latent',\n",
       "        'score': 0.12642635154761875},\n",
       "       {'bg_count': 144,\n",
       "        'doc_count': 17,\n",
       "        'key': 'large',\n",
       "        'score': 0.11221951494443401},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'estimating',\n",
       "        'score': 0.1098278715092376},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'identifying',\n",
       "        'score': 0.1098278715092376},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'infinite',\n",
       "        'score': 0.09953399637205229},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 8,\n",
       "        'key': 'prior',\n",
       "        'score': 0.08583536623506599},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cascade',\n",
       "        'score': 0.08526605601426158},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'average',\n",
       "        'score': 0.08526605601426158},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'level',\n",
       "        'score': 0.08526605601426158},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'action',\n",
       "        'score': 0.08335790687076106},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 6,\n",
       "        'key': 'functional',\n",
       "        'score': 0.08136141865265527},\n",
       "       {'bg_count': 89,\n",
       "        'doc_count': 11,\n",
       "        'key': 'structured',\n",
       "        'score': 0.07778787786143657},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'structural',\n",
       "        'score': 0.07688747107024457},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'construction',\n",
       "        'score': 0.07465049727903922},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'novel',\n",
       "        'score': 0.07465049727903922},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 13,\n",
       "        'key': 'regression',\n",
       "        'score': 0.07122328600657388},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'norm',\n",
       "        'score': 0.06780118221054607},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'relational',\n",
       "        'score': 0.06623028269292328},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'case',\n",
       "        'score': 0.06623028269292328},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'implicit',\n",
       "        'score': 0.06615805029086132},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'specific',\n",
       "        'score': 0.06615805029086132},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'sparsity',\n",
       "        'score': 0.06453485644586225},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 8,\n",
       "        'key': 'task',\n",
       "        'score': 0.06026670863625081},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 8,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.06026670863625081},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'max',\n",
       "        'score': 0.05920968457326122},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 8,\n",
       "        'key': 'variable',\n",
       "        'score': 0.05752720960780633},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'joint',\n",
       "        'score': 0.05341937980859448},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuronal',\n",
       "        'score': 0.05341937980859448},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cost',\n",
       "        'score': 0.05341937980859448},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 7,\n",
       "        'key': 'point',\n",
       "        'score': 0.05033630840683054},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'margin',\n",
       "        'score': 0.04922395442390692},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'dirichlet',\n",
       "        'score': 0.04922395442390692},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 8,\n",
       "        'key': 'active',\n",
       "        'score': 0.049154374549039405},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'instance',\n",
       "        'score': 0.04851989116156877},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'diffusion',\n",
       "        'score': 0.04851989116156877},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multivariate',\n",
       "        'score': 0.04851989116156877},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'covariance',\n",
       "        'score': 0.045379261954686},\n",
       "       {'bg_count': 180,\n",
       "        'doc_count': 14,\n",
       "        'key': 'multi',\n",
       "        'score': 0.04452810616959612},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 5,\n",
       "        'key': 'constraint',\n",
       "        'score': 0.04353704795860923},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'global',\n",
       "        'score': 0.042917683117532995},\n",
       "       {'bg_count': 78,\n",
       "        'doc_count': 8,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.042284356039281906},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'boltzmann',\n",
       "        'score': 0.040680709326327634},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'short',\n",
       "        'score': 0.040680709326327634},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'scene',\n",
       "        'score': 0.04065303058735222},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 8,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.039703555435087294},\n",
       "       {'bg_count': 99,\n",
       "        'doc_count': 9,\n",
       "        'key': 'scale',\n",
       "        'score': 0.03866173936778177},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 4,\n",
       "        'key': 'movement',\n",
       "        'score': 0.03856258209795458},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 6,\n",
       "        'key': 'rate',\n",
       "        'score': 0.03824591855882904},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 5,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.03731547507349721},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 5,\n",
       "        'key': 'coding',\n",
       "        'score': 0.03731547507349721},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'differential',\n",
       "        'score': 0.03468604086408441},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 6,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.034046356861378434},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 7,\n",
       "        'key': 'detection',\n",
       "        'score': 0.03382321704092909},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 5,\n",
       "        'key': 'label',\n",
       "        'score': 0.033426992020302214},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'domain',\n",
       "        'score': 0.033156249797788286},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'projection',\n",
       "        'score': 0.033156249797788286},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'manifold',\n",
       "        'score': 0.03159442046662913},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'single',\n",
       "        'score': 0.03159442046662913},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 5,\n",
       "        'key': 'discriminative',\n",
       "        'score': 0.031129252034323342},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 4,\n",
       "        'key': 'trees',\n",
       "        'score': 0.030133354318125406},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'consistency',\n",
       "        'score': 0.029953407867576613},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'class',\n",
       "        'score': 0.029953407867576613},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'predictive',\n",
       "        'score': 0.029953407867576613},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'search',\n",
       "        'score': 0.029953407867576613},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 7,\n",
       "        'key': 'selection',\n",
       "        'score': 0.028702068452769767},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 8,\n",
       "        'key': 'object',\n",
       "        'score': 0.028635379648025875},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 5,\n",
       "        'key': 'generalized',\n",
       "        'score': 0.0280493026914155},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exponential',\n",
       "        'score': 0.027942038844060792},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dual',\n",
       "        'score': 0.027942038844060792},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 16,\n",
       "        'key': 'inference',\n",
       "        'score': 0.027869602226714144},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 6,\n",
       "        'key': 'policy',\n",
       "        'score': 0.0272220691030212},\n",
       "       {'bg_count': 180,\n",
       "        'doc_count': 12,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.026843685494464243},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 4,\n",
       "        'key': 'binary',\n",
       "        'score': 0.026265826277968484},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'layer',\n",
       "        'score': 0.026122228775165537},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'parameter',\n",
       "        'score': 0.026122228775165537},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'game',\n",
       "        'score': 0.026122228775165537},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 4,\n",
       "        'key': 'set',\n",
       "        'score': 0.025123984666112643},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'framework',\n",
       "        'score': 0.024467855985260748},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.023025464947026213},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 5,\n",
       "        'key': 'brain',\n",
       "        'score': 0.022935424537153415},\n",
       "       {'bg_count': 110,\n",
       "        'doc_count': 8,\n",
       "        'key': 'random',\n",
       "        'score': 0.022013340384516963},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 9,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.021688632271354057},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.021572703602927377},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'low',\n",
       "        'score': 0.021572703602927377},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'support',\n",
       "        'score': 0.021572703602927377},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'family',\n",
       "        'score': 0.020298836554700692},\n",
       "       {'bg_count': 114,\n",
       "        'doc_count': 8,\n",
       "        'key': 'processe',\n",
       "        'score': 0.020279635098254176},\n",
       "       {'bg_count': 200,\n",
       "        'doc_count': 12,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.020049727903921934},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'spatial',\n",
       "        'score': 0.01912295927941452},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'inverse',\n",
       "        'score': 0.01912295927941452},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dependent',\n",
       "        'score': 0.01912295927941452},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'likelihood',\n",
       "        'score': 0.01803418402451992},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tree',\n",
       "        'score': 0.01803418402451992},\n",
       "       {'bg_count': 232,\n",
       "        'doc_count': 13,\n",
       "        'key': 'data',\n",
       "        'score': 0.017342535718676597},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 7,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.017228377697968707},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'convergence',\n",
       "        'score': 0.017023178430689217},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'maximization',\n",
       "        'score': 0.017023178430689217},\n",
       "       {'bg_count': 212,\n",
       "        'doc_count': 12,\n",
       "        'key': 'classification',\n",
       "        'score': 0.016588655169117356},\n",
       "       {'bg_count': 146,\n",
       "        'doc_count': 9,\n",
       "        'key': 'graph',\n",
       "        'score': 0.01629371274263078},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 5,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.01605033337633702},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 5,\n",
       "        'key': 'state',\n",
       "        'score': 0.01605033337633702},\n",
       "       {'bg_count': 192,\n",
       "        'doc_count': 11,\n",
       "        'key': 'feature',\n",
       "        'score': 0.015848875746700443},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 6,\n",
       "        'key': 'robust',\n",
       "        'score': 0.015420065567979865},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'segmentation',\n",
       "        'score': 0.015203368361793958},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 4,\n",
       "        'key': 'temporal',\n",
       "        'score': 0.014609526490273346},\n",
       "       {'bg_count': 130,\n",
       "        'doc_count': 8,\n",
       "        'key': 'process',\n",
       "        'score': 0.014411709513980103},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'risk',\n",
       "        'score': 0.01438151865326061},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.01438151865326061},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 4,\n",
       "        'key': 'motion',\n",
       "        'score': 0.014031809008084374},\n",
       "       {'bg_count': 69,\n",
       "        'doc_count': 5,\n",
       "        'key': 'decision',\n",
       "        'score': 0.013646447793179785},\n",
       "       {'bg_count': 69,\n",
       "        'doc_count': 5,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.013646447793179785},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 3,\n",
       "        'key': 'metric',\n",
       "        'score': 0.0136110345515106},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 3,\n",
       "        'key': 'population',\n",
       "        'score': 0.0136110345515106},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 5,\n",
       "        'key': 'bound',\n",
       "        'score': 0.013206880143688175},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'high',\n",
       "        'score': 0.012944340806316896},\n",
       "       {'bg_count': 93,\n",
       "        'doc_count': 6,\n",
       "        'key': 'space',\n",
       "        'score': 0.01232604313585433},\n",
       "       {'bg_count': 117,\n",
       "        'doc_count': 7,\n",
       "        'key': 'approach',\n",
       "        'score': 0.011594055586830966},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 3,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.011563748224003433},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 5,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.011184868956026769},\n",
       "       {'bg_count': 211,\n",
       "        'doc_count': 11,\n",
       "        'key': 'image',\n",
       "        'score': 0.011029529472554265},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rank',\n",
       "        'score': 0.009839717632418444},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 4,\n",
       "        'key': 'human',\n",
       "        'score': 0.009728809830400994},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 4,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.009728809830400994},\n",
       "       {'bg_count': 1547,\n",
       "        'doc_count': 65,\n",
       "        'key': 'learning',\n",
       "        'score': 0.009333837682192403},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 4,\n",
       "        'key': 'control',\n",
       "        'score': 0.009331734576716464},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 4,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.008947895164821416},\n",
       "       {'bg_count': 247,\n",
       "        'doc_count': 12,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.00841473170635351},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'matching',\n",
       "        'score': 0.008367984200577603},\n",
       "       {'bg_count': 129,\n",
       "        'doc_count': 7,\n",
       "        'key': 'online',\n",
       "        'score': 0.008285529230872174},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sample',\n",
       "        'score': 0.008217362090569554},\n",
       "       {'bg_count': 153,\n",
       "        'doc_count': 8,\n",
       "        'key': 'function',\n",
       "        'score': 0.008126700983764992},\n",
       "       {'bg_count': 177,\n",
       "        'doc_count': 9,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.008041822646154235},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 5,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.00785449758811387},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 4,\n",
       "        'key': 'fast',\n",
       "        'score': 0.007532487333458433},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 3,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.007500916442283773},\n",
       "       {'bg_count': 204,\n",
       "        'doc_count': 10,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.007383066756445491},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 3,\n",
       "        'key': 'convex',\n",
       "        'score': 0.007096941691260511},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 4,\n",
       "        'key': 'structure',\n",
       "        'score': 0.006283598070491094},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 3,\n",
       "        'key': 'complexity',\n",
       "        'score': 0.005649365500093826},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 3,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.005649365500093826},\n",
       "       {'bg_count': 141,\n",
       "        'doc_count': 7,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.005540156297204245},\n",
       "       {'bg_count': 94,\n",
       "        'doc_count': 5,\n",
       "        'key': 'visual',\n",
       "        'score': 0.005463007510091311},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 4,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.005173474281186797},\n",
       "       {'bg_count': 119,\n",
       "        'doc_count': 6,\n",
       "        'key': 'time',\n",
       "        'score': 0.005143491061277204},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 3,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.004712698552868322},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 3,\n",
       "        'key': 'theory',\n",
       "        'score': 0.004712698552868322},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.0035012624972980398},\n",
       "       {'bg_count': 106,\n",
       "        'doc_count': 5,\n",
       "        'key': 'markov',\n",
       "        'score': 0.0029060684329602687},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 5,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.0027188781889958486},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 3,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.002680607548718079},\n",
       "       {'bg_count': 157,\n",
       "        'doc_count': 7,\n",
       "        'key': 'method',\n",
       "        'score': 0.0025324865864342775},\n",
       "       {'bg_count': 163,\n",
       "        'doc_count': 7,\n",
       "        'key': 'linear',\n",
       "        'score': 0.001556839126575619},\n",
       "       {'bg_count': 90,\n",
       "        'doc_count': 4,\n",
       "        'key': 'problem',\n",
       "        'score': 0.0013990533975521767},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.0009660307639663093},\n",
       "       {'bg_count': 293,\n",
       "        'doc_count': 12,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.0006417330121679448},\n",
       "       {'bg_count': 145,\n",
       "        'doc_count': 6,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.0005367507651691133},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variational',\n",
       "        'score': 0.000341586132482642}],\n",
       "      'doc_count': 292}},\n",
       "    {'doc_count': 262,\n",
       "     'key': '2009',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'compressed',\n",
       "        'score': 0.322288910902628},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'slow',\n",
       "        'score': 0.2258937416234485},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'smooth',\n",
       "        'score': 0.15351086766505448},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'additive',\n",
       "        'score': 0.14677903385583593},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'class',\n",
       "        'score': 0.1381674091889114},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'law',\n",
       "        'score': 0.12417483163651803},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'annotation',\n",
       "        'score': 0.12417483163651803},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 9,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.12100137203278047},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'alignment',\n",
       "        'score': 0.10722167997202961},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 13,\n",
       "        'key': 'graph',\n",
       "        'score': 0.09414926662293158},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 6,\n",
       "        'key': 'factor',\n",
       "        'score': 0.08559940729394726},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'output',\n",
       "        'score': 0.08348726764174581},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'program',\n",
       "        'score': 0.08348726764174581},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'parametric',\n",
       "        'score': 0.08234514305693141},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 7,\n",
       "        'key': 'detection',\n",
       "        'score': 0.08096635054289765},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'functional',\n",
       "        'score': 0.07849840400391067},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sparsity',\n",
       "        'score': 0.07849840400391067},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'structural',\n",
       "        'score': 0.07849840400391067},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'domain',\n",
       "        'score': 0.07849840400391067},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sensing',\n",
       "        'score': 0.07485657224891536},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'driven',\n",
       "        'score': 0.07485657224891536},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connectivity',\n",
       "        'score': 0.07485657224891536},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 6,\n",
       "        'key': 'label',\n",
       "        'score': 0.06972133353988981},\n",
       "       {'bg_count': 293,\n",
       "        'doc_count': 20,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.0676726914527404},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 6,\n",
       "        'key': 'matching',\n",
       "        'score': 0.06751604551849293},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 7,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.06558293514363965},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 10,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.06326117359128255},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'joint',\n",
       "        'score': 0.06157857933686848},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'topic',\n",
       "        'score': 0.06157857933686848},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'context',\n",
       "        'score': 0.06145011680819617},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 8,\n",
       "        'key': 'semi',\n",
       "        'score': 0.0594806052483344},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 8,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.05829619792736897},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'infinite',\n",
       "        'score': 0.05811458235686072},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 8,\n",
       "        'key': 'map',\n",
       "        'score': 0.05714255443292213},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'noisy',\n",
       "        'score': 0.05505700910980323},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 9,\n",
       "        'key': 'object',\n",
       "        'score': 0.05373533371340075},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 6,\n",
       "        'key': 'game',\n",
       "        'score': 0.05304935609812948},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'neighbor',\n",
       "        'score': 0.052244041722510334},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'conditional',\n",
       "        'score': 0.052244041722510334},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dirichlet',\n",
       "        'score': 0.051841384534700774},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'relational',\n",
       "        'score': 0.051841384534700774},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 12,\n",
       "        'key': 'regression',\n",
       "        'score': 0.04854644776307393},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'discovery',\n",
       "        'score': 0.04788564914632014},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'activity',\n",
       "        'score': 0.04788564914632014},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'manifold',\n",
       "        'score': 0.044395294391866645},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'segmentation',\n",
       "        'score': 0.044395294391866645},\n",
       "       {'bg_count': 170,\n",
       "        'doc_count': 12,\n",
       "        'key': 'image',\n",
       "        'score': 0.04355155499645229},\n",
       "       {'bg_count': 192,\n",
       "        'doc_count': 13,\n",
       "        'key': 'feature',\n",
       "        'score': 0.04323157947769167},\n",
       "       {'bg_count': 74,\n",
       "        'doc_count': 7,\n",
       "        'key': 'human',\n",
       "        'score': 0.04313146402039823},\n",
       "       {'bg_count': 58,\n",
       "        'doc_count': 6,\n",
       "        'key': 'prior',\n",
       "        'score': 0.04257347755234906},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'baye',\n",
       "        'score': 0.04129275683235242},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'code',\n",
       "        'score': 0.040992172173338774},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'binary',\n",
       "        'score': 0.040992172173338774},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 7,\n",
       "        'key': 'process',\n",
       "        'score': 0.03871063988923958},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'energy',\n",
       "        'score': 0.0385168021738397},\n",
       "       {'bg_count': 204,\n",
       "        'doc_count': 13,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.037769820648963504},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 4,\n",
       "        'key': 'likelihood',\n",
       "        'score': 0.03747596293922265},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'matrix',\n",
       "        'score': 0.03601844298117825},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'nearest',\n",
       "        'score': 0.03601844298117825},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'separation',\n",
       "        'score': 0.03601844298117825},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'filter',\n",
       "        'score': 0.03601844298117825},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 5,\n",
       "        'key': 'metric',\n",
       "        'score': 0.03585679981741546},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 6,\n",
       "        'key': 'random',\n",
       "        'score': 0.034637205926757814},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'region',\n",
       "        'score': 0.03375802275924646},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'natural',\n",
       "        'score': 0.03375802275924646},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.03375802275924646},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.03375802275924646},\n",
       "       {'bg_count': 263,\n",
       "        'doc_count': 15,\n",
       "        'key': 'inference',\n",
       "        'score': 0.032993005406413825},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 6,\n",
       "        'key': 'unsupervised',\n",
       "        'score': 0.032944912712477334},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 4,\n",
       "        'key': 'rank',\n",
       "        'score': 0.031615614215695795},\n",
       "       {'bg_count': 244,\n",
       "        'doc_count': 14,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.031299763760948625},\n",
       "       {'bg_count': 222,\n",
       "        'doc_count': 13,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.03068429568196482},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 5,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.030673708375616938},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 6,\n",
       "        'key': 'search',\n",
       "        'score': 0.030585236258762298},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.029148098963684484},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'robust',\n",
       "        'score': 0.029148098963684484},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.028106972204416995},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 4,\n",
       "        'key': 'free',\n",
       "        'score': 0.028009245770448495},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 4,\n",
       "        'key': 'belief',\n",
       "        'score': 0.028009245770448495},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 5,\n",
       "        'key': 'field',\n",
       "        'score': 0.02800811849126341},\n",
       "       {'bg_count': 116,\n",
       "        'doc_count': 8,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.027664974109297545},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tree',\n",
       "        'score': 0.02652467804906474},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'maximum',\n",
       "        'score': 0.025064098828739584},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'source',\n",
       "        'score': 0.025064098828739584},\n",
       "       {'bg_count': 80,\n",
       "        'doc_count': 6,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.024568061301788936},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 5,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.024148111281680446},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'nonlinear',\n",
       "        'score': 0.023711710661771846},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 4,\n",
       "        'key': 'brain',\n",
       "        'score': 0.02309147061783854},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'ranking',\n",
       "        'score': 0.02245592164958752},\n",
       "       {'bg_count': 883,\n",
       "        'doc_count': 37,\n",
       "        'key': 'model',\n",
       "        'score': 0.022324167568183634},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 5,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.022121607496649383},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'scene',\n",
       "        'score': 0.02128673877617453},\n",
       "       {'bg_count': 66,\n",
       "        'doc_count': 5,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.02087295364930701},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'order',\n",
       "        'score': 0.02019550142765573},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 4,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.018488433074995626},\n",
       "       {'bg_count': 115,\n",
       "        'doc_count': 7,\n",
       "        'key': 'fast',\n",
       "        'score': 0.01822876947983674},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'problem',\n",
       "        'score': 0.017826558395632595},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'high',\n",
       "        'score': 0.017826558395632595},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 5,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.017041467871160558},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'programming',\n",
       "        'score': 0.016472456356238667},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variable',\n",
       "        'score': 0.016472456356238667},\n",
       "       {'bg_count': 1547,\n",
       "        'doc_count': 60,\n",
       "        'key': 'learning',\n",
       "        'score': 0.01646786562652563},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 5,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.015164821775741886},\n",
       "       {'bg_count': 125,\n",
       "        'doc_count': 7,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.014633063341297126},\n",
       "       {'bg_count': 226,\n",
       "        'doc_count': 11,\n",
       "        'key': 'multi',\n",
       "        'score': 0.014492433192570311},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 4,\n",
       "        'key': 'latent',\n",
       "        'score': 0.014343007451616631},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 5,\n",
       "        'key': 'selection',\n",
       "        'score': 0.014297763769630727},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 3,\n",
       "        'key': 'boosting',\n",
       "        'score': 0.01420844246145802},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 9,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.013651037202855444},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'local',\n",
       "        'score': 0.013533210247225196},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 4,\n",
       "        'key': 'convex',\n",
       "        'score': 0.013339272433849317},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 3,\n",
       "        'key': 'generative',\n",
       "        'score': 0.012892605326029951},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 4,\n",
       "        'key': 'complexity',\n",
       "        'score': 0.012862498300409845},\n",
       "       {'bg_count': 156,\n",
       "        'doc_count': 8,\n",
       "        'key': 'function',\n",
       "        'score': 0.01274207019792941},\n",
       "       {'bg_count': 132,\n",
       "        'doc_count': 7,\n",
       "        'key': 'application',\n",
       "        'score': 0.012440227400748346},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'policy',\n",
       "        'score': 0.011705142545277795},\n",
       "       {'bg_count': 86,\n",
       "        'doc_count': 5,\n",
       "        'key': 'approach',\n",
       "        'score': 0.011580645948154483},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 5,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.011228179104317663},\n",
       "       {'bg_count': 112,\n",
       "        'doc_count': 6,\n",
       "        'key': 'online',\n",
       "        'score': 0.011005539970198208},\n",
       "       {'bg_count': 166,\n",
       "        'doc_count': 8,\n",
       "        'key': 'method',\n",
       "        'score': 0.010135056864015704},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.009646873725307382},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'component',\n",
       "        'score': 0.009646873725307382},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variational',\n",
       "        'score': 0.00918823773824876},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rate',\n",
       "        'score': 0.00918823773824876},\n",
       "       {'bg_count': 95,\n",
       "        'doc_count': 5,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.008675577119478379},\n",
       "       {'bg_count': 96,\n",
       "        'doc_count': 5,\n",
       "        'key': 'markov',\n",
       "        'score': 0.008386415175883302},\n",
       "       {'bg_count': 123,\n",
       "        'doc_count': 6,\n",
       "        'key': 'visual',\n",
       "        'score': 0.007973268940777515},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sample',\n",
       "        'score': 0.007537148184837713},\n",
       "       {'bg_count': 178,\n",
       "        'doc_count': 8,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.0073932990207086985},\n",
       "       {'bg_count': 127,\n",
       "        'doc_count': 6,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.007000858474649762},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 4,\n",
       "        'key': 'vector',\n",
       "        'score': 0.0069404616955827},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 4,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.006097133698059847},\n",
       "       {'bg_count': 132,\n",
       "        'doc_count': 6,\n",
       "        'key': 'dynamic',\n",
       "        'score': 0.005868221283989594},\n",
       "       {'bg_count': 106,\n",
       "        'doc_count': 5,\n",
       "        'key': 'processe',\n",
       "        'score': 0.005794869454984041},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 3,\n",
       "        'key': 'active',\n",
       "        'score': 0.005502769985099104},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.005502769985099104},\n",
       "       {'bg_count': 135,\n",
       "        'doc_count': 6,\n",
       "        'key': 'optimal',\n",
       "        'score': 0.0052289105141503},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 3,\n",
       "        'key': 'bound',\n",
       "        'score': 0.0052053462716870275},\n",
       "       {'bg_count': 86,\n",
       "        'doc_count': 4,\n",
       "        'key': 'large',\n",
       "        'score': 0.004358178292315051},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 4,\n",
       "        'key': 'scale',\n",
       "        'score': 0.004132599512259486},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 3,\n",
       "        'key': 'space',\n",
       "        'score': 0.0038621424046647404},\n",
       "       {'bg_count': 171,\n",
       "        'doc_count': 7,\n",
       "        'key': 'classification',\n",
       "        'score': 0.0035095045852301444},\n",
       "       {'bg_count': 64,\n",
       "        'doc_count': 3,\n",
       "        'key': 'control',\n",
       "        'score': 0.0033836260270380513},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.001041414283917941}],\n",
       "      'doc_count': 262}},\n",
       "    {'doc_count': 250,\n",
       "     'key': '2008',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dependence',\n",
       "        'score': 0.24867599999999998},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'template',\n",
       "        'score': 0.24867599999999998},\n",
       "       {'bg_count': 5, 'doc_count': 3, 'key': 'armed', 'score': 0.1965408},\n",
       "       {'bg_count': 5, 'doc_count': 3, 'key': 'unlabeled', 'score': 0.1965408},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'short',\n",
       "        'score': 0.1369577142857143},\n",
       "       {'bg_count': 8, 'doc_count': 3, 'key': 'rational', 'score': 0.118338},\n",
       "       {'bg_count': 8, 'doc_count': 3, 'key': 'text', 'score': 0.118338},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'motor',\n",
       "        'score': 0.11640685714285715},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'logistic',\n",
       "        'score': 0.10385600000000002},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'correlated',\n",
       "        'score': 0.10385600000000002},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dynamical',\n",
       "        'score': 0.0922704},\n",
       "       {'bg_count': 10, 'doc_count': 3, 'key': 'prior', 'score': 0.0922704},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.09140000000000001},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'transition',\n",
       "        'score': 0.08279127272727273},\n",
       "       {'bg_count': 20, 'doc_count': 4, 'key': 'making', 'score': 0.0766848},\n",
       "       {'bg_count': 154,\n",
       "        'doc_count': 13,\n",
       "        'key': 'online',\n",
       "        'score': 0.0751406753246753},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'partially',\n",
       "        'score': 0.07489200000000001},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'orientation',\n",
       "        'score': 0.07489200000000001},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 5,\n",
       "        'key': 'sequential',\n",
       "        'score': 0.07051249999999999},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'measure',\n",
       "        'score': 0.06825890909090909},\n",
       "       {'bg_count': 247,\n",
       "        'doc_count': 17,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.06755621052631579},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 5,\n",
       "        'key': 'regularized',\n",
       "        'score': 0.06518823529411763},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 5,\n",
       "        'key': 'performance',\n",
       "        'score': 0.06518823529411763},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 5,\n",
       "        'key': 'adaptation',\n",
       "        'score': 0.06518823529411763},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 5,\n",
       "        'key': 'noise',\n",
       "        'score': 0.06275428571428572},\n",
       "       {'bg_count': 171,\n",
       "        'doc_count': 13,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.06250095906432748},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'level',\n",
       "        'score': 0.06247885714285714},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cluster',\n",
       "        'score': 0.06247885714285714},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'effect',\n",
       "        'score': 0.06247885714285714},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 5,\n",
       "        'key': 'reduction',\n",
       "        'score': 0.06045555555555556},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 5,\n",
       "        'key': 'ranking',\n",
       "        'score': 0.05622105263157895},\n",
       "       {'bg_count': 102,\n",
       "        'doc_count': 9,\n",
       "        'key': 'process',\n",
       "        'score': 0.05600329411764705},\n",
       "       {'bg_count': 87,\n",
       "        'doc_count': 8,\n",
       "        'key': 'fast',\n",
       "        'score': 0.053227402298850576},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'loss',\n",
       "        'score': 0.053168999999999994},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'weighted',\n",
       "        'score': 0.053168999999999994},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'differential',\n",
       "        'score': 0.053168999999999994},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'factor',\n",
       "        'score': 0.05265540740740741},\n",
       "       {'bg_count': 73,\n",
       "        'doc_count': 7,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.04976635616438357},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimensionality',\n",
       "        'score': 0.0493355294117647},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'causal',\n",
       "        'score': 0.0493355294117647},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'computational',\n",
       "        'score': 0.0493355294117647},\n",
       "       {'bg_count': 74,\n",
       "        'doc_count': 7,\n",
       "        'key': 'high',\n",
       "        'score': 0.04871545945945946},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 4,\n",
       "        'key': 'response',\n",
       "        'score': 0.047920551724137936},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recursive',\n",
       "        'score': 0.04592800000000001},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 5,\n",
       "        'key': 'semi',\n",
       "        'score': 0.04582727272727273},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'variable',\n",
       "        'score': 0.045789866666666665},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 6,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.04437403278688525},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 7,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.04386005063291139},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 4,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.04379664516129032},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'estimating',\n",
       "        'score': 0.04287915789473685},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 8,\n",
       "        'key': 'convex',\n",
       "        'score': 0.04141370297029703},\n",
       "       {'bg_count': 102,\n",
       "        'doc_count': 8,\n",
       "        'key': 'efficient',\n",
       "        'score': 0.040693960784313726},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'conditional',\n",
       "        'score': 0.0401352},\n",
       "       {'bg_count': 20, 'doc_count': 3, 'key': 'signal', 'score': 0.0401352},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 7,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.03878757647058824},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 4,\n",
       "        'key': 'temporal',\n",
       "        'score': 0.03852047058823529},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'real',\n",
       "        'score': 0.03765257142857143},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'consistency',\n",
       "        'score': 0.03765257142857143},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.03765257142857143},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'design',\n",
       "        'score': 0.03765257142857143},\n",
       "       {'bg_count': 883,\n",
       "        'doc_count': 38,\n",
       "        'key': 'model',\n",
       "        'score': 0.03746326613816534},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 4,\n",
       "        'key': 'discriminative',\n",
       "        'score': 0.03549155555555556},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.035395636363636365},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'density',\n",
       "        'score': 0.035395636363636365},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 5,\n",
       "        'key': 'matching',\n",
       "        'score': 0.034649056603773584},\n",
       "       {'bg_count': 53,\n",
       "        'doc_count': 5,\n",
       "        'key': 'policy',\n",
       "        'score': 0.034649056603773584},\n",
       "       {'bg_count': 113,\n",
       "        'doc_count': 8,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.0336175575221239},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'programming',\n",
       "        'score': 0.03333495652173913},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'margin',\n",
       "        'score': 0.03333495652173913},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 4,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.03278147368421053},\n",
       "       {'bg_count': 74,\n",
       "        'doc_count': 6,\n",
       "        'key': 'human',\n",
       "        'score': 0.03236237837837838},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 10,\n",
       "        'key': 'regression',\n",
       "        'score': 0.031960248447204964},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 5,\n",
       "        'key': 'natural',\n",
       "        'score': 0.03172142857142857},\n",
       "       {'bg_count': 75, 'doc_count': 6, 'key': 'field', 'score': 0.03161088},\n",
       "       {'bg_count': 168,\n",
       "        'doc_count': 10,\n",
       "        'key': 'classification',\n",
       "        'score': 0.028961904761904765},\n",
       "       {'bg_count': 124,\n",
       "        'doc_count': 8,\n",
       "        'key': 'bound',\n",
       "        'score': 0.027796645161290323},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sample',\n",
       "        'score': 0.027109209302325583},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 6,\n",
       "        'key': 'large',\n",
       "        'score': 0.026863609756097567},\n",
       "       {'bg_count': 175,\n",
       "        'doc_count': 10,\n",
       "        'key': 'multi',\n",
       "        'score': 0.026203428571428572},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 4,\n",
       "        'key': 'robust',\n",
       "        'score': 0.025193244444444447},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.023955310344827587},\n",
       "       {'bg_count': 110,\n",
       "        'doc_count': 7,\n",
       "        'key': 'random',\n",
       "        'score': 0.02360858181818182},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sequence',\n",
       "        'score': 0.02163561290322581},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'support',\n",
       "        'score': 0.02163561290322581},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'continuous',\n",
       "        'score': 0.020346980392156863},\n",
       "       {'bg_count': 223,\n",
       "        'doc_count': 11,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.018863569506726453},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'memory',\n",
       "        'score': 0.018667764705882352},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'binary',\n",
       "        'score': 0.018667764705882352},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'search',\n",
       "        'score': 0.018667764705882352},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'coding',\n",
       "        'score': 0.018667764705882352},\n",
       "       {'bg_count': 224,\n",
       "        'doc_count': 11,\n",
       "        'key': 'based',\n",
       "        'score': 0.018582928571428566},\n",
       "       {'bg_count': 150,\n",
       "        'doc_count': 8,\n",
       "        'key': 'feature',\n",
       "        'score': 0.017431893333333334},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 3,\n",
       "        'key': 'motion',\n",
       "        'score': 0.016964000000000003},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 4,\n",
       "        'key': 'space',\n",
       "        'score': 0.016520982456140355},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 4,\n",
       "        'key': 'brain',\n",
       "        'score': 0.016520982456140355},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 3,\n",
       "        'key': 'system',\n",
       "        'score': 0.01618118918918919},\n",
       "       {'bg_count': 81,\n",
       "        'doc_count': 5,\n",
       "        'key': 'graph',\n",
       "        'score': 0.015758024691358022},\n",
       "       {'bg_count': 130, 'doc_count': 7, 'key': 'image', 'score': 0.0156688},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'component',\n",
       "        'score': 0.015439578947368422},\n",
       "       {'bg_count': 293,\n",
       "        'doc_count': 13,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.014824791808873711},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 3,\n",
       "        'key': 'theory',\n",
       "        'score': 0.014735999999999999},\n",
       "       {'bg_count': 212,\n",
       "        'doc_count': 10,\n",
       "        'key': 'data',\n",
       "        'score': 0.014649056603773582},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 5,\n",
       "        'key': 'processe',\n",
       "        'score': 0.012913636363636365},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approach',\n",
       "        'score': 0.012248930232558141},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.011697818181818182},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spike',\n",
       "        'score': 0.011667104477611939},\n",
       "       {'bg_count': 200,\n",
       "        'doc_count': 9,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.010921679999999996},\n",
       "       {'bg_count': 69,\n",
       "        'doc_count': 4,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.010865159420289858},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 4,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.010108394366197185},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 3,\n",
       "        'key': 'structure',\n",
       "        'score': 0.009723000000000002},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 4,\n",
       "        'key': 'decision',\n",
       "        'score': 0.008715946666666667},\n",
       "       {'bg_count': 52, 'doc_count': 3, 'key': 'detection', 'score': 0.008052},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 6,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.007838290076335878},\n",
       "       {'bg_count': 161,\n",
       "        'doc_count': 7,\n",
       "        'key': 'function',\n",
       "        'score': 0.007260521739130433},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 5,\n",
       "        'key': 'machine',\n",
       "        'score': 0.007069158878504674},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 4,\n",
       "        'key': 'control',\n",
       "        'score': 0.006606048780487807},\n",
       "       {'bg_count': 109,\n",
       "        'doc_count': 5,\n",
       "        'key': 'recognition',\n",
       "        'score': 0.006572477064220185},\n",
       "       {'bg_count': 110,\n",
       "        'doc_count': 5,\n",
       "        'key': 'markov',\n",
       "        'score': 0.006330909090909093},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unsupervised',\n",
       "        'score': 0.006293052631578949},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 3,\n",
       "        'key': 'latent',\n",
       "        'score': 0.006293052631578949},\n",
       "       {'bg_count': 167,\n",
       "        'doc_count': 7,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.005993676646706589},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.005672949152542375},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.005093508196721313},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 3,\n",
       "        'key': 'gradient',\n",
       "        'score': 0.005093508196721313},\n",
       "       {'bg_count': 63,\n",
       "        'doc_count': 3,\n",
       "        'key': 'structured',\n",
       "        'score': 0.004550857142857143},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 3,\n",
       "        'key': 'local',\n",
       "        'score': 0.0035627462686567157},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vector',\n",
       "        'score': 0.0033338823529411757},\n",
       "       {'bg_count': 155,\n",
       "        'doc_count': 6,\n",
       "        'key': 'linear',\n",
       "        'score': 0.0029084903225806446},\n",
       "       {'bg_count': 71,\n",
       "        'doc_count': 3,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.0026859718309859165},\n",
       "       {'bg_count': 330,\n",
       "        'doc_count': 12,\n",
       "        'key': 'algorithm',\n",
       "        'score': 0.002555345454545459},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recurrent',\n",
       "        'score': 0.002482000000000001},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 4,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.0023534257425742574},\n",
       "       {'bg_count': 130,\n",
       "        'doc_count': 5,\n",
       "        'key': 'application',\n",
       "        'score': 0.0022799999999999995},\n",
       "       {'bg_count': 79,\n",
       "        'doc_count': 3,\n",
       "        'key': 'object',\n",
       "        'score': 0.0011987848101265818},\n",
       "       {'bg_count': 171,\n",
       "        'doc_count': 6,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.0003907368421052643}],\n",
       "      'doc_count': 250}}],\n",
       "   'doc_count_error_upper_bound': 133,\n",
       "   'sum_other_doc_count': 3341}},\n",
       " 'hits': {'hits': [{'_id': '1001',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1001',\n",
       "     'pdf_name': '1001-neural-network-ensembles-cross-validation-and-active-learning.pdf',\n",
       "     'title': 'Neural Network Ensembles, Cross Validation, and Active Learning',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1004',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1004',\n",
       "     'pdf_name': '1004-iceg-morphology-classification-using-an-analogue-vlsi-neural-network.pdf',\n",
       "     'title': 'ICEG Morphology Classification using an Analogue VLSI Neural Network',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1006',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1006',\n",
       "     'pdf_name': '1006-pulsestream-synapses-with-non-volatile-analogue-amorphous-silicon-memories.pdf',\n",
       "     'title': 'Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1007',\n",
       "     'pdf_name': '1007-learning-to-play-the-game-of-chess.pdf',\n",
       "     'title': 'Learning to Play the Game of Chess',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1013',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1013',\n",
       "     'pdf_name': '1013-ocular-dominance-and-patterned-lateral-connections-in-a-self-organizing-model-of-the-primary-visual-cortex.pdf',\n",
       "     'title': 'Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex',\n",
       "     'year': '1994'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1032',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1032',\n",
       "     'pdf_name': '1032-vlsi-model-of-primate-visual-smooth-pursuit.pdf',\n",
       "     'title': 'VLSI Model of Primate Visual Smooth Pursuit',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1033',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1033',\n",
       "     'pdf_name': '1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf',\n",
       "     'title': 'Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1035',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1035',\n",
       "     'pdf_name': '1035-a-dynamical-model-of-context-dependencies-for-the-vestibulo-ocular-reflex.pdf',\n",
       "     'title': 'A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1036',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1036',\n",
       "     'pdf_name': '1036-improved-gaussian-mixture-density-estimates-using-bayesian-penalty-terms-and-network-averaging.pdf',\n",
       "     'title': 'Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '1044',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Abstract Missing',\n",
       "     'event_type': '',\n",
       "     'id': '1044',\n",
       "     'pdf_name': '1044-learning-with-ensembles-how-overfitting-can-be-useful.pdf',\n",
       "     'title': 'Learning with ensembles: How overfitting can be useful',\n",
       "     'year': '1995'},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': 7241},\n",
       " 'timed_out': False,\n",
       " 'took': 65}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregations from slack\n",
    "#bg_count: number of documents with the significant term in the whole index (regardless of buckets)\n",
    "#doc_count: number of documents with the significant term in the specific bucket (year)\n",
    "es.search(index = \"nips_papers\", _source_exclude=\"paper_text\", body =\n",
    "         {\n",
    "             \"aggs\": {\n",
    "                \"years\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\":\"year\"\n",
    "                    },\n",
    "                    \"aggs\": {\n",
    "                        \"keywords\": {\n",
    "                            \"significant_terms\": {\n",
    "                                \"field\": \"title\",\n",
    "                                \"size\": 200\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_name': 'elastic56',\n",
       " 'cluster_uuid': 'gkJJypQTTaWcZoxzoEMWYQ',\n",
       " 'name': 'dY-sJap',\n",
       " 'tagline': 'You Know, for Search',\n",
       " 'version': {'build_date': '2017-09-23T13:16:45.703Z',\n",
       "  'build_hash': '57e20f3',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '6.6.1',\n",
       "  'number': '5.6.2'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.get_source(index=\"nips_papers\", doc_type=\"document\", id='1001')\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'aggregations': {'terms': {'bg_count': 7241,\n",
       "   'buckets': [{'bg_count': 153,\n",
       "     'doc_count': 55,\n",
       "     'key': 'deep',\n",
       "     'score': 0.22952093651433012},\n",
       "    {'bg_count': 34,\n",
       "     'doc_count': 22,\n",
       "     'key': 'adversarial',\n",
       "     'score': 0.19117554557161373},\n",
       "    {'bg_count': 16,\n",
       "     'doc_count': 14,\n",
       "     'key': 'gan',\n",
       "     'score': 0.17177702200021255},\n",
       "    {'bg_count': 23,\n",
       "     'doc_count': 12,\n",
       "     'key': 'generative',\n",
       "     'score': 0.08065867573976962},\n",
       "    {'bg_count': 16,\n",
       "     'doc_count': 9,\n",
       "     'key': 'scalable',\n",
       "     'score': 0.06625563127791237},\n",
       "    {'bg_count': 4,\n",
       "     'doc_count': 4,\n",
       "     'key': 'disentangled',\n",
       "     'score': 0.05693202990623394},\n",
       "    {'bg_count': 4,\n",
       "     'doc_count': 4,\n",
       "     'key': 'fairness',\n",
       "     'score': 0.05693202990623394},\n",
       "    {'bg_count': 3,\n",
       "     'doc_count': 3,\n",
       "     'key': 'assumption',\n",
       "     'score': 0.042699022429675444},\n",
       "    {'bg_count': 3,\n",
       "     'doc_count': 3,\n",
       "     'key': 'acceleration',\n",
       "     'score': 0.042699022429675444},\n",
       "    {'bg_count': 3,\n",
       "     'doc_count': 3,\n",
       "     'key': 'predicting',\n",
       "     'score': 0.042699022429675444}],\n",
       "   'doc_count': 679}},\n",
       " 'hits': {'hits': [{'_id': '6608',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Deep Subspace Clustering Networks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6618',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6621',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Label Efficient Learning of Transferable Representations acrosss Domains and Tasks'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6622',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Decoding with Value Networks for Neural Machine Translation'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6623',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Parametric Simplex Method for Sparse Learning'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6627',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'From Parity to Preference-based Notions of Fairness in Classification'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6636',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'MaskRNN: Instance Level Video Object Segmentation'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6640',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Learning a Multi-View Stereo Machine'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6649',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'f-GANs in an Information Geometric Nutshell'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6652',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {'title': 'Non-monotone Continuous DR-submodular  Maximization: Structure and Algorithms'},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 2.416312,\n",
       "  'total': 679},\n",
       " 'timed_out': False,\n",
       " 'took': 14}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index=\"nips_papers\", body={\n",
    "    \"_source\": \"title\",\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"year\": \"2017\"\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"terms\": { #aggregation name\n",
    "            \"significant_terms\": {\n",
    "                \"field\": \"title\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'aggregations': {'keywords_by_years': {'bg_count': 7241,\n",
       "   'buckets': [{'bg_count': 694,\n",
       "     'doc_count': 675,\n",
       "     'key': '2017',\n",
       "     'score': 9.31704028935845},\n",
       "    {'bg_count': 690,\n",
       "     'doc_count': 662,\n",
       "     'key': 'beach',\n",
       "     'score': 9.000335013745987},\n",
       "    {'bg_count': 780,\n",
       "     'doc_count': 655,\n",
       "     'key': '31st',\n",
       "     'score': 7.6740183266419555},\n",
       "    {'bg_count': 1220,\n",
       "     'doc_count': 644,\n",
       "     'key': '2016',\n",
       "     'score': 4.390681402006101},\n",
       "    {'bg_count': 1471,\n",
       "     'doc_count': 627,\n",
       "     'key': '2015',\n",
       "     'score': 3.273993355541531},\n",
       "    {'bg_count': 1998,\n",
       "     'doc_count': 663,\n",
       "     'key': 'usa',\n",
       "     'score': 2.478902347333788},\n",
       "    {'bg_count': 1806,\n",
       "     'doc_count': 618,\n",
       "     'key': '2014',\n",
       "     'score': 2.4112152203642627},\n",
       "    {'bg_count': 1647,\n",
       "     'doc_count': 513,\n",
       "     'key': 'arxiv',\n",
       "     'score': 1.7540518870676873},\n",
       "    {'bg_count': 1211,\n",
       "     'doc_count': 417,\n",
       "     'key': 'preprint',\n",
       "     'score': 1.6410714422956212},\n",
       "    {'bg_count': 2077,\n",
       "     'doc_count': 558,\n",
       "     'key': '2013',\n",
       "     'score': 1.5326609829468658}],\n",
       "   'doc_count': 679}},\n",
       " 'hits': {'hits': [{'_id': '6608',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6618',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6621',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6622',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6623',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6627',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6636',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6640',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6649',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6652',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 2.416312,\n",
       "    '_source': {},\n",
       "    '_type': 'document'}],\n",
       "  'max_score': 2.416312,\n",
       "  'total': 679},\n",
       " 'timed_out': False,\n",
       " 'took': 215}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#single-set analysis--aggregations based on one category (year = 2017)\n",
    "es.search(index=\"nips_papers\", body={\n",
    "    \"_source\": \"\",\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"year\": \"2017\"\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"keywords_by_years\": {\n",
    "            \"significant_terms\": {\n",
    "                \"field\": \"paper_text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'aggregations': {'keywords_by_years': {'buckets': [{'doc_count': 679,\n",
       "     'key': '2017',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 153,\n",
       "        'doc_count': 55,\n",
       "        'key': 'deep',\n",
       "        'score': 0.22952093651433012},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 22,\n",
       "        'key': 'adversarial',\n",
       "        'score': 0.19117554557161373},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 14,\n",
       "        'key': 'gan',\n",
       "        'score': 0.17177702200021255},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 12,\n",
       "        'key': 'generative',\n",
       "        'score': 0.08065867573976962},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 9,\n",
       "        'key': 'scalable',\n",
       "        'score': 0.06625563127791237},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 4,\n",
       "        'key': 'fairness',\n",
       "        'score': 0.05693202990623394},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'gated',\n",
       "        'score': 0.04436742068492824},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'predicting',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'acceleration',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'assumption',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'safe',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'disentangled',\n",
       "        'score': 0.042699022429675444},\n",
       "       {'bg_count': 653,\n",
       "        'doc_count': 82,\n",
       "        'key': 'learning',\n",
       "        'score': 0.04095781338412156},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'convolution',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'decentralized',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'practical',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'evaluation',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'knowledge',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'invariance',\n",
       "        'score': 0.03091970128470136},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'imitation',\n",
       "        'score': 0.03091970128470136}],\n",
       "      'doc_count': 679}},\n",
       "    {'doc_count': 569,\n",
       "     'key': '2016',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 150,\n",
       "        'doc_count': 37,\n",
       "        'key': 'deep',\n",
       "        'score': 0.1390939406949365},\n",
       "       {'bg_count': 130,\n",
       "        'doc_count': 28,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.08567060269766893},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 19,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.06506979214794173},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'interpretable',\n",
       "        'score': 0.06453896547144343},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'budget',\n",
       "        'score': 0.06453896547144343},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 6,\n",
       "        'key': 'convolutional',\n",
       "        'score': 0.06265059158397146},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'posterior',\n",
       "        'score': 0.061823382062694394},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'composition',\n",
       "        'score': 0.061823382062694394},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'maximizing',\n",
       "        'score': 0.061823382062694394},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'safe',\n",
       "        'score': 0.052610825063344045},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 8,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.04817402208795188},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'path',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'asynchronous',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'crowdsourcing',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'aggregation',\n",
       "        'score': 0.04504943461380463},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'recovery',\n",
       "        'score': 0.04409072477184448},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.04409072477184448},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'domain',\n",
       "        'score': 0.04409072477184448},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'chain',\n",
       "        'score': 0.034985066144470764},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'adversarial',\n",
       "        'score': 0.03273059104984507}],\n",
       "      'doc_count': 569}},\n",
       "    {'doc_count': 411,\n",
       "     'key': '2014',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 39,\n",
       "        'doc_count': 11,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.10623111789929414},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 14,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.10144921724241776},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 9,\n",
       "        'key': 'rank',\n",
       "        'score': 0.09384090788001492},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subset',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exploiting',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'permutation',\n",
       "        'score': 0.08914966167616815},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 4,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.08824750707642712},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 7,\n",
       "        'key': 'parallel',\n",
       "        'score': 0.07429199602695677},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 6,\n",
       "        'key': 'factorization',\n",
       "        'score': 0.07113384363104647},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'strategy',\n",
       "        'score': 0.06985987532633597},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hilbert',\n",
       "        'score': 0.06985987532633597},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 8,\n",
       "        'key': 'low',\n",
       "        'score': 0.06903322434938569},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'positive',\n",
       "        'score': 0.06647420326003807},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 9,\n",
       "        'key': 'convex',\n",
       "        'score': 0.060772702708874075},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 5,\n",
       "        'key': 'coordinate',\n",
       "        'score': 0.05927820302587206},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'balanced',\n",
       "        'score': 0.057000017759781196},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'attribute',\n",
       "        'score': 0.057000017759781196},\n",
       "       {'bg_count': 107,\n",
       "        'doc_count': 15,\n",
       "        'key': 'inference',\n",
       "        'score': 0.05364283818565411},\n",
       "       {'bg_count': 42,\n",
       "        'doc_count': 8,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.04585519125451994},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tensor',\n",
       "        'score': 0.040925195801587724}],\n",
       "      'doc_count': 411}},\n",
       "    {'doc_count': 403,\n",
       "     'key': '2015',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'frank',\n",
       "        'score': 0.1263107340110462},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'proposal',\n",
       "        'score': 0.1263107340110462},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mixing',\n",
       "        'score': 0.1263107340110462},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sgd',\n",
       "        'score': 0.1263107340110462},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 5,\n",
       "        'key': 'mcmc',\n",
       "        'score': 0.11144018428097516},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'path',\n",
       "        'score': 0.10896768857226713},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 15,\n",
       "        'key': 'optimization',\n",
       "        'score': 0.0807981881618412},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'walk',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'poisson',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measurement',\n",
       "        'score': 0.07280877291283118},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 7,\n",
       "        'key': 'bound',\n",
       "        'score': 0.06354373263842568},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 5,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.061901331412257544},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 6,\n",
       "        'key': 'convolutional',\n",
       "        'score': 0.061543035528467384},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'infinite',\n",
       "        'score': 0.061410389818298244},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'guarantees',\n",
       "        'score': 0.05943328263827744},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 6,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.05489682918086302},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'subspace',\n",
       "        'score': 0.049879361013596184},\n",
       "       {'bg_count': 82,\n",
       "        'doc_count': 12,\n",
       "        'key': 'stochastic',\n",
       "        'score': 0.04851887788869083},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 7,\n",
       "        'key': 'graph',\n",
       "        'score': 0.04688507917374906},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 5,\n",
       "        'key': 'generative',\n",
       "        'score': 0.04332426158648844}],\n",
       "      'doc_count': 403}},\n",
       "    {'doc_count': 368,\n",
       "     'key': '2012',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 42,\n",
       "        'doc_count': 11,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.12415063940273655},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 5,\n",
       "        'key': 'ranking',\n",
       "        'score': 0.12008580458412096},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hashing',\n",
       "        'score': 0.11215331108223062},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 10,\n",
       "        'key': 'prior',\n",
       "        'score': 0.10323853681589747},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'collaborative',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'conjugate',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'geometric',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unknown',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'weighted',\n",
       "        'score': 0.08809221408317579},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'newton',\n",
       "        'score': 0.07205148275047259},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 9,\n",
       "        'key': 'object',\n",
       "        'score': 0.07178786625708886},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 5,\n",
       "        'key': 'topic',\n",
       "        'score': 0.06995851916942343},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 5,\n",
       "        'key': '3d',\n",
       "        'score': 0.06067568853707204},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'semantic',\n",
       "        'score': 0.052000568584593565},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'metric',\n",
       "        'score': 0.05023798271671617},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 5,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.05006673924295616},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'margin',\n",
       "        'score': 0.04616414618777567},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'expectation',\n",
       "        'score': 0.04531693052930057},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'parametric',\n",
       "        'score': 0.03997002008506616},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.03945429778716779}],\n",
       "      'doc_count': 368}},\n",
       "    {'doc_count': 360,\n",
       "     'key': '2013',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'thompson',\n",
       "        'score': 0.1592824074074074},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 6,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.10165032679738562},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sery',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'coupled',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'optimality',\n",
       "        'score': 0.0922361111111111},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 6,\n",
       "        'key': 'low',\n",
       "        'score': 0.08919590643274854},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'setting',\n",
       "        'score': 0.08821673525377231},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'transfer',\n",
       "        'score': 0.07547453703703705},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'lasso',\n",
       "        'score': 0.07547453703703705},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 5,\n",
       "        'key': 'high',\n",
       "        'score': 0.07341097608024691},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 7,\n",
       "        'key': 'rank',\n",
       "        'score': 0.07181301440329219},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'message',\n",
       "        'score': 0.0701571268237935},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 8,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.0694650205761317},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 5,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.059626786874593884},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 5,\n",
       "        'key': 'graphical',\n",
       "        'score': 0.059626786874593884},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'agent',\n",
       "        'score': 0.05765432098765433},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 5,\n",
       "        'key': 'binary',\n",
       "        'score': 0.055951003086419746},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.048485596707818934},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'submodular',\n",
       "        'score': 0.047538580246913584},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'inverse',\n",
       "        'score': 0.047538580246913584}],\n",
       "      'doc_count': 360}},\n",
       "    {'doc_count': 306,\n",
       "     'key': '2011',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'passing',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'message',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multiclass',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measurement',\n",
       "        'score': 0.10619313084711007},\n",
       "       {'bg_count': 91,\n",
       "        'doc_count': 12,\n",
       "        'key': 'sparse',\n",
       "        'score': 0.08315483055967485},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'tree',\n",
       "        'score': 0.08210517322397369},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 6,\n",
       "        'key': 'bandit',\n",
       "        'score': 0.07638971748266582},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'low',\n",
       "        'score': 0.0675274467085309},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 5,\n",
       "        'key': 'structure',\n",
       "        'score': 0.060991498996112614},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'measure',\n",
       "        'score': 0.05979430988081507},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'knowledge',\n",
       "        'score': 0.05979430988081507},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'observation',\n",
       "        'score': 0.05346719793086575},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'embedding',\n",
       "        'score': 0.05346719793086575},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'scale',\n",
       "        'score': 0.048194604639241315},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 5,\n",
       "        'key': 'object',\n",
       "        'score': 0.04810293761658622},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rank',\n",
       "        'score': 0.043733179546328334},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'manifold',\n",
       "        'score': 0.043733179546328334},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'noise',\n",
       "        'score': 0.043733179546328334},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sampling',\n",
       "        'score': 0.038482350093268974},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 6,\n",
       "        'key': 'inference',\n",
       "        'score': 0.038390683070613864}],\n",
       "      'doc_count': 306}},\n",
       "    {'doc_count': 292,\n",
       "     'key': '2010',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 14,\n",
       "        'doc_count': 6,\n",
       "        'key': 'group',\n",
       "        'score': 0.19782926306195212},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 7,\n",
       "        'key': 'latent',\n",
       "        'score': 0.18409234847063236},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'matrice',\n",
       "        'score': 0.14259007318446235},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'identifying',\n",
       "        'score': 0.11711273221992867},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'estimating',\n",
       "        'score': 0.1098278715092376},\n",
       "       {'bg_count': 62,\n",
       "        'doc_count': 10,\n",
       "        'key': 'large',\n",
       "        'score': 0.10272837607975834},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'infinite',\n",
       "        'score': 0.09891463153097606},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'action',\n",
       "        'score': 0.08335790687076106},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 6,\n",
       "        'key': 'regression',\n",
       "        'score': 0.07499208341152185},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'novel',\n",
       "        'score': 0.07465049727903922},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 5,\n",
       "        'key': 'task',\n",
       "        'score': 0.07133970178895352},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'margin',\n",
       "        'score': 0.044320329464118166},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'prior',\n",
       "        'score': 0.040680709326327634},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.03482963836688738},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'point',\n",
       "        'score': 0.032188262338149747},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'processe',\n",
       "        'score': 0.03159442046662913},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'selection',\n",
       "        'score': 0.029953407867576613},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'search',\n",
       "        'score': 0.029953407867576613},\n",
       "       {'bg_count': 46,\n",
       "        'doc_count': 5,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.02903131552538611},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.026122228775165537}],\n",
       "      'doc_count': 292}},\n",
       "    {'doc_count': 262,\n",
       "     'key': '2009',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 5,\n",
       "        'doc_count': 4,\n",
       "        'key': 'compressed',\n",
       "        'score': 0.322288910902628},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'slow',\n",
       "        'score': 0.2258937416234485},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'additive',\n",
       "        'score': 0.14677903385583593},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'class',\n",
       "        'score': 0.1381674091889114},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'smooth',\n",
       "        'score': 0.12417483163651803},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'law',\n",
       "        'score': 0.12417483163651803},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'factor',\n",
       "        'score': 0.12417483163651803},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.0955750272990071},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'label',\n",
       "        'score': 0.08401402633193594},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 6,\n",
       "        'key': 'graph',\n",
       "        'score': 0.07973453320461071},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 5,\n",
       "        'key': 'detection',\n",
       "        'score': 0.07510020644817568},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'domain',\n",
       "        'score': 0.07485657224891536},\n",
       "       {'bg_count': 72,\n",
       "        'doc_count': 8,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.06323122843139159},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'semi',\n",
       "        'score': 0.05811458235686072},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'matching',\n",
       "        'score': 0.051841384534700774},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 6,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.05012819765747917},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 5,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.042245261361957824},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.04129275683235242},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'field',\n",
       "        'score': 0.03601844298117825},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.028106972204416995}],\n",
       "      'doc_count': 262}},\n",
       "    {'doc_count': 250,\n",
       "     'key': '2008',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dependence',\n",
       "        'score': 0.24867599999999998},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'template',\n",
       "        'score': 0.24867599999999998},\n",
       "       {'bg_count': 5, 'doc_count': 3, 'key': 'armed', 'score': 0.1965408},\n",
       "       {'bg_count': 5, 'doc_count': 3, 'key': 'unlabeled', 'score': 0.1965408},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'short',\n",
       "        'score': 0.1369577142857143},\n",
       "       {'bg_count': 8, 'doc_count': 3, 'key': 'rational', 'score': 0.118338},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'motor',\n",
       "        'score': 0.11640685714285715},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 6,\n",
       "        'key': 'fast',\n",
       "        'score': 0.10633799999999999},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dimensional',\n",
       "        'score': 0.0922704},\n",
       "       {'bg_count': 10, 'doc_count': 3, 'key': 'prior', 'score': 0.0922704},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dynamical',\n",
       "        'score': 0.0922704},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'process',\n",
       "        'score': 0.09140000000000001},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'making',\n",
       "        'score': 0.07489200000000001},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'high',\n",
       "        'score': 0.07227123809523811},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'reduction',\n",
       "        'score': 0.06825890909090909},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.06247885714285714},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'online',\n",
       "        'score': 0.058147840000000006},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variable',\n",
       "        'score': 0.057513600000000005},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sequential',\n",
       "        'score': 0.057513600000000005},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'robust',\n",
       "        'score': 0.057513600000000005}],\n",
       "      'doc_count': 250}},\n",
       "    {'doc_count': 217,\n",
       "     'key': '2007',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'theoretical',\n",
       "        'score': 0.21683408014610636},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pomdp',\n",
       "        'score': 0.15916933891142307},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 6,\n",
       "        'key': 'discriminative',\n",
       "        'score': 0.13516832331267575},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'natural',\n",
       "        'score': 0.12629401396354586},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'programming',\n",
       "        'score': 0.12457049417061311},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.08408191580482349},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'search',\n",
       "        'score': 0.08408191580482349},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.0784387011828665},\n",
       "       {'bg_count': 104,\n",
       "        'doc_count': 9,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.07829057741723121},\n",
       "       {'bg_count': 106,\n",
       "        'doc_count': 9,\n",
       "        'key': 'inference',\n",
       "        'score': 0.07603085606260646},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 5,\n",
       "        'key': 'image',\n",
       "        'score': 0.07553073258439694},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'latent',\n",
       "        'score': 0.06758416165633788},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.06758416165633788},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'processe',\n",
       "        'score': 0.06357889669915834},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'support',\n",
       "        'score': 0.06306143685361762},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.05207767661843987},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vector',\n",
       "        'score': 0.05207767661843987},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.04908210564520957},\n",
       "       {'bg_count': 166,\n",
       "        'doc_count': 9,\n",
       "        'key': 'model',\n",
       "        'score': 0.03355898481905707},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'feature',\n",
       "        'score': 0.026879638431855576}],\n",
       "      'doc_count': 217}},\n",
       "    {'doc_count': 207,\n",
       "     'key': '2002',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'array',\n",
       "        'score': 0.28968704053770217},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'scaling',\n",
       "        'score': 0.23899040817755376},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'string',\n",
       "        'score': 0.23899040817755376},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'color',\n",
       "        'score': 0.20277852792030487},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'cortex',\n",
       "        'score': 0.16093102133849876},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'real',\n",
       "        'score': 0.15952886120826723},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'response',\n",
       "        'score': 0.1397245868871948},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 5,\n",
       "        'key': 'motion',\n",
       "        'score': 0.12672824503322297},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 4,\n",
       "        'key': 'natural',\n",
       "        'score': 0.1229826649519549},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 6,\n",
       "        'key': 'approach',\n",
       "        'score': 0.10620551238068567},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'filtering',\n",
       "        'score': 0.1035772554361086},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'spike',\n",
       "        'score': 0.08690051109710846},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'density',\n",
       "        'score': 0.08056343205208989},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'representation',\n",
       "        'score': 0.07497189171824999},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 5,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.07409469814768684},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'support',\n",
       "        'score': 0.07000163364372565},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 5,\n",
       "        'key': 'vector',\n",
       "        'score': 0.06972806314681271},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'selection',\n",
       "        'score': 0.06555456062967753},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.05793100689130935},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'machine',\n",
       "        'score': 0.05793100689130935}],\n",
       "      'doc_count': 207}},\n",
       "    {'doc_count': 207,\n",
       "     'key': '2004',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 8,\n",
       "        'doc_count': 4,\n",
       "        'key': 'tracking',\n",
       "        'score': 0.3186538775700716},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'interface',\n",
       "        'score': 0.20277852792030487},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.18866251254405},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 5,\n",
       "        'key': 'regularization',\n",
       "        'score': 0.17702252316829087},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'surface',\n",
       "        'score': 0.17561961772736823},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'path',\n",
       "        'score': 0.17561961772736823},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 6,\n",
       "        'key': 'semi',\n",
       "        'score': 0.16112686410417978},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'discriminant',\n",
       "        'score': 0.15449602091063971},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'plasticity',\n",
       "        'score': 0.13759714345725688},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'minimization',\n",
       "        'score': 0.13088857253248487},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 6,\n",
       "        'key': 'decision',\n",
       "        'score': 0.12700413078484912},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'population',\n",
       "        'score': 0.1237707891772164},\n",
       "       {'bg_count': 102,\n",
       "        'doc_count': 10,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.1173660904068505},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 6,\n",
       "        'key': 'detection',\n",
       "        'score': 0.1124934667819444},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'error',\n",
       "        'score': 0.11224882727718268},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'entropy',\n",
       "        'score': 0.11224882727718268},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 6,\n",
       "        'key': 'classification',\n",
       "        'score': 0.109278035554028},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'exponential',\n",
       "        'score': 0.09414288714855823},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 5,\n",
       "        'key': 'supervised',\n",
       "        'score': 0.084171548149704},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'real',\n",
       "        'score': 0.08056343205208989}],\n",
       "      'doc_count': 207}},\n",
       "    {'doc_count': 207,\n",
       "     'key': '2005',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'law',\n",
       "        'score': 0.23899040817755376},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'neuronal',\n",
       "        'score': 0.2059946945475196},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'laplacian',\n",
       "        'score': 0.20277852792030487},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'activity',\n",
       "        'score': 0.16093102133849876},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'eye',\n",
       "        'score': 0.15449602091063971},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cue',\n",
       "        'score': 0.15449602091063971},\n",
       "       {'bg_count': 91,\n",
       "        'doc_count': 9,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.10694031866054549},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'approximation',\n",
       "        'score': 0.09823373687377412},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'design',\n",
       "        'score': 0.09414288714855823},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'policy',\n",
       "        'score': 0.08690051109710846},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vector',\n",
       "        'score': 0.05793100689130935},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'active',\n",
       "        'score': 0.054639017777014},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 4,\n",
       "        'key': 'method',\n",
       "        'score': 0.04827183831594669},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'process',\n",
       "        'score': 0.04400336063852132},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.041836837888087626},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'variational',\n",
       "        'score': 0.039825066762684914},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'estimation',\n",
       "        'score': 0.022602343225700688}],\n",
       "      'doc_count': 207}},\n",
       "    {'doc_count': 204,\n",
       "     'key': '2006',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'eeg',\n",
       "        'score': 0.2462874855824683},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spiking',\n",
       "        'score': 0.17924424671829517},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 4,\n",
       "        'key': 'relational',\n",
       "        'score': 0.16598744072792518},\n",
       "       {'bg_count': 102,\n",
       "        'doc_count': 11,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.1524849511123173},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'source',\n",
       "        'score': 0.12765413652091853},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'extraction',\n",
       "        'score': 0.11579080161476357},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'laplacian',\n",
       "        'score': 0.10575259515570934},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'localization',\n",
       "        'score': 0.09714841819080573},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 5,\n",
       "        'key': 'field',\n",
       "        'score': 0.09632045901149129},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.08350064789895624},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'effect',\n",
       "        'score': 0.08316663062283737},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dirichlet',\n",
       "        'score': 0.08316663062283737},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'large',\n",
       "        'score': 0.07981820179052013},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 7,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.0778676344118659},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'adaptation',\n",
       "        'score': 0.07740942397720332},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'random',\n",
       "        'score': 0.07740942397720332},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'nonparametric',\n",
       "        'score': 0.0677130759424513},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 4,\n",
       "        'key': 'markov',\n",
       "        'score': 0.06475364952873602},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 4,\n",
       "        'key': 'multiple',\n",
       "        'score': 0.05177495834935281},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 5,\n",
       "        'key': 'based',\n",
       "        'score': 0.04679985944876182}],\n",
       "      'doc_count': 204}},\n",
       "    {'doc_count': 198,\n",
       "     'key': '2003',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 8,\n",
       "        'doc_count': 4,\n",
       "        'key': 'expression',\n",
       "        'score': 0.34919906132027345},\n",
       "       {'bg_count': 76,\n",
       "        'doc_count': 10,\n",
       "        'key': 'classification',\n",
       "        'score': 0.19252197681224792},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'agent',\n",
       "        'score': 0.16954902560963167},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'boosting',\n",
       "        'score': 0.12337389041934498},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.1141256458060866},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 6,\n",
       "        'key': 'detection',\n",
       "        'score': 0.11116972432252899},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'markov',\n",
       "        'score': 0.10293167363874438},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'trees',\n",
       "        'score': 0.09566880930517295},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 5,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.08736975569939354},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'sample',\n",
       "        'score': 0.08534114594720657},\n",
       "       {'bg_count': 90,\n",
       "        'doc_count': 7,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.06520564794975568},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'probabilistic',\n",
       "        'score': 0.06400585946040492},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'prediction',\n",
       "        'score': 0.051340679522497705},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.04852376240677863},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'process',\n",
       "        'score': 0.04641533176886713},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'fast',\n",
       "        'score': 0.0442165158074249},\n",
       "       {'bg_count': 47,\n",
       "        'doc_count': 3,\n",
       "        'key': 'linear',\n",
       "        'score': 0.02021667350487467}],\n",
       "      'doc_count': 198}},\n",
       "    {'doc_count': 197,\n",
       "     'key': '2001',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'geometrical',\n",
       "        'score': 0.5445128707258626},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'asymmetric',\n",
       "        'score': 0.4772432511359049},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hmm',\n",
       "        'score': 0.26464222216496175},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'direction',\n",
       "        'score': 0.22466070094197593},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'binding',\n",
       "        'score': 0.19467456002473654},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'technique',\n",
       "        'score': 0.17135200597799483},\n",
       "       {'bg_count': 159,\n",
       "        'doc_count': 14,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.15893253056291975},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'propagation',\n",
       "        'score': 0.12470689788451134},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'estimate',\n",
       "        'score': 0.11394264217063052},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'support',\n",
       "        'score': 0.10471613727301841},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'local',\n",
       "        'score': 0.10471613727301841},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'distribution',\n",
       "        'score': 0.08972306681439872},\n",
       "       {'bg_count': 57,\n",
       "        'doc_count': 6,\n",
       "        'key': 'machine',\n",
       "        'score': 0.08738342028639588},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'generalization',\n",
       "        'score': 0.07806178979102786},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.06473461605003258},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vector',\n",
       "        'score': 0.05778130801125244},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 4,\n",
       "        'key': 'spectral',\n",
       "        'score': 0.056241249881900233},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 4,\n",
       "        'key': 'function',\n",
       "        'score': 0.05250730752044566},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 3,\n",
       "        'key': 'clustering',\n",
       "        'score': 0.037247320209229814},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 4,\n",
       "        'key': 'dynamic',\n",
       "        'score': 0.0349785225458394}],\n",
       "      'doc_count': 197}},\n",
       "    {'doc_count': 158,\n",
       "     'key': '1993',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cursive',\n",
       "        'score': 0.851185707418683},\n",
       "       {'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'central',\n",
       "        'score': 0.6336424451209742},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'internal',\n",
       "        'score': 0.503116487742349},\n",
       "       {'bg_count': 526,\n",
       "        'doc_count': 33,\n",
       "        'key': 'network',\n",
       "        'score': 0.3916579037311828},\n",
       "       {'bg_count': 530,\n",
       "        'doc_count': 33,\n",
       "        'key': 'neural',\n",
       "        'score': 0.3871256874049575},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'backpropagation',\n",
       "        'score': 0.3061780392336393},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'artificial',\n",
       "        'score': 0.2420645729850985},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'generation',\n",
       "        'score': 0.2420645729850985},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'computational',\n",
       "        'score': 0.19855592052555684},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.14417010495112964},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 5,\n",
       "        'key': 'dynamic',\n",
       "        'score': 0.10264039476104853},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.07769855258238531},\n",
       "       {'bg_count': 67,\n",
       "        'doc_count': 5,\n",
       "        'key': 'function',\n",
       "        'score': 0.07658490913482581},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 3,\n",
       "        'key': 'machine',\n",
       "        'score': 0.05352707899375101}],\n",
       "      'doc_count': 158}},\n",
       "    {'doc_count': 152,\n",
       "     'key': '1995',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'robot',\n",
       "        'score': 0.4503765581717451},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multilayer',\n",
       "        'score': 0.4503765581717451},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'term',\n",
       "        'score': 0.38321750098931534},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'improving',\n",
       "        'score': 0.29367209141274236},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dependency',\n",
       "        'score': 0.29367209141274236},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'artificial',\n",
       "        'score': 0.21531985803324097},\n",
       "       {'bg_count': 517,\n",
       "        'doc_count': 24,\n",
       "        'key': 'network',\n",
       "        'score': 0.19128039992070167},\n",
       "       {'bg_count': 274,\n",
       "        'doc_count': 16,\n",
       "        'key': 'neural',\n",
       "        'score': 0.18755686758193982},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'motion',\n",
       "        'score': 0.16830851800554017},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 5,\n",
       "        'key': 'recurrent',\n",
       "        'score': 0.14517852083857968},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'attention',\n",
       "        'score': 0.11458127225959634},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.10290143622786943},\n",
       "       {'bg_count': 59,\n",
       "        'doc_count': 5,\n",
       "        'key': 'system',\n",
       "        'score': 0.09990565871637166},\n",
       "       {'bg_count': 43,\n",
       "        'doc_count': 4,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.09030148811441087},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'training',\n",
       "        'score': 0.08100174366838146},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'visual',\n",
       "        'score': 0.07752799933135925},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 3,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.07125284827093199},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 3,\n",
       "        'key': 'dynamic',\n",
       "        'score': 0.04294494459833795},\n",
       "       {'bg_count': 167,\n",
       "        'doc_count': 6,\n",
       "        'key': 'model',\n",
       "        'score': 0.028087523014912005}],\n",
       "      'doc_count': 152}},\n",
       "    {'doc_count': 152,\n",
       "     'key': '1996',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rbf',\n",
       "        'score': 0.9204899584487534},\n",
       "       {'bg_count': 3,\n",
       "        'doc_count': 3,\n",
       "        'key': 'tuned',\n",
       "        'score': 0.9204899584487534},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'analog',\n",
       "        'score': 0.3915627885503231},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'unit',\n",
       "        'score': 0.38321750098931534},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'expert',\n",
       "        'score': 0.33186584883260783},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'multilayer',\n",
       "        'score': 0.29367209141274236},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'organizing',\n",
       "        'score': 0.29367209141274236},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'contour',\n",
       "        'score': 0.29367209141274236},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vector',\n",
       "        'score': 0.2623311980609418},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'statistic',\n",
       "        'score': 0.2623311980609418},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'orientation',\n",
       "        'score': 0.2623311980609418},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'basi',\n",
       "        'score': 0.21531985803324097},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cortical',\n",
       "        'score': 0.21531985803324097},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vlsi',\n",
       "        'score': 0.19723857340720222},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'input',\n",
       "        'score': 0.19723857340720222},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'independent',\n",
       "        'score': 0.18174032944202612},\n",
       "       {'bg_count': 70,\n",
       "        'doc_count': 7,\n",
       "        'key': 'algorithm',\n",
       "        'score': 0.1733336218836565},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'size',\n",
       "        'score': 0.16830851800554017},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 6,\n",
       "        'key': 'visual',\n",
       "        'score': 0.14857167590027698},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hidden',\n",
       "        'score': 0.1369676246537396}],\n",
       "      'doc_count': 152}},\n",
       "    {'doc_count': 152,\n",
       "     'key': '2000',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 9,\n",
       "        'doc_count': 4,\n",
       "        'key': 'video',\n",
       "        'score': 0.530855647891659},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'competition',\n",
       "        'score': 0.4503765581717451},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'probability',\n",
       "        'score': 0.2623311980609418},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 6,\n",
       "        'key': 'speech',\n",
       "        'score': 0.24982686980609414},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 5,\n",
       "        'key': 'code',\n",
       "        'score': 0.23728537825962367},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'spatial',\n",
       "        'score': 0.21531985803324097},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 5,\n",
       "        'key': 'support',\n",
       "        'score': 0.19096878709932727},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'representation',\n",
       "        'score': 0.1461855344630927},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 5,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.14122133733456446},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 5,\n",
       "        'key': 'vector',\n",
       "        'score': 0.14122133733456446},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 4,\n",
       "        'key': 'machine',\n",
       "        'score': 0.13544366008399603},\n",
       "       {'bg_count': 88,\n",
       "        'doc_count': 7,\n",
       "        'key': 'kernel',\n",
       "        'score': 0.1284591609481239},\n",
       "       {'bg_count': 33,\n",
       "        'doc_count': 4,\n",
       "        'key': 'natural',\n",
       "        'score': 0.12564005708050027},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 3,\n",
       "        'key': 'error',\n",
       "        'score': 0.10847590342483002},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'structure',\n",
       "        'score': 0.10290143622786943},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 3,\n",
       "        'key': 'image',\n",
       "        'score': 0.049060240862103914},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 3,\n",
       "        'key': 'based',\n",
       "        'score': 0.030632450781559153},\n",
       "       {'bg_count': 332,\n",
       "        'doc_count': 9,\n",
       "        'key': 'learning',\n",
       "        'score': 0.017253701440109466}],\n",
       "      'doc_count': 152}},\n",
       "    {'doc_count': 151,\n",
       "     'key': '1998',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'discrimination',\n",
       "        'score': 0.4564931362659533},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'contour',\n",
       "        'score': 0.38844160970383507},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'observable',\n",
       "        'score': 0.3374029647822464},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'em',\n",
       "        'score': 0.26594886189202227},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classifier',\n",
       "        'score': 0.2183127932985396},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'general',\n",
       "        'score': 0.2183127932985396},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 4,\n",
       "        'key': 'cell',\n",
       "        'score': 0.2154709171068103},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'invariant',\n",
       "        'score': 0.1999912284548923},\n",
       "       {'bg_count': 127,\n",
       "        'doc_count': 10,\n",
       "        'key': 'algorithm',\n",
       "        'score': 0.1838329372900139},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 5,\n",
       "        'key': 'space',\n",
       "        'score': 0.1653710363580545},\n",
       "       {'bg_count': 101,\n",
       "        'doc_count': 8,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.14825474477626263},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'analog',\n",
       "        'score': 0.138919345642735},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'support',\n",
       "        'score': 0.138919345642735},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'independent',\n",
       "        'score': 0.138919345642735},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vector',\n",
       "        'score': 0.11623550345536225},\n",
       "       {'bg_count': 97,\n",
       "        'doc_count': 7,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.11406625771975096},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 4,\n",
       "        'key': 'system',\n",
       "        'score': 0.11083914053082072},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hierarchical',\n",
       "        'score': 0.10440045535760255},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'processe',\n",
       "        'score': 0.07540458751809131},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approach',\n",
       "        'score': 0.055347295478730164}],\n",
       "      'doc_count': 151}},\n",
       "    {'doc_count': 150,\n",
       "     'key': '1997',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 4,\n",
       "        'key': 'recording',\n",
       "        'score': 0.831525925925926},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 6,\n",
       "        'key': 'cell',\n",
       "        'score': 0.38909629629629633},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'auditory',\n",
       "        'score': 0.34113015873015884},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'channel',\n",
       "        'score': 0.3018222222222222},\n",
       "       {'bg_count': 10, 'doc_count': 3, 'key': 'coding', 'score': 0.26964},\n",
       "       {'bg_count': 13, 'doc_count': 3, 'key': 'rule', 'score': 0.2028},\n",
       "       {'bg_count': 13, 'doc_count': 3, 'key': 'study', 'score': 0.2028},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'identification',\n",
       "        'score': 0.161025},\n",
       "       {'bg_count': 16, 'doc_count': 3, 'key': 'factor', 'score': 0.161025},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'natural',\n",
       "        'score': 0.13244210526315792},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'processing',\n",
       "        'score': 0.13244210526315792},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 3,\n",
       "        'key': 'regression',\n",
       "        'score': 0.09140000000000001},\n",
       "       {'bg_count': 45,\n",
       "        'doc_count': 4,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.08775901234567902},\n",
       "       {'bg_count': 49,\n",
       "        'doc_count': 3,\n",
       "        'key': 'classification',\n",
       "        'score': 0.039110204081632655},\n",
       "       {'bg_count': 139,\n",
       "        'doc_count': 5,\n",
       "        'key': 'neural',\n",
       "        'score': 0.024548361310951245}],\n",
       "      'doc_count': 150}},\n",
       "    {'doc_count': 150,\n",
       "     'key': '1999',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'modelling',\n",
       "        'score': 0.3018222222222222},\n",
       "       {'bg_count': 137,\n",
       "        'doc_count': 12,\n",
       "        'key': 'machine',\n",
       "        'score': 0.2582656934306569},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 5,\n",
       "        'key': 'density',\n",
       "        'score': 0.2540079365079365},\n",
       "       {'bg_count': 68,\n",
       "        'doc_count': 8,\n",
       "        'key': 'vector',\n",
       "        'score': 0.2495581699346405},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 5,\n",
       "        'key': 'support',\n",
       "        'score': 0.19653968253968251},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'selection',\n",
       "        'score': 0.1878814814814815},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'environment',\n",
       "        'score': 0.1868857142857143},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'predictive',\n",
       "        'score': 0.1868857142857143},\n",
       "       {'bg_count': 40,\n",
       "        'doc_count': 5,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.16780555555555554},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'approximate',\n",
       "        'score': 0.15037647058823528},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'circuit',\n",
       "        'score': 0.15037647058823528},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 4,\n",
       "        'key': 'system',\n",
       "        'score': 0.14497185185185188},\n",
       "       {'bg_count': 90,\n",
       "        'doc_count': 7,\n",
       "        'key': 'gaussian',\n",
       "        'score': 0.12854765432098766},\n",
       "       {'bg_count': 20, 'doc_count': 3, 'key': 'factor', 'score': 0.12482},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 5,\n",
       "        'key': 'data',\n",
       "        'score': 0.12442265795206972},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'robust',\n",
       "        'score': 0.10593043478260869},\n",
       "       {'bg_count': 60,\n",
       "        'doc_count': 5,\n",
       "        'key': 'analysi',\n",
       "        'score': 0.10075925925925926},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 3,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.08344285714285714},\n",
       "       {'bg_count': 131,\n",
       "        'doc_count': 7,\n",
       "        'key': 'bayesian',\n",
       "        'score': 0.07370958439355386},\n",
       "       {'bg_count': 34,\n",
       "        'doc_count': 3,\n",
       "        'key': 'application',\n",
       "        'score': 0.06518823529411763}],\n",
       "      'doc_count': 150}},\n",
       "    {'doc_count': 144,\n",
       "     'key': '1991',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 4,\n",
       "        'doc_count': 3,\n",
       "        'key': 'processor',\n",
       "        'score': 0.7648654513888887},\n",
       "       {'bg_count': 689,\n",
       "        'doc_count': 43,\n",
       "        'key': 'network',\n",
       "        'score': 0.6385003566853016},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'basi',\n",
       "        'score': 0.6077256944444444},\n",
       "       {'bg_count': 140,\n",
       "        'doc_count': 17,\n",
       "        'key': 'recognition',\n",
       "        'score': 0.60279190090388},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'integrated',\n",
       "        'score': 0.5029658564814814},\n",
       "       {'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'character',\n",
       "        'score': 0.42813740079365076},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'architecture',\n",
       "        'score': 0.4020061728395062},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hybrid',\n",
       "        'score': 0.3283661265432098},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'separation',\n",
       "        'score': 0.3283661265432098},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'forward',\n",
       "        'score': 0.3008805374001452},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hand',\n",
       "        'score': 0.2934461805555555},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'organizing',\n",
       "        'score': 0.2934461805555555},\n",
       "       {'bg_count': 391,\n",
       "        'doc_count': 22,\n",
       "        'key': 'neural',\n",
       "        'score': 0.2794793541410123},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'silicon',\n",
       "        'score': 0.2648753156565656},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'segmentation',\n",
       "        'score': 0.25158179012345677},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'rule',\n",
       "        'score': 0.24106626157407407},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.21514358561460006},\n",
       "       {'bg_count': 27,\n",
       "        'doc_count': 4,\n",
       "        'key': 'pattern',\n",
       "        'score': 0.17915523548239595},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'control',\n",
       "        'score': 0.1755913628472222},\n",
       "       {'bg_count': 61,\n",
       "        'doc_count': 6,\n",
       "        'key': 'adaptive',\n",
       "        'score': 0.16441826047358835}],\n",
       "      'doc_count': 144}},\n",
       "    {'doc_count': 143,\n",
       "     'key': '1990',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 510,\n",
       "        'doc_count': 42,\n",
       "        'key': 'neural',\n",
       "        'score': 0.9310646572678658},\n",
       "       {'bg_count': 860,\n",
       "        'doc_count': 53,\n",
       "        'key': 'network',\n",
       "        'score': 0.7859615015006135},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 9,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.6725023228519733},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 4,\n",
       "        'key': 'development',\n",
       "        'score': 0.5385886840432295},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'artificial',\n",
       "        'score': 0.5101716465352829},\n",
       "       {'bg_count': 35,\n",
       "        'doc_count': 7,\n",
       "        'key': 'generalization',\n",
       "        'score': 0.4467895740623014},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 4,\n",
       "        'key': 'basi',\n",
       "        'score': 0.4078439043474008},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'competitive',\n",
       "        'score': 0.3773839796567069},\n",
       "       {'bg_count': 32,\n",
       "        'doc_count': 6,\n",
       "        'key': 'speech',\n",
       "        'score': 0.35640495867768596},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'architecture',\n",
       "        'score': 0.3499267530888085},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'radial',\n",
       "        'score': 0.3331214240305149},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 4,\n",
       "        'key': 'implementation',\n",
       "        'score': 0.30529897909577053},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'navigation',\n",
       "        'score': 0.29771137952956134},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'design',\n",
       "        'score': 0.29771137952956134},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 4,\n",
       "        'key': 'analog',\n",
       "        'score': 0.2867839231475595},\n",
       "       {'bg_count': 140,\n",
       "        'doc_count': 12,\n",
       "        'key': 'recognition',\n",
       "        'score': 0.2803015166651531},\n",
       "       {'bg_count': 41,\n",
       "        'doc_count': 6,\n",
       "        'key': 'control',\n",
       "        'score': 0.2689594219527701},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'pattern',\n",
       "        'score': 0.26873952493787207},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'perceptron',\n",
       "        'score': 0.26873952493787207},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'chip',\n",
       "        'score': 0.2553083280356008}],\n",
       "      'doc_count': 143}},\n",
       "    {'doc_count': 140,\n",
       "     'key': '1994',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 7,\n",
       "        'doc_count': 3,\n",
       "        'key': 'direction',\n",
       "        'score': 0.453564139941691},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 5,\n",
       "        'key': 'cortex',\n",
       "        'score': 0.36584960070984907},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'perceptron',\n",
       "        'score': 0.34801020408163263},\n",
       "       {'bg_count': 510,\n",
       "        'doc_count': 26,\n",
       "        'key': 'neural',\n",
       "        'score': 0.3039731892757103},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'computational',\n",
       "        'score': 0.26697959183673464},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'input',\n",
       "        'score': 0.25565051020408164},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'organizing',\n",
       "        'score': 0.25565051020408164},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'implementation',\n",
       "        'score': 0.25565051020408164},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'statistical',\n",
       "        'score': 0.2160677842565598},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'associative',\n",
       "        'score': 0.2160677842565598},\n",
       "       {'bg_count': 348,\n",
       "        'doc_count': 18,\n",
       "        'key': 'network',\n",
       "        'score': 0.21538881069669238},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'backpropagation',\n",
       "        'score': 0.20023469387755105},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.18638073979591838},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 7,\n",
       "        'key': 'reinforcement',\n",
       "        'score': 0.1850974025974026},\n",
       "       {'bg_count': 17,\n",
       "        'doc_count': 3,\n",
       "        'key': 'active',\n",
       "        'score': 0.17415666266506602},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'point',\n",
       "        'score': 0.1632908163265306},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'mixture',\n",
       "        'score': 0.11156938775510204},\n",
       "       {'bg_count': 30,\n",
       "        'doc_count': 3,\n",
       "        'key': 'problem',\n",
       "        'score': 0.0894030612244898},\n",
       "       {'bg_count': 75,\n",
       "        'doc_count': 5,\n",
       "        'key': 'function',\n",
       "        'score': 0.08743197278911564},\n",
       "       {'bg_count': 37,\n",
       "        'doc_count': 3,\n",
       "        'key': 'recognition',\n",
       "        'score': 0.06843491450634308}],\n",
       "      'doc_count': 140}},\n",
       "    {'doc_count': 127,\n",
       "     'key': '1992',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'unit',\n",
       "        'score': 0.5670944675222683},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'knowledge',\n",
       "        'score': 0.4814387128774257},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'hippocampal',\n",
       "        'score': 0.4814387128774257},\n",
       "       {'bg_count': 371,\n",
       "        'doc_count': 23,\n",
       "        'key': 'neural',\n",
       "        'score': 0.45903454610143724},\n",
       "       {'bg_count': 511,\n",
       "        'doc_count': 26,\n",
       "        'key': 'network',\n",
       "        'score': 0.389180480904993},\n",
       "       {'bg_count': 50,\n",
       "        'doc_count': 7,\n",
       "        'key': 'hidden',\n",
       "        'score': 0.3848459296918594},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 4,\n",
       "        'key': 'rule',\n",
       "        'score': 0.3465611141748599},\n",
       "       {'bg_count': 31,\n",
       "        'doc_count': 5,\n",
       "        'key': 'analog',\n",
       "        'score': 0.3226806453612907},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connection',\n",
       "        'score': 0.31308512617025236},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'circuit',\n",
       "        'score': 0.26498410139677425},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 5,\n",
       "        'key': 'speech',\n",
       "        'score': 0.25598709092155025},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 4,\n",
       "        'key': 'cell',\n",
       "        'score': 0.2558273916547833},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vlsi',\n",
       "        'score': 0.24574369148738298},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'development',\n",
       "        'score': 0.22890833281666562},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'computation',\n",
       "        'score': 0.1784022568045136},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'context',\n",
       "        'score': 0.168782051849818},\n",
       "       {'bg_count': 77,\n",
       "        'doc_count': 6,\n",
       "        'key': 'recognition',\n",
       "        'score': 0.16265128634153372},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'modeling',\n",
       "        'score': 0.14473153946307893},\n",
       "       {'bg_count': 56,\n",
       "        'doc_count': 4,\n",
       "        'key': 'based',\n",
       "        'score': 0.09677333640381566},\n",
       "       {'bg_count': 321,\n",
       "        'doc_count': 8,\n",
       "        'key': 'learning',\n",
       "        'score': 0.026516738391732235}],\n",
       "      'doc_count': 127}},\n",
       "    {'doc_count': 101,\n",
       "     'key': '1989',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'chip',\n",
       "        'score': 1.0350455837662973},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 4,\n",
       "        'key': 'simulation',\n",
       "        'score': 0.9928794859683989},\n",
       "       {'bg_count': 860,\n",
       "        'doc_count': 41,\n",
       "        'key': 'network',\n",
       "        'score': 0.9815340721269918},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': '2',\n",
       "        'score': 0.7688584452504655},\n",
       "       {'bg_count': 508,\n",
       "        'doc_count': 27,\n",
       "        'key': 'neural',\n",
       "        'score': 0.7513106635369237},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 4,\n",
       "        'key': 'computer',\n",
       "        'score': 0.6702284089795119},\n",
       "       {'bg_count': 26,\n",
       "        'doc_count': 5,\n",
       "        'key': 'speech',\n",
       "        'score': 0.6330261739045192},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'basi',\n",
       "        'score': 0.6091461621409664},\n",
       "       {'bg_count': 39,\n",
       "        'doc_count': 6,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.5958239388295264},\n",
       "       {'bg_count': 55,\n",
       "        'doc_count': 7,\n",
       "        'key': 'analog',\n",
       "        'score': 0.5630891802051494},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 4,\n",
       "        'key': 'associative',\n",
       "        'score': 0.5581496138149511},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'organizing',\n",
       "        'score': 0.5510689682829669},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'behavior',\n",
       "        'score': 0.5282619351044014},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'artificial',\n",
       "        'score': 0.5026713067346338},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'vlsi',\n",
       "        'score': 0.5026713067346338},\n",
       "       {'bg_count': 13,\n",
       "        'doc_count': 3,\n",
       "        'key': 'cortical',\n",
       "        'score': 0.46171943927065967},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 4,\n",
       "        'key': 'performance',\n",
       "        'score': 0.43361761918766145},\n",
       "       {'bg_count': 15,\n",
       "        'doc_count': 3,\n",
       "        'key': 'simple',\n",
       "        'score': 0.3961964513283011},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'backpropagation',\n",
       "        'score': 0.3065334151966525},\n",
       "       {'bg_count': 21,\n",
       "        'doc_count': 3,\n",
       "        'key': 'circuit',\n",
       "        'score': 0.2745109022924923}],\n",
       "      'doc_count': 101}},\n",
       "    {'doc_count': 94,\n",
       "     'key': '1988',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 42,\n",
       "        'doc_count': 9,\n",
       "        'key': 'analog',\n",
       "        'score': 1.4846973420422946},\n",
       "       {'bg_count': 6,\n",
       "        'doc_count': 3,\n",
       "        'key': 'annealing',\n",
       "        'score': 1.1973177908555905},\n",
       "       {'bg_count': 860,\n",
       "        'doc_count': 37,\n",
       "        'key': 'network',\n",
       "        'score': 0.9108942539505408},\n",
       "       {'bg_count': 645,\n",
       "        'doc_count': 31,\n",
       "        'key': 'neural',\n",
       "        'score': 0.8911870396299844},\n",
       "       {'bg_count': 8,\n",
       "        'doc_count': 3,\n",
       "        'key': 'winner',\n",
       "        'score': 0.8900096197374375},\n",
       "       {'bg_count': 10,\n",
       "        'doc_count': 3,\n",
       "        'key': 'sensory',\n",
       "        'score': 0.7056247170665457},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 3,\n",
       "        'key': 'perceptron',\n",
       "        'score': 0.5827014486192846},\n",
       "       {'bg_count': 22,\n",
       "        'doc_count': 4,\n",
       "        'key': 'learn',\n",
       "        'score': 0.5534384131034199},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 4,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.5275257346428641},\n",
       "       {'bg_count': 16,\n",
       "        'doc_count': 3,\n",
       "        'key': 'net',\n",
       "        'score': 0.42904736306020813},\n",
       "       {'bg_count': 18,\n",
       "        'doc_count': 3,\n",
       "        'key': 'expert',\n",
       "        'score': 0.37782933454051604},\n",
       "       {'bg_count': 19,\n",
       "        'doc_count': 3,\n",
       "        'key': 'associative',\n",
       "        'score': 0.35626384884801404},\n",
       "       {'bg_count': 96,\n",
       "        'doc_count': 6,\n",
       "        'key': 'system',\n",
       "        'score': 0.24347838388411042},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 4,\n",
       "        'key': 'training',\n",
       "        'score': 0.23060962728232987},\n",
       "       {'bg_count': 48,\n",
       "        'doc_count': 4,\n",
       "        'key': 'temporal',\n",
       "        'score': 0.23060962728232987},\n",
       "       {'bg_count': 29,\n",
       "        'doc_count': 3,\n",
       "        'key': 'propagation',\n",
       "        'score': 0.2224091100669674},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 4,\n",
       "        'key': 'speech',\n",
       "        'score': 0.21454122617811267},\n",
       "       {'bg_count': 52,\n",
       "        'doc_count': 4,\n",
       "        'key': 'distributed',\n",
       "        'score': 0.20959710276143048},\n",
       "       {'bg_count': 36,\n",
       "        'doc_count': 3,\n",
       "        'key': 'field',\n",
       "        'score': 0.17295722046174739},\n",
       "       {'bg_count': 38,\n",
       "        'doc_count': 3,\n",
       "        'key': 'representation',\n",
       "        'score': 0.1621744776154964}],\n",
       "      'doc_count': 94}},\n",
       "    {'doc_count': 90,\n",
       "     'key': '1987',\n",
       "     'keywords': {'bg_count': 7241,\n",
       "      'buckets': [{'bg_count': 35,\n",
       "        'doc_count': 9,\n",
       "        'key': 'associative',\n",
       "        'score': 1.9688571428571435},\n",
       "       {'bg_count': 645,\n",
       "        'doc_count': 39,\n",
       "        'key': 'neural',\n",
       "        'score': 1.6747269595176575},\n",
       "       {'bg_count': 5,\n",
       "        'doc_count': 3,\n",
       "        'key': 'electronic',\n",
       "        'score': 1.5757777777777777},\n",
       "       {'bg_count': 860,\n",
       "        'doc_count': 43,\n",
       "        'key': 'network',\n",
       "        'score': 1.4442160493827163},\n",
       "       {'bg_count': 12,\n",
       "        'doc_count': 4,\n",
       "        'key': 'simulation',\n",
       "        'score': 1.1474897119341565},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 4,\n",
       "        'key': 'artificial',\n",
       "        'score': 0.9772134038800705},\n",
       "       {'bg_count': 85,\n",
       "        'doc_count': 10,\n",
       "        'key': 'memory',\n",
       "        'score': 0.9405954974582426},\n",
       "       {'bg_count': 9,\n",
       "        'doc_count': 3,\n",
       "        'key': 'organization',\n",
       "        'score': 0.8606172839506173},\n",
       "       {'bg_count': 11,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connection',\n",
       "        'score': 0.6980808080808081},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 4,\n",
       "        'key': 'cortex',\n",
       "        'score': 0.670716049382716},\n",
       "       {'bg_count': 44,\n",
       "        'doc_count': 6,\n",
       "        'key': 'pattern',\n",
       "        'score': 0.6647474747474748},\n",
       "       {'bg_count': 51,\n",
       "        'doc_count': 6,\n",
       "        'key': 'processing',\n",
       "        'score': 0.5643572984749455},\n",
       "       {'bg_count': 14,\n",
       "        'doc_count': 3,\n",
       "        'key': 'connectionist',\n",
       "        'score': 0.5413492063492064},\n",
       "       {'bg_count': 28,\n",
       "        'doc_count': 4,\n",
       "        'key': 'learn',\n",
       "        'score': 0.4663844797178131},\n",
       "       {'bg_count': 120,\n",
       "        'doc_count': 8,\n",
       "        'key': 'neuron',\n",
       "        'score': 0.3878847736625515},\n",
       "       {'bg_count': 20,\n",
       "        'doc_count': 3,\n",
       "        'key': 'computational',\n",
       "        'score': 0.3689444444444444},\n",
       "       {'bg_count': 54,\n",
       "        'doc_count': 5,\n",
       "        'key': 'net',\n",
       "        'score': 0.3583104709647919},\n",
       "       {'bg_count': 23,\n",
       "        'doc_count': 3,\n",
       "        'key': 'property',\n",
       "        'score': 0.31647342995169075},\n",
       "       {'bg_count': 24,\n",
       "        'doc_count': 3,\n",
       "        'key': 'computer',\n",
       "        'score': 0.3018981481481482},\n",
       "       {'bg_count': 25,\n",
       "        'doc_count': 3,\n",
       "        'key': 'speech',\n",
       "        'score': 0.2884888888888889}],\n",
       "      'doc_count': 90}}],\n",
       "   'doc_count_error_upper_bound': 0,\n",
       "   'sum_other_doc_count': 0}},\n",
       " 'hits': {'hits': [], 'max_score': 0.0, 'total': 7241},\n",
       " 'timed_out': False,\n",
       " 'took': 13}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multi-set analysis: each year in aggregation of years contains an aggregation of significant terms\n",
    "sig_terms_res = es.search(index = \"nips_papers\", body = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"keywords_by_years\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"year\",\n",
    "                \"size\": 50\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"keywords\": {\n",
    "                    \"significant_terms\": {\n",
    "                        \"field\": \"title\",\n",
    "                        \"size\": 20\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'aggregations': {'keywords_by_years': {'buckets': [{'doc_count': 679,\n",
       "     'key': '2017',\n",
       "     'keywords': {'buckets': [{'doc_count': 180, 'key': 'learning'},\n",
       "       {'doc_count': 85, 'key': 'network'},\n",
       "       {'doc_count': 66, 'key': 'model'},\n",
       "       {'doc_count': 63, 'key': 'neural'},\n",
       "       {'doc_count': 61, 'key': 'deep'},\n",
       "       {'doc_count': 32, 'key': 'algorithm'},\n",
       "       {'doc_count': 32, 'key': 'multi'},\n",
       "       {'doc_count': 31, 'key': 'data'},\n",
       "       {'doc_count': 29, 'key': 'optimization'},\n",
       "       {'doc_count': 27, 'key': 'gaussian'},\n",
       "       {'doc_count': 26, 'key': 'online'},\n",
       "       {'doc_count': 25, 'key': 'gradient'},\n",
       "       {'doc_count': 23, 'key': 'adversarial'},\n",
       "       {'doc_count': 23, 'key': 'inference'},\n",
       "       {'doc_count': 22, 'key': 'efficient'},\n",
       "       {'doc_count': 22, 'key': 'stochastic'},\n",
       "       {'doc_count': 20, 'key': 'analysi'},\n",
       "       {'doc_count': 19, 'key': 'dynamic'},\n",
       "       {'doc_count': 18, 'key': 'bayesian'},\n",
       "       {'doc_count': 18, 'key': 'reinforcement'}],\n",
       "      'doc_count_error_upper_bound': 15,\n",
       "      'sum_other_doc_count': 3235}},\n",
       "    {'doc_count': 569,\n",
       "     'key': '2016',\n",
       "     'keywords': {'buckets': [{'doc_count': 121, 'key': 'learning'},\n",
       "       {'doc_count': 73, 'key': 'network'},\n",
       "       {'doc_count': 61, 'key': 'model'},\n",
       "       {'doc_count': 47, 'key': 'neural'},\n",
       "       {'doc_count': 43, 'key': 'deep'},\n",
       "       {'doc_count': 39, 'key': 'optimization'},\n",
       "       {'doc_count': 38, 'key': 'stochastic'},\n",
       "       {'doc_count': 26, 'key': 'method'},\n",
       "       {'doc_count': 24, 'key': 'inference'},\n",
       "       {'doc_count': 22, 'key': 'bayesian'},\n",
       "       {'doc_count': 22, 'key': 'optimal'},\n",
       "       {'doc_count': 21, 'key': 'data'},\n",
       "       {'doc_count': 19, 'key': 'bandit'},\n",
       "       {'doc_count': 19, 'key': 'based'},\n",
       "       {'doc_count': 17, 'key': 'estimation'},\n",
       "       {'doc_count': 16, 'key': 'fast'},\n",
       "       {'doc_count': 16, 'key': 'feature'},\n",
       "       {'doc_count': 16, 'key': 'gradient'},\n",
       "       {'doc_count': 16, 'key': 'online'},\n",
       "       {'doc_count': 14, 'key': 'convex'}],\n",
       "      'doc_count_error_upper_bound': 14,\n",
       "      'sum_other_doc_count': 2783}},\n",
       "    {'doc_count': 411,\n",
       "     'key': '2014',\n",
       "     'keywords': {'buckets': [{'doc_count': 85, 'key': 'learning'},\n",
       "       {'doc_count': 66, 'key': 'model'},\n",
       "       {'doc_count': 37, 'key': 'network'},\n",
       "       {'doc_count': 30, 'key': 'inference'},\n",
       "       {'doc_count': 28, 'key': 'neural'},\n",
       "       {'doc_count': 22, 'key': 'deep'},\n",
       "       {'doc_count': 20, 'key': 'estimation'},\n",
       "       {'doc_count': 19, 'key': 'data'},\n",
       "       {'doc_count': 19, 'key': 'sparse'},\n",
       "       {'doc_count': 18, 'key': 'based'},\n",
       "       {'doc_count': 17, 'key': 'bayesian'},\n",
       "       {'doc_count': 15, 'key': 'convex'},\n",
       "       {'doc_count': 14, 'key': 'classification'},\n",
       "       {'doc_count': 14, 'key': 'distributed'},\n",
       "       {'doc_count': 14, 'key': 'graphical'},\n",
       "       {'doc_count': 14, 'key': 'optimization'},\n",
       "       {'doc_count': 14, 'key': 'stochastic'},\n",
       "       {'doc_count': 13, 'key': 'algorithm'},\n",
       "       {'doc_count': 13, 'key': 'efficient'},\n",
       "       {'doc_count': 13, 'key': 'gaussian'}],\n",
       "      'doc_count_error_upper_bound': 10,\n",
       "      'sum_other_doc_count': 2020}},\n",
       "    {'doc_count': 403,\n",
       "     'key': '2015',\n",
       "     'keywords': {'buckets': [{'doc_count': 78, 'key': 'learning'},\n",
       "       {'doc_count': 48, 'key': 'model'},\n",
       "       {'doc_count': 44, 'key': 'network'},\n",
       "       {'doc_count': 31, 'key': 'optimization'},\n",
       "       {'doc_count': 26, 'key': 'inference'},\n",
       "       {'doc_count': 22, 'key': 'neural'},\n",
       "       {'doc_count': 20, 'key': 'deep'},\n",
       "       {'doc_count': 20, 'key': 'stochastic'},\n",
       "       {'doc_count': 16, 'key': 'algorithm'},\n",
       "       {'doc_count': 16, 'key': 'fast'},\n",
       "       {'doc_count': 15, 'key': 'bayesian'},\n",
       "       {'doc_count': 15, 'key': 'gradient'},\n",
       "       {'doc_count': 14, 'key': 'bandit'},\n",
       "       {'doc_count': 14, 'key': 'linear'},\n",
       "       {'doc_count': 13, 'key': 'analysi'},\n",
       "       {'doc_count': 13, 'key': 'efficient'},\n",
       "       {'doc_count': 13, 'key': 'estimation'},\n",
       "       {'doc_count': 13, 'key': 'graph'},\n",
       "       {'doc_count': 13, 'key': 'optimal'},\n",
       "       {'doc_count': 12, 'key': 'data'}],\n",
       "      'doc_count_error_upper_bound': 11,\n",
       "      'sum_other_doc_count': 1973}},\n",
       "    {'doc_count': 368,\n",
       "     'key': '2012',\n",
       "     'keywords': {'buckets': [{'doc_count': 92, 'key': 'learning'},\n",
       "       {'doc_count': 52, 'key': 'model'},\n",
       "       {'doc_count': 21, 'key': 'bayesian'},\n",
       "       {'doc_count': 17, 'key': 'classification'},\n",
       "       {'doc_count': 17, 'key': 'sparse'},\n",
       "       {'doc_count': 16, 'key': 'based'},\n",
       "       {'doc_count': 16, 'key': 'multi'},\n",
       "       {'doc_count': 15, 'key': 'algorithm'},\n",
       "       {'doc_count': 15, 'key': 'feature'},\n",
       "       {'doc_count': 14, 'key': 'inference'},\n",
       "       {'doc_count': 13, 'key': 'estimation'},\n",
       "       {'doc_count': 13, 'key': 'method'},\n",
       "       {'doc_count': 13, 'key': 'network'},\n",
       "       {'doc_count': 13, 'key': 'processe'},\n",
       "       {'doc_count': 12, 'key': 'function'},\n",
       "       {'doc_count': 11, 'key': 'analysi'},\n",
       "       {'doc_count': 11, 'key': 'clustering'},\n",
       "       {'doc_count': 11, 'key': 'image'},\n",
       "       {'doc_count': 11, 'key': 'neural'},\n",
       "       {'doc_count': 11, 'key': 'nonparametric'}],\n",
       "      'doc_count_error_upper_bound': 11,\n",
       "      'sum_other_doc_count': 1866}},\n",
       "    {'doc_count': 360,\n",
       "     'key': '2013',\n",
       "     'keywords': {'buckets': [{'doc_count': 77, 'key': 'learning'},\n",
       "       {'doc_count': 46, 'key': 'model'},\n",
       "       {'doc_count': 22, 'key': 'inference'},\n",
       "       {'doc_count': 21, 'key': 'sparse'},\n",
       "       {'doc_count': 20, 'key': 'algorithm'},\n",
       "       {'doc_count': 20, 'key': 'bayesian'},\n",
       "       {'doc_count': 16, 'key': 'network'},\n",
       "       {'doc_count': 16, 'key': 'online'},\n",
       "       {'doc_count': 16, 'key': 'optimization'},\n",
       "       {'doc_count': 16, 'key': 'process'},\n",
       "       {'doc_count': 15, 'key': 'multi'},\n",
       "       {'doc_count': 15, 'key': 'stochastic'},\n",
       "       {'doc_count': 14, 'key': 'data'},\n",
       "       {'doc_count': 14, 'key': 'estimation'},\n",
       "       {'doc_count': 14, 'key': 'robust'},\n",
       "       {'doc_count': 12, 'key': 'based'},\n",
       "       {'doc_count': 12, 'key': 'neural'},\n",
       "       {'doc_count': 11, 'key': 'adaptive'},\n",
       "       {'doc_count': 11, 'key': 'dimensional'},\n",
       "       {'doc_count': 11, 'key': 'gaussian'}],\n",
       "      'doc_count_error_upper_bound': 11,\n",
       "      'sum_other_doc_count': 1843}},\n",
       "    {'doc_count': 306,\n",
       "     'key': '2011',\n",
       "     'keywords': {'buckets': [{'doc_count': 79, 'key': 'learning'},\n",
       "       {'doc_count': 39, 'key': 'model'},\n",
       "       {'doc_count': 18, 'key': 'multi'},\n",
       "       {'doc_count': 17, 'key': 'algorithm'},\n",
       "       {'doc_count': 17, 'key': 'sparse'},\n",
       "       {'doc_count': 16, 'key': 'image'},\n",
       "       {'doc_count': 14, 'key': 'based'},\n",
       "       {'doc_count': 14, 'key': 'efficient'},\n",
       "       {'doc_count': 13, 'key': 'data'},\n",
       "       {'doc_count': 13, 'key': 'gaussian'},\n",
       "       {'doc_count': 13, 'key': 'large'},\n",
       "       {'doc_count': 12, 'key': 'analysi'},\n",
       "       {'doc_count': 12, 'key': 'inference'},\n",
       "       {'doc_count': 11, 'key': 'network'},\n",
       "       {'doc_count': 9, 'key': 'optimization'},\n",
       "       {'doc_count': 8, 'key': 'bandit'},\n",
       "       {'doc_count': 8, 'key': 'bayesian'},\n",
       "       {'doc_count': 8, 'key': 'classification'},\n",
       "       {'doc_count': 8, 'key': 'kernel'},\n",
       "       {'doc_count': 8, 'key': 'neural'}],\n",
       "      'doc_count_error_upper_bound': 10,\n",
       "      'sum_other_doc_count': 1465}},\n",
       "    {'doc_count': 292,\n",
       "     'key': '2010',\n",
       "     'keywords': {'buckets': [{'doc_count': 65, 'key': 'learning'},\n",
       "       {'doc_count': 31, 'key': 'model'},\n",
       "       {'doc_count': 21, 'key': 'network'},\n",
       "       {'doc_count': 17, 'key': 'large'},\n",
       "       {'doc_count': 15, 'key': 'inference'},\n",
       "       {'doc_count': 13, 'key': 'multi'},\n",
       "       {'doc_count': 12, 'key': 'data'},\n",
       "       {'doc_count': 11, 'key': 'analysi'},\n",
       "       {'doc_count': 11, 'key': 'bayesian'},\n",
       "       {'doc_count': 11, 'key': 'classification'},\n",
       "       {'doc_count': 11, 'key': 'regression'},\n",
       "       {'doc_count': 10, 'key': 'kernel'},\n",
       "       {'doc_count': 10, 'key': 'sparse'},\n",
       "       {'doc_count': 10, 'key': 'structured'},\n",
       "       {'doc_count': 9, 'key': 'feature'},\n",
       "       {'doc_count': 9, 'key': 'gaussian'},\n",
       "       {'doc_count': 9, 'key': 'image'},\n",
       "       {'doc_count': 9, 'key': 'latent'},\n",
       "       {'doc_count': 8, 'key': 'estimation'},\n",
       "       {'doc_count': 8, 'key': 'prior'}],\n",
       "      'doc_count_error_upper_bound': 10,\n",
       "      'sum_other_doc_count': 1471}},\n",
       "    {'doc_count': 262,\n",
       "     'key': '2009',\n",
       "     'keywords': {'buckets': [{'doc_count': 60, 'key': 'learning'},\n",
       "       {'doc_count': 37, 'key': 'model'},\n",
       "       {'doc_count': 20, 'key': 'bayesian'},\n",
       "       {'doc_count': 15, 'key': 'inference'},\n",
       "       {'doc_count': 13, 'key': 'analysi'},\n",
       "       {'doc_count': 12, 'key': 'feature'},\n",
       "       {'doc_count': 12, 'key': 'graph'},\n",
       "       {'doc_count': 12, 'key': 'image'},\n",
       "       {'doc_count': 12, 'key': 'kernel'},\n",
       "       {'doc_count': 11, 'key': 'regression'},\n",
       "       {'doc_count': 10, 'key': 'sparse'},\n",
       "       {'doc_count': 9, 'key': 'multi'},\n",
       "       {'doc_count': 8, 'key': 'based'},\n",
       "       {'doc_count': 8, 'key': 'method'},\n",
       "       {'doc_count': 7, 'key': 'data'},\n",
       "       {'doc_count': 7, 'key': 'detection'},\n",
       "       {'doc_count': 7, 'key': 'efficient'},\n",
       "       {'doc_count': 7, 'key': 'hierarchical'},\n",
       "       {'doc_count': 7, 'key': 'nonparametric'},\n",
       "       {'doc_count': 7, 'key': 'object'}],\n",
       "      'doc_count_error_upper_bound': 9,\n",
       "      'sum_other_doc_count': 1297}},\n",
       "    {'doc_count': 250,\n",
       "     'key': '2008',\n",
       "     'keywords': {'buckets': [{'doc_count': 51, 'key': 'learning'},\n",
       "       {'doc_count': 38, 'key': 'model'},\n",
       "       {'doc_count': 17, 'key': 'kernel'},\n",
       "       {'doc_count': 13, 'key': 'bayesian'},\n",
       "       {'doc_count': 12, 'key': 'online'},\n",
       "       {'doc_count': 11, 'key': 'based'},\n",
       "       {'doc_count': 11, 'key': 'network'},\n",
       "       {'doc_count': 10, 'key': 'algorithm'},\n",
       "       {'doc_count': 10, 'key': 'multi'},\n",
       "       {'doc_count': 9, 'key': 'classification'},\n",
       "       {'doc_count': 9, 'key': 'data'},\n",
       "       {'doc_count': 9, 'key': 'gaussian'},\n",
       "       {'doc_count': 9, 'key': 'sparse'},\n",
       "       {'doc_count': 8, 'key': 'analysi'},\n",
       "       {'doc_count': 7, 'key': 'clustering'},\n",
       "       {'doc_count': 7, 'key': 'efficient'},\n",
       "       {'doc_count': 7, 'key': 'image'},\n",
       "       {'doc_count': 7, 'key': 'neural'},\n",
       "       {'doc_count': 7, 'key': 'process'},\n",
       "       {'doc_count': 7, 'key': 'regression'}],\n",
       "      'doc_count_error_upper_bound': 9,\n",
       "      'sum_other_doc_count': 1235}},\n",
       "    {'doc_count': 217,\n",
       "     'key': '2007',\n",
       "     'keywords': {'buckets': [{'doc_count': 48, 'key': 'learning'},\n",
       "       {'doc_count': 29, 'key': 'model'},\n",
       "       {'doc_count': 14, 'key': 'analysi'},\n",
       "       {'doc_count': 12, 'key': 'inference'},\n",
       "       {'doc_count': 11, 'key': 'bayesian'},\n",
       "       {'doc_count': 10, 'key': 'algorithm'},\n",
       "       {'doc_count': 8, 'key': 'neural'},\n",
       "       {'doc_count': 7, 'key': 'based'},\n",
       "       {'doc_count': 7, 'key': 'image'},\n",
       "       {'doc_count': 7, 'key': 'kernel'},\n",
       "       {'doc_count': 7, 'key': 'neuron'},\n",
       "       {'doc_count': 7, 'key': 'time'},\n",
       "       {'doc_count': 6, 'key': 'application'},\n",
       "       {'doc_count': 6, 'key': 'discriminative'},\n",
       "       {'doc_count': 6, 'key': 'linear'},\n",
       "       {'doc_count': 6, 'key': 'machine'},\n",
       "       {'doc_count': 6, 'key': 'processe'},\n",
       "       {'doc_count': 5, 'key': 'data'},\n",
       "       {'doc_count': 5, 'key': 'latent'},\n",
       "       {'doc_count': 5, 'key': 'network'}],\n",
       "      'doc_count_error_upper_bound': 6,\n",
       "      'sum_other_doc_count': 1053}},\n",
       "    {'doc_count': 207,\n",
       "     'key': '2002',\n",
       "     'keywords': {'buckets': [{'doc_count': 35, 'key': 'learning'},\n",
       "       {'doc_count': 27, 'key': 'model'},\n",
       "       {'doc_count': 17, 'key': 'kernel'},\n",
       "       {'doc_count': 12, 'key': 'based'},\n",
       "       {'doc_count': 11, 'key': 'bayesian'},\n",
       "       {'doc_count': 9, 'key': 'analysi'},\n",
       "       {'doc_count': 9, 'key': 'approach'},\n",
       "       {'doc_count': 9, 'key': 'image'},\n",
       "       {'doc_count': 9, 'key': 'neural'},\n",
       "       {'doc_count': 8, 'key': 'algorithm'},\n",
       "       {'doc_count': 8, 'key': 'estimation'},\n",
       "       {'doc_count': 7, 'key': 'function'},\n",
       "       {'doc_count': 7, 'key': 'representation'},\n",
       "       {'doc_count': 6, 'key': 'classification'},\n",
       "       {'doc_count': 6, 'key': 'dynamic'},\n",
       "       {'doc_count': 6, 'key': 'machine'},\n",
       "       {'doc_count': 6, 'key': 'natural'},\n",
       "       {'doc_count': 6, 'key': 'time'},\n",
       "       {'doc_count': 5, 'key': 'feature'},\n",
       "       {'doc_count': 5, 'key': 'motion'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 951}},\n",
       "    {'doc_count': 207,\n",
       "     'key': '2004',\n",
       "     'keywords': {'buckets': [{'doc_count': 38, 'key': 'learning'},\n",
       "       {'doc_count': 24, 'key': 'model'},\n",
       "       {'doc_count': 15, 'key': 'kernel'},\n",
       "       {'doc_count': 14, 'key': 'algorithm'},\n",
       "       {'doc_count': 13, 'key': 'classification'},\n",
       "       {'doc_count': 11, 'key': 'clustering'},\n",
       "       {'doc_count': 11, 'key': 'network'},\n",
       "       {'doc_count': 10, 'key': 'analysi'},\n",
       "       {'doc_count': 10, 'key': 'machine'},\n",
       "       {'doc_count': 9, 'key': 'bayesian'},\n",
       "       {'doc_count': 8, 'key': 'application'},\n",
       "       {'doc_count': 8, 'key': 'data'},\n",
       "       {'doc_count': 8, 'key': 'detection'},\n",
       "       {'doc_count': 8, 'key': 'semi'},\n",
       "       {'doc_count': 7, 'key': 'image'},\n",
       "       {'doc_count': 7, 'key': 'inference'},\n",
       "       {'doc_count': 7, 'key': 'supervised'},\n",
       "       {'doc_count': 6, 'key': 'bound'},\n",
       "       {'doc_count': 6, 'key': 'decision'},\n",
       "       {'doc_count': 6, 'key': 'gaussian'}],\n",
       "      'doc_count_error_upper_bound': 6,\n",
       "      'sum_other_doc_count': 990}},\n",
       "    {'doc_count': 207,\n",
       "     'key': '2005',\n",
       "     'keywords': {'buckets': [{'doc_count': 40, 'key': 'learning'},\n",
       "       {'doc_count': 22, 'key': 'model'},\n",
       "       {'doc_count': 16, 'key': 'gaussian'},\n",
       "       {'doc_count': 13, 'key': 'analysi'},\n",
       "       {'doc_count': 10, 'key': 'kernel'},\n",
       "       {'doc_count': 10, 'key': 'network'},\n",
       "       {'doc_count': 9, 'key': 'bayesian'},\n",
       "       {'doc_count': 9, 'key': 'visual'},\n",
       "       {'doc_count': 8, 'key': 'algorithm'},\n",
       "       {'doc_count': 8, 'key': 'based'},\n",
       "       {'doc_count': 8, 'key': 'clustering'},\n",
       "       {'doc_count': 8, 'key': 'fast'},\n",
       "       {'doc_count': 7, 'key': 'activity'},\n",
       "       {'doc_count': 7, 'key': 'process'},\n",
       "       {'doc_count': 7, 'key': 'sparse'},\n",
       "       {'doc_count': 6, 'key': 'approach'},\n",
       "       {'doc_count': 6, 'key': 'data'},\n",
       "       {'doc_count': 6, 'key': 'method'},\n",
       "       {'doc_count': 6, 'key': 'regression'},\n",
       "       {'doc_count': 5, 'key': 'active'}],\n",
       "      'doc_count_error_upper_bound': 6,\n",
       "      'sum_other_doc_count': 988}},\n",
       "    {'doc_count': 204,\n",
       "     'key': '2006',\n",
       "     'keywords': {'buckets': [{'doc_count': 41, 'key': 'learning'},\n",
       "       {'doc_count': 20, 'key': 'model'},\n",
       "       {'doc_count': 17, 'key': 'bayesian'},\n",
       "       {'doc_count': 12, 'key': 'clustering'},\n",
       "       {'doc_count': 12, 'key': 'data'},\n",
       "       {'doc_count': 11, 'key': 'classification'},\n",
       "       {'doc_count': 10, 'key': 'algorithm'},\n",
       "       {'doc_count': 10, 'key': 'based'},\n",
       "       {'doc_count': 9, 'key': 'graph'},\n",
       "       {'doc_count': 9, 'key': 'kernel'},\n",
       "       {'doc_count': 8, 'key': 'dynamic'},\n",
       "       {'doc_count': 8, 'key': 'image'},\n",
       "       {'doc_count': 8, 'key': 'random'},\n",
       "       {'doc_count': 7, 'key': 'analysi'},\n",
       "       {'doc_count': 7, 'key': 'approach'},\n",
       "       {'doc_count': 7, 'key': 'field'},\n",
       "       {'doc_count': 7, 'key': 'gaussian'},\n",
       "       {'doc_count': 7, 'key': 'method'},\n",
       "       {'doc_count': 7, 'key': 'network'},\n",
       "       {'doc_count': 7, 'key': 'time'}],\n",
       "      'doc_count_error_upper_bound': 6,\n",
       "      'sum_other_doc_count': 962}},\n",
       "    {'doc_count': 198,\n",
       "     'key': '2003',\n",
       "     'keywords': {'buckets': [{'doc_count': 31, 'key': 'learning'},\n",
       "       {'doc_count': 25, 'key': 'model'},\n",
       "       {'doc_count': 13, 'key': 'classification'},\n",
       "       {'doc_count': 11, 'key': 'data'},\n",
       "       {'doc_count': 11, 'key': 'kernel'},\n",
       "       {'doc_count': 9, 'key': 'based'},\n",
       "       {'doc_count': 9, 'key': 'linear'},\n",
       "       {'doc_count': 9, 'key': 'network'},\n",
       "       {'doc_count': 8, 'key': 'clustering'},\n",
       "       {'doc_count': 7, 'key': 'algorithm'},\n",
       "       {'doc_count': 7, 'key': 'approximate'},\n",
       "       {'doc_count': 7, 'key': 'gaussian'},\n",
       "       {'doc_count': 7, 'key': 'multi'},\n",
       "       {'doc_count': 7, 'key': 'system'},\n",
       "       {'doc_count': 6, 'key': 'detection'},\n",
       "       {'doc_count': 6, 'key': 'vector'},\n",
       "       {'doc_count': 5, 'key': 'analysi'},\n",
       "       {'doc_count': 5, 'key': 'inference'},\n",
       "       {'doc_count': 5, 'key': 'large'},\n",
       "       {'doc_count': 5, 'key': 'neural'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 956}},\n",
       "    {'doc_count': 197,\n",
       "     'key': '2001',\n",
       "     'keywords': {'buckets': [{'doc_count': 38, 'key': 'learning'},\n",
       "       {'doc_count': 22, 'key': 'model'},\n",
       "       {'doc_count': 16, 'key': 'kernel'},\n",
       "       {'doc_count': 10, 'key': 'time'},\n",
       "       {'doc_count': 9, 'key': 'algorithm'},\n",
       "       {'doc_count': 9, 'key': 'bayesian'},\n",
       "       {'doc_count': 8, 'key': 'machine'},\n",
       "       {'doc_count': 7, 'key': 'clustering'},\n",
       "       {'doc_count': 7, 'key': 'neural'},\n",
       "       {'doc_count': 6, 'key': 'method'},\n",
       "       {'doc_count': 5, 'key': 'analysi'},\n",
       "       {'doc_count': 5, 'key': 'classification'},\n",
       "       {'doc_count': 5, 'key': 'data'},\n",
       "       {'doc_count': 5, 'key': 'probabilistic'},\n",
       "       {'doc_count': 5, 'key': 'reinforcement'},\n",
       "       {'doc_count': 5, 'key': 'vector'},\n",
       "       {'doc_count': 4, 'key': 'adaptive'},\n",
       "       {'doc_count': 4, 'key': 'approximation'},\n",
       "       {'doc_count': 4, 'key': 'belief'},\n",
       "       {'doc_count': 4, 'key': 'classifier'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 899}},\n",
       "    {'doc_count': 158,\n",
       "     'key': '1993',\n",
       "     'keywords': {'buckets': [{'doc_count': 46, 'key': 'network'},\n",
       "       {'doc_count': 38, 'key': 'neural'},\n",
       "       {'doc_count': 29, 'key': 'learning'},\n",
       "       {'doc_count': 12, 'key': 'model'},\n",
       "       {'doc_count': 11, 'key': 'dynamic'},\n",
       "       {'doc_count': 9, 'key': 'algorithm'},\n",
       "       {'doc_count': 7, 'key': 'function'},\n",
       "       {'doc_count': 6, 'key': 'classification'},\n",
       "       {'doc_count': 6, 'key': 'data'},\n",
       "       {'doc_count': 6, 'key': 'machine'},\n",
       "       {'doc_count': 5, 'key': 'adaptive'},\n",
       "       {'doc_count': 5, 'key': 'analog'},\n",
       "       {'doc_count': 5, 'key': 'application'},\n",
       "       {'doc_count': 5, 'key': 'backpropagation'},\n",
       "       {'doc_count': 5, 'key': 'connectionist'},\n",
       "       {'doc_count': 5, 'key': 'neuron'},\n",
       "       {'doc_count': 5, 'key': 'system'},\n",
       "       {'doc_count': 4, 'key': 'analysi'},\n",
       "       {'doc_count': 4, 'key': 'architecture'},\n",
       "       {'doc_count': 4, 'key': 'based'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 762}},\n",
       "    {'doc_count': 152,\n",
       "     'key': '1995',\n",
       "     'keywords': {'buckets': [{'doc_count': 38, 'key': 'network'},\n",
       "       {'doc_count': 35, 'key': 'learning'},\n",
       "       {'doc_count': 28, 'key': 'neural'},\n",
       "       {'doc_count': 17, 'key': 'model'},\n",
       "       {'doc_count': 9, 'key': 'dynamic'},\n",
       "       {'doc_count': 8, 'key': 'based'},\n",
       "       {'doc_count': 7, 'key': 'algorithm'},\n",
       "       {'doc_count': 7, 'key': 'system'},\n",
       "       {'doc_count': 6, 'key': 'adaptive'},\n",
       "       {'doc_count': 6, 'key': 'data'},\n",
       "       {'doc_count': 6, 'key': 'visual'},\n",
       "       {'doc_count': 5, 'key': 'bayesian'},\n",
       "       {'doc_count': 5, 'key': 'continuous'},\n",
       "       {'doc_count': 5, 'key': 'neuron'},\n",
       "       {'doc_count': 5, 'key': 'recognition'},\n",
       "       {'doc_count': 5, 'key': 'recurrent'},\n",
       "       {'doc_count': 4, 'key': 'attention'},\n",
       "       {'doc_count': 4, 'key': 'control'},\n",
       "       {'doc_count': 4, 'key': 'time'},\n",
       "       {'doc_count': 3, 'key': 'analysi'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 692}},\n",
       "    {'doc_count': 152,\n",
       "     'key': '1996',\n",
       "     'keywords': {'buckets': [{'doc_count': 33, 'key': 'learning'},\n",
       "       {'doc_count': 31, 'key': 'network'},\n",
       "       {'doc_count': 20, 'key': 'neural'},\n",
       "       {'doc_count': 16, 'key': 'model'},\n",
       "       {'doc_count': 13, 'key': 'algorithm'},\n",
       "       {'doc_count': 10, 'key': 'based'},\n",
       "       {'doc_count': 9, 'key': 'visual'},\n",
       "       {'doc_count': 6, 'key': 'adaptive'},\n",
       "       {'doc_count': 6, 'key': 'analog'},\n",
       "       {'doc_count': 6, 'key': 'bayesian'},\n",
       "       {'doc_count': 6, 'key': 'data'},\n",
       "       {'doc_count': 5, 'key': 'classification'},\n",
       "       {'doc_count': 4, 'key': 'feature'},\n",
       "       {'doc_count': 4, 'key': 'function'},\n",
       "       {'doc_count': 4, 'key': 'multi'},\n",
       "       {'doc_count': 4, 'key': 'temporal'},\n",
       "       {'doc_count': 3, 'key': 'analysi'},\n",
       "       {'doc_count': 3, 'key': 'approach'},\n",
       "       {'doc_count': 3, 'key': 'approximation'},\n",
       "       {'doc_count': 3, 'key': 'architecture'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 731}},\n",
       "    {'doc_count': 152,\n",
       "     'key': '2000',\n",
       "     'keywords': {'buckets': [{'doc_count': 34, 'key': 'learning'},\n",
       "       {'doc_count': 15, 'key': 'model'},\n",
       "       {'doc_count': 10, 'key': 'kernel'},\n",
       "       {'doc_count': 9, 'key': 'based'},\n",
       "       {'doc_count': 9, 'key': 'machine'},\n",
       "       {'doc_count': 8, 'key': 'algorithm'},\n",
       "       {'doc_count': 8, 'key': 'network'},\n",
       "       {'doc_count': 8, 'key': 'recognition'},\n",
       "       {'doc_count': 7, 'key': 'representation'},\n",
       "       {'doc_count': 7, 'key': 'sparse'},\n",
       "       {'doc_count': 6, 'key': 'classification'},\n",
       "       {'doc_count': 6, 'key': 'linear'},\n",
       "       {'doc_count': 5, 'key': 'approach'},\n",
       "       {'doc_count': 5, 'key': 'code'},\n",
       "       {'doc_count': 5, 'key': 'image'},\n",
       "       {'doc_count': 5, 'key': 'neural'},\n",
       "       {'doc_count': 5, 'key': 'reinforcement'},\n",
       "       {'doc_count': 5, 'key': 'speech'},\n",
       "       {'doc_count': 5, 'key': 'structure'},\n",
       "       {'doc_count': 5, 'key': 'support'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 710}},\n",
       "    {'doc_count': 151,\n",
       "     'key': '1998',\n",
       "     'keywords': {'buckets': [{'doc_count': 40, 'key': 'learning'},\n",
       "       {'doc_count': 21, 'key': 'model'},\n",
       "       {'doc_count': 17, 'key': 'network'},\n",
       "       {'doc_count': 16, 'key': 'algorithm'},\n",
       "       {'doc_count': 10, 'key': 'neural'},\n",
       "       {'doc_count': 10, 'key': 'reinforcement'},\n",
       "       {'doc_count': 9, 'key': 'processe'},\n",
       "       {'doc_count': 7, 'key': 'based'},\n",
       "       {'doc_count': 7, 'key': 'data'},\n",
       "       {'doc_count': 7, 'key': 'gaussian'},\n",
       "       {'doc_count': 7, 'key': 'vector'},\n",
       "       {'doc_count': 5, 'key': 'machine'},\n",
       "       {'doc_count': 5, 'key': 'space'},\n",
       "       {'doc_count': 5, 'key': 'support'},\n",
       "       {'doc_count': 4, 'key': 'bayesian'},\n",
       "       {'doc_count': 4, 'key': 'bound'},\n",
       "       {'doc_count': 4, 'key': 'cell'},\n",
       "       {'doc_count': 4, 'key': 'classification'},\n",
       "       {'doc_count': 4, 'key': 'dynamic'},\n",
       "       {'doc_count': 4, 'key': 'estimation'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 742}},\n",
       "    {'doc_count': 150,\n",
       "     'key': '1997',\n",
       "     'keywords': {'buckets': [{'doc_count': 28, 'key': 'learning'},\n",
       "       {'doc_count': 21, 'key': 'network'},\n",
       "       {'doc_count': 20, 'key': 'model'},\n",
       "       {'doc_count': 16, 'key': 'neural'},\n",
       "       {'doc_count': 8, 'key': 'algorithm'},\n",
       "       {'doc_count': 8, 'key': 'analysi'},\n",
       "       {'doc_count': 6, 'key': 'cell'},\n",
       "       {'doc_count': 6, 'key': 'multi'},\n",
       "       {'doc_count': 6, 'key': 'reinforcement'},\n",
       "       {'doc_count': 5, 'key': 'application'},\n",
       "       {'doc_count': 4, 'key': 'auditory'},\n",
       "       {'doc_count': 4, 'key': 'based'},\n",
       "       {'doc_count': 4, 'key': 'bayesian'},\n",
       "       {'doc_count': 4, 'key': 'classification'},\n",
       "       {'doc_count': 4, 'key': 'data'},\n",
       "       {'doc_count': 4, 'key': 'time'},\n",
       "       {'doc_count': 3, 'key': 'adaptive'},\n",
       "       {'doc_count': 3, 'key': 'analog'},\n",
       "       {'doc_count': 3, 'key': 'channel'},\n",
       "       {'doc_count': 3, 'key': 'coding'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 707}},\n",
       "    {'doc_count': 150,\n",
       "     'key': '1999',\n",
       "     'keywords': {'buckets': [{'doc_count': 30, 'key': 'learning'},\n",
       "       {'doc_count': 24, 'key': 'model'},\n",
       "       {'doc_count': 12, 'key': 'machine'},\n",
       "       {'doc_count': 11, 'key': 'bayesian'},\n",
       "       {'doc_count': 10, 'key': 'neural'},\n",
       "       {'doc_count': 9, 'key': 'analysi'},\n",
       "       {'doc_count': 9, 'key': 'gaussian'},\n",
       "       {'doc_count': 9, 'key': 'network'},\n",
       "       {'doc_count': 8, 'key': 'vector'},\n",
       "       {'doc_count': 7, 'key': 'algorithm'},\n",
       "       {'doc_count': 7, 'key': 'mixture'},\n",
       "       {'doc_count': 6, 'key': 'approach'},\n",
       "       {'doc_count': 6, 'key': 'based'},\n",
       "       {'doc_count': 6, 'key': 'image'},\n",
       "       {'doc_count': 5, 'key': 'data'},\n",
       "       {'doc_count': 5, 'key': 'density'},\n",
       "       {'doc_count': 5, 'key': 'dynamic'},\n",
       "       {'doc_count': 5, 'key': 'estimation'},\n",
       "       {'doc_count': 5, 'key': 'support'},\n",
       "       {'doc_count': 4, 'key': 'approximate'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 658}},\n",
       "    {'doc_count': 144,\n",
       "     'key': '1991',\n",
       "     'keywords': {'buckets': [{'doc_count': 50, 'key': 'network'},\n",
       "       {'doc_count': 34, 'key': 'neural'},\n",
       "       {'doc_count': 28, 'key': 'learning'},\n",
       "       {'doc_count': 19, 'key': 'recognition'},\n",
       "       {'doc_count': 18, 'key': 'model'},\n",
       "       {'doc_count': 10, 'key': 'adaptive'},\n",
       "       {'doc_count': 7, 'key': 'function'},\n",
       "       {'doc_count': 6, 'key': 'based'},\n",
       "       {'doc_count': 6, 'key': 'recurrent'},\n",
       "       {'doc_count': 5, 'key': 'connectionist'},\n",
       "       {'doc_count': 5, 'key': 'data'},\n",
       "       {'doc_count': 5, 'key': 'neuron'},\n",
       "       {'doc_count': 5, 'key': 'system'},\n",
       "       {'doc_count': 4, 'key': 'analysi'},\n",
       "       {'doc_count': 4, 'key': 'architecture'},\n",
       "       {'doc_count': 4, 'key': 'control'},\n",
       "       {'doc_count': 4, 'key': 'modeling'},\n",
       "       {'doc_count': 4, 'key': 'pattern'},\n",
       "       {'doc_count': 4, 'key': 'segmentation'},\n",
       "       {'doc_count': 4, 'key': 'speech'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 698}},\n",
       "    {'doc_count': 143,\n",
       "     'key': '1990',\n",
       "     'keywords': {'buckets': [{'doc_count': 53, 'key': 'network'},\n",
       "       {'doc_count': 45, 'key': 'neural'},\n",
       "       {'doc_count': 24, 'key': 'learning'},\n",
       "       {'doc_count': 14, 'key': 'model'},\n",
       "       {'doc_count': 14, 'key': 'recognition'},\n",
       "       {'doc_count': 10, 'key': 'function'},\n",
       "       {'doc_count': 9, 'key': 'connectionist'},\n",
       "       {'doc_count': 9, 'key': 'control'},\n",
       "       {'doc_count': 7, 'key': 'adaptive'},\n",
       "       {'doc_count': 7, 'key': 'architecture'},\n",
       "       {'doc_count': 7, 'key': 'generalization'},\n",
       "       {'doc_count': 7, 'key': 'system'},\n",
       "       {'doc_count': 7, 'key': 'time'},\n",
       "       {'doc_count': 6, 'key': 'application'},\n",
       "       {'doc_count': 6, 'key': 'based'},\n",
       "       {'doc_count': 6, 'key': 'speech'},\n",
       "       {'doc_count': 5, 'key': 'algorithm'},\n",
       "       {'doc_count': 5, 'key': 'analog'},\n",
       "       {'doc_count': 5, 'key': 'classification'},\n",
       "       {'doc_count': 5, 'key': 'classifier'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 639}},\n",
       "    {'doc_count': 140,\n",
       "     'key': '1994',\n",
       "     'keywords': {'buckets': [{'doc_count': 39, 'key': 'learning'},\n",
       "       {'doc_count': 36, 'key': 'network'},\n",
       "       {'doc_count': 30, 'key': 'neural'},\n",
       "       {'doc_count': 25, 'key': 'model'},\n",
       "       {'doc_count': 10, 'key': 'algorithm'},\n",
       "       {'doc_count': 10, 'key': 'dynamic'},\n",
       "       {'doc_count': 7, 'key': 'reinforcement'},\n",
       "       {'doc_count': 7, 'key': 'time'},\n",
       "       {'doc_count': 6, 'key': 'cortex'},\n",
       "       {'doc_count': 6, 'key': 'mixture'},\n",
       "       {'doc_count': 5, 'key': 'based'},\n",
       "       {'doc_count': 5, 'key': 'function'},\n",
       "       {'doc_count': 4, 'key': 'active'},\n",
       "       {'doc_count': 4, 'key': 'application'},\n",
       "       {'doc_count': 4, 'key': 'computational'},\n",
       "       {'doc_count': 4, 'key': 'connectionist'},\n",
       "       {'doc_count': 4, 'key': 'training'},\n",
       "       {'doc_count': 3, 'key': 'analysi'},\n",
       "       {'doc_count': 3, 'key': 'associative'},\n",
       "       {'doc_count': 3, 'key': 'backpropagation'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 627}},\n",
       "    {'doc_count': 127,\n",
       "     'key': '1992',\n",
       "     'keywords': {'buckets': [{'doc_count': 38, 'key': 'network'},\n",
       "       {'doc_count': 33, 'key': 'neural'},\n",
       "       {'doc_count': 30, 'key': 'learning'},\n",
       "       {'doc_count': 18, 'key': 'model'},\n",
       "       {'doc_count': 9, 'key': 'based'},\n",
       "       {'doc_count': 8, 'key': 'recognition'},\n",
       "       {'doc_count': 7, 'key': 'analog'},\n",
       "       {'doc_count': 7, 'key': 'hidden'},\n",
       "       {'doc_count': 6, 'key': 'algorithm'},\n",
       "       {'doc_count': 5, 'key': 'cell'},\n",
       "       {'doc_count': 5, 'key': 'speech'},\n",
       "       {'doc_count': 4, 'key': 'analysi'},\n",
       "       {'doc_count': 4, 'key': 'computation'},\n",
       "       {'doc_count': 4, 'key': 'control'},\n",
       "       {'doc_count': 4, 'key': 'estimation'},\n",
       "       {'doc_count': 4, 'key': 'function'},\n",
       "       {'doc_count': 4, 'key': 'optimal'},\n",
       "       {'doc_count': 4, 'key': 'rule'},\n",
       "       {'doc_count': 4, 'key': 'space'},\n",
       "       {'doc_count': 4, 'key': 'structure'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 630}},\n",
       "    {'doc_count': 101,\n",
       "     'key': '1989',\n",
       "     'keywords': {'buckets': [{'doc_count': 41, 'key': 'network'},\n",
       "       {'doc_count': 28, 'key': 'neural'},\n",
       "       {'doc_count': 11, 'key': 'learning'},\n",
       "       {'doc_count': 10, 'key': 'algorithm'},\n",
       "       {'doc_count': 9, 'key': 'model'},\n",
       "       {'doc_count': 7, 'key': 'analog'},\n",
       "       {'doc_count': 5, 'key': 'associative'},\n",
       "       {'doc_count': 5, 'key': 'connectionist'},\n",
       "       {'doc_count': 5, 'key': 'recognition'},\n",
       "       {'doc_count': 5, 'key': 'representation'},\n",
       "       {'doc_count': 5, 'key': 'speech'},\n",
       "       {'doc_count': 4, 'key': 'behavior'},\n",
       "       {'doc_count': 4, 'key': 'computer'},\n",
       "       {'doc_count': 4, 'key': 'feature'},\n",
       "       {'doc_count': 4, 'key': 'function'},\n",
       "       {'doc_count': 3, 'key': '2'},\n",
       "       {'doc_count': 3, 'key': 'application'},\n",
       "       {'doc_count': 3, 'key': 'architecture'},\n",
       "       {'doc_count': 3, 'key': 'artificial'},\n",
       "       {'doc_count': 3, 'key': 'backpropagation'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 487}},\n",
       "    {'doc_count': 94,\n",
       "     'key': '1988',\n",
       "     'keywords': {'buckets': [{'doc_count': 37, 'key': 'network'},\n",
       "       {'doc_count': 31, 'key': 'neural'},\n",
       "       {'doc_count': 14, 'key': 'learning'},\n",
       "       {'doc_count': 9, 'key': 'analog'},\n",
       "       {'doc_count': 6, 'key': 'model'},\n",
       "       {'doc_count': 6, 'key': 'system'},\n",
       "       {'doc_count': 5, 'key': 'adaptive'},\n",
       "       {'doc_count': 4, 'key': 'algorithm'},\n",
       "       {'doc_count': 4, 'key': 'connectionist'},\n",
       "       {'doc_count': 4, 'key': 'distributed'},\n",
       "       {'doc_count': 4, 'key': 'dynamic'},\n",
       "       {'doc_count': 4, 'key': 'training'},\n",
       "       {'doc_count': 3, 'key': 'annealing'},\n",
       "       {'doc_count': 3, 'key': 'application'},\n",
       "       {'doc_count': 3, 'key': 'approach'},\n",
       "       {'doc_count': 3, 'key': 'associative'},\n",
       "       {'doc_count': 3, 'key': 'control'},\n",
       "       {'doc_count': 3, 'key': 'expert'},\n",
       "       {'doc_count': 3, 'key': 'field'},\n",
       "       {'doc_count': 3, 'key': 'net'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 398}},\n",
       "    {'doc_count': 90,\n",
       "     'key': '1987',\n",
       "     'keywords': {'buckets': [{'doc_count': 43, 'key': 'network'},\n",
       "       {'doc_count': 39, 'key': 'neural'},\n",
       "       {'doc_count': 11, 'key': 'learning'},\n",
       "       {'doc_count': 9, 'key': 'associative'},\n",
       "       {'doc_count': 9, 'key': 'memory'},\n",
       "       {'doc_count': 7, 'key': 'neuron'},\n",
       "       {'doc_count': 6, 'key': 'application'},\n",
       "       {'doc_count': 6, 'key': 'system'},\n",
       "       {'doc_count': 5, 'key': 'model'},\n",
       "       {'doc_count': 4, 'key': 'analysi'},\n",
       "       {'doc_count': 4, 'key': 'artificial'},\n",
       "       {'doc_count': 4, 'key': 'based'},\n",
       "       {'doc_count': 4, 'key': 'classification'},\n",
       "       {'doc_count': 4, 'key': 'cortex'},\n",
       "       {'doc_count': 4, 'key': 'pattern'},\n",
       "       {'doc_count': 3, 'key': 'approach'},\n",
       "       {'doc_count': 3, 'key': 'computational'},\n",
       "       {'doc_count': 3, 'key': 'computer'},\n",
       "       {'doc_count': 3, 'key': 'connection'},\n",
       "       {'doc_count': 3, 'key': 'connectionist'}],\n",
       "      'doc_count_error_upper_bound': 5,\n",
       "      'sum_other_doc_count': 348}}],\n",
       "   'doc_count_error_upper_bound': 0,\n",
       "   'sum_other_doc_count': 0}},\n",
       " 'hits': {'hits': [], 'max_score': 0.0, 'total': 7241},\n",
       " 'timed_out': False,\n",
       " 'took': 6}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_terms_res = es.search(index = \"nips_papers\", body = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"keywords_by_years\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"year\",\n",
    "                \"size\": 50\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"keywords\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"title\",\n",
    "                        \"size\": 20\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "tot_terms_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
