{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "from elasticsearch import Elasticsearch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{\n",
    "    'host': 'nyuvis-web.poly.edu',\n",
    "    'port': 80,\n",
    "    'url_prefix': 'es'\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list = []\n",
    "tot_num_docs = es.count(index = 'nips_papers')['count']\n",
    "\n",
    "res = es.search(index = 'nips_papers', body = {\n",
    "    \"size\": tot_num_docs,\n",
    "    \"_source\": ['id', 'year', 'title']\n",
    "})\n",
    "\n",
    "for entry in res['hits']['hits']:\n",
    "    title_list.append(entry['_source'])\n",
    "\n",
    "len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RAKE score': 9.0,\n",
       "  'doc_id': '1001',\n",
       "  'keyword': 'neural network ensembles',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1001',\n",
       "  'keyword': 'cross validation',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1001',\n",
       "  'keyword': 'active learning',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '1004',\n",
       "  'keyword': 'iceg morphology classification using',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '1004',\n",
       "  'keyword': 'analogue vlsi neural network',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1006',\n",
       "  'keyword': 'volatile analogue amorphous',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1006',\n",
       "  'keyword': 'silicon memories',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1006',\n",
       "  'keyword': 'pulsestream synapses',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1006', 'keyword': 'non', 'year': '1994'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1007', 'keyword': 'play', 'year': '1994'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1007', 'keyword': 'learning', 'year': '1994'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1007', 'keyword': 'game', 'year': '1994'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1007', 'keyword': 'chess', 'year': '1994'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1013',\n",
       "  'keyword': 'primary visual cortex',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1013',\n",
       "  'keyword': 'patterned lateral connections',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1013',\n",
       "  'keyword': 'organizing model',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1013',\n",
       "  'keyword': 'ocular dominance',\n",
       "  'year': '1994'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1013', 'keyword': 'self', 'year': '1994'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '1032',\n",
       "  'keyword': 'primate visual smooth pursuit',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1032',\n",
       "  'keyword': 'vlsi model',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1033',\n",
       "  'keyword': 'hamiltonian dynamics applied',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1033',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1033', 'keyword': 'learning', 'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1033', 'keyword': 'gradient', 'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1035',\n",
       "  'keyword': 'ocular reflex',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1035',\n",
       "  'keyword': 'dynamical model',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1035',\n",
       "  'keyword': 'context dependencies',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1035', 'keyword': 'vestibulo', 'year': '1995'},\n",
       " {'RAKE score': 81.0,\n",
       "  'doc_id': '1036',\n",
       "  'keyword': 'improved gaussian mixture density estimates using bayesian penalty terms',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1036',\n",
       "  'keyword': 'network averaging',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1044', 'keyword': 'useful', 'year': '1995'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '1044',\n",
       "  'keyword': 'overfitting',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1044', 'keyword': 'learning', 'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1044', 'keyword': 'ensembles', 'year': '1995'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '1045',\n",
       "  'keyword': 'object recognition using multiple visual cues',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1045',\n",
       "  'keyword': 'based approach',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1045', 'keyword': 'view', 'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1045', 'keyword': 'seemore', 'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1045', 'keyword': '3', 'year': '1995'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '105',\n",
       "  'keyword': 'handwritten signature verification',\n",
       "  'year': '1988'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '105',\n",
       "  'keyword': 'backpropagation',\n",
       "  'year': '1988'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '105',\n",
       "  'keyword': 'application',\n",
       "  'year': '1988'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1052', 'keyword': 'structure', 'year': '1995'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '1052',\n",
       "  'keyword': 'similarity',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1052', 'keyword': 'learning', 'year': '1995'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1054',\n",
       "  'keyword': 'fourier transform algorithm',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1054',\n",
       "  'keyword': 'implementation issues',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1055',\n",
       "  'keyword': 'surround receptive field',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1055',\n",
       "  'keyword': 'adaptive retina',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1055', 'keyword': 'center', 'year': '1995'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '1061',\n",
       "  'keyword': 'stable dynamic parameter adaption',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1064',\n",
       "  'keyword': 'sample data',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1064',\n",
       "  'keyword': 'bayes risk',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '1064',\n",
       "  'keyword': 'estimating',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1069',\n",
       "  'keyword': 'perception guides production',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1069',\n",
       "  'keyword': 'birdsong learning',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '1073',\n",
       "  'keyword': 'improving elevator performance using reinforcement learning',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '1075',\n",
       "  'keyword': 'improving committee diagnosis',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1075',\n",
       "  'keyword': 'resampling techniques',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '108',\n",
       "  'keyword': 'perceptual organization',\n",
       "  'year': '1988'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '108',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '1988'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '108',\n",
       "  'keyword': 'model matching',\n",
       "  'year': '1988'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '1082',\n",
       "  'keyword': 'beta sheets',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1082', 'keyword': 'proteins', 'year': '1995'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '1082',\n",
       "  'keyword': 'prediction',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 64.0,\n",
       "  'doc_id': '1084',\n",
       "  'keyword': 'speed airborne particle monitoring using artificial neural networks',\n",
       "  'year': '1995'},\n",
       " {'RAKE score': 1.0, 'doc_id': '1084', 'keyword': 'high', 'year': '1995'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2372',\n",
       "  'keyword': 'bounded finite state controllers',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2373',\n",
       "  'keyword': 'minimax embeddings',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2380',\n",
       "  'keyword': 'reward maximization',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2380',\n",
       "  'keyword': 'eye movements',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2385',\n",
       "  'keyword': 'online classification',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2385', 'keyword': 'budget', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2396',\n",
       "  'keyword': 'microarray expression data',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2396',\n",
       "  'keyword': 'based clustering',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2396', 'keyword': 'ica', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2396', 'keyword': 'genes', 'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '24',\n",
       "  'keyword': 'optimal neural spike classification',\n",
       "  'year': '1987'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '240',\n",
       "  'keyword': 'view representation',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '240',\n",
       "  'keyword': 'organizing multiple',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0, 'doc_id': '240', 'keyword': '3d objects', 'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '240', 'keyword': 'self', 'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2409',\n",
       "  'keyword': 'based image vectors',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2409',\n",
       "  'keyword': 'time generation',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2409',\n",
       "  'keyword': 'signal vlsi',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2409', 'keyword': 'real', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2409', 'keyword': 'mixed', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2409', 'keyword': 'edge', 'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2414',\n",
       "  'keyword': 'party processor',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2414',\n",
       "  'keyword': 'based cocktail',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2414',\n",
       "  'keyword': 'classification',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2415',\n",
       "  'keyword': 'spiking neural systems',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2415',\n",
       "  'keyword': 'decaying cmos synapse',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2415', 'keyword': 'summating', 'year': '2003'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2415',\n",
       "  'keyword': 'exponentially',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0, 'doc_id': '2417', 'keyword': 'find pre', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2417', 'keyword': 'learning', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2417', 'keyword': 'images', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2418',\n",
       "  'keyword': 'estimating internal variables',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2418',\n",
       "  'keyword': 'particle filter',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2418',\n",
       "  'keyword': 'learning agent',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2418', 'keyword': 'paramters', 'year': '2003'},\n",
       " {'RAKE score': 81.0,\n",
       "  'doc_id': '2421',\n",
       "  'keyword': 'eigenvoice speaker adaptation via composite kernel principal component analysis',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2422',\n",
       "  'keyword': 'asd brain computer interface',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2422',\n",
       "  'keyword': 'energy normalization transform',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2422',\n",
       "  'keyword': 'performance',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2422', 'keyword': 'lf', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2422', 'keyword': 'impact', 'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2427',\n",
       "  'keyword': 'model uncertainty',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2427',\n",
       "  'keyword': 'corrected bootstrap',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2427', 'keyword': 'bias', 'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2433',\n",
       "  'keyword': 'margin maximizing loss functions',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2434',\n",
       "  'keyword': 'perceptron learning',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2434',\n",
       "  'keyword': 'definite programming',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2434', 'keyword': 'semi', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2439',\n",
       "  'keyword': 'bayesian posterior distributions',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2439',\n",
       "  'keyword': 'learning bounds',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2439',\n",
       "  'keyword': 'generalized family',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2448',\n",
       "  'keyword': 'distributed optimization',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2448',\n",
       "  'keyword': 'adaptive networks',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2454',\n",
       "  'keyword': 'probability estimates',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2454',\n",
       "  'keyword': 'pairwise coupling',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2454',\n",
       "  'keyword': 'class classification',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2454', 'keyword': 'multi', 'year': '2003'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '2455',\n",
       "  'keyword': 'autonomous helicopter flight via reinforcement learning',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2456',\n",
       "  'keyword': 'policy language bias',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2456',\n",
       "  'keyword': 'approximate policy iteration',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2457',\n",
       "  'keyword': 'information bottleneck',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2457',\n",
       "  'keyword': 'gaussian variables',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2460',\n",
       "  'keyword': 'reconstructing meg sources',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2460',\n",
       "  'keyword': 'unknown correlations',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2468',\n",
       "  'keyword': 'unbiased estimator',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2468',\n",
       "  'keyword': 'fold cross',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2468', 'keyword': 'variance', 'year': '2003'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2468',\n",
       "  'keyword': 'validation',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2468', 'keyword': 'k', 'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2472',\n",
       "  'keyword': 'recurrent model',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2472',\n",
       "  'keyword': 'orientation maps',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2472',\n",
       "  'keyword': 'complex cells',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2472', 'keyword': 'simple', 'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2473',\n",
       "  'keyword': 'experiments via information theory',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2473', 'keyword': 'design', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2475',\n",
       "  'keyword': 'motion pattern processing',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2475',\n",
       "  'keyword': 'functional architecture',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2475', 'keyword': 'mstd', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2476',\n",
       "  'keyword': 'global reward games',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '2476',\n",
       "  'keyword': 'agent learning',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.5, 'doc_id': '2476', 'keyword': 'learning', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2476', 'keyword': 'multi', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2476', 'keyword': 'local', 'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2477',\n",
       "  'keyword': 'support vector machines ---',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2477',\n",
       "  'keyword': 'asymptotically sharp bounds',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2477',\n",
       "  'keyword': 'sparseness',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '2482',\n",
       "  'keyword': 'salient boundary detection using ratio contour',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2490',\n",
       "  'keyword': 'neural symbolic learning systems',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2490', 'keyword': 'time', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2490', 'keyword': 'reasoning', 'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2490', 'keyword': 'knowledge', 'year': '2003'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '2496',\n",
       "  'keyword': 'supervised protein classification using cluster kernels',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2496', 'keyword': 'semi', 'year': '2003'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '25',\n",
       "  'keyword': 'computing motion using resistive networks',\n",
       "  'year': '1987'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2501',\n",
       "  'keyword': 'sample covariance eigenspectrum',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2501',\n",
       "  'keyword': 'limiting form',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '2501',\n",
       "  'keyword': 'kernel pca',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.5, 'doc_id': '2501', 'keyword': 'pca', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2503',\n",
       "  'keyword': 'general adaptive multi',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2503',\n",
       "  'keyword': 'extending q',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2503',\n",
       "  'keyword': 'agent systems',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2503', 'keyword': 'learning', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2505',\n",
       "  'keyword': 'fast fourier transforms',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2505',\n",
       "  'keyword': 'missing data',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2505',\n",
       "  'keyword': 'generalised propagation',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2505', 'keyword': 'partial', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2508',\n",
       "  'keyword': 'parameterized novelty detectors',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2508',\n",
       "  'keyword': 'environmental sensor monitoring',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2509',\n",
       "  'keyword': 'rigid 3d shape',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2509',\n",
       "  'keyword': 'learning non',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0, 'doc_id': '2509', 'keyword': '2d motion', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2511',\n",
       "  'keyword': 'stochastic gradient descent',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2511',\n",
       "  'keyword': 'linear feedforward networks',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2511',\n",
       "  'keyword': 'learning curves',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2516',\n",
       "  'keyword': 'nonlinear time series prediction',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2516',\n",
       "  'keyword': 'dynamical modeling',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2516', 'keyword': 'kernels', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2521',\n",
       "  'keyword': 'necessary intransitive likelihood',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2521',\n",
       "  'keyword': 'ratio classifiers',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2522',\n",
       "  'keyword': 'limited biochemical signal',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2522',\n",
       "  'keyword': 'relay channel',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2522', 'keyword': 'diffusion', 'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '253',\n",
       "  'keyword': 'subgrouping reduces complexity',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '253',\n",
       "  'keyword': 'recurrent networks',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '253', 'keyword': 'speeds', 'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '253', 'keyword': 'learning', 'year': '1989'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2533',\n",
       "  'keyword': 'silicon central pattern generators',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2533',\n",
       "  'keyword': 'legged locomotory control',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2533',\n",
       "  'keyword': 'entrainment',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2537',\n",
       "  'keyword': 'auction mechanism design',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2537',\n",
       "  'keyword': 'robot coordination',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2537', 'keyword': 'multi', 'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '254',\n",
       "  'keyword': 'neural implementation',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '254',\n",
       "  'keyword': 'motivated behavior',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '254',\n",
       "  'keyword': 'artificial insect',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '254', 'keyword': 'feeding', 'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2542',\n",
       "  'keyword': 'linear programming',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2542',\n",
       "  'keyword': 'image reconstruction',\n",
       "  'year': '2003'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2544',\n",
       "  'keyword': 'large deviation bound',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0, 'doc_id': '2544', 'keyword': 'roc curve', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2544', 'keyword': 'area', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2554',\n",
       "  'keyword': 'category detection',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2554',\n",
       "  'keyword': 'active learning',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2554', 'keyword': 'rare', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2554', 'keyword': 'anomaly', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '256',\n",
       "  'keyword': 'simd processor arrays',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '256',\n",
       "  'keyword': 'connectionist learning algorithms',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '256',\n",
       "  'keyword': 'performance',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '256', 'keyword': '2', 'year': '1989'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '2571',\n",
       "  'keyword': 'break visual human interaction proofs',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2571',\n",
       "  'keyword': 'using machine learning',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2571', 'keyword': 'hips', 'year': '2004'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '2573',\n",
       "  'keyword': 'microwatt analog vlsi support vector machine',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2573',\n",
       "  'keyword': 'sequence estimation',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2573',\n",
       "  'keyword': 'pattern classification',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2573', 'keyword': 'sub', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2578',\n",
       "  'keyword': 'towards bridging theory',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2578', 'keyword': 'training', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2578', 'keyword': 'practice', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2578', 'keyword': 'expansion', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2578', 'keyword': 'co', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2579',\n",
       "  'keyword': 'multiclass problems',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2579',\n",
       "  'keyword': 'learning preferences',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2581',\n",
       "  'keyword': 'natural images using',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2581',\n",
       "  'keyword': 'limited capacity units',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2581',\n",
       "  'keyword': 'sparse coding',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2581',\n",
       "  'keyword': 'overcomplete set',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2588',\n",
       "  'keyword': 'timing dependent plasticity',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2588',\n",
       "  'keyword': 'spiking neuron model',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2588',\n",
       "  'keyword': 'mutual information maximization',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2588', 'keyword': 'spike', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '259',\n",
       "  'keyword': 'conventional pattern classifiers',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '259',\n",
       "  'keyword': 'speech problems',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '259',\n",
       "  'keyword': 'practical characteristics',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '259',\n",
       "  'keyword': 'neural network',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '259', 'keyword': 'artificial', 'year': '1989'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '2591',\n",
       "  'keyword': 'detecting significant multidimensional spatial clusters',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '26',\n",
       "  'keyword': 'traditional classifiers',\n",
       "  'year': '1987'},\n",
       " {'RAKE score': 4.0, 'doc_id': '26', 'keyword': 'neural net', 'year': '1987'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '260',\n",
       "  'keyword': 'discovering high order features',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '260',\n",
       "  'keyword': 'mean field modules',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2614',\n",
       "  'keyword': 'talairach space',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0, 'doc_id': '2614', 'keyword': 'mass meta', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2614', 'keyword': 'analysis', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2617',\n",
       "  'keyword': 'contrastive divergences',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2617',\n",
       "  'keyword': 'convergence',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2619',\n",
       "  'keyword': 'tuning spectral clustering',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2619', 'keyword': 'self', 'year': '2004'},\n",
       " {'RAKE score': 81.0,\n",
       "  'doc_id': '2620',\n",
       "  'keyword': 'learning efficient auditory codes using spikes predicts cochlear filters',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2623',\n",
       "  'keyword': 'stable adaptive control',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2623',\n",
       "  'keyword': 'online learning',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2631',\n",
       "  'keyword': 'time pitch determination',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2631',\n",
       "  'keyword': 'nonnegative matrix factorization',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2631', 'keyword': 'voices', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2631', 'keyword': 'real', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2631', 'keyword': 'one', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2639',\n",
       "  'keyword': 'online document clustering',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2639',\n",
       "  'keyword': 'probabilistic model',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2639',\n",
       "  'keyword': 'novelty detection',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2639',\n",
       "  'keyword': 'application',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '264',\n",
       "  'keyword': 'model based image compression',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '264',\n",
       "  'keyword': 'interacting filter banks',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '264',\n",
       "  'keyword': 'adaptive data representation',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2640',\n",
       "  'keyword': 'articulated object action modeling',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2640',\n",
       "  'keyword': 'three tiered approach',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2640',\n",
       "  'keyword': 'recognition',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2644',\n",
       "  'keyword': 'tracking hand movements',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2644',\n",
       "  'keyword': 'temporal kernel',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2644',\n",
       "  'keyword': 'neural activities',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2644',\n",
       "  'keyword': 'based model',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '265',\n",
       "  'keyword': 'data signal separation comparison',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '265',\n",
       "  'keyword': 'cocktail party problem',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '265', 'keyword': 'speech', 'year': '1989'},\n",
       " {'RAKE score': 1.0, 'doc_id': '265', 'keyword': 'sonn', 'year': '1989'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '265',\n",
       "  'keyword': 'backpropagation',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2651',\n",
       "  'keyword': 'multiple relational embedding',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '2653',\n",
       "  'keyword': 'classification using local label configurations',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2653',\n",
       "  'keyword': 'topographic support vector machine',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2656',\n",
       "  'keyword': 'class kernel fisher discriminants',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2656',\n",
       "  'keyword': 'outlier detection',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2656', 'keyword': 'one', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2658',\n",
       "  'keyword': 'generative affine localisation',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2658', 'keyword': 'tracking', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2659',\n",
       "  'keyword': 'learning syntactic patterns',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2659',\n",
       "  'keyword': 'automatic hypernym discovery',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2664',\n",
       "  'keyword': 'dynamic bayesian networks',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2664',\n",
       "  'keyword': 'computer interfaces',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2664', 'keyword': 'brain', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2674',\n",
       "  'keyword': 'spiking network',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2674',\n",
       "  'keyword': 'maximising sensitivity',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2678',\n",
       "  'keyword': 'alternative splicing events',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2678',\n",
       "  'keyword': 'probabilistic inference',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2678',\n",
       "  'keyword': 'microarray data',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2680',\n",
       "  'keyword': 'new criteria',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2680',\n",
       "  'keyword': 'new algorithm',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2680',\n",
       "  'keyword': 'agent systems',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2680', 'keyword': 'multi', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2680', 'keyword': 'learning', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2681',\n",
       "  'keyword': 'proximity graphs',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2681',\n",
       "  'keyword': 'manifold learning',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2681',\n",
       "  'keyword': 'clustering',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2682',\n",
       "  'keyword': 'optimal sub',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2682',\n",
       "  'keyword': 'graphical models',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2688',\n",
       "  'keyword': 'modelling uncertainty',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2688', 'keyword': 'go', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2688', 'keyword': 'game', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2692',\n",
       "  'keyword': 'spectral clustering',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2692', 'keyword': 'limits', 'year': '2004'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '270',\n",
       "  'keyword': 'coupled markov random fields',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '270',\n",
       "  'keyword': 'mean field theory',\n",
       "  'year': '1989'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2700',\n",
       "  'keyword': 'generalization error bounds',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2700',\n",
       "  'keyword': 'rank matrices',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2700',\n",
       "  'keyword': 'collaborative prediction',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2700', 'keyword': 'low', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2701',\n",
       "  'keyword': 'local image representations',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2701',\n",
       "  'keyword': 'algebraic set kernels',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2701', 'keyword': 'inference', 'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2701',\n",
       "  'keyword': 'application',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2704',\n",
       "  'keyword': 'approximate scalable algorithm',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2704',\n",
       "  'keyword': 'large pomdps',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2704', 'keyword': 'vdcbpi', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2708', 'keyword': 'picture', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2714',\n",
       "  'keyword': 'probabilistic inference',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2714',\n",
       "  'keyword': 'harmonising chorales',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2722',\n",
       "  'keyword': 'posed inverse problems',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2722',\n",
       "  'keyword': 'regularization',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2722', 'keyword': 'learning', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2722', 'keyword': 'ill', 'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2724',\n",
       "  'keyword': 'implicit surface modeling',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2724',\n",
       "  'keyword': 'kernel methods',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2731',\n",
       "  'keyword': 'individual model neurons',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2731',\n",
       "  'keyword': 'synaptic plasticity',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2731', 'keyword': 'synergies', 'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2731', 'keyword': 'intrinsic', 'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2734',\n",
       "  'keyword': 'reactive environments',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2734',\n",
       "  'keyword': 'exploitation tradeoffs',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2734',\n",
       "  'keyword': 'experts algorithms',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2734',\n",
       "  'keyword': 'exploration',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2737',\n",
       "  'keyword': 'neural microcircuits',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2737',\n",
       "  'keyword': 'generalization capability',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2737',\n",
       "  'keyword': 'computational power',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2737', 'keyword': 'methods', 'year': '2004'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2737',\n",
       "  'keyword': 'estimating',\n",
       "  'year': '2004'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2750',\n",
       "  'keyword': 'dynamical synapses give rise',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2750',\n",
       "  'keyword': 'neuronal avalanches',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2750',\n",
       "  'keyword': 'law distribution',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2750', 'keyword': 'power', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2758',\n",
       "  'keyword': 'rapid object detection',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2758',\n",
       "  'keyword': 'hierarchical compositional system',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2761',\n",
       "  'keyword': 'tilt perception',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2761',\n",
       "  'keyword': 'bayesian framework',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2761',\n",
       "  'keyword': 'confidence',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2765',\n",
       "  'keyword': 'recurrent neural networks',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2765',\n",
       "  'keyword': 'whole relationships',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2765',\n",
       "  'keyword': 'representing part',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2787',\n",
       "  'keyword': 'subsequence kernels',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2787',\n",
       "  'keyword': 'relation extraction',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2793',\n",
       "  'keyword': 'density matrices',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2793',\n",
       "  'keyword': 'bayes rule',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '2812',\n",
       "  'keyword': 'gradient flow independent component analysis',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2812',\n",
       "  'keyword': 'micropower vlsi',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2813',\n",
       "  'keyword': 'bounded rationality',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2813', 'keyword': 'optimal', 'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2813', 'keyword': 'frugal', 'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2813', 'keyword': 'fast', 'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2813', 'keyword': 'far', 'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2813', 'keyword': 'accuracy', 'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2819',\n",
       "  'keyword': 'bayesian spatial scan statistic',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2820',\n",
       "  'keyword': 'topic discovery',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2820', 'keyword': 'relations', 'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2820', 'keyword': 'group', 'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2820',\n",
       "  'keyword': 'attributes',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2821',\n",
       "  'keyword': 'unseen cases',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2821',\n",
       "  'keyword': 'generalization',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2823',\n",
       "  'keyword': 'extracting dynamical structure embedded',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2823',\n",
       "  'keyword': 'neural activity',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2828',\n",
       "  'keyword': 'gaussian regularized least squares',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2828',\n",
       "  'keyword': 'asymptotics',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2830',\n",
       "  'keyword': 'saliency based',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2830',\n",
       "  'keyword': 'information maximization',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '2835',\n",
       "  'keyword': 'fast gaussian process regression using kd',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2835', 'keyword': 'trees', 'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2838',\n",
       "  'keyword': 'pattern recognition',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2838',\n",
       "  'keyword': 'one example',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2838', 'keyword': 'chopping', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2844',\n",
       "  'keyword': 'set covering machine',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2844',\n",
       "  'keyword': 'bayes approach',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2844', 'keyword': 'pac', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2847',\n",
       "  'keyword': 'road obstacle avoidance',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '2847',\n",
       "  'keyword': 'end learning',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.5, 'doc_id': '2847', 'keyword': 'end', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2850',\n",
       "  'keyword': 'maximum margin semi',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2850',\n",
       "  'keyword': 'supervised learning',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2850',\n",
       "  'keyword': 'structured variables',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2860',\n",
       "  'keyword': 'generalization error bounds',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2860',\n",
       "  'keyword': 'interdependent data',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2860',\n",
       "  'keyword': 'classifiers trained',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2872',\n",
       "  'keyword': 'efficient estimation',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2872', 'keyword': 'ooms', 'year': '2005'},\n",
       " {'RAKE score': 64.0,\n",
       "  'doc_id': '2879',\n",
       "  'keyword': 'dynamic social network analysis using latent space models',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2882',\n",
       "  'keyword': 'infinite latent feature models',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2882',\n",
       "  'keyword': 'indian buffet process',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '2890',\n",
       "  'keyword': 'efficient multiple kernel learning algorithm',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2890', 'keyword': 'general', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '29',\n",
       "  'keyword': 'electronic neural networks',\n",
       "  'year': '1987'},\n",
       " {'RAKE score': 1.0, 'doc_id': '29', 'keyword': 'basins', 'year': '1987'},\n",
       " {'RAKE score': 1.0, 'doc_id': '29', 'keyword': 'attraction', 'year': '1987'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2905',\n",
       "  'keyword': 'surrogate loss functions',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2905',\n",
       "  'keyword': 'experimental design',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2905',\n",
       "  'keyword': 'divergences',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2908',\n",
       "  'keyword': 'perturbed gaussian markov processes',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2908',\n",
       "  'keyword': 'iterative estimation',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2908', 'keyword': 'non', 'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2913',\n",
       "  'keyword': 'consensus propagation',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2914',\n",
       "  'keyword': 'local manifold parzen windows',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2914', 'keyword': 'non', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2937',\n",
       "  'keyword': 'inferring motor programs',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2937',\n",
       "  'keyword': 'handwritten digits',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2937', 'keyword': 'images', 'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2939',\n",
       "  'keyword': 'invariant visual responses',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2939',\n",
       "  'keyword': 'learning cue',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2945',\n",
       "  'keyword': 'intrinsic dimensionality using high',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2945',\n",
       "  'keyword': 'rate vector quantization',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2945',\n",
       "  'keyword': 'estimation',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2949',\n",
       "  'keyword': 'object class detection',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2949',\n",
       "  'keyword': 'eye movements',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2949',\n",
       "  'keyword': 'computational model',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '295',\n",
       "  'keyword': 'spoken letter recognition',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2950',\n",
       "  'keyword': 'exploitation model based',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2950',\n",
       "  'keyword': 'dopamine activity',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2950',\n",
       "  'keyword': 'norepinepherine',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2950',\n",
       "  'keyword': 'exploration',\n",
       "  'year': '2005'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2964',\n",
       "  'keyword': 'switched gaussian process',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2964',\n",
       "  'keyword': 'estimating disparity',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2964',\n",
       "  'keyword': 'binocular stereo',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2964',\n",
       "  'keyword': 'segmentation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2967',\n",
       "  'keyword': 'large margin multi',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2967',\n",
       "  'keyword': 'neural prosthesis',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2967',\n",
       "  'keyword': 'digital conversion',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2967',\n",
       "  'keyword': 'channel analog',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2967',\n",
       "  'keyword': 'applications',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2971',\n",
       "  'keyword': 'nonsmooth cost functions',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2971', 'keyword': 'rank', 'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2971', 'keyword': 'learning', 'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2983',\n",
       "  'keyword': 'domain adaptation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2983',\n",
       "  'keyword': 'representations',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '2983', 'keyword': 'analysis', 'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '2992',\n",
       "  'keyword': 'training conditional random fields',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '2992',\n",
       "  'keyword': 'maximum labelwise accuracy',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '2999',\n",
       "  'keyword': 'sequential data using generalized suffix trees',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '2999',\n",
       "  'keyword': 'similarity measures',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '2999',\n",
       "  'keyword': 'computation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '300',\n",
       "  'keyword': 'spline receptive field functions',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '300',\n",
       "  'keyword': 'layer perceptrons',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '300', 'keyword': 'multi', 'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '300', 'keyword': 'b', 'year': '1990'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3004',\n",
       "  'keyword': 'general independent subspace analysis',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3004', 'keyword': 'towards', 'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3007',\n",
       "  'keyword': 'linear threshold functions',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3007',\n",
       "  'keyword': 'unconcentrated distributions',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3007',\n",
       "  'keyword': 'efficient learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3007',\n",
       "  'keyword': 'decision lists',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3007', 'keyword': 'attribute', 'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3011',\n",
       "  'keyword': 'stereo computation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3011',\n",
       "  'keyword': 'neurophysiological evidence',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3011',\n",
       "  'keyword': 'cooperative mechanisms',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3014',\n",
       "  'keyword': 'generalized regularized least',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3014',\n",
       "  'keyword': 'squares learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3014',\n",
       "  'keyword': 'predefined features',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3014',\n",
       "  'keyword': 'hilbert space',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3019',\n",
       "  'keyword': 'mixture regression',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3019',\n",
       "  'keyword': 'covariate shift',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '302',\n",
       "  'keyword': 'interacting oscillators',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '302',\n",
       "  'keyword': 'dimensional networks',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '302', 'keyword': 'two', 'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '302', 'keyword': 'phase', 'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '302', 'keyword': 'coupling', 'year': '1990'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3021',\n",
       "  'keyword': 'unsupervised learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3021',\n",
       "  'keyword': 'probabilistic grammar',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3021',\n",
       "  'keyword': 'object detection',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3021', 'keyword': 'parsing', 'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3027',\n",
       "  'keyword': 'switching linear dynamical systems',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3027',\n",
       "  'keyword': 'novel gaussian sum smoother',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3027',\n",
       "  'keyword': 'approximate inference',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3030',\n",
       "  'keyword': 'high dimensions',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3030',\n",
       "  'keyword': 'approximate correspondences',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3038',\n",
       "  'keyword': 'implicit online learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3038', 'keyword': 'kernels', 'year': '2006'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '3041',\n",
       "  'keyword': 'parameter expanded variational bayesian methods',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3043',\n",
       "  'keyword': 'graph based semi',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '3043',\n",
       "  'keyword': 'supervised learning algorithms',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '3043',\n",
       "  'keyword': 'hyperparameter learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '3044',\n",
       "  'keyword': 'large scale hierarchical classification kernel methods',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3044',\n",
       "  'keyword': 'validation optimization',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3044', 'keyword': 'cross', 'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3049',\n",
       "  'keyword': 'doubly stochastic normalization',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3049',\n",
       "  'keyword': 'spectral clustering',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3050',\n",
       "  'keyword': 'cortical population activity',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3050',\n",
       "  'keyword': 'nonlinear physically',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3050',\n",
       "  'keyword': 'decoding motor',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3050',\n",
       "  'keyword': 'based models',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3053',\n",
       "  'keyword': 'markov decision processes',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3053',\n",
       "  'keyword': 'performance tradeoff',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3053',\n",
       "  'keyword': 'robustness',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3058',\n",
       "  'keyword': 'tighter pac',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3058',\n",
       "  'keyword': 'bayes bounds',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3059',\n",
       "  'keyword': 'svm models',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3059',\n",
       "  'keyword': 'efficient method',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3059',\n",
       "  'keyword': 'based adaptation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3059',\n",
       "  'keyword': 'hyperparameters',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3059', 'keyword': 'gradient', 'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3062',\n",
       "  'keyword': 'combining causal',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3062',\n",
       "  'keyword': 'based reasoning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3062',\n",
       "  'keyword': 'similarity',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3064',\n",
       "  'keyword': 'occurrence data',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3064', 'keyword': 'non co', 'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3064',\n",
       "  'keyword': 'information bottleneck',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3071',\n",
       "  'keyword': 'small world threshold',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3071',\n",
       "  'keyword': 'economic network formation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3073',\n",
       "  'keyword': 'simplifying mixture models',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3073',\n",
       "  'keyword': 'function approximation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 49.0,\n",
       "  'doc_id': '3078',\n",
       "  'keyword': 'modeling human motion using binary latent variables',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3079',\n",
       "  'keyword': 'bayesian skill rating system',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3079', 'keyword': 'trueskill', 'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3086',\n",
       "  'keyword': 'support vector machines',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3086', 'keyword': 'budget', 'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3087',\n",
       "  'keyword': 'road traffic optimisation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3087',\n",
       "  'keyword': 'natural actor',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3087', 'keyword': 'critic', 'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3095',\n",
       "  'keyword': 'based visual saliency',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3095', 'keyword': 'graph', 'year': '2006'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '3113',\n",
       "  'keyword': 'collapsed variational bayesian inference algorithm',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3113',\n",
       "  'keyword': 'latent dirichlet allocation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '3121',\n",
       "  'keyword': 'kernel maximum entropy data transformation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3121',\n",
       "  'keyword': 'enhanced spectral clustering algorithm',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '313',\n",
       "  'keyword': 'video signal processing',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '313',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '313',\n",
       "  'keyword': 'applications',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3139',\n",
       "  'keyword': 'scalable discriminative learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3139',\n",
       "  'keyword': 'natural language parsing',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3139',\n",
       "  'keyword': 'translation',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '314',\n",
       "  'keyword': 'second order properties',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '314',\n",
       "  'keyword': 'learning time',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '314',\n",
       "  'keyword': 'error surfaces',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '314',\n",
       "  'keyword': 'generalization',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3140', 'keyword': 'time', 'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3140', 'keyword': 'recipe', 'year': '2006'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3140',\n",
       "  'keyword': 'optimizing',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3140', 'keyword': 'histogram', 'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3141',\n",
       "  'keyword': 'sensorimotor integration',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3141',\n",
       "  'keyword': 'causal inference',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3143',\n",
       "  'keyword': 'task feature learning',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3143', 'keyword': 'multi', 'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3149',\n",
       "  'keyword': 'markov random fields',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3149',\n",
       "  'keyword': 'bayesian model scoring',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3152',\n",
       "  'keyword': 'isotonic conditional random fields',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3152',\n",
       "  'keyword': 'local sentiment flow',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3156',\n",
       "  'keyword': 'network pca',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3156',\n",
       "  'keyword': 'anomaly detection',\n",
       "  'year': '2006'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3161',\n",
       "  'keyword': 'classify complex patterns using',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3161',\n",
       "  'keyword': 'vlsi network',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3161',\n",
       "  'keyword': 'spiking neurons',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3161', 'keyword': 'learning', 'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'information bottleneck optimization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'theoretical analysis',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'spiking neurons',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'simplified rules',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'pca', 'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3170',\n",
       "  'keyword': 'posterior constraints',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3170',\n",
       "  'keyword': 'expectation maximization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'continuous time particle filtering',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'fmri', 'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'performs independent component analysis',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'online hebbian learning rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'discriminative k',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'means', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'clustering',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'side information',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'greedy algorithm',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'armed bandits',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'multi', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'epoch', 'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3187',\n",
       "  'keyword': 'task structure learning',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3187',\n",
       "  'keyword': 'spectral regularization framework',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3187', 'keyword': 'multi', 'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3193',\n",
       "  'keyword': 'penalized convex risk minimization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3193',\n",
       "  'keyword': 'estimating divergence functionals',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3193',\n",
       "  'keyword': 'likelihood ratio',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'convergent message passing algorithms',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3200', 'keyword': 'map lp', 'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'fixing max',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'relaxations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'product', 'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'modelling motion primitives',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'biologically executed movements',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'timing', 'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3208',\n",
       "  'keyword': 'probabilistic matrix factorization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'recurrent neural networks',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'line handwriting recognition',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'unconstrained',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '322',\n",
       "  'keyword': 'interaction among ocularity',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '322',\n",
       "  'keyword': 'center pathways',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 1.5, 'doc_id': '322', 'keyword': 'center', 'year': '1990'},\n",
       " {'RAKE score': 1.0, 'doc_id': '322', 'keyword': 'retinotopy', 'year': '1990'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '322',\n",
       "  'keyword': 'development',\n",
       "  'year': '1990'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '3229',\n",
       "  'keyword': 'spike trains using gaussian processes',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3229',\n",
       "  'keyword': 'inferring neural firing rates',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3232',\n",
       "  'keyword': 'semantic structure',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3232',\n",
       "  'keyword': 'retrieved context',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3232', 'keyword': 'discovery', 'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'direct importance estimation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'covariate shift adaptation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'model selection',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'application',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3253',\n",
       "  'keyword': 'hierarchical apprenticeship learning',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3253',\n",
       "  'keyword': 'quadruped locomotion',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3253',\n",
       "  'keyword': 'application',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3256',\n",
       "  'keyword': 'partially observed populations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3256',\n",
       "  'keyword': 'spiking neurons',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3256',\n",
       "  'keyword': 'neural characterization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3260',\n",
       "  'keyword': 'bayesian co',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3260', 'keyword': 'training', 'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'supervised factored logistic regression',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'dimensional neuroimaging data',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'semi', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'high', 'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'training deep neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'binary weights',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'propagations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'binaryconnect',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'convolutional neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'spectral representations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5651',\n",
       "  'keyword': 'bidirectional recurrent neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5651',\n",
       "  'keyword': 'generative models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'deep knowledge tracing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 64.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'statistical model criticism using kernel two sample tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'calibrated structured prediction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'learning deep multidimensional recurrent neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'free optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'hessian', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'copula variational inference',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'based relative entropy stochastic search',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'model', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'sample testing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'probability measures',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5685', 'keyword': 'fast two', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'analytic representations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'zero temperature',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'online prediction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'limit', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'average submodular partitioning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'mixed robust',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'fast algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'guarantees',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'applications',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '571',\n",
       "  'keyword': 'globally optimal path planning',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '571',\n",
       "  'keyword': 'oscillatory neural fields',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'structured discrete distributions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'differentially private learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5714',\n",
       "  'keyword': 'robust portfolio optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '572',\n",
       "  'keyword': 'data analysis using g',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 1.0, 'doc_id': '572', 'keyword': 'splines', 'year': '1991'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5720',\n",
       "  'keyword': 'regularized em algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5720',\n",
       "  'keyword': 'unified framework',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5720',\n",
       "  'keyword': 'statistical guarantees',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5748',\n",
       "  'keyword': 'normalized estimator',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5748',\n",
       "  'keyword': 'counterfactual learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5748', 'keyword': 'self', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '575',\n",
       "  'keyword': 'perturbing hebbian rules',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5750',\n",
       "  'keyword': 'second order method',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5750',\n",
       "  'keyword': 'glms via stein',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5750',\n",
       "  'keyword': 'stein method',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5750', 'keyword': 'newton', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5750', 'keyword': 'lemma', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5756',\n",
       "  'keyword': 'latent bayesian melding',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5756',\n",
       "  'keyword': 'population models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5756',\n",
       "  'keyword': 'integrating individual',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5760',\n",
       "  'keyword': 'stochastic expectation propagation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5764',\n",
       "  'keyword': 'diverse complex characters',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5764',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5764',\n",
       "  'keyword': 'interactive control',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5780',\n",
       "  'keyword': 'perceiving physical object properties',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5780',\n",
       "  'keyword': 'physics engine',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5780',\n",
       "  'keyword': 'deep learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5780',\n",
       "  'keyword': 'integrating',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5780', 'keyword': 'galileo', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5782',\n",
       "  'keyword': 'level convolutional networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5782',\n",
       "  'keyword': 'text classification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5782', 'keyword': 'character', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5785',\n",
       "  'keyword': 'unsupervised learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5785',\n",
       "  'keyword': 'program synthesis',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5789',\n",
       "  'keyword': 'making perceptual decisions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5789',\n",
       "  'keyword': 'brain uses reliability',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5789',\n",
       "  'keyword': 'stimulus information',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5795',\n",
       "  'keyword': 'preconditioned spectral descent',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5795',\n",
       "  'keyword': 'deep learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5798',\n",
       "  'keyword': 'group invariant features',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5798',\n",
       "  'keyword': 'kernel perspective',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5798', 'keyword': 'learning', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5801',\n",
       "  'keyword': 'hamiltonian monte carlo',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5801',\n",
       "  'keyword': 'refraction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5801',\n",
       "  'keyword': 'reflection',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5804',\n",
       "  'keyword': 'parallel predictive entropy search',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5804',\n",
       "  'keyword': 'expensive objective functions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5804',\n",
       "  'keyword': 'batch global optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '5808',\n",
       "  'keyword': 'community detection via measure space embedding',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '5810',\n",
       "  'keyword': 'determinantal point processes without spectral knowledge',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5810', 'keyword': 'inference', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '582',\n",
       "  'keyword': 'probabilistic pattern recognition',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '582',\n",
       "  'keyword': 'combined neural network',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '582',\n",
       "  'keyword': 'based framework',\n",
       "  'year': '1991'},\n",
       " {'RAKE score': 1.0, 'doc_id': '582', 'keyword': 'rule', 'year': '1991'},\n",
       " {'RAKE score': 1.0, 'doc_id': '582', 'keyword': 'discovery', 'year': '1991'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5826',\n",
       "  'keyword': 'dual convex optimization framework',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5826',\n",
       "  'keyword': 'universal primal',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5835',\n",
       "  'keyword': 'subsampled power iteration',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5835',\n",
       "  'keyword': 'unified algorithm',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5835',\n",
       "  'keyword': 'planted csp',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5835',\n",
       "  'keyword': 'block models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5836',\n",
       "  'keyword': 'stationary time series',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5836',\n",
       "  'keyword': 'learning theory',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5836',\n",
       "  'keyword': 'forecasting non',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5836',\n",
       "  'keyword': 'algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5841',\n",
       "  'keyword': 'theoretic lower bounds',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5841',\n",
       "  'keyword': 'erroneous oracles',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5841',\n",
       "  'keyword': 'convex optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5841',\n",
       "  'keyword': 'information',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5846',\n",
       "  'keyword': 'end memory networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5846', 'keyword': 'end', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5854',\n",
       "  'keyword': 'spatial transformer networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5862',\n",
       "  'keyword': 'efficient neuromorphic computing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5862', 'keyword': 'energy', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5862',\n",
       "  'keyword': 'backpropagation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5863',\n",
       "  'keyword': 'optimal point process filtering',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5863',\n",
       "  'keyword': 'tractable approximation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5863',\n",
       "  'keyword': 'neural encoding',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5863',\n",
       "  'keyword': 'application',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5866',\n",
       "  'keyword': 'pointer networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5877',\n",
       "  'keyword': 'length poisson mrf',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5877',\n",
       "  'keyword': 'adding dependencies',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5877',\n",
       "  'keyword': 'multinomial',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5877', 'keyword': 'fixed', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '588',\n",
       "  'keyword': 'radial basis functions',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '588',\n",
       "  'keyword': 'analog vlsi chip',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5885',\n",
       "  'keyword': 'adaptive dimensionality reduction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5885',\n",
       "  'keyword': 'normative theory',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5885',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5888',\n",
       "  'keyword': 'variational consensus monte carlo',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5890',\n",
       "  'keyword': 'free hamiltonian monte carlo',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5890',\n",
       "  'keyword': 'efficient kernel exponential families',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5890', 'keyword': 'gradient', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '591',\n",
       "  'keyword': 'silicon auditory processors',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '591',\n",
       "  'keyword': 'computer peripherals',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5910',\n",
       "  'keyword': 'form games',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5910',\n",
       "  'keyword': 'based pruning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5910', 'keyword': 'regret', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5910', 'keyword': 'extensive', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5912',\n",
       "  'keyword': 'bounding errors',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5912',\n",
       "  'keyword': 'propagation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5912',\n",
       "  'keyword': 'expectation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5913',\n",
       "  'keyword': 'variance reduced optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5913',\n",
       "  'keyword': 'local smoothness',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5916',\n",
       "  'keyword': 'monotonic single index models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5916',\n",
       "  'keyword': 'matrix completion',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5918',\n",
       "  'keyword': 'sampled newton methods',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5918',\n",
       "  'keyword': 'convergence rates',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5918', 'keyword': 'sub', 'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5922',\n",
       "  'keyword': 'filter restless bandits indexable',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5922', 'keyword': 'kalman', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5923',\n",
       "  'keyword': 'coherent risk measures',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5923',\n",
       "  'keyword': 'policy gradient',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5932',\n",
       "  'keyword': 'approximate map inference',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5932',\n",
       "  'keyword': 'continuous mrfs',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5932', 'keyword': 'exactness', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5938',\n",
       "  'keyword': 'scalable methods',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5938',\n",
       "  'keyword': 'graph information',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5938',\n",
       "  'keyword': 'collaborative filtering',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5938',\n",
       "  'keyword': 'consistency',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5941',\n",
       "  'keyword': 'symmetric label noise',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5941', 'keyword': 'unhinged', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5941', 'keyword': 'learning', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5941',\n",
       "  'keyword': 'importance',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5943',\n",
       "  'keyword': 'spherical random features',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5943',\n",
       "  'keyword': 'polynomial kernels',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5947',\n",
       "  'keyword': 'supervised learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5947',\n",
       "  'keyword': 'ladder networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5947', 'keyword': 'semi', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '595',\n",
       "  'keyword': 'moving obstacle avoidance',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '595',\n",
       "  'keyword': 'dynamic programming teacher',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '595',\n",
       "  'keyword': 'temporal planning',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '595',\n",
       "  'keyword': 'learning spatio',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '595',\n",
       "  'keyword': 'forward neurocontrol',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 1.0, 'doc_id': '595', 'keyword': 'feed', 'year': '1992'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5951',\n",
       "  'keyword': 'uncertainty',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5951', 'keyword': 'linearize', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5951', 'keyword': 'learning', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5969',\n",
       "  'keyword': 'sparse local embeddings',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5969',\n",
       "  'keyword': 'label classification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5969',\n",
       "  'keyword': 'extreme multi',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5972',\n",
       "  'keyword': 'learn parametric nonlinear embeddings',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5972',\n",
       "  'keyword': 'universal algorithm',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5972', 'keyword': 'fast', 'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5982',\n",
       "  'keyword': 'resource constrained prediction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5982',\n",
       "  'keyword': 'directed acyclic graph',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5982',\n",
       "  'keyword': 'efficient learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '5987',\n",
       "  'keyword': 'fast lifted map inference via partitioning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5988',\n",
       "  'keyword': 'strong labelers',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5988',\n",
       "  'keyword': 'active learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5988', 'keyword': 'weak', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '60',\n",
       "  'keyword': 'level neurons',\n",
       "  'year': '1987'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '60',\n",
       "  'keyword': 'hopfield model',\n",
       "  'year': '1987'},\n",
       " {'RAKE score': 1.0, 'doc_id': '60', 'keyword': 'multi', 'year': '1987'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6017',\n",
       "  'keyword': 'rank tensor decomposition',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6017', 'keyword': 'sparse', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6017', 'keyword': 'low', 'year': '2015'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '6018',\n",
       "  'keyword': 'robust pca via local incoherence',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6018', 'keyword': 'analysis', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6019',\n",
       "  'keyword': 'uniform generalization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6019',\n",
       "  'keyword': 'algorithmic stability',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6021',\n",
       "  'keyword': 'efficient compressive phase retrieval',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6021',\n",
       "  'keyword': 'constrained sensing vectors',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6022',\n",
       "  'keyword': 'general structural constraints',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6022',\n",
       "  'keyword': 'unified view',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6022',\n",
       "  'keyword': 'matrix completion',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6034',\n",
       "  'keyword': 'concave empirical risk minimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6034',\n",
       "  'keyword': 'fast rates',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6034', 'keyword': 'exp', 'year': '2015'},\n",
       " {'RAKE score': 49.0,\n",
       "  'doc_id': '6038',\n",
       "  'keyword': 'based active learning via adaptive submodular maximization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6038',\n",
       "  'keyword': 'budgeted stream',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6043',\n",
       "  'keyword': 'exploiting piecewise linear structure',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6043',\n",
       "  'keyword': 'unified methods',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6043',\n",
       "  'keyword': 'convex optimization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6047',\n",
       "  'keyword': 'two gaussians',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6047',\n",
       "  'keyword': 'global analysis',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6047',\n",
       "  'keyword': 'expectation maximization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6047', 'keyword': 'mixtures', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6049',\n",
       "  'keyword': 'distributed statistical learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6049',\n",
       "  'keyword': 'bootstrap model aggregation',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6052',\n",
       "  'keyword': 'tight episodic pac rl',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6052', 'keyword': 'near', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6052', 'keyword': 'left', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6052', 'keyword': 'h', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6055', 'keyword': 'process', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6055', 'keyword': 'forget', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6056',\n",
       "  'keyword': 'estimator composition',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '6056',\n",
       "  'keyword': 'robustness',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6063',\n",
       "  'keyword': 'blind signal recovery',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6063', 'keyword': 'structure', 'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6067',\n",
       "  'keyword': 'deep unsupervised perceptual grouping',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6067', 'keyword': 'tagger', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6068',\n",
       "  'keyword': 'shot learners',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6068',\n",
       "  'keyword': 'learning feed',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6068',\n",
       "  'keyword': 'forward one',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6071',\n",
       "  'keyword': 'little training data',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6071',\n",
       "  'keyword': 'joint text recognition',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6071',\n",
       "  'keyword': 'generative shape models',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '6071',\n",
       "  'keyword': 'segmentation',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6072',\n",
       "  'keyword': 'identifying statistical alternatives',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6072', 'keyword': 'power', 'year': '2016'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '6072',\n",
       "  'keyword': 'adaptivity',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6074',\n",
       "  'keyword': 'proximal deep structured models',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6084',\n",
       "  'keyword': 'fast ?- free inference',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6084',\n",
       "  'keyword': 'bayesian conditional density estimation',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6084',\n",
       "  'keyword': 'simulation models',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6085',\n",
       "  'keyword': 'objective online matching',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6085',\n",
       "  'keyword': 'submodular allocations',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6085', 'keyword': 'bi', 'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6092',\n",
       "  'keyword': 'multiple quantile graphical model',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6098',\n",
       "  'keyword': 'discriminative gaifman models',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '610',\n",
       "  'keyword': 'parameterising feature sensitive cell formation',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '610',\n",
       "  'keyword': 'linsker networks',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '610',\n",
       "  'keyword': 'auditory system',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6104',\n",
       "  'keyword': 'fast learning rates',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6104',\n",
       "  'keyword': 'tailed losses',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6104', 'keyword': 'heavy', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6107',\n",
       "  'keyword': 'response learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6107',\n",
       "  'keyword': 'interventional priors',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '6107',\n",
       "  'keyword': 'observational',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6107', 'keyword': 'dose', 'year': '2016'},\n",
       " {'RAKE score': 36.0,\n",
       "  'doc_id': '6108',\n",
       "  'keyword': 'latent variable models via collaborative filtering',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6108',\n",
       "  'keyword': 'nonparametric regression',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6108',\n",
       "  'keyword': 'blind regression',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6126',\n",
       "  'keyword': 'theoretical revisit',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0, 'doc_id': '6126', 'keyword': 'robust k', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6126', 'keyword': 'means', 'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6128',\n",
       "  'keyword': 'normalized spectral map synchronization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6129',\n",
       "  'keyword': 'spreading models',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6129',\n",
       "  'keyword': 'reconstructing parameters',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6129',\n",
       "  'keyword': 'partial observations',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '613',\n",
       "  'keyword': 'bayesian learning via stochastic dynamics',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6131',\n",
       "  'keyword': 'neural representational similarity analysis',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6131',\n",
       "  'keyword': 'reducing bias',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6131',\n",
       "  'keyword': 'bayesian method',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6144',\n",
       "  'keyword': 'order factorization machines',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6144', 'keyword': 'higher', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6148',\n",
       "  'keyword': 'maximum testing power',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6148',\n",
       "  'keyword': 'interpretable distribution features',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6159',\n",
       "  'keyword': 'free online learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6159',\n",
       "  'keyword': 'coin betting',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6159', 'keyword': 'parameter', 'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6160',\n",
       "  'keyword': 'temporal regularized matrix factorization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6160',\n",
       "  'keyword': 'dimensional time series prediction',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6160', 'keyword': 'high', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '617',\n",
       "  'keyword': 'large committee machine',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '617',\n",
       "  'keyword': 'statistical mechanics',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 1.0, 'doc_id': '617', 'keyword': 'learning', 'year': '1992'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6172',\n",
       "  'keyword': 'symmetric skip connections',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6172',\n",
       "  'keyword': 'image restoration using',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6172',\n",
       "  'keyword': 'deep convolutional encoder',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6172',\n",
       "  'keyword': 'decoder networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6177',\n",
       "  'keyword': 'disease trajectory maps',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6178',\n",
       "  'keyword': 'regret bounds',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6178',\n",
       "  'keyword': 'missing labels',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6178',\n",
       "  'keyword': 'decomposable metrics',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6178', 'keyword': 'non', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '618',\n",
       "  'keyword': 'ontogenetic model',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '618',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '618',\n",
       "  'keyword': 'genetic algorithms',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '618',\n",
       "  'keyword': 'interposing',\n",
       "  'year': '1992'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6181',\n",
       "  'keyword': 'learning influence functions',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6181',\n",
       "  'keyword': 'incomplete observations',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 15.5,\n",
       "  'doc_id': '6184',\n",
       "  'keyword': 'supervised convolutional kernel networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '6184',\n",
       "  'keyword': 'end kernel learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 2.0, 'doc_id': '6184', 'keyword': 'end', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6191',\n",
       "  'keyword': 'spectral learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6191',\n",
       "  'keyword': 'nonequilibrium data',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6191',\n",
       "  'keyword': 'dynamic systems',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6192',\n",
       "  'keyword': 'metric learning approach',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6192',\n",
       "  'keyword': 'makes objects similar',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6192',\n",
       "  'keyword': 'unified multi',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6194',\n",
       "  'keyword': 'scene dynamics',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6194',\n",
       "  'keyword': 'generating videos',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6199',\n",
       "  'keyword': 'structured conditional probability matrices',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6199',\n",
       "  'keyword': 'optimal smoothing',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6199', 'keyword': 'near', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6211',\n",
       "  'keyword': 'tensor networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6211',\n",
       "  'keyword': 'supervised learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6214',\n",
       "  'keyword': 'recurrent neural networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6214',\n",
       "  'keyword': 'relu activations',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6214',\n",
       "  'keyword': 'normalized optimization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6214', 'keyword': 'path', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6215',\n",
       "  'keyword': 'recurrent neural networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6215',\n",
       "  'keyword': 'multiplicative integration',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6226',\n",
       "  'keyword': 'product cut',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6238',\n",
       "  'keyword': 'generalized polynomial neural networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6238',\n",
       "  'keyword': 'nonlinear spectral methods',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6238',\n",
       "  'keyword': 'globally optimal training',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '6242',\n",
       "  'keyword': 'conic constrained distributed optimization problems',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6242',\n",
       "  'keyword': 'dual method',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6242', 'keyword': 'primal', 'year': '2016'},\n",
       " {'RAKE score': 49.0,\n",
       "  'doc_id': '6244',\n",
       "  'keyword': 'incomplete tracing data using nonnegative spline regression',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6244',\n",
       "  'keyword': 'high resolution neural connectivity',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6249',\n",
       "  'keyword': 'limited time',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6249',\n",
       "  'keyword': 'human decision',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6249', 'keyword': 'making', 'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6253',\n",
       "  'keyword': 'efficient monte',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6253',\n",
       "  'keyword': 'carlo planning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6253', 'keyword': 'trails', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6253', 'keyword': 'sample', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6253', 'keyword': 'path', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6253', 'keyword': 'blazing', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6253', 'keyword': 'beating', 'year': '2016'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '6260',\n",
       "  'keyword': 'learning user perceived clusters',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6260',\n",
       "  'keyword': 'level supervision',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6260', 'keyword': 'feature', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6264',\n",
       "  'keyword': 'based language learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6264', 'keyword': 'dialog', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6274',\n",
       "  'keyword': 'identifying functional regions',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6274',\n",
       "  'keyword': 'generalized correspondence',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '6274',\n",
       "  'keyword': 'lda models',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.5, 'doc_id': '6274', 'keyword': 'lda', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6274', 'keyword': 'gc', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6274', 'keyword': 'brain', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6275',\n",
       "  'keyword': 'ladder variational autoencoders',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6278',\n",
       "  'keyword': 'modeling structured information',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6278',\n",
       "  'keyword': 'human pose estimation',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6278', 'keyword': 'crf', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6278', 'keyword': 'cnn', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6285',\n",
       "  'keyword': 'step inertial forward',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6285',\n",
       "  'keyword': 'backward splitting method',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6285',\n",
       "  'keyword': 'convex optimization',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6285', 'keyword': 'non', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6285', 'keyword': 'multi', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6288',\n",
       "  'keyword': 'iterative regularization path',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6288',\n",
       "  'keyword': 'structural sparsity',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0, 'doc_id': '6288', 'keyword': 'split lbi', 'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6289',\n",
       "  'keyword': 'supervised binary hashing',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '6289',\n",
       "  'keyword': 'ensemble diversity approach',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6291',\n",
       "  'keyword': 'unsupervised learning',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '6291',\n",
       "  'keyword': 'noisy networks',\n",
       "  'year': '2016'},\n",
       " {'RAKE score': 4.0, 'doc_id': '6291', 'keyword': 'c data', 'year': '2016'},\n",
       " {'RAKE score': 1.0, 'doc_id': '6291', 'keyword': 'hi', 'year': '2016'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '6291',\n",
       "  'keyword': 'applications',\n",
       "  'year': '2016'},\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_list = []\n",
    "\n",
    "for doc in title_list:\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(doc['title'])\n",
    "    keywords = rake.get_ranked_phrases_with_scores()\n",
    "    for word in keywords:\n",
    "        keywords_list.append({\n",
    "            'doc_id': doc['id'],\n",
    "            'year': doc['year'],\n",
    "            'keyword': word[1],\n",
    "            'RAKE score': word[0]\n",
    "        })\n",
    "\n",
    "keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('RAKE_title_keywords.csv', 'w', newline = '') as f:\\n    header_present = False\\n    for doc in keywords_list:\\n        if not header_present:\\n            w = csv.DictWriter(f, doc.keys())\\n            w.writeheader()\\n            header_present = True\\n        w.writerow(doc)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open('RAKE_title_keywords.csv', 'w', newline = '') as f:\n",
    "    header_present = False\n",
    "    for doc in keywords_list:\n",
    "        if not header_present:\n",
    "            w = csv.DictWriter(f, doc.keys())\n",
    "            w.writeheader()\n",
    "            header_present = True\n",
    "        w.writerow(doc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '3168',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \\\\cite{KlampflETAL:07b}. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis {(PCA)} with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals $X$ that are related or are not related to some additional target signal $Y_T$. In a biological interpretation, this target signal $Y_T$ (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.',\n",
       "     'id': '3168',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3172',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.',\n",
       "     'id': '3172',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3174',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Independent component analysis (ICA) is a powerful method to decouple signals. Most of the algorithms performing ICA do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution. Moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations. In this paper, we are interested in understanding the neural mechanism responsible for solving ICA. We present an online learning rule that exploits delayed correlations in the input. This rule performs ICA by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based Hebbian learning rule.',\n",
       "     'id': '3174',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3176',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship, we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering, as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.',\n",
       "     'id': '3176',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3178',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present Epoch-Greedy, an algorithm for multi-armed bandits with observable side information. Epoch-Greedy has the following properties: No knowledge of a time horizon $T$ is necessary. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. The regret scales as $O(T^{2/3} S^{1/3})$ or better (sometimes, much better). Here $S$ is the complexity term in a sample complexity bound for standard supervised learning.',\n",
       "     'id': '3178',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3200',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.',\n",
       "     'id': '3200',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3204',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.',\n",
       "     'id': '3204',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3213',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'On-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e. the movement of the pen, is recorded directly. However, the raw data can be difficult to interpret because each letter is spread over many pen locations. As a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as HMMs. In this paper we describe a system capable of directly transcribing raw on-line handwriting data. The system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark HMM in both cases.',\n",
       "     'id': '3213',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3248',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'When training and test samples follow different input distributions (i.e., the situation called \\\\emph{covariate shift}), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the \\\\emph{importance} (i.e., the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.',\n",
       "     'id': '3248',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5646',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Imaging neuroscience links human behavior to aspects of brain biology in ever-increasing datasets. Existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks. However, testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations. We therefore propose to blend representation modelling and task classification into a unified statistical learning problem. A multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder. We show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset, as well as better generalization to other datasets.',\n",
       "     'id': '5646',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5647',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.',\n",
       "     'id': '5647',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5649',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs).We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality.  This representation also enables a new form of stochastic regularization by randomized modification of resolution.  We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.  Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.',\n",
       "     'id': '5649',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5654',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.In this paper we explore the benefit of using recurrent neural networks to model student learning.This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.We show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.These results suggest a promising new line of research for knowledge tracing.',\n",
       "     'id': '5654',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5657',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose an exploratory approach to statistical model criticism using maximum mean discrepancy (MMD) two sample tests. Typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model. MMD two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy. We demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on. We then apply the procedure to real data where the models being assessed are restricted Boltzmann machines, deep belief networks and Gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.',\n",
       "     'id': '5657',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5658',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In user-facing applications, displaying calibrated confidence measures---probabilities that correspond to true frequency---can be as important as obtaining high accuracy. We are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis. Structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output. We extend the notion of calibration so as to handle various subtleties pertaining to the structured setting,  and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest. We explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.',\n",
       "     'id': '5658',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5664',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of  CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.',\n",
       "     'id': '5664',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5669',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.',\n",
       "     'id': '5669',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5672',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Stochastic search algorithms are general black-box optimizers. Due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search. Yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely. To alleviate these problems, we introduce a new surrogate-based stochastic search approach. We learn simple, quadratic surrogate models of the objective function. As the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models. The algorithm can be misled by an inaccurate optimum introduced by the surrogate. Instead, we use information theoretic constraints to bound the `distance' between the new and old data distribution while maximizing the objective function. Additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence. We compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task.The proposed method considerably outperforms the existing approaches.\",\n",
       "     'id': '5672',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5685',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both  based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than  competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.',\n",
       "     'id': '5685',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5690',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We design an online algorithm to classify the vertices of a graph.   Underpinning the algorithm is the probability distribution of an  Ising model isomorphic to the graph.   Each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far.  Computing these classifications is unfortunately based on a $\\\\#P$-complete problem.  This motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework.    Our algorithm is optimal when the graph is a tree matching the prior results in [1].For a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound. The algorithm is efficient as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.',\n",
       "     'id': '5690',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5706',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call Submodular Partitioning. These problems generalize purely robust instances of the problem, namely max-min submodular fair allocation (SFA) and \\\\emph{min-max submodular load balancing} (SLB), and also average-case instances, that is the submodular welfare problem (SWP) and submodular multiway partition (SMP). While the robust versions have been studied in the theory community, existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This contrasts the average case instances, where most of the algorithms are scalable.  In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.',\n",
       "     'id': '5706',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5713',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate the problem of learning an unknown probability distribution over a discrete population from random samples. Our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing Differential Privacy to the individuals of the population.We describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families. Our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient - both in terms of sample size and running time - as their non-private counterparts. We complement our theoretical guarantees with an experimental evaluation. Our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set.',\n",
       "     'id': '5713',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5714',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a robust portfolio optimization approach based on quantile statistics. The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data. The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.',\n",
       "     'id': '5714',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5748',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem.In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy.This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs.The Counterfactual Risk Minimization (CRM) principle offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator.We show that this conventional estimator suffers from apropensity overfitting problem when used for learning over complex hypothesis spaces.We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem.This naturally gives rise to a new learning algorithm -- Normalized Policy Optimizer for Exponential Models (Norm-POEM) --for structured output prediction using linear rules.We evaluate the empirical effectiveness of Norm-POEM on severalmulti-label classification problems, finding that it consistently outperforms the conventional estimator.',\n",
       "     'id': '5748',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5750',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs)when the number of observations is much larger than the number of coefficients (n > > p > > 1). In this regime, optimization algorithms can immensely benefit fromapproximate second order information.We propose an alternative way of constructing the curvature information by formulatingit as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling andeigenvalue thresholding.Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the case where the rows of the design matrix are i.i.d. samples with bounded support.We show that the convergence has two phases, aquadratic phase followed by a linear phase. Finally,we empirically demonstrate that our algorithm achieves the highest performancecompared to various algorithms on several datasets.',\n",
       "     'id': '5750',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5756',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour,  whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matchingexpectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.',\n",
       "     'id': '5756',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5760',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local-approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale datasets settings. However, EP has a crucial limitation in this context: the number approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP).  Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.',\n",
       "     'id': '5760',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5764',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -- swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions.',\n",
       "     'id': '5764',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5780',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to fit key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.',\n",
       "     'id': '5780',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5782',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.',\n",
       "     'id': '5782',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5785',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce an unsupervised learning algorithmthat combines probabilistic modeling with solver-based techniques for program synthesis.We apply our techniques to both a visual learning domain and a language learning problem,showing that our algorithm can learn many visual concepts from only a few examplesand that it can recover some English inflectional morphology.Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures,and a technique for applying program synthesis tools to noisy data.',\n",
       "     'id': '5785',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5789',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus. Basic statistical considerations state that the reliability of the stimulus information, i.e., the amount of noise in the samples, should be taken into account when the decision is made. However, for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability. We here show that even the basic drift diffusion model, which has frequently been used to explain experimental findings in perceptual decision making, implicitly relies on estimates of stimulus reliability. We then show that only those variants of the drift diffusion model which allow stimulus-specific reliabilities are consistent with neurophysiological findings. Our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds.',\n",
       "     'id': '5789',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5795',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that ex- ploits the so far unused ?geometry? in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADA-grad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.',\n",
       "     'id': '5795',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5798',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We analyze in this paper a random feature map based on a  theory of invariance (\\\\emph{I-theory}) introduced in \\\\cite{AnselmiLRMTP13}. More specifically, a group invariant  signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a  set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting',\n",
       "     'id': '5798',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5801',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.',\n",
       "     'id': '5801',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5804',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop \\\\textit{parallel predictive entropy search} (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a \\\\textit{batch} of points which will maximize the information gain about the global maximizer of the objective.  Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.',\n",
       "     'id': '5804',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5808',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We present a new algorithm for community detection. The algorithm uses random walks to embed the graph in a space of measures, after which a modification of $k$-means in that space is applied. The algorithm is therefore fast and easily parallelizable. We evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks,  and find its performance to be better or at least as good as previously known  algorithms. We also prove a linear time (in number of edges) guarantee for the algorithm on a $p,q$-stochastic block model with  where $p \\\\geq c\\\\cdot N^{-\\\\half +  \\\\epsilon}$ and $p-q \\\\geq c' \\\\sqrt{p N^{-\\\\half +  \\\\epsilon} \\\\log N}$.\",\n",
       "     'id': '5808',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5810',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Determinantal point processes (DPPs) are point process models thatnaturally encode diversity between the points of agiven realization, through a positive definite kernel $K$. DPPs possess desirable properties, such as exactsampling or analyticity of the moments, but learning the parameters ofkernel $K$ through likelihood-based inference is notstraightforward. First, the kernel that appears in thelikelihood is not $K$, but another kernel $L$ related to $K$ throughan often intractable spectral decomposition. This issue is typically bypassed in machine learning bydirectly parametrizing the kernel $L$, at the price of someinterpretability of the model parameters. We follow this approachhere. Second, the likelihood has an intractable normalizingconstant, which takes the form of large determinant in the case of aDPP over a finite set of objects, and the form of a Fredholm determinant in thecase of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood ofa DPP, both for finite and continuous domains. Unlike previous work, our bounds arecheap to evaluate since they do not rely on approximating the spectrumof a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variationalinference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs.',\n",
       "     'id': '5810',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5826',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template. The algorithmic instances of our framework are universal since they can automatically adapt to the unknown Holder continuity degree and constant within the dual formulation. They are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each Holder smoothness degree. In contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function. We instead leverage computationally cheaper, Fenchel-type operators, which are the main workhorses of the generalized conditional gradient (GCG)-type methods. In contrast to the GCG-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.',\n",
       "     'id': '5826',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5835',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present an algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems (CSP), via a common generalization in terms of random bipartite graphs. Our algorithm matches up to a constant factor the best-known bounds  for the number of edges (or constraints) needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is significantly better than both spectral and SDP-based approaches.The main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted CSP.  Here our algorithm succeeds at a significantly lower density than the spectral approaches, surpassing a barrier based on the spectral norm of a random matrix.Other significant features of the algorithm and analysis include (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) the algorithm can be implemented statistically, i.e., with very limited access to the input distribution (iii) the algorithm is extremely simple to implement and runs in linear time, and thus is practical even for very large instances.',\n",
       "     'id': '5835',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5836',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. We use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.',\n",
       "     'id': '5836',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5841',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle. In particular, for a given function $x \\\\to f(x)$ we consider optimization when one is given access to absolute error oracles that return values in [f(x) - \\\\epsilon,f(x)+\\\\epsilon] or relative error oracles that return value in [(1+\\\\epsilon)f(x), (1 +\\\\epsilon)f (x)], for some \\\\epsilon larger than 0. We show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.',\n",
       "     'id': '5841',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5846',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.',\n",
       "     'id': '5846',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5854',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Convolutional Neural Networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner. In this work we introduce a new learnable module, theSpatial Transformer, which explicitly allows the spatial manipulation ofdata within the network. This differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself,without any extra training supervision or modification to the optimisation process. We show that the useof spatial transformers results in models which learn invariance to translation,scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.',\n",
       "     'id': '5854',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5862',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient. For the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets.  For the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses.  Our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging. To demonstrate, we trained a sparsely connected network that runs on the TrueNorth chip using the MNIST dataset. With a high performance network (ensemble of $64$), we achieve $99.42\\\\%$ accuracy at $121 \\\\mu$J per image, and with a high efficiency network (ensemble of $1$) we achieve $92.7\\\\%$ accuracy at $0.408 \\\\mu$J per image.',\n",
       "     'id': '5862',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5863',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance.',\n",
       "     'id': '5863',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5866',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence.Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in eachstep of the output depends on the length of the input, which is variable.Problems such as sorting variable sized sequences, and various combinatorialoptimization problems belong to this class.  Our model solvesthe problem of variable size output dictionaries using a recently proposedmechanism of neural attention. It differs from the previous attentionattempts in that, instead of using attention to blend hidden units of anencoder to a context vector at each decoder step, it uses attention asa pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).We show Ptr-Nets can be used to learn approximate solutions to threechallenging geometric problems -- finding planar convex hulls, computingDelaunay triangulations, and the planar Travelling Salesman Problem-- using training examples alone. Ptr-Nets not only improve oversequence-to-sequence with input attention, butalso allow us to generalize to variable size output dictionaries.We show that the learnt models generalize beyond the maximum lengthsthey were trained on. We hope our results on these taskswill encourage a broader exploration of neural learning for discreteproblems.',\n",
       "     'id': '5866',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5885',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.',\n",
       "     'id': '5885',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5888',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (Scott et al, 2013). A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC, achieving near-ideal speedup in some instances.',\n",
       "     'id': '5888',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5890',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.\",\n",
       "     'id': '5890',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5910',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Counterfactual Regret Minimization (CFR) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games. CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set.We introduce an improvement to CFR that prunes any path of play in the tree, and its descendants, that has negative regret. It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration. The new algorithm maintains CFR's convergence guarantees while making iterations significantly faster---even if previously known pruning techniques are used in the comparison. This improvement carries over to CFR+, a recent variant of CFR. Experiments show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game.\",\n",
       "     'id': '5910',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5912',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Expectation Propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP's convergence with respect to the true posterior. In particular, we show that EP converges at a rate of $O(n^{-2})$ for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.\",\n",
       "     'id': '5912',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5918',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting -- maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extraround of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation.  On the empirical side, the recent work in (Gu et al. 2012) and (Gu et al. 2014) (on active linear and logistic regression) shows the promise of this approach.',\n",
       "     'id': '5918',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5922',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the restless bandit associated with an extremely simple scalar Kalman filter model in discrete time. Under certain assumptions, we prove that the problem is {\\\\it indexable} in the sense that the {\\\\it Whittle index} is a non-decreasing function of the relevant belief state. In spite of the long history of this problem, this appears to be the first such proof. We use results about {\\\\it Schur-convexity} and {\\\\it mechanical words}, which are particularbinary strings intimately related to {\\\\it palindromes}.',\n",
       "     'id': '5922',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5923',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.',\n",
       "     'id': '5923',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5932',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Computing the MAP assignment in graphical models is generally intractable.  As a result, for discrete graphical models, the MAP problem is often approximated using linear programming relaxations.  Much research has focused on characterizing when these LP relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog.  In this work, we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight.  We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models.  We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.',\n",
       "     'id': '5932',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5938',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.',\n",
       "     'id': '5938',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5941',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss? SLN-robustness.',\n",
       "     'id': '5941',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5943',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials. Meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit l2 norm. The question we address in this work is: if we know a priori that data is so normalized, can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting, and introduce a new approximation paradigm, Spherical Random Fourier (SRF) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work, SRF features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy.',\n",
       "     'id': '5943',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5947',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.',\n",
       "     'id': '5947',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5951',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Training deep feature hierarchies to solve supervised learning tasks has achieving state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabelednatural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing a latent variables that are non-deterministic functions of the input into the network architecture.',\n",
       "     'id': '5951',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5969',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches make training and prediction tractable by assuming that the training label matrix is low-rank and hence the effective number of labels can be reduced by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies or scale to large problems as the low rank assumption is violated in most real world applications.This paper develops the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world as well as benchmark data sets and compare our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embeddings (by as much as 35%) as well as trees (by as much as 6%). SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.',\n",
       "     'id': '5969',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5972',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N-body methods developed for nonlinear embeddings, yielding linear-time iterations.\",\n",
       "     'id': '5972',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5982',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.',\n",
       "     'id': '5982',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5987',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recently, there has been growing interest in lifting MAP inference algorithms for Markov logic networks (MLNs). A key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the MLN and these symmetries can be detected using lifted inference rules. Unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries. This is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the MLN, and search for solutions in the much larger propositional space. In this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding. Our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable. We show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space MAP inference algorithms. Our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.',\n",
       "     'id': '5987',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5988',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible.This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.',\n",
       "     'id': '5988',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6017',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition. We present an efficient computational algorithm that modifies Leurgans' algoirthm for tensor factorization. Our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction. We use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor. We delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm. We validate our algorithm with numerical experiments.\",\n",
       "     'id': '6017',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6018',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate the robust PCA problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming Principal Component Pursuit (PCP). In contrast to previous studies that assume the support of the error matrix is generated by uniform Bernoulli sampling, we allow non-uniform sampling, i.e., entries of the low-rank matrix are corrupted by errors with unequal probabilities. We characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix, under which correct matrix decomposition by PCP is guaranteed. Such a refined analysis of robust PCA captures how robust each entry of the low rank matrix combats error corruption. In order to deal with non-uniform error corruption, our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies.',\n",
       "     'id': '6018',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6019',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result.  For instance,  a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.',\n",
       "     'id': '6019',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6021',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements. The proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially.In recent years, various methods are proposed for compressive phase retrieval, but they have suboptimal sample complexity or lack robustness guarantees. The main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target. Given a set of underdetermined measurements, there is a standard framework for recovering a sparse matrix, and a standard framework for recovering a low-rank matrix. However, a general, efficient method for recovering a jointly sparse and low-rank matrix has remained elusive.Deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace, then the low-rank and sparse structures of the target signal can be effectively decoupled. We show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is $\\\\mathsf{O}(k\\\\,\\\\log\\\\frac{d}{k})$, where $k$ and $d$ denote the sparsity level and the dimension of the input signal. We also evaluate the algorithm through numerical simulation.',\n",
       "     'id': '6021',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6022',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms. In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by {\\\\em any} norm regularization.We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain \\\\textit{\\\\modified}~complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably including the recently proposed spectral $k$-support norm.',\n",
       "     'id': '6022',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6034',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider Empirical Risk Minimization (ERM) in the context of stochastic optimization with exp-concave and smooth losses---a general optimization framework that captures several important learning problems including linear and logistic regression, learning SVMs with the squared hinge-loss, portfolio selection and more. In this setting, we establish the first evidence that ERM is able to attain fast generalization rates, and show that the expected loss of the ERM solution in $d$ dimensions converges to the optimal expected loss in a rate of $d/n$. This rate matches existing lower bounds up to constants and improves by a $\\\\log{n}$ factor upon the state-of-the-art, which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.',\n",
       "     'id': '6034',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6038',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, and prove this class includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness comparing with existing heuristics on common benchmark datasets.',\n",
       "     'id': '6038',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6043',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times. By considering a novel problem formulation?the minimization of a sum of piecewise functions?we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization. This result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization. In empirical comparisons, we study the scalability of our methods. We find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.',\n",
       "     'id': '6043',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6047',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models.  However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer.  This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties.  Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians.  This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.',\n",
       "     'id': '6047',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6049',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.',\n",
       "     'id': '6049',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6052',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many applications such as advertisement placement or automated dialog systems, an intelligent system optimizes performance over a sequence of interactions with each user. Such tasks often involve many states and potentially time-dependent transition dynamics, and can be modeled well as episodic Markov decision processes (MDPs). In this paper, we present a PAC algorithm for reinforcement learning in episodic finite MDPs with time-dependent transitions that acts epsilon-optimal in all but O(S A H^3  / epsilon^2 log(1 / delta)) episodes. Our algorithm has a polynomial computational complexity, and our sample complexity bound accounts for the fact that we may only be able to approximately solve the internal planning problems. In addition, our PAC sample complexity bound has only linear dependency on the number of states S and actions A and strictly improves previous bounds with S^2 dependency in this setting. Compared against other methods for infinite horizon reinforcement learning with linear state space sample complexity our method has much lower dependency on the (effective) horizon. Indeed, our bound is optimal up to a factor of H.',\n",
       "     'id': '6052',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6055',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce the Forget-me-not Process, an efficient, non-parametric meta-algorithm for online probabilistic sequence prediction for piecewise stationary, repeating sources. Our method works by taking a Bayesian approach to partition a stream of data into postulated task-specific segments, while simultaneously building a model for each task. We provide regret guarantees with respect to piecewise stationary data sources under the logarithmic loss, and validate the method empirically across a range of sequence prediction and task identification problems.',\n",
       "     'id': '6055',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6056',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We formalize notions of robustness for composite estimators via the notion of a breakdown point.  A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.',\n",
       "     'id': '6056',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6063',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of recovering a signal observed in Gaussian noise. If the set of signals is convex and compact, and can be specified beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk. However, when the set is unspecified, designing an estimator that is blind to the hidden structure of the signal remains a challenging problem. We propose a new family of estimators to recover signals observed in Gaussian noise. Instead of specifying the set where the signal lives, we assume the existence of a well-performing linear estimator. Proposed estimators enjoy exact oracle inequalities and can be efficiently computed through convex optimization. We present several numerical illustrations that show the potential of the approach.',\n",
       "     'id': '6063',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6067',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features.  Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism.  We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations.  In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.',\n",
       "     'id': '6067',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6068',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.',\n",
       "     'id': '6068',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6071',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.',\n",
       "     'id': '6071',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6072',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. We focus on the most biased coin problem, asking how many total coin flips are required to identify a ``heavy\\'\\' coin from an infinite bag containing both ``heavy\\'\\' coins with mean $\\\\theta_1 \\\\in (0,1)$, and ``light\" coins with mean $\\\\theta_0 \\\\in (0,\\\\theta_1)$, where heavy coins are drawn from the bag with proportion $\\\\alpha \\\\in (0,1/2)$. When $\\\\alpha,\\\\theta_0,\\\\theta_1$ are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters $\\\\theta_0,\\\\theta_1,\\\\alpha$, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples.  In characterizing this gap between adaptive and nonadaptive strategies,  we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.',\n",
       "     'id': '6072',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6074',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many problems in real-world applications involve predicting continuous-valued random variables that are statistically related.  In this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the   dependencies between continuous output variables.  We show that  inference in our  model using proximal methods can be efficiently solved as a feed-foward pass of a special  type of  deep recurrent neural network. We demonstrate the  effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.',\n",
       "     'id': '6074',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6084',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ?-ball around the observed data, which is only correct in the limit ??0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ??0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.',\n",
       "     'id': '6084',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6085',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability.  In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as ?share of voice?, and ?buyer surplus?. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.',\n",
       "     'id': '6085',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6092',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce the Multiple Quantile Graphical Model (MQGM), which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models.  The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others.  Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates.  We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model. We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers.  We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate. Lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data.',\n",
       "     'id': '6092',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6104',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i)  the envelope function $\\\\sup_{f \\\\in \\\\mathcal{F}}|\\\\ell \\\\circ f|$, where $\\\\ell$ is the loss function and $\\\\mathcal{F}$ is the hypothesis class, exists and is $L^r$-integrable, and (ii) $\\\\ell$ satisfies the multi-scale Bernstein's condition on $\\\\mathcal{F}$. Under these assumptions, we prove that learning rate faster than $O(n^{-1/2})$ can be obtained and, depending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily close to $O(n^{-1})$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.\",\n",
       "     'id': '6104',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6107',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.',\n",
       "     'id': '6107',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6108',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We introduce the framework of {\\\\em blind regression} motivated by {\\\\em matrix completion} for recommendation systems: given $m$ users, $n$ movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user $u$ and movie $i$ have features $x_1(u)$ and $x_2(i)$ respectively, and their corresponding rating $y(u,i)$ is a noisy measurement of $f(x_1(u), x_2(i))$ for some unknown function $f$. In contrast with classical regression, the features $x = (x_1(u), x_2(i))$ are not observed, making it challenging to apply standard regression methods to  predict the unobserved ratings.  Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least $\\\\max(m^{-1+\\\\delta},n^{-1/2+\\\\delta})$ with $\\\\delta > 0$, we prove that the expected fraction of our estimates with error greater than $\\\\epsilon$ is less than $\\\\gamma^2 / \\\\epsilon^2$ plus a polynomially decaying term, where $\\\\gamma^2$ is the variance of the additive entry-wise noise term.  Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods.\",\n",
       "     'id': '6108',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6126',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Over the last years, many variations of the quadratic k-means clustering procedure have been proposed, all aiming to robustify the performance of the algorithm in the presence of outliers. In general terms, two main approaches have been developed: one based on penalized regularization methods, and one based on trimming functions. In this work, we present a theoretical analysis of the robustness and consistency properties of a variant of the classical quadratic k-means algorithm, the robust k-means, which borrows ideas from outlier detection in regression. We show that two outliers in a dataset are enough to breakdown this clustering procedure. However, if we focus on ?well-structured? datasets, then robust k-means can recover the underlying cluster structure in spite of the outliers. Finally, we show that, with slight modifications, the most general non-asymptotic results for consistency of quadratic k-means remain valid for this robust variant.',\n",
       "     'id': '6126',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6128',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The algorithmic advancement of synchronizing maps is important in order to solve a wide range of practice problems  with possible large-scale dataset. In this paper, we provide theoretical justifications for spectral techniques for the map synchronization problem, i.e., it takes as input a collection of objects and noisy maps estimated between pairs of objects, and outputs clean maps between all pairs of objects. We show that a simple normalized spectral method that projects the blocks of the top eigenvectors of a data matrix to the map space leads to surprisingly good results. As the noise is modelled naturally as random permutation matrix, this algorithm NormSpecSync leads to competing theoretical guarantees as state-of-the-art convex optimization techniques, yet it is much more efficient. We demonstrate the usefulness of our algorithm in a couple of applications, where it is optimal in both complexity and exactness among existing methods.',\n",
       "     'id': '6128',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6129',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs.',\n",
       "     'id': '6129',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6131',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In neuroscience, the similarity matrix of neural activity patterns in response to different sensory stimuli or under different cognitive states reflects the structure of neural representational space. Existing methods derive point estimations of neural activity patterns from noisy neural imaging data, and the similarity is calculated from these point estimations. We show that this approach translates structured noise from estimated patterns into spurious bias structure in the resulting similarity matrix, which is especially severe when signal-to-noise ratio is low and experimental conditions cannot be fully randomized in a cognitive task. We propose an alternative Bayesian framework for computing representational similarity in which we treat the covariance structure of neural activity patterns as a hyper-parameter in a generative model of the neural data, and directly estimate this covariance structure from imaging data while marginalizing over the unknown activity patterns. Converting the estimated covariance structure into a correlation matrix offers a much less biased estimate of neural representational similarity. Our method can also simultaneously estimate a signal-to-noise map that informs where the learned representational structure is supported more strongly, and the learned covariance matrix can be used as a structured prior to constrain Bayesian estimation of neural activity patterns. Our code is freely available in Brain Imaging Analysis Kit (Brainiak) (https://github.com/IntelPNI/brainiak), a python toolkit for brain imaging analysis.',\n",
       "     'id': '6131',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6144',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy.  We demonstrate the proposed approaches on four different link prediction tasks.',\n",
       "     'id': '6144',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6148',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.',\n",
       "     'id': '6148',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6159',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice.  These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices.  We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator.  The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.',\n",
       "     'id': '6159',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6172',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.',\n",
       "     'id': '6172',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6177',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Longitudinal data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of longitudinal EHR data. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled longitudinal data. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.',\n",
       "     'id': '6177',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6184',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard ``deep learning'' datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.\",\n",
       "     'id': '6184',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6191',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.',\n",
       "     'id': '6191',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6192',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Linkages are essentially determined by similarity measures that may be derived from multiple perspectives. For example, spatial linkages are usually generated based on localities of heterogeneous data, whereas semantic linkages can come from various properties, such as different physical meanings behind social relations. Many existing metric learning models focus on spatial linkages, but leave the rich semantic factors unconsidered. Similarities based on these models are usually overdetermined on linkages. We propose a Unified Multi-Metric Learning (UM2L) framework to exploit multiple types of metrics. In UM2L, a type of combination operator is introduced for distance characterization from multiple perspectives, and thus can introduce flexibilities for representing and utilizing both spatial and semantic linkages. Besides, we propose a uniform solver for UM2L which is guaranteed to converge. Extensive experiments on diverse applications exhibit the superior classification performance and comprehensibility of UM2L. Visualization results also validate its ability on physical meanings discovery.',\n",
       "     'id': '6192',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6194',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.\",\n",
       "     'id': '6194',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6199',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Utilizing the structure of a probabilistic model can significantly increase its learning speed. Motivated by several recent applications, in particular bigram models in language processing, we consider learning low-rank conditional probability matrices under expected KL-risk. This choice makes smoothing, that is the careful handling of low-probability elements, paramount. We derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk. We then derive sample-complexity bounds for the global minimizer of the penalized risk and show that it is within a small factor of the optimal sample complexity. This framework generalizes to more sophisticated smoothing techniques, including absolute-discounting.',\n",
       "     'id': '6199',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6211',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1% test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.',\n",
       "     'id': '6211',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6214',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.',\n",
       "     'id': '6214',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6215',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a general simple structural design called ?Multiplicative Integration? (MI) to improve recurrent neural networks (RNNs). MI changes the way of how the information flow gets integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.',\n",
       "     'id': '6215',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6226',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a theoretical and algorithmic framework for multi-way graph partitioning that relies on a multiplicative cut-based objective. We refer to this objective as the Product Cut. We provide a detailed investigation of the mathematical properties of this objective and an effective algorithm for its optimization. The proposed model has strong mathematical underpinnings, and the corresponding algorithm achieves state-of-the-art performance on benchmark data sets.',\n",
       "     'id': '6226',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6238',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward  neural networks can be trained globally optimal with a linear convergence rate. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one- and two hidden layer networks. Our experiments confirms that these models are already rich enough to achieve good performance on a series of real-world datasets.',\n",
       "     'id': '6238',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6242',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider cooperative multi-agent consensus optimization problems over an undirected network of agents, where only those agents connected by an edge can directly communicate. The objective is to minimize the sum of agent-specific composite convex functions over agent-specific private conic constraint sets; hence, the optimal consensus decision should lie in the intersection of these private sets. We provide convergence rates in sub-optimality, infeasibility and consensus violation; examine the effect of underlying network topology on the convergence rates of the proposed decentralized algorithms; and show how to extend these methods to handle time-varying communication networks.',\n",
       "     'id': '6242',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6253',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the sampling-based planning problem in Markov decision processes (MDPs) that we can access only through a generative model, usually referred to as Monte-Carlo planning. Our objective is to return a good estimate of the optimal value function at any state while minimizing the number of calls to the generative model, i.e. the sample complexity. We propose a new algorithm, TrailBlazer, able to handle MDPs with a finite or an infinite number of transitions from state-action to next states. TrailBlazer is an adaptive algorithm that exploits possible structures of the MDP by exploring only a subset of states reachable by following near-optimal policies. We provide bounds on its sample complexity that depend on a measure of the quantity of near-optimal states. The algorithm behavior can be considered as an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). Finally, another appealing feature of TrailBlazer is that it is simple to implement and computationally efficient.',\n",
       "     'id': '6253',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6260',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Semi-supervised clustering algorithms have been proposed to identify data clusters that align with user perceived ones via the aid of side information such as seeds or pairwise constrains. However, traditional side information is mostly at the instance level and subject to the sampling bias, where non-randomly sampled instances in the supervision can mislead the algorithms to wrong clusters. In this paper, we propose learning from the feature-level supervision. We show that this kind of supervision can be easily obtained in the form of perception vectors in many applications. Then we present novel algorithms, called Perception Embedded (PE) clustering, that exploit the perception vectors as well as traditional side information to find clusters perceived by the user. Extensive experiments are conducted on real datasets and the results demonstrate the effectiveness of PE empirically.',\n",
       "     'id': '6260',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6264',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\",\n",
       "     'id': '6264',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6275',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.',\n",
       "     'id': '6275',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6278',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks. Finally, a neural network implementation of end-to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.',\n",
       "     'id': '6278',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6285',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we propose a multi-step inertial Forward--Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the scheme with the help of the Kurdyka??ojasiewicz property. Then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on a few problems arising from statistics and machine learning.',\n",
       "     'id': '6285',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6288',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called \\\\emph{Split LBI}. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some $\\\\ell_2$ error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking.',\n",
       "     'id': '6288',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6289',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.',\n",
       "     'id': '6289',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6291',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Complex networks play an important role in a plethora of disciplines in natural sciences. Cleaning up noisy observed networks, poses an important challenge in network analysis Existing methods utilize labeled data to alleviate the noise effect in the network.  However, labeled data is usually expensive to collect while unlabeled data can be gathered cheaply. In this paper, we propose an optimization framework to mine useful structures from noisy networks in an unsupervised manner. The key feature of our optimization framework is its ability to utilize local structures as well as global patterns in the network. We extend our method to incorporate multi-resolution networks in order to add further resistance to high-levels of noise. We also generalize our framework to utilize partial labels to enhance the performance. We specifically focus our method on multi-resolution Hi-C data by recovering clusters of genomic regions that co-localize in 3D space. Additionally, we use Capture-C-generated partial labels to further denoise the Hi-C network. We empirically demonstrate the effectiveness of our framework in denoising the network and improving community detection results.',\n",
       "     'id': '6291',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6292',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the sum of these vectors doesn't exceed the budget in each dimension. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits  (linContextual),  bandits with knapsacks (BwK), and the online stochastic packing problem (OSPP). We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem, where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through  an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases.\",\n",
       "     'id': '6292',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6293',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications. These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset. However, the high variance inherent in these noisy gradients degrades performance and leads to slower mixing. In this paper, we present techniques for reducing variance in stochastic gradient Langevin dynamics, yielding novel  stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics. This is complemented by impressive empirical results obtained on  a variety of real world datasets, and on four different machine learning tasks (regression, classification, independent component analysis and mixture modeling). These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods.',\n",
       "     'id': '6293',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6302',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization  of a least square criterion under a multilinear rank constraint, a difficult  non convex problem.  HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR computes accurate solutions while being computationally very competitive.',\n",
       "     'id': '6302',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6310',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. Current RNN models are ill suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors which generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences.   The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information.  It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes.',\n",
       "     'id': '6310',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6317',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Experiments reveal that in the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas, where visual and vestibular cues are integrated to infer heading direction, there are two types of neurons with roughly the same number. One is ?congruent? cells, whose preferred heading directions are similar in response to visual and vestibular cues; and the other is ?opposite? cells, whose preferred heading directions are nearly ?opposite? (with an offset of 180 degree) in response to visual vs. vestibular cues. Congruent neurons are known to be responsible for cue integration, but the computational role of opposite neurons remains largely unknown. Here, we propose that opposite neurons may serve to encode the disparity information between cues necessary for multisensory segregation. We build a computational model composed of two reciprocally coupled modules, MSTd and VIP, and each module consists of groups of congruent and opposite neurons. In the model, congruent neurons in two modules are reciprocally connected with each other in the congruent manner, whereas opposite neurons are reciprocally connected in the opposite manner. Mimicking the experimental protocol, our model reproduces the characteristics of congruent and opposite neurons, and demonstrates that in each module, the sisters of congruent and opposite neurons can jointly achieve optimal multisensory information integration and segregation. This study sheds light on our understanding of how the brain implements optimal multisensory integration and segregation concurrently in a distributed manner.',\n",
       "     'id': '6317',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6323',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and capture into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.',\n",
       "     'id': '6323',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6324',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with $M \\\\geq 3$ components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro (2007). Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least $1-e^{-\\\\Omega(M)}$. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.',\n",
       "     'id': '6324',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6325',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the cost function for  hierarchical clusterings introduced by [Dasgupta, 2015]  where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [Dasgupta, 2015] that a top-down algorithm  returns a hierarchical clustering of cost at most \\\\(O\\\\left(\\\\alpha_n \\\\log n\\\\right)\\\\) times the cost of the optimal hierarchical clustering, where \\\\(\\\\alpha_n\\\\) is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani,  the top down algorithm returns a hierarchical clustering of cost at most  \\\\(O\\\\left(\\\\log^{3/2} n\\\\right)\\\\) times the cost of the optimal solution. We improve this by giving an \\\\(O(\\\\log{n})\\\\)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by  using the idea of \\\\emph{sphere growing} which has been extensively used in the context of graph  partitioning. We also prove that our algorithm returns an \\\\(O(\\\\log{n})\\\\)-approximate  hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2015]. Experiments show that the hierarchies found by using the ILP formulation as well  as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with an inapproximability result for this problem, namely that no polynomial sized LP or SDP can be used to obtain a constant factor approximation for this problem.',\n",
       "     'id': '6325',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6326',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results. Here we introduce KiloSort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering. KiloSort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved. Unlike previous algorithms that compress the data with PCA, KiloSort operates on the raw data which allows it to construct a more accurate model of the waveforms. Processing times are faster than in previous algorithms thanks to batch-based optimization on GPUs. We compare KiloSort to an established algorithm and show favorable performance, at much reduced processing times. A novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings.',\n",
       "     'id': '6326',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6332',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.\",\n",
       "     'id': '6332',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6334',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.',\n",
       "     'id': '6334',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6336',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new boosting algorithm for the key scenario of binary classification with abstention where the algorithm can abstain from predicting the label of a point, at the price of a fixed cost.  At each round, our algorithm selects a pair of functions, a base predictor and a base abstention function.  We define convex upper bounds for the natural loss function associated to this problem, which we prove to be calibrated with respect to the Bayes solution. Our algorithm benefits from general margin-based learning guarantees which we derive for ensembles of pairs of base predictor and abstention functions, in terms of the Rademacher complexities of the corresponding function classes.  We give convergence guarantees for our algorithm along with a linear-time weak-learning algorithm for abstention stumps. We also report the results of several experiments suggesting that our algorithm provides a significant improvement in practice over two confidence-based algorithms.',\n",
       "     'id': '6336',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6338',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein?s identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.',\n",
       "     'id': '6338',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6342',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this work, we propose an infinite restricted Boltzmann machine (RBM), whose maximum likelihood estimation (MLE) corresponds to a constrained convex optimization.  We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity.  As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization. The resulting model can also be used as an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.',\n",
       "     'id': '6342',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6343',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Several works have shown that deep CNN classifiers can be easily transferred across datasets, e.g. the transfer of a CNN trained to recognize objects on ImageNET to an object detector on Pascal VOC. Less clear, however, is the ability of CNNs to transfer knowledge across tasks. A common example of such transfer is the problem of scene classification that should leverage localized object detections to recognize holistic visual concepts. While this problem is currently addressed with Fisher vector representations, these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern CNNs. It is argued that this is mostly due to the reliance on a model, the Gaussian mixture of diagonal covariances, which has a very limited ability to capture the second order statistics of CNN features. This problem is addressed by the adoption of a better model, the mixture of factor analyzers (MFA), which approximates the non-linear data manifold by a collection of local subspaces. The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an image representation for holistic image classifiers. Extensive experiments show that the MFA-FS has state of the art performance for object-to-scene transfer and this transfer actually  outperforms the training of a scene CNN from a large scene dataset. The two representations are also shown to be complementary, in the sense that their combination outperforms each of the representations by itself. When combined, they produce a state of the art scene classifier.',\n",
       "     'id': '6343',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6348',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bayesian nonparametric  methods based on the Dirichlet process (DP), gamma process and beta process, have proven effective in capturing aspects of various datasets arising in machine learning.  However, it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior. As such there is now considerable interest in models based on the Stable Processs (SP), Generalized Gamma process (GGP) and Stable-beta process (SBP). These models present new challenges in terms of practical statistical implementation. In analogy to tractable processes such as the finite-dimensional Dirichlet process, we describe a class of random processes, we call iid finite-dimensional BFRY processes, that enables one to begin to develop efficient posterior inference algorithms such as variational Bayes that readily scale to massive datasets. For illustrative purposes, we describe a simple variational Bayes algorithm for normalized SP mixture models, and demonstrate its usefulness with experiments on synthetic and real-world datasets.',\n",
       "     'id': '6348',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6355',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce the study of fairness in multi-armed bandit problems. Our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm?s uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on ?chained? confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.',\n",
       "     'id': '6355',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6363',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider tractable representations of probability distributions and the polytime operations they support.  In particular, we consider a recently proposed arithmetic circuit representation, the Probabilistic Sentential Decision Diagram (PSDD).  We show that PSDD supports a polytime multiplication operator, while they do not support a polytime operator for summing-out variables.  A polytime multiplication operator make PSDDs suitable for a broader class of applications compared to arithmetic circuits, which do not in general support multiplication.  As one example, we show that PSDD multiplication leads to a very simple but effective compilation algorithm for probabilistic graphical models: represent each model factor as a PSDD, and then multiply them.',\n",
       "     'id': '6363',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6365',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms. For the {\\\\it detection} problem in symmetric SBMs, Decelle et al.\\\\ conjectured that the so-called Kesten-Stigum (KS) threshold can be achieved efficiently. This was proved for two communities, but remained open from three communities. We prove this conjecture here, obtaining a more general result that applies to arbitrary SBMs with linear size communities. The developed algorithm is a linearized acyclic belief propagation (ABP) algorithm, which mitigates the effects of cycles while provably achieving the KS threshold in $O(n \\\\ln n)$ time. This extends prior methods by achieving universally the KS threshold while reducing or preserving the computational complexity. ABP is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in Krzakala et al., and extending results from Bordenave et al.',\n",
       "     'id': '6365',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6374',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.',\n",
       "     'id': '6374',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6380',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.',\n",
       "     'id': '6380',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6382',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary?solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.',\n",
       "     'id': '6382',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6383',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.\",\n",
       "     'id': '6383',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6387',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of convolutional neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.',\n",
       "     'id': '6387',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6394',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure, and limited progress has been made on matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector based on sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures such as Gaussian widths of suitable sets associated with the structure of the underlying true matrix. Further, we derive general bounds on these geometric measures for structures characterized by unitarily invariant norms, a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development.',\n",
       "     'id': '6394',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6395',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL),  finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.',\n",
       "     'id': '6395',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6400',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a new oracle-based algorithm, BISTRO+, for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order $O((KT)^{\\\\frac{2}{3}}(\\\\log N)^{\\\\frac{1}{3}})$, where $K$ is the number of actions, $T$ is the number of iterations, and $N$ is the number of baseline policies. Our result is the first to break the $O(T^{\\\\frac{3}{4}})$ barrier achieved by recent algorithms, which was left as a major open problem. Our analysis employs the recent relaxation framework of (Rakhlin and Sridharan, ICML'16).\",\n",
       "     'id': '6400',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6417',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Non-negative matrix factorization is a popular tool for  decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.',\n",
       "     'id': '6417',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6421',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables.  To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages.  We present applications of our method to a number of tasks including engineering design and parameter optimization.',\n",
       "     'id': '6421',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6422',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many applications of machine learning involve structured output with large domain, where learning of structured predictor is prohibitive due to repetitive calls to expensive inference oracle. In this work, we show that, by decomposing training of Structural Support Vector Machine (SVM) into a series of multiclass SVM problems connected through messages, one can replace expensive structured oracle with Factorwise Maximization Oracle (FMO) that allows efficient implementation of complexity sublinear to the factor domain. A Greedy Direction Method of Multiplier (GDMM) algorithm is proposed to exploit sparsity of messages which guarantees $\\\\epsilon$ sub-optimality after $O(log(1/\\\\epsilon))$ passes of FMO calls. We conduct experiments on chain-structured problems and fully-connected problems of large output domains. The proposed approach is orders-of-magnitude faster than the state-of-the-art training algorithms for Structural SVM.',\n",
       "     'id': '6422',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6437',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables. Our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression. We give numerical results to illustrate these tools on simulated data and on health record data.',\n",
       "     'id': '6437',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6441',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.\",\n",
       "     'id': '6441',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6451',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model  remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.',\n",
       "     'id': '6451',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6452',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called ?Bayesian optimization? only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.',\n",
       "     'id': '6452',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6453',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly ? rather than exponentially? with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy.',\n",
       "     'id': '6453',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6455',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are positively curved. In this paper we ask whether there are other \"lucky\" settings when FTL achieves sublinear, \"small\" regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of  the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polyhedral domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL.',\n",
       "     'id': '6455',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6466',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Despite the success of kernel-based nonparametric methods, kernel selection still requires considerable expertise, and is often described as a ?black art.? We present a sophisticated method for automatically searching for an appropriate kernel from an infinite space of potential choices. Previous efforts in this direction have focused on traversing a kernel grammar, only examining the data via computation of marginal likelihood. Our proposed search method is based on Bayesian optimization in model space, where we reason about model evidence as a function to be maximized. We explicitly reason about the data distribution and how it induces similarity between potential model choices in terms of the explanations they can offer for observed data. In this light, we construct a novel kernel between models to explain a given dataset. Our method is capable of finding a model that explains a given dataset well without any human assistance, often with fewer computations of model evidence than previous approaches, a claim we demonstrate empirically.',\n",
       "     'id': '6466',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6467',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In stochastic convex optimization the goal is to minimize a convex function $F(x) \\\\doteq \\\\E_{f\\\\sim D}[f(x)]$ over a convex set $\\\\K \\\\subset \\\\R^d$ where $D$ is some unknown distribution and each $f(\\\\cdot)$ in the support of $D$ is convex over $\\\\K$. The optimization is based on i.i.d.~samples $f^1,f^2,\\\\ldots,f^n$ from $D$. A common approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \\\\doteq \\\\frac{1}{n}\\\\sum_{i\\\\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\\\\K$. We demonstrate that in the standard $\\\\ell_p/\\\\ell_q$ setting of Lipschitz-bounded functions over a $\\\\K$ of bounded radius, ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\\\\Omega(\\\\log d)$ dependence proved for $\\\\ell_2/\\\\ell_2$ setting in (Shalev-Shwartz et al.  2009). In stark contrast, these problems can be solved using dimension-independent number of samples for $\\\\ell_2/\\\\ell_2$ setting and $\\\\log d$ dependence for $\\\\ell_1/\\\\ell_\\\\infty$ setting using other approaches. We also demonstrate that for a more general class of range-bounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.',\n",
       "     'id': '6467',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6473',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference.  However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than  the recent state-of-the-art incremental solutions to variational sparse GPR.',\n",
       "     'id': '6473',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6474',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.',\n",
       "     'id': '6474',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6480',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Matching users to the right items at the right time is a fundamental task in recommendation systems. As users interact with different items over time, users' and items' feature may evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature. To learn parameters, we design an efficient convex optimization algorithm with a novel low rank space sharing constraints. Extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.\",\n",
       "     'id': '6480',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6483',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on $\\\\R^d$ and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its $U$-statistic variant, which are usually employed in applications.',\n",
       "     'id': '6483',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6485',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We present a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition. These are the tightest margin bounds known for both standard multi-class and general structured prediction problems.  Our guarantees are expressed in terms of a data-dependent complexity measure, \\\\emph{factor graph complexity}, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets, and a sparsity measure for features and graphs. Our proof techniques include generalizations of Talagrand's contraction lemma that can be of independent interest. We further extend our theory by leveraging the principle of Voted Risk Minimization (VRM) and show that learning is possible even with complex factor graphs.  We present new learning bounds for this advanced setting, which we use to devise two new algorithms, \\\\emph{Voted Conditional Random Field} (VCRF) and \\\\emph{Voted Structured Boosting} (StructBoost). These algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees. We also report the results of experiments with VCRF on several datasets to validate our theory.\",\n",
       "     'id': '6485',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6489',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset ?Depth in the Wild? consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.',\n",
       "     'id': '6489',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6491',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned  regularizations suppress down the eigenvalues associated with localized  eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the  theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.',\n",
       "     'id': '6491',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6505',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse nonnegative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed:  more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. The algorithm enables real-time simultaneous deconvolution of $O(10^5)$ traces of whole-brain zebrafish imaging data on a laptop.',\n",
       "     'id': '6505',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6507',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [1] proved the first gap-free convergence result using the block Krylov method, Shamir [2] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [3] provided the fastest $O(\\\\mathsf{nnz}(A) + \\\\mathsf{poly}(1/\\\\varepsilon))$-time algorithm using alternating minimization.\\n\\nIn this paper, we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [1], and the first accelerated and stochastic method outperforming [2]. In the $O(\\\\mathsf{nnz}(A) + \\\\mathsf{poly}(1/\\\\varepsilon))$ running-time regime, LazySVD outperforms [3] in certain parameter regimes without even using alternating minimization.',\n",
       "     'id': '6507',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6518',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability $1-\\\\alpha$. Although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by $\\\\alpha$. In this paper, we characterize the effect of $\\\\alpha$ by establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model. For small $\\\\alpha$, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly, we also show that this gap narrows as $\\\\alpha$ increases. In other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy.',\n",
       "     'id': '6518',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6519',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right---similar to why we study the human brain---and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization, which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network. The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).',\n",
       "     'id': '6519',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6529',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many cases of network analysis, it is more attractive to study how a network varies under  different conditions than an individual static network. We propose a novel graphical model, namely Latent Differential Graph Model, where the networks under two different conditions are represented by two semiparametric elliptical distributions respectively, and the variation of these two networks (i.e., differential graph) is characterized by the difference between their latent precision matrices. We propose an estimator for the differential graph based on quasi likelihood maximization with nonconvex regularization. We show that our estimator attains a faster statistical rate in parameter estimation than the state-of-the-art methods, and enjoys oracle property under mild conditions. Thorough experiments on both synthetic and real world data support our theory.',\n",
       "     'id': '6529',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6547',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A key problem in structured output prediction is enabling direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient method that incorporates task reward into maximum likelihood training. We establish a connection between maximum likelihood and regularized expected reward, showing that they are approximately equivalent in the vicinity of the optimal solution. Then we show how maximum likelihood can be generalized by optimizing the conditional probability of auxiliary outputs that are sampled proportional to their exponentiated scaled rewards. We apply this framework to optimize edit distance in the output space, by sampling from edited targets. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over maximum likelihood baseline by simply sampling from target output augmentations.',\n",
       "     'id': '6547',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6551',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Asynchronous parallel optimization received substantial successes and extensive attention recently. One of core theoretical questions is how much speedup (or benefit) the asynchronous parallelization can bring to us. This paper provides a comprehensive and generic analysis to study the speedup property for a broad range of asynchronous parallel stochastic algorithms from the zeroth order to the first order methods. Our result recovers or improves existing analysis on special cases, provides more insights for understanding the asynchronous parallel behaviors, and suggests a novel asynchronous parallel zeroth order method for the first time. Our experiments provide novel applications of the proposed asynchronous parallel zeroth order method on hyper parameter tuning and model blending problems.',\n",
       "     'id': '6551',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6552',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach which models future frames in a probabilistic manner. Our proposed method is therefore able to synthesize multiple possible next frames using the same model. Solving this challenging problem involves low- and high-level image and motion understanding for successful image synthesis. Here, we propose a novel network structure, namely a Cross Convolutional Network, that encodes images as feature maps and motion information as convolutional kernels to aid in synthesizing future frames. In experiments, our model performs well on both synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold video data. We show that our model can also be applied to tasks such as visual analogy-making, and present analysis of the learned network representations.',\n",
       "     'id': '6552',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6553',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate $\\\\eta(t)$, and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights $w(t)$. Using a Lyapunov argument, we give sufficient conditions on $\\\\eta$ and $w$ to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme. We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases.',\n",
       "     'id': '6553',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6560',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.',\n",
       "     'id': '6560',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6567',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of completing a binary matrix in an online learning setting. On each trial we predict a matrix entry and then receive the true entry. We propose a Matrix Exponentiated Gradient algorithm [1] to solve this problem. We provide a mistake bound for the algorithm, which scales with the margin complexity [2, 3] of the underlying matrix. The bound suggests an interpretation where each row of the matrix is a prediction task over a finite set of objects, the columns. Using this we show that the algorithm makes a number of mistakes which is comparable up to a logarithmic factor to the number of mistakes made by the Kernel Perceptron with an optimal kernel in hindsight. We discuss applications of the algorithm to predicting as well as the best biclustering and to the problem of predicting the labeling of a graph without knowing the graph in advance.',\n",
       "     'id': '6567',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6569',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.\",\n",
       "     'id': '6569',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6574',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in community detection under the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities. For such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on SBM parameters for exact recoverability. Our results exploit the tradeoffs among the various parameters of heterogenous SBM and provide recovery guarantees for many new interesting SBM configurations.',\n",
       "     'id': '6574',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6594',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.',\n",
       "     'id': '6594',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6603',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes S2FO2 and S2RU of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules.',\n",
       "     'id': '6603',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6604',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present Cyclades, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. Cyclades is asynchronous during model updates, and requires no memory locking mechanisms, similar to Hogwild!-type algorithms. Unlike Hogwild!, Cyclades introduces no conflicts during parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms.  Due to its inherent cache locality and conflict-free nature,  our multi-core implementation of Cyclades consistently outperforms Hogwild!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to Hogwild!, and up to 5\\\\times gains over asynchronous implementations of variance reduction algorithms.',\n",
       "     'id': '6604',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6609',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII (12.5% relative improvement) and HMDB (RGB) datasets. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.',\n",
       "     'id': '6609',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6611',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints.   In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.',\n",
       "     'id': '6611',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6612',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Synthesizing realistic profile faces is promising for more efficiently  training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively.\",\n",
       "     'id': '6612',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6616',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper studies the numerical computation of integrals, representing estimates or predictions, over the output $f(x)$ of a computational model with respect to a distribution $p(\\\\mathrm{d}x)$ over uncertain inputs $x$ to the model. For the functional cardiac models that motivate this work, neither $f$ nor $p$ possess a closed-form expression and evaluation of either requires $\\\\approx$ 100 CPU hours, precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem, with a joint model for both the a priori unknown function $f$ and the a priori unknown distribution $p$. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account, in a statistically principled manner, for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment.',\n",
       "     'id': '6616',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6631',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM).   We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits.   Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.',\n",
       "     'id': '6631',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6634',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called \"calibration function\" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.',\n",
       "     'id': '6634',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6635',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce \\'safe\\' and generic responses like \"I don\\'t know\", \"I can\\'t tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users.   Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, a RNN is augmented with a sequence of GS samplers, which coupled with the straight-through gradient estimator enables end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). The source code can be downloaded from https://github.com/jiasenlu/visDial.pytorch',\n",
       "     'id': '6635',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6639',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Convolutional neural networks (CNNs) have recently been applied to the optical flow estimation problem. As training the CNNs requires sufficiently large ground truth training data, existing approaches resort to synthetic, unrealistic datasets. On the other hand, unsupervised methods are capable of leveraging real-world videos for training where the ground truth flow fields are not available. These methods, however, rely on the fundamental assumptions of brightness constancy and spatial smoothness priors which do not hold near motion boundaries. In this paper, we propose to exploit unlabeled videos for semi-supervised learning of optical flow with a Generative Adversarial Network. Our key insight is that the adversarial loss can capture the structural patterns of flow warp errors without making explicit assumptions. Extensive experiments on benchmark datasets demonstrate that the proposed semi-supervised algorithm performs favorably against purely supervised and semi-supervised learning schemes.',\n",
       "     'id': '6639',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6641',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we study the {\\\\em pooled data} problem of identifying the labels associated with a large collection of items, based on a sequence of pooled tests revealing the counts of each label within the pool.  In the noiseless setting, we identify an exact asymptotic threshold on the required number of tests with optimal decoding, and prove a {\\\\em phase transition} between complete success and complete failure.  In addition, we present a novel {\\\\em noisy} variation of the problem, and provide an information-theoretic framework for characterizing the required number of tests for general random noise models.  Our results reveal that noise can make the problem considerably more difficult, with strict increases in the scaling laws even at low noise levels.  Finally, we demonstrate similar behavior in an {\\\\em approximate recovery} setting, where a given number of errors is allowed in the decoded labels.',\n",
       "     'id': '6641',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6643',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process ($\\\\Gamma$P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal $\\\\Gamma$P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the $\\\\Gamma$P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed.\",\n",
       "     'id': '6643',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6653',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this work, we introduce the average top-$k$ (\\\\atk) loss as a new ensemble loss for supervised learning. The \\\\atk loss provides a natural generalization of the two widely used ensemble losses, namely the average loss and the maximum loss. Furthermore, the \\\\atk loss combines the advantages of them and can alleviate their corresponding drawbacks to better adapt to different data distributions. We show that the \\\\atk loss affords an intuitive interpretation that reduces the penalty of continuous and convex individual losses on correctly classified data. The \\\\atk loss can lead to convex optimization problems that can be solved effectively with conventional sub-gradient based method. We further study the Statistical Learning Theory of \\\\matk by establishing its classification calibration and statistical consistency of \\\\matk which provide useful insights on the practical choice of the  parameter $k$. We demonstrate the applicability of \\\\matk learning combined with different individual loss functions for binary and multi-class classification and regression using synthetic and real datasets.',\n",
       "     'id': '6653',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6655',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the penalty is a separable sum of support functions, is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra's algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.\",\n",
       "     'id': '6655',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6675',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recently, there has been a growing research interest in the analysis of dynamic regret, which measures the performance of an online learner against a sequence of local minimizers. By exploiting the strong convexity, previous studies have shown that the dynamic regret can be upper bounded by the path-length of the comparator sequence. In this paper, we illustrate that the dynamic regret can be further improved by allowing the learner to query the gradient of the function multiple times, and meanwhile the strong convexity can be weakened to other non-degenerate conditions. Specifically, we introduce the squared path-length, which could be much smaller than the path-length, as a new regularity of the comparator sequence. When multiple gradients are accessible to the learner, we first demonstrate that the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. We then extend our theoretical guarantee to functions that are semi-strongly convex or self-concordant. To the best of our knowledge, this is the first time that semi-strong convexity and self-concordance are utilized to tighten the dynamic regret.',\n",
       "     'id': '6675',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6678',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we introduce a natural image prior that directly represents a Gaussian-smoothed version of the natural image distribution. We include our prior in a formulation of image restoration as a Bayes estimator that also allows us to solve noise-blind image restoration problems. We show that the gradient of our prior corresponds to the mean-shift vector on the natural image distribution. In addition, we learn the mean-shift vector field using denoising autoencoders, and use it in a gradient descent approach to perform Bayes risk minimization. We demonstrate competitive results for noise-blind deblurring, super-resolution, and demosaicing.',\n",
       "     'id': '6678',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6679',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e^{-t})) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.',\n",
       "     'id': '6679',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6683',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we consider the problem of minimizing a submodular function from training data. Submodular functions can be efficiently minimized and are conse- quently heavily applied in machine learning. There are many cases, however, in which we do not know the function we aim to optimize, but rather have access to training data that is used to learn the function. In this paper we consider the question of whether submodular functions can be minimized in such cases. We show that even learnable submodular functions cannot be minimized within any non-trivial approximation when given access to polynomially-many samples. Specifically, we show that there is a class of submodular functions with range in [0, 1] such that, despite being PAC-learnable and minimizable in polynomial-time, no algorithm can obtain an approximation strictly better than 1/2 ? o(1) using polynomially-many samples drawn from any distribution. Furthermore, we show that this bound is tight using a trivial algorithm that obtains an approximation of 1/2.',\n",
       "     'id': '6683',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6684',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.',\n",
       "     'id': '6684',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6692',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.',\n",
       "     'id': '6692',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6700',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.',\n",
       "     'id': '6700',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6708',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications, e.g., neuroscience, genetics, systems biology, etc. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce the Union of Intersections (UoI) method, a flexible, modular, and scalable framework for enhanced model selection and estimation. The method performs model selection and model estimation through intersection and union operations, respectively. We show that UoI can satisfy the bi-criteria of low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm ($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as the accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the $UoI_{L1Logistic}$ and $UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields.',\n",
       "     'id': '6708',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6712',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The {\\\\L}ojasiewicz inequality shows that H\\\\\"olderian error bounds on the minimum of convex optimization problems hold almost generically. Here, we clarify results of \\\\citet{Nemi85} who show that H\\\\\"olderian error bounds directly controls the performance of restart schemes. The constants quantifying error bounds are of course unobservable, but we show that optimal restart strategies are robust, and searching for the best scheme only increases the complexity by a logarithmic factor compared to the optimal bound. Overall then, restart schemes generically accelerate accelerated methods.',\n",
       "     'id': '6712',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6714',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy.  Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training.  We showcase this method for two challenging applications: Image compression and neural network compression.  While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.',\n",
       "     'id': '6714',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6715',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a hierarchical model which allows to estimate a group-average piecewise-geodesic trajectory in the Riemannian space of measurements and individual variability. This model falls into the well defined mixed-effect models. The subject-specific trajectories are defined through spatial and temporal transformations of the group-average piecewise-geodesic path, component by component. Thus we can apply our model to a wide variety of situations. Due to the non-linearity of the model, we use the Stochastic Approximation Expectation-Maximization algorithm to estimate the model parameters. Experiments on synthetic data validate this choice. The model is then applied to the metastatic renal cancer chemotherapy monitoring: we run estimations on RECIST scores of treated patients and estimate the time they escape from the treatment. Experiments highlight the role of the different parameters on the response to treatment.',\n",
       "     'id': '6715',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6716',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of 1/p*, where p* is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing a triggering probability modulated (TPM) bounded smoothness condition into the influence maximization bandit and combinatorial cascading bandit satisfy this TPM condition. As a result, we completely remove the factor of 1/p* from the regret bounds, achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally, we provide lower bound results showing that the factor 1/p* is unavoidable for general CMAB-T problems, suggesting that the TPM condition is crucial in removing this factor.',\n",
       "     'id': '6716',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6718',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\\\\tilde{O}(D\\\\sqrt{SAT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$, when $T\\\\ge S^5A$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result improves over the best previously known upper bound of $\\\\tilde{O}(DS\\\\sqrt{AT})$ achieved by any algorithm in this setting, and matches the dependence on $S$ in the established lower bound of $\\\\Omega(\\\\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.',\n",
       "     'id': '6718',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6722',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Several important applications, such as streaming PCA and semidefinite programming, involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates.  Because of storage limitations, it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nystr?m approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix.  Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.',\n",
       "     'id': '6722',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6730',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Consider a dataset where data is collected on multiple features of multiple individuals over multiple times. This type of data can be represented as a three dimensional individual/feature/time tensor and has become increasingly prominent in various areas of science. The tensor biclustering problem computes a subset of individuals and a subset of features whose signal trajectories over time lie in a low-dimensional subspace, modeling similarity among the signal trajectories while allowing different scalings across different individuals or different features. We study the information-theoretic limit of this problem under a generative model. Moreover, we propose an efficient spectral algorithm to solve the tensor biclustering problem and analyze its achievability bound in an asymptotic regime. Finally, we show the efficiency of our proposed method in several synthetic and real datasets.',\n",
       "     'id': '6730',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6738',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Adaptive stimulus selection methods in neuroscience have primarily focused on maximizing the firing rate of a single recorded neuron. When recording from a population of neurons, it is usually not possible to find a single stimulus that maximizes the firing rates of all neurons. This motivates optimizing an objective function that takes into account the responses of all recorded neurons together. We propose ?Adept,? an adaptive stimulus selection method that can optimize population objective functions. In simulations, we first confirmed that population objective functions elicited more diverse stimulus responses than single-neuron objective functions. Then, we tested Adept in a closed-loop electrophysiological experiment in which population activity was recorded from macaque V4, a cortical area known for mid-level visual processing. To predict neural responses, we used the outputs of a deep convolutional neural network model as feature embeddings. Images chosen by Adept elicited mean neural responses that were 20% larger than those for randomly-chosen natural images, and also evoked a larger diversity of neural responses. Such adaptive stimulus selection methods can facilitate experiments that involve neurons far from the sensory periphery, for which it is often unclear which stimuli to present.',\n",
       "     'id': '6738',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6762',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an unbiased estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to state-of-arts.',\n",
       "     'id': '6762',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6764',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A new model, named as deep dynamic poisson factorization model, is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks, representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution, and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.',\n",
       "     'id': '6764',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6765',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning, in which the state of the art is unbiased PU learning. However, if its model is very flexible, empirical risks on training data will go negative, and we will suffer from serious overfitting. In this paper, we propose a non-negative risk estimator for PU learning: when getting minimized, it is more robust against overfitting, and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover, we analyze the bias, consistency, and mean-squared-error reduction of the proposed risk estimator, and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.',\n",
       "     'id': '6765',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6766',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight $\\\\ell_\\\\infty$ estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in $\\\\ell_\\\\infty$ error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model.',\n",
       "     'id': '6766',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6767',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Answering \"What if?\" questions is important in many domains. For example, would a patient\\'s disease progression slow down if I were to give them a dose of drug A? Ideally, we answer our question using an experiment, but this is not always possible (e.g., it may be unethical). As an alternative, we can use non-experimental data to learn models that make counterfactual predictions of what we would observe had we run an experiment. In this paper, we propose the counterfactual GP, a counterfactual model of continuous-time trajectories (time series) under sequences of actions taken in continuous-time. We develop our model within the potential outcomes framework of Neyman and Rubin. The counterfactual GP is trained using a joint maximum likelihood objective that adjusts for dependencies between observed actions and outcomes in the training data. We report two sets of experimental results using the counterfactual GP. The first shows that it can be used to learn the natural progression (i.e. untreated progression) of biomarker trajectories from observational data. In the second, we show how the CGP can be used for medical decision support by learning counterfactual models of renal health under different types of dialysis.',\n",
       "     'id': '6767',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6768',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to its excellent scalability properties. A fundamental barrier when parallelizing SGD is the high bandwidth cost of communicating gradient updates between nodes; consequently, several lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always guarantee convergence, and it is not clear whether they can be improved.  In this paper, we propose Quantized SGD (QSGD), a family of compression schemes for gradient updates which provides convergence guarantees. QSGD allows the user to smoothly trade off \\\\emph{communication bandwidth} and \\\\emph{convergence time}: nodes can adjust the number of bits sent per iteration, at the cost of possibly higher variance. We show that this trade-off is inherent, in the sense that improving it past some threshold would violate  information-theoretic lower bounds. QSGD guarantees convergence for convex and non-convex objectives,  under asynchrony, and can be extended to stochastic variance-reduced techniques.   When applied to  training deep neural networks for image classification and  automated speech recognition, QSGD leads to significant reductions in  end-to-end training time. For example, on 16GPUs, we can train the ResNet152  network to full accuracy on ImageNet 1.8x faster than the full-precision  variant.',\n",
       "     'id': '6768',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6769',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.',\n",
       "     'id': '6769',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6775',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.\",\n",
       "     'id': '6775',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6780',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Computational models in fields such as computational neuroscience  are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning.  Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.\",\n",
       "     'id': '6780',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6782',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types, and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community.  We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.',\n",
       "     'id': '6782',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6791',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Adversarial training has proved to be competitive against supervised learning methods on computer vision tasks. However, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing the existence of information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques. We also show that supervised training of our adversarial model produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques.',\n",
       "     'id': '6791',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6794',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Key to multitask learning is exploiting the relationships between different tasks to improve prediction performance. Most previous methods have focused on the case where tasks relations can be modeled as linear operators and regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related is often restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that our algorithm can be efficiently implemented and study its generalization properties, proving universal consistency and learning rates. Our theoretical analysis highlights the benefits of non-linear multitask learning over learning the tasks independently. Encouraging experimental results show the benefits of the proposed method in practice.',\n",
       "     'id': '6794',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6796',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form $\\\\vct{x}\\\\mapsto \\\\max(0,\\\\langle \\\\vct{w},\\\\vct{x}\\\\rangle)$ with $\\\\vct{w}\\\\in\\\\R^d$ denoting the weight vector.  We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialized at $\\\\vct{0}$, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.',\n",
       "     'id': '6796',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6798',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Technological breakthroughs allow us to collect data with increasing spatio-temporal resolution from complex interaction systems. The combination of high-resolution observations, expressive dynamic models, and efficient machine learning algorithms can lead to crucial insights into complex interaction dynamics and the functions of these systems. In this paper, we formulate the dynamics of a complex interacting network as a stochastic process driven by a sequence of events, and develop expectation propagation algorithms to make inferences from noisy observations. To avoid getting stuck at a local optimum, we formulate the problem of minimizing Bethe free energy as a constrained primal problem and take advantage of the concavity of dual problem in the feasible domain of dual variables guaranteed by duality theorem. Our expectation propagation algorithms demonstrate better performance in inferring the interaction dynamics in complex transportation networks than competing models such as particle filter, extended Kalman filter, and deep neural networks.',\n",
       "     'id': '6798',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6799',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a data-efficient reinforcement learning method for continuous state-action systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.',\n",
       "     'id': '6799',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6807',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neural networks have a smooth initial inductive bias, such that small changes in input do not lead to large changes in output. However, in reinforcement learning domains with sparse rewards, value functions have non-smooth structure with a characteristic asymmetric discontinuity whenever rewards arrive. We propose a mechanism that learns an interpolation between a direct value estimate and a projected value estimate computed from the encountered reward and the previous estimate. This reduces the need to learn about discontinuities, and thus improves the value function approximation. Furthermore, as the interpolation is learned and state-dependent, our method can deal with heterogeneous observability. We demonstrate that this one change leads to significant improvements on multiple Atari games, when applied to the state-of-the-art A3C algorithm.',\n",
       "     'id': '6807',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6823',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error -- TreeHist and Bitstogram. In both algorithms, server running time is $\\\\tilde O(n)$ and user running time is $\\\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $\\\\tilde O(n^{5/2})$ server time and $\\\\tilde O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for the use of such algorithms in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.\",\n",
       "     'id': '6823',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6831',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion.  To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.',\n",
       "     'id': '6831',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6835',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world data sets.',\n",
       "     'id': '6835',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6836',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space of the network, as defined in Daniely, Frostig and Singer. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two.  As corollaries, it follows that for neural networks of any depth between 2 and log(n), SGD is guaranteed to learn, in polynomial time, constant degree polynomials with polynomially bounded coefficients. Likewise, it follows  that SGD on large enough networks can learn any continuous function (not in polynomial time), complementing classical expressivity results.',\n",
       "     'id': '6836',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6837',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of interactively learning a binary classifier using noisy labeling and pairwise comparison oracles, where the comparison oracle answers which one in the given two instances is more likely to be positive. Learning from such oracles has multiple applications where obtaining direct labels is harder but pairwise comparisons are easier, and the algorithm can leverage both types of oracles. In this paper, we attempt to characterize how the access to an easier comparison oracle helps in improving the label and total query complexity. We show that the comparison oracle reduces the learning problem to that of learning a threshold function. We then present an algorithm that interactively queries the label and comparison oracles and we characterize its query complexity under Tsybakov and adversarial noise conditions for the comparison and labeling oracles. Our lower bounds show that our label and total query complexity is almost optimal.',\n",
       "     'id': '6837',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6838',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neural networks have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.',\n",
       "     'id': '6838',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6841',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In this paper, we develop an algorithm that approximates the residual error of Tucker decomposition, one of the most popular tensor decomposition methods, with a provable guarantee.  Given an order-$K$ tensor $X\\\\in\\\\mathbb{R}^{N_1\\\\times\\\\cdots\\\\times N_K}$, our algorithm randomly samples a constant number $s$ of indices for each mode and creates a ``mini'' tensor $\\\\tilde{X}\\\\in\\\\mathbb{R}^{s\\\\times\\\\cdots\\\\times s}$, whose elements are given by the intersection of the sampled indices on $X$.  Then, we show that the residual error of the Tucker decomposition of $\\\\tilde{X}$ is sufficiently close to that of $X$ with high probability.  This result implies that we can figure out how much we can fit a low-rank tensor to $X$ \\\\emph{in constant time}, regardless of the size of $X$. This is useful for guessing the favorable rank of Tucker decomposition.  Finally, we demonstrate how the sampling method works quickly and accurately using multiple real datasets.\",\n",
       "     'id': '6841',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6847',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of computing a restricted nonnegative matrix factorization (NMF) of an m\\\\times n matrix X.  Specifically, we seek a factorization X\\\\approx BC, where the k columns of B are a subset of those from X and C\\\\in\\\\Re_{\\\\geq 0}^{k\\\\times n}.  Equivalently, given the matrix X, consider the problem of finding a small subset, S, of the columns of X such that the conic hull of S \\\\eps-approximates the conic hull of the columns of X, i.e., the distance of every column of X to the conic hull of the columns of S should be at most an \\\\eps-fraction of the angular diameter of X.   If k is the size of the smallest \\\\eps-approximation, then we produce an O(k/\\\\eps^{2/3}) sized O(\\\\eps^{1/3})-approximation, yielding the first provable, polynomial time \\\\eps-approximation for this class of NMF problems, where also desirably the approximation is independent of n and m. Furthermore, we prove an approximate conic Carath?odory theorem, a general sparsity result, that shows that any column of X can be \\\\eps-approximated with an O(1/\\\\eps^2) sparse combination from S.   Our results are facilitated by a reduction to the problem of approximating convex hulls,  and we prove that both the convex and conic hull variants are d-sum-hard, resolving an open problem.  Finally, we provide experimental results for the convex and conic algorithms on a variety of feature selection tasks.',\n",
       "     'id': '6847',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6851',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.',\n",
       "     'id': '6851',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6852',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of estimating multiple related functions computed by weighted  automata~(WFA). We first present a natural notion of relatedness between WFAs by considering to which extent several WFAs can share a common underlying representation. We then introduce the model of vector-valued WFA which conveniently helps us formalize this notion of relatedness. Finally, we propose a spectral learning algorithm for vector-valued WFAs to tackle the multitask learning problem. By jointly learning multiple tasks in the form of a vector-valued WFA, our algorithm enforces the discovery of a representation space shared between tasks. The benefits of the proposed multitask approach are theoretically motivated and showcased through experiments on both synthetic and real world datasets.',\n",
       "     'id': '6852',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6857',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix $Y^TY$, $Y=f(WX)$, where $W$ is a random weight matrix, $X$ is a random data matrix, and $f$ is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature methods on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.',\n",
       "     'id': '6857',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6860',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.\",\n",
       "     'id': '6860',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6869',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Kernel machines as well as neural networks possess universal function approximation properties. Nevertheless in practice their ways of choosing the appropriate function class differ. Specifically neural networks learn a representation by adapting their basis functions to the data and the task at hand, while kernel methods typically use a basis that is not adapted during training. In this work, we contrast random features of approximated kernel machines with learned features of neural networks. Our analysis reveals how these random and adaptive basis functions affect the quality of learning. Furthermore, we present basis adaptation schemes that allow for a more compact representation, while retaining the generalization properties of kernel machines.',\n",
       "     'id': '6869',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6871',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%.',\n",
       "     'id': '6871',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5074',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.\",\n",
       "     'id': '5074',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5082',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.',\n",
       "     'id': '5082',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5088',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering  datasets with high-dimensional inputs.',\n",
       "     'id': '5088',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5091',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider design of linear projection measurements for a vector Poisson signal model. The projections are performed on the vector Poisson rate, $X\\\\in\\\\mathbb{R}_+^n$, and the observed data are a vector of counts, $Y\\\\in\\\\mathbb{Z}_+^m$. The projection matrix is designed by maximizing mutual information between $Y$ and $X$, $I(Y;X)$. When there is a latent class label $C\\\\in\\\\{1,\\\\dots,L\\\\}$ associated with $X$, we consider the mutual information with respect to $Y$ and $C$, $I(Y;C)$. New analytic expressions for the gradient of $I(Y;X)$ and $I(Y;C)$ are presented, with gradient performed with respect to the measurement matrix. Connections are made to the more widely studied Gaussian measurement model. Example results are presented for compressive topic modeling of a document corpora (word counting), and hyperspectral compressive sensing for chemical classification (photon counting).',\n",
       "     'id': '5091',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5099',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. and Amini et al. proposed variations on the algorithm that artificially inflate the node degrees for improved statistical performance.  The current paper extends the previous theoretical results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and  provides guidance on the choice of tuning parameter.  Moreover, our results show how the star shape\" in the eigenvectors--which are consistently observed in empirical networks--can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical model that allow for highly heterogeneous degrees.  Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.  \"',\n",
       "     'id': '5099',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5104',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of \\\\emph{Preconditioned Lasso} algorithms that pre-multiply $X$ and $y$ by matrices $P_X$, $P_y$ prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter $\\\\lambda$. In this paper we propose an agnostic, theoretical framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose $\\\\lambda$. We apply our framework to three Preconditioned Lasso instances and highlight when they will outperform the Lasso. Additionally, our theory offers insights into the fragilities of these algorithms to which we provide partial solutions.',\n",
       "     'id': '5104',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5117',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many scientific data occur as sequences of multidimensional arrays called tensors.  How can hidden, evolving trends in such data be extracted while preserving the tensor structure?  The model that is traditionally used is the linear dynamical system (LDS), which treats the observation at each time slice as a vector.  In this paper, we propose the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters.  The MLDS models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent, low-dimensional tensors.  Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets.',\n",
       "     'id': '5117',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5118',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H. Instead of assuming a generative model for H, we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i,j) of interest. We do this, for all the cells of H simultaneously, without generating realizations but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with running time equal to the time required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.',\n",
       "     'id': '5118',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5120',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we first introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner.',\n",
       "     'id': '5120',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5123',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming (DP)-based solution schemes can be applied. If the conditional expectations in the DP recursions are estimated via kernel regression, however, the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions. The resulting data-driven DP scheme is asymptotically consistent and admits efficient computational solution when combined with parametric value function approximations. If training data is sparse, however, the estimated cost-to-go functions display a high variability and an optimistic bias, while the corresponding control policies perform poorly in out-of-sample tests. To mitigate these small sample effects, we propose a robust data-driven DP scheme, which replaces the expectations in the DP recursions with worst-case expectations over a set of distributions close to the best estimate. We show that the arising min-max problems in the DP recursions reduce to tractable conic programs. We also demonstrate that this robust algorithm dominates state-of-the-art benchmark algorithms in out-of-sample tests across several application domains.',\n",
       "     'id': '5123',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5125',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we introduce a novel method that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. Our method can be regarded as a natural extension of the one-class SVM (OCSVM) algorithm that finds multiple parallel separating hyperplanes in a reproducing kernel Hilbert space. We call our method q-OCSVM, as it can be used to estimate $q$ quantiles of a high-dimensional distribution. For this purpose, we introduce a new global convex optimization program that finds all estimated sets at once and show that it can be solved efficiently. We prove the correctness of our method and present empirical results that demonstrate its superiority over existing methods.',\n",
       "     'id': '5125',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5126',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Stochastic And-Or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events. We present a unified formalization of stochastic And-Or grammars that is agnostic to the type of the data being modeled, and propose an unsupervised approach to learning the structures as well as the parameters of such grammars. Starting from a trivial initial grammar, our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar. In our empirical evaluation, we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.',\n",
       "     'id': '5126',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5128',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also used a visual Turing test\" to show that our model produces human-like performance on other conceptual tasks, including generating new examples and parsing.\"',\n",
       "     'id': '5128',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5131',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Robust PCA methods are typically based on batch optimization and have to load all the samples into memory. This prevents them from efficiently processing big data. In this paper, we develop  an Online Robust Principal Component Analysis (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the data size,  significantly enhancing the computation and storage efficiency. The proposed method is based on stochastic optimization of an equivalent reformulation of the batch RPCA method. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust  to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the OR-PCA over online PCA and batch RPCA methods.',\n",
       "     'id': '5131',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5138',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R?nyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.',\n",
       "     'id': '5138',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5140',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In text analysis documents are represented as disorganized bags of words, models of count features are typically based on mixing a small number of topics \\\\cite{lda,sam}. Recently, it has been observed that for many text corpora documents evolve into one another in a smooth way, with some features dropping and new ones being introduced. The counting grid \\\\cite{cgUai} models this spatial metaphor literally: it is multidimensional grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid. The major drawback of this method is that it is essentially a mixture and all the content much be generated by a single contiguous area on the grid. This may be problematic especially for lower dimensional grids. In this paper, we overcome to this issue with the \\\\emph{Componential Counting Grid} which brings the componential nature of topic models to the basic counting grid. We also introduce a generative kernel based on the document's grid usage and a visualization strategy useful for understanding large text corpora. We evaluate our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.\",\n",
       "     'id': '5140',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5145',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with perfect information and simultaneous moves. We present a general template of MCTS algorithms for these games, which can be instantiated by various selection methods. We formally prove that if a selection method is $\\\\epsilon$-Hannan consistent in a matrix game and satisfies additional requirements on exploration, then the MCTS algorithm eventually converges to an approximate Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this claim using regret matching and Exp3 as the selection methods on randomly generated and worst case games. We confirm the formal result and show that additional MCTS variants also converge to approximate NE on the evaluated games.',\n",
       "     'id': '5145',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5147',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror-Prox algorithm, prove an extension to Holder-smooth functions, and apply the results to saddle-point type problems. Second, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of O(log T / T). This addresses a question of Daskalakis et al, 2011. Further, we consider a partial information version of the problem. We then apply the results to approximate convex programming  and show a simple algorithm for the approximate Max-Flow problem.',\n",
       "     'id': '5147',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5159',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider energy minimization for undirected graphical models, also known as MAP-inference problem for Markov random fields. Although combinatorial methods, which return a provably optimal integral solution of the problem, made a big progress in the past decade, they are still typically unable to cope with large-scale datasets. On the other hand, large scale datasets are typically defined on sparse graphs, and convex relaxation methods, such as linear programming relaxations often provide good approximations to integral solutions.   We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation, our method confines application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve big problems.   We demonstrate the power of our approach on a computer vision energy minimization benchmark.',\n",
       "     'id': '5159',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5163',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Inspired by a two-level theory that unifies agenda setting and ideological  framing, we propose supervised hierarchical latent Dirichlet allocation  (SHLDA) which jointly captures documents' multi-level topic structure and  their polar response variables. Our model extends the nested Chinese restaurant  process to discover a tree-structured topic hierarchy and uses both per-topic  hierarchical and per-word lexical regression parameters to model the response  variables. Experiments in a political domain and on sentiment analysis tasks  show that SHLDA improves predictive accuracy while adding a new dimension of  insight into how topics under discussion are framed.\",\n",
       "     'id': '5163',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5167',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the K-SVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We fit the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The flexibility of the block-based representation is reflected in the variability of the recovered cell shapes.',\n",
       "     'id': '5167',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5171',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models~(LFMs). FAB inference has not been applicable to models, including LFMs, without a specific condition on the Hesqsian matrix of a complete log-likelihood, which is required to derive a factorized information criterion\\'\\'~(FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models.  FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efficiency.\"',\n",
       "     'id': '5171',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5172',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary. In real-world environments, however, such changes often occur without warning or signal. Real-world data often come from generating models that are only locally stationary. In this paper, we present LoSST, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner. We show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and significantly better when it is only locally stationary.',\n",
       "     'id': '5172',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5173',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix. Our method, named ALICE, is applicable to the elliptical family. Computationally, we develop an efficient dual inexact iterative projection (${\\\\rm D_2}$P) algorithm based on the alternating direction method of multipliers (ADMM). Theoretically, we prove that the ALICE estimator achieves the parametric rate of convergence in both parameter estimation and model selection. Moreover, ALICE calibrates regularizations when estimating each column of the inverse covariance matrix. So it not only is asymptotically tuning free, but also achieves an improved finite sample performance. We present numerical simulations to support our theory, and a real data example to illustrate the effectiveness of the proposed estimator.',\n",
       "     'id': '5173',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5185',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most provably efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\\\\tilde{O}(\\\\tau S \\\\sqrt{AT} )$ bound on the expected regret, where $T$ is time, $\\\\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.',\n",
       "     'id': '5185',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5187',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A long term goal of Interactive Reinforcement Learning is to incorporate non-expert human feedback to solve complex tasks. State-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy. We compare Advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.',\n",
       "     'id': '5187',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5188',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD($\\\\lambda$)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of soft-greedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward any optimal policy, except in a certain pathological case. Consequently, in the context of approximations, the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.',\n",
       "     'id': '5188',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5196',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Human eye movements provide a rich source of information into the human visual processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood. This has precluded the development of reliable dynamic eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement annotations collected under the task constraints of action and context recognition. Our dataset is unique among eyetracking datasets for still images in terms of its large scale (over 1 million fixations, 9157 images), task control and action from a single image emphasis. Second, we introduce models to automatically discover areas of interest (AOI) and introduce novel dynamic consistency metrics, based on them. Our method can automatically determine the number and spatial support of the AOIs, in addition to their locations. Based on such encodings, we show that, on unconstrained read-world stimuli, task instructions have significant influence on visual behavior. Finally, we leverage our large scale dataset in conjunction with powerful machine learning techniques and computer vision features, to introduce novel dynamic eye movement prediction methods which learn task-sensitive reward functions from eye movement data and efficiently integrate these rewards to plan future saccades based on inverse optimal control. We show that the propose methodology achieves state of the art scanpath modeling results.',\n",
       "     'id': '5196',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5200',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multi-task learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state-of-the-art.',\n",
       "     'id': '5200',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5213',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a novel formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle infinitely many tasks parameterized by a continuous parameter. Our key finding is that, for a certain class of PTL problems, the path of optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks efficiently. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression, and demonstrate the usefulness of the proposed method experimentally in these scenarios.',\n",
       "     'id': '5213',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5222',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.',\n",
       "     'id': '5222',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5229',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE, based on the whole dataset jointly. We study the statistical properties of this framework, showing that the loss of efficiency compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the full-exponential-family-ness\" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of the KL and linear combination methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.\"',\n",
       "     'id': '5229',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5236',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources. Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios. In this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data. We demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients. Our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.',\n",
       "     'id': '5236',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5237',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the noise assumption\" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.\"',\n",
       "     'id': '5237',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5241',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.',\n",
       "     'id': '5241',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5252',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees. In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping. Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of $1/\\\\sqrt{t}$ within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level $s^*$, dimension $d$ and sample size $n$. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.',\n",
       "     'id': '5252',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5257',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'State of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs. A key facet of thesestatistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however, proximal optimization methods attain only a sublinear rate. In this paper, we investigate a novel variant of strong convexity, which we call Constant Nullspace Strong Convexity (CNSC), where we require that the objective function be strongly convex only over a constant subspace. As we show, the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method, when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.',\n",
       "     'id': '5257',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5260',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\\\\mathcal{M}$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold $\\\\mathcal{M}$ in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.',\n",
       "     'id': '5260',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5261',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We focus on the problem of maximum a posteriori (MAP) inference in Markov random fields with binary variables and pairwise interactions. For this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation, followed by randomized rounding. We develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases. In practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark MAP inference tasks.',\n",
       "     'id': '5261',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5262',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite ?push-pull? fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.',\n",
       "     'id': '5262',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5263',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures the temporal dynamics, effective network connectivity in large population recordings, and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a relatively large number of idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional latent dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations. Inference scales linearly in both population size and recording duration. We apply this model to multi-channel recordings from primary visual cortex and show that it accounts for a large number of individual neural receptive fields using a small number of nonlinear inputs and a low-dimensional dynamical model.',\n",
       "     'id': '5263',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5266',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time. Recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces. However, animals and robots are commonly confronted with unbounded environments. To address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric Bayesian models. Specifically, we demonstrate that the Chinese Restaurant Process (CRP) model is able to solve this problem and that an Empirical Bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.',\n",
       "     'id': '5266',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5270',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.',\n",
       "     'id': '5270',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5272',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. We apply convolutional neural networks (CNNs) to analyze and classify EEG data recorded within a rhythm perception study in Kigali, Rwanda which comprises 12 East African and 12 Western rhythmic stimuli ? each presented in a loop for 32 seconds to 13 participants. We investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures. Using CNNs, we are able to recognize individual rhythms from the EEG with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel. Aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.',\n",
       "     'id': '5272',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5286',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P). In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.',\n",
       "     'id': '5286',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5287',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many situations we have some measurement of confidence on ``positiveness for a binary label. The ``positiveness\" is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called \\\\emph{expectation loss SVM} (e-SVM) that is devoted to the problems where only the ``positiveness\" instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.\"',\n",
       "     'id': '5287',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5291',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.',\n",
       "     'id': '5291',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5295',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.',\n",
       "     'id': '5295',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5298',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.',\n",
       "     'id': '5298',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5300',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. (2014) is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models (Mimno et al. 2011, Newman et al. 2010) and measures of model fitness (Mimno & Blei 2011) provide strong support that explicitly modeling word dependencies---as in APM---could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because $O(p^2)$ parameters must be estimated where $p$ is the number of words (Inouye et al. could only provide results for datasets with $p = 200$). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle $p = 10^4$ as an important step towards scaling to large datasets. In addition, Inouye et al. only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word brings to mind\" another word (Boyd-Graber et al. 2006)). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies. (MATLAB code available at: http://bigdata.ices.utexas.edu/software/apm/)\"',\n",
       "     'id': '5300',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5305',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.',\n",
       "     'id': '5305',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5312',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.',\n",
       "     'id': '5312',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5314',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs. In this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.',\n",
       "     'id': '5314',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5315',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing of univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters.',\n",
       "     'id': '5315',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5322',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov?s accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov?s scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov?s scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov?s scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.',\n",
       "     'id': '5322',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5327',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.',\n",
       "     'id': '5327',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5337',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a biologically-plausible learning rule that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves. It achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide both proofs of convergence, and a close fit to experimental data on STDP.',\n",
       "     'id': '5337',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5343',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.',\n",
       "     'id': '5343',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5355',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We improve a recent gurantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.',\n",
       "     'id': '5355',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5366',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Coverage functions are an important class of discrete functions that capture laws of diminishing returns. In this paper, we propose a new problem of learning time-varying coverage functions which arise naturally from applications in social network analysis, machine learning, and algorithmic game theory. We develop a novel parametrization of the time-varying coverage function by illustrating the connections with counting processes. We present an efficient algorithm to learn the parameters by maximum likelihood estimation, and provide a rigorous theoretic analysis of its sample complexity. Empirical experiments from information diffusion in social network analysis demonstrate that with few assumptions about the underlying diffusion process, our method performs significantly better than existing approaches on both synthetic and real world data.',\n",
       "     'id': '5366',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5376',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.',\n",
       "     'id': '5376',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5378',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler\\'s objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation\" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.\"',\n",
       "     'id': '5378',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5381',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.',\n",
       "     'id': '5381',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5383',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.',\n",
       "     'id': '5383',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5394',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.',\n",
       "     'id': '5394',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5396',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'With the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing~(DGC) frameworks has attracted much attention from big data machine learning community. In DGC frameworks, the graph partitioning~(GP) strategy plays a key role to affect the performance, including the workload balance and communication cost. Typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes GP a challenging task. Recently, many methods have been proposed to solve the GP problem. However, the existing GP methods cannot achieve satisfactory performance for applications with power-law graphs. In this paper, we propose a novel vertex-cut method, called \\\\emph{degree-based hashing}~(DBH), for GP. DBH makes effective use of the skewed degree distributions for GP. We theoretically prove that DBH can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance. Furthermore, empirical results on several large power-law graphs also show that DBH can outperform the state of the art.',\n",
       "     'id': '5396',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5397',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.',\n",
       "     'id': '5397',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5399',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.',\n",
       "     'id': '5399',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5402',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of recovering the sparsest vector in a subspace $ \\\\mathcal{S} \\\\in \\\\mathbb{R}^p $ with $ \\\\text{dim}(\\\\mathcal{S})=n$. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/ \\\\sqrt{n}$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\\\Omega(1)$. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.',\n",
       "     'id': '5402',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5404',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.',\n",
       "     'id': '5404',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5408',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l_1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.',\n",
       "     'id': '5408',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5412',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability. In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.',\n",
       "     'id': '5412',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5419',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.',\n",
       "     'id': '5419',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5424',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.\",\n",
       "     'id': '5424',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5429',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.',\n",
       "     'id': '5429',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5431',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.',\n",
       "     'id': '5431',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5443',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Large Markov Decision Processes (MDPs) are usually solved using Approximate Dynamic Programming (ADP) methods such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API). The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) $T^*Q-Q$, where $T^*$ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning (RL) problem.',\n",
       "     'id': '5443',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5448',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Tensor factorizations have become popular methods for learning from multi-relational data. In this context, the rank of a factorization is an important parameter that determines runtime as well as generalization ability. To determine conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model for learning from latent and observable patterns in multi-relational data and present a scalable algorithm for computing the factorization. Experimentally, we show that the proposed approach does not only improve the predictive performance over pure latent variable methods but that it also reduces the required rank --- and therefore runtime and memory complexity --- significantly.',\n",
       "     'id': '5448',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5449',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.',\n",
       "     'id': '5449',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5455',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We present two new methods for inference in Gaussian process (GP) models with general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a Taylor series expansion or statistical linearization. We show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented Kalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.\",\n",
       "     'id': '5455',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5460',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\\\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the $G$-optimality criterion used in optimal experimental design.',\n",
       "     'id': '5460',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5466',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We consider the problem of learning sparse additive models, i.e., functions of the form: $f(\\\\vecx) = \\\\sum_{l \\\\in S} \\\\phi_{l}(x_l)$, $\\\\vecx \\\\in \\\\matR^d$ from point queries of $f$. Here $S$ is an unknown subset of coordinate variables with $\\\\abs{S} = k \\\\ll d$. Assuming $\\\\phi_l$'s to be smooth, we propose a set of points at which to sample $f$ and an efficient randomized algorithm that recovers a \\\\textit{uniform approximation} to each unknown $\\\\phi_l$. We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise -- either arbitrary but bounded, or stochastic -- on the performance of our algorithm.\",\n",
       "     'id': '5466',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5476',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications, existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty, we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix SNM problem. Finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.',\n",
       "     'id': '5476',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5484',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.',\n",
       "     'id': '5484',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5500',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Jacobi metric introduced in mathematical physics can be used to analyze Hamiltonian Monte Carlo (HMC). In a geometrical setting, each step of HMC corresponds to a geodesic on a Riemannian manifold with a Jacobi metric. Our calculation of the sectional curvature of this HMC manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate Gaussian. We show that positive curvature can be used to prove theoretical concentration results for HMC Markov chains.',\n",
       "     'id': '5500',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5501',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty. It finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty. However, the Bayes-adaptive solution is typically intractable in domains with large or continuous state spaces. We present a tractable method for approximating the Bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises over belief space. Our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.',\n",
       "     'id': '5501',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5506',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost, together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI). We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.',\n",
       "     'id': '5506',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5509',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than $2\\\\sqrt{2}$ times the fully supervised case. These theoretical findings are also validated through experiments.',\n",
       "     'id': '5509',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5511',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present the Convex Polytope Machine (CPM), a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition, text topic, and web security) demonstrate that the CPM trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.',\n",
       "     'id': '5511',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5514',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble?s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.',\n",
       "     'id': '5514',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5516',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.',\n",
       "     'id': '5516',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5522',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram -- the (power) magnitude of the short-time Fourier transform (STFT) -- has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.',\n",
       "     'id': '5522',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5546',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.',\n",
       "     'id': '5546',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5551',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture --- a deep recursive neural network (deep RNN) --- constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.',\n",
       "     'id': '5551',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5555',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.',\n",
       "     'id': '5555',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5556',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.',\n",
       "     'id': '5556',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5557',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.',\n",
       "     'id': '5557',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5558',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics. In this paper, we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically, we analytically derive the leading term of the VB free energy under an asymptotic setup, and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model, which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last.FM music data.',\n",
       "     'id': '5558',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5559',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. These methods are fast and easy to use, while being reasonably accurate. A difficulty remains in computation of the lower bound when the latent dimensionality $L$ is large. Even though the lower bound is concave for many models, its computation requires optimization over $O(L^2)$ variational parameters. Efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First, it maximizes a Lagrangian of the lower bound reducing the number of parameters to $O(N)$, where $N$ is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when the bound is not concave. Second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples and computes gradient efficiently. Overall, our approach avoids all direct computations of the covariance, only requiring its linear projections. Theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.',\n",
       "     'id': '5559',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5569',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan) but no proof was known. Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).',\n",
       "     'id': '5569',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5588',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player, zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.',\n",
       "     'id': '5588',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5592',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables in order to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.',\n",
       "     'id': '5592',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5608',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.',\n",
       "     'id': '5608',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5613',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of $O(s\\\\log d/T)$ for $s$-sparse problems in $d$ dimensions in $T$ steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish $O(1/T)$ rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.',\n",
       "     'id': '5613',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5624',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a ``brain state,'' relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.\",\n",
       "     'id': '5624',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5625',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how the latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.',\n",
       "     'id': '5625',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5626',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.\",\n",
       "     'id': '5626',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5632',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations $X$ across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels $Y$ and adjusting conditional distributions $P(X|Y)$, such that $P(X)$ can be matched across domains. However, covariate shift assumes that the support of test $P(X)$ is contained in the support of training $P(X)$, i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for $P(Y)$. Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both $X$ and $Y$ by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.',\n",
       "     'id': '5632',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5639',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is in particular challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long- term dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability of disentangling latent data factors without using object class labels.',\n",
       "     'id': '5639',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6880',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.',\n",
       "     'id': '6880',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6882',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'MicroRNAs (miRNAs) are small non-coding ribonucleic acids (RNAs) which play key roles in post-transcriptional gene regulation. Direct identification of mature miRNAs is infeasible due to their short lengths, and researchers instead aim at identifying precursor miRNAs (pre-miRNAs). Many of the known pre-miRNAs have distinctive stem-loop secondary structure, and structure-based filtering is usually the first step to predict the possibility of a given sequence being a pre-miRNA. To identify new pre-miRNAs that often have non-canonical structure, however, we need to consider additional features other than structure. To obtain such additional characteristics, existing computational methods rely on manual feature extraction, which inevitably limits the efficiency, robustness, and generalization of computational identification. To address the limitations of existing approaches, we propose a pre-miRNA identification method that incorporates (1) a deep recurrent neural network (RNN) for automated feature learning and classification, (2) multimodal architecture for seamless integration of prior knowledge (secondary structure), (3) an attention mechanism for improving long-term dependence modeling, and (4) an RNN-based class activation mapping for highlighting the learned representations that can contrast pre-miRNAs and non-pre-miRNAs. In our experiments with recent benchmarks, the proposed approach outperformed the compared state-of-the-art alternatives in terms of various performance metrics.',\n",
       "     'id': '6882',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6888',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we determine whether $X \\\\independent Y \\\\vert Z$. We approach this by converting the conditional independence test into a classification problem.  This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks.  These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution $f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X \\\\independent Y \\\\vert Z.$ -- when given access only to i.i.d.  samples from the true joint distribution $f(x,y,z)$.  To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $f^{CI}$ in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d \\\\textit{near-independent} samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.',\n",
       "     'id': '6888',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6889',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a technique for augmenting neural text-to-speech (TTS) with low-dimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-of-the-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.',\n",
       "     'id': '6889',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6892',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (?generator?) and a task solving model (?solver?). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.',\n",
       "     'id': '6892',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6896',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study minimax strategies for the online prediction problem with expert advice. It has been conjectured that a simple adversary strategy, called COMB, is near optimal in this game for any number of experts. Our results and new insights make progress in this direction by showing that, up to a small additive term, COMB is minimax optimal in the finite-time three expert problem. In addition, we provide for this setting a new near minimax optimal COMB-based learner. Prior to this work, in this problem, learners obtaining the optimal multiplicative constant in their regret rate were known only when $K=2$ or $K\\\\rightarrow\\\\infty$. We characterize, when $K=3$, the regret of the game scaling as $\\\\sqrt{8/(9\\\\pi)T}\\\\pm \\\\log(T)^2$ which gives for the first time the optimal constant in the leading ($\\\\sqrt{T}$) term of the regret.',\n",
       "     'id': '6896',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6903',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant  in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, i.e., leveraging  the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the H\\\\\"{o}lderian error bound (HEB) condition.   {\\\\it The key technique} for our development is a novel synthesis of  {\\\\it adaptive regularization and a conditional restarting scheme}, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic, PG\\'s convergence speed is essentially $o(\\\\frac{1}{t})$, where $t$ is the total number of iterations; (ii) if the objective function consists of an $\\\\ell_1$, $\\\\ell_\\\\infty$, $\\\\ell_{1,\\\\infty}$, or huber  norm regularization and a convex smooth piecewise quadratic loss (e.g., square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a {\\\\it faster linear convergence} than PG without any other assumptions  (e.g., restricted eigen-value condition).   It is notable that  our linear convergence results for the aforementioned problems  are global instead of local.  To the best of our knowledge, these improved results are first shown in this work.',\n",
       "     'id': '6903',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6914',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the generalization properties of ridge regression with random features in the statistical learning  framework. We show for the first time that $O(1/\\\\sqrt{n})$ learning bounds can be achieved with only  $O(\\\\sqrt{n}\\\\log n)$  random features rather than $O({n})$  as suggested by previous results. Further,  we prove  faster learning rates and show that they might require more random features, unless they are sampled according to  a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential  effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties.',\n",
       "     'id': '6914',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6916',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.',\n",
       "     'id': '6916',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6919',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present extensive experimental and theoretical support for the efficacy of recurrent highway networks (RHNs) and recurrent hypernetworks complimentary to the original works. Where the original RHN work primarily provides theoretical treatment of the subject, we demonstrate experimentally that RHNs benefit from far better gradient flow than LSTMs in addition to their improved task accuracy. The original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved--we consider these in depth and frame several feasible solutions that we believe will yield further gains in the future. We demonstrate that these approaches are complementary: by combining RHNs and hypernetworks, we make a significant improvement over current state-of-the-art character-level language modeling performance on Penn Treebank while relying on much simpler regularization. Finally, we argue for RHNs as a drop-in replacement for LSTMs (analogous to LSTMs for vanilla RNNs) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.',\n",
       "     'id': '6919',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6927',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.',\n",
       "     'id': '6927',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6929',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented.  In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus  allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while  fulfilling  any continuous and convex constraining criterion.',\n",
       "     'id': '6929',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6934',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Predicated on the increasing abundance of electronic health records, we investigate the problem of inferring individualized treatment effects using observational data. Stemming from the potential outcomes model, we propose a novel multi-task learning framework in which factual and counterfactual outcomes are modeled as the outputs of a function in a vector-valued reproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian method for learning the treatment effects using a multi-task Gaussian process (GP) with a linear coregionalization kernel as a prior over the vvRKHS. The Bayesian approach allows us to compute individualized measures of confidence in our estimates via pointwise credible intervals, which are crucial for realizing the full potential of precision medicine. The impact of selection bias is alleviated via a risk-based empirical Bayes method for adapting the multi-task GP prior, which jointly minimizes the empirical error in factual outcomes and the uncertainty in (unobserved) counterfactual outcomes. We conduct experiments on observational datasets for an interventional social program applied to premature infants, and a left ventricular assist device applied to cardiac patients wait-listed for a heart transplant. In both experiments, we show that our method significantly outperforms the state-of-the-art.',\n",
       "     'id': '6934',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6949',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary?a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout?s discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.',\n",
       "     'id': '6949',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6950',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.',\n",
       "     'id': '6950',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6963',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a  non-linear transformation between the joint feature/label space distributions of the two domain $\\\\ps$ and $\\\\pt$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target $\\\\pt^f=(X,f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.',\n",
       "     'id': '6963',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6968',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been  particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have  encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow  architecture.   In this paper we  identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data.  To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a  significant performance  boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.',\n",
       "     'id': '6968',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6970',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Influence maximization is the problem of selecting $k$ nodes in a social network to maximize their influence spread. The problem has been extensively studied but most works focus on the submodular influence diffusion models. In this paper, motivated by empirical evidences, we explore influence maximization in the non-submodular regime. In particular, we study the general threshold model in which a fraction of nodes have non-submodular threshold functions, but their threshold functions are closely upper- and lower-bounded by some submodular functions (we call them $\\\\varepsilon$-almost submodular). We first show a strong hardness result: there is no $1/n^{\\\\gamma/c}$ approximation for influence maximization (unless P = NP) for all networks with up to $n^{\\\\gamma}$ $\\\\varepsilon$-almost submodular nodes, where $\\\\gamma$ is in (0,1) and $c$ is a parameter depending on $\\\\varepsilon$. This indicates that influence maximization is still hard to approximate even though threshold functions are close to submodular. We then provide $(1-\\\\varepsilon)^{\\\\ell}(1-1/e)$ approximation algorithms when the number of $\\\\varepsilon$-almost submodular nodes is $\\\\ell$. Finally, we conduct experiments on a number of real-world datasets, and the results demonstrate that our approximation algorithms outperform other baseline algorithms.',\n",
       "     'id': '6970',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6975',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules.  When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.',\n",
       "     'id': '6975',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6979',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.',\n",
       "     'id': '6979',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6988',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.',\n",
       "     'id': '6988',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6997',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.',\n",
       "     'id': '6997',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7010',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can  be learnt either from simple synthetic 2D datasets or  from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.',\n",
       "     'id': '7010',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7011',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A new method for learning variational autoencoders (VAEs) is developed, based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.',\n",
       "     'id': '7011',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7014',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Predicting the future from a sequence of video frames has been recently a sought after yet challenging task in the field of computer vision and machine learning. Although there have been efforts for tracking using motion trajectories and flow features, the complex problem of generating unseen frames has not been studied extensively. In this paper, we deal with this problem using convolutional models within a multi-stage Generative Adversarial Networks (GAN) framework. The proposed method uses two stages of GANs to generate a crisp and clear set of future frames. Although GANs have been used in the past for predicting the future, none of the works consider the relation between subsequent frames in the temporal dimension. Our main contribution lies in formulating two objective functions based on the Normalized Cross Correlation (NCC) and the Pairwise Contrastive Divergence (PCD) for solving this problem. This method, coupled with the traditional L1 loss, has been experimented with three real-world video datasets, viz. Sports-1M, UCF-101 and the KITTI. Performance analysis reveals superior results over the recent state-of-the-art methods.',\n",
       "     'id': '7014',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7019',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Decentralized (PO)MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity, recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDec-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing CDec-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this, we show how a particular decomposition of the approximate action-value function over agents leads to effective updates, and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches.',\n",
       "     'id': '7019',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7020',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmarks datasets.',\n",
       "     'id': '7020',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7025',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants -- using importance values defined by the complete gradient information which changes during optimization -- enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is  (i) provably the \\\\emph{best sampling} with respect to the given bounds,  (ii) always better than uniform sampling and fixed importance sampling and  (iii) can efficiently be computed -- in many applications  at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.',\n",
       "     'id': '7025',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7031',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Non-parametric multivariate density estimation faces strong statistical and computational bottlenecks, and the more practical approaches impose near-parametric assumptions on the form of the density functions. In this paper, we leverage recent developments to propose a class of non-parametric models which have very attractive computational and statistical properties. Our approach relies on the simple function space assumption that the conditional distribution of each variable conditioned on the other variables has a non-parametric exponential family form.',\n",
       "     'id': '7031',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7032',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We address the problem of setting the kernel bandwidth, epps, used by Manifold Learning algorithms to construct the graph Laplacian. Exploiting the connection between manifold geometry, represented by the Riemannian metric, and the Laplace-Beltrami operator, we set epps by optimizing the Laplacian's ability to preserve the geometry of the data. Experiments show that this principled approach is effective and robust\",\n",
       "     'id': '7032',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7035',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).',\n",
       "     'id': '7035',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7041',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This article introduces an energy-based model that is adversarial regarding data: it minimizes the energy for a given data distribution (the positive samples) while maximizing the energy for another given data distribution (the negative or unlabeled samples). The model is especially instantiated with autoencoders where the energy, represented by the reconstruction error, provides a general distance measure for unknown data. The resulting neural network thus learns to reconstruct data from the first distribution while crushing data from the second distribution. This solution can handle different problems such as Positive and Unlabeled (PU) learning or covariate shift, especially with imbalanced data. Using autoencoders allows handling a large variety of data, such as images, text or even dialogues. Our experiments show the flexibility of the proposed approach in dealing with different types of data in different settings: images with CIFAR-10 and CIFAR-100 (not-in-training setting), text with Amazon reviews (PU learning) and dialogues with Facebook bAbI (next response classification and dialogue completion).',\n",
       "     'id': '7041',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7044',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical examples of potential correlations.',\n",
       "     'id': '7044',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7046',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We formulate a supervised learning problem, referred to as continuous ranking, where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space X and the goal is to order all possible observations x in X by means of a scoring function s : X ? R so that s(X) and Y tend to increase or decrease together with highest probability. This problem generalizes bi/multi-partite ranking to a certain extent and the task of finding optimal scoring functions s(x) can be naturally cast as optimization of a dedicated functional cri- terion, called the IROC curve here, or as maximization of the Kendall ? related to the pair (s(X), Y ). From the theoretical side, we describe the optimal elements of this problem and provide statistical guarantees for empirical Kendall ? maximiza- tion under appropriate conditions for the class of scoring function candidates. We also propose a recursive statistical learning algorithm tailored to empirical IROC curve optimization and producing a piecewise constant scoring function that is fully described by an oriented binary tree. Preliminary numerical experiments highlight the difference in nature between regression and continuous ranking and provide strong empirical evidence of the performance of empirical optimizers of the criteria proposed.',\n",
       "     'id': '7046',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7054',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Suppose, we are given a set of $n$ elements to be clustered into $k$ (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, ``do two elements $u$ and $v$ belong to the same cluster?''. The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some  function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution $f_+$ when the underlying pair of elements belong to the same cluster, and from some $f_-$ otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from $\\\\Theta(nk)$ (no similarity matrix) to $O(\\\\frac{k^2\\\\log{n}}{\\\\cH^2(f_+\\\\|f_-)})$ where $\\\\cH^2$ denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an $O(\\\\log{n})$ factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of $k, f_+$ and $f_-$, and only depend logarithmically with $n$.\",\n",
       "     'id': '7054',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7058',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.',\n",
       "     'id': '7058',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7063',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose novel first-order stochastic approximation algorithms for canonical correlation analysis (CCA). Algorithms presented are instances of inexact matrix stochastic gradient (MSG) and inexact matrix exponentiated gradient (MEG), and achieve $\\\\epsilon$-suboptimality in the population objective in $\\\\operatorname{poly}(\\\\frac{1}{\\\\epsilon})$ iterations. We also consider practical variants of the proposed algorithms and compare them with other methods for CCA both theoretically and empirically.',\n",
       "     'id': '7063',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7070',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets.\",\n",
       "     'id': '7070',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7072',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In this paper, we propose an accelerated first-order method for geodesically convex optimization, which is the generalization of the standard Nesterov's accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular, we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems, which show that our method improves the convergence rate from O((1-\\\\mu/L)^{k}) to O((1-\\\\sqrt{\\\\mu/L})^{k}). Moreover, our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k^{2}). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments.\",\n",
       "     'id': '7072',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7073',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, with almost 60% test coverage.',\n",
       "     'id': '7073',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7074',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The inverse covariance matrix provides considerable insight for understanding statistical models in the multivariate setting.  In particular, when the distribution over variables is assumed to be multivariate normal, the sparsity pattern in the inverse covariance matrix, commonly referred to as the precision matrix, corresponds to the adjacency matrix representation of the Gauss-Markov graph, which encodes conditional independence statements between variables.  Minimax results under the spectral norm have previously been established for covariance matrices, both sparse and banded, and for sparse precision matrices.  We establish minimax estimation bounds for estimating banded precision matrices under the spectral norm. Our results greatly improve upon the existing bounds; in particular, we find that the minimax rate for estimating banded precision matrices matches that of estimating banded covariance matrices.  The key insight in our analysis is that we are able to obtain barely-noisy estimates of $k \\\\times k$ subblocks of the precision matrix by inverting slightly wider blocks of the empirical covariance matrix along the diagonal.  Our theoretical results are complemented by experiments demonstrating the sharpness of our bounds.',\n",
       "     'id': '7074',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7077',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of recovering a signal x in R^n, from magnitude-only measurements, y_i = |a_i^T x|  for i={1,2...m}.  Also known as the phase retrieval problem, it is a fundamental challenge in nano-, bio- and astronomical imaging systems, astronomical imaging, and speech processing. The problem is ill-posed, and therefore additional assumptions on the signal and/or the measurements are necessary.  In this paper, we first study the case where the underlying signal x is s-sparse. We develop a novel recovery algorithm that we call Compressive Phase Retrieval with Alternating Minimization, or CoPRAM. Our algorithm is simple and can be obtained via a natural combination of the classical alternating minimization approach for phase retrieval, with the CoSaMP algorithm for sparse recovery. Despite its simplicity, we prove that our algorithm achieves a sample complexity of O(s^2 log n) with Gaussian samples, which matches the best known existing results. It also demonstrates linear convergence in theory and practice and requires no extra tuning parameters other than the signal sparsity level s.  We then consider the case where the underlying signal x arises from to structured sparsity models. We specifically examine the case of block-sparse signals with uniform block size of b and block sparsity k=s/b. For this problem, we design a recovery algorithm that we call Block CoPRAM that further reduces the sample complexity to O(ks log n). For sufficiently large block lengths of b=Theta(s), this bound equates to O(s log n). To our knowledge, this constitutes the first end-to-end linearly convergent algorithm for phase retrieval where the Gaussian sample complexity has a sub-quadratic dependence on the sparsity level of the signal.',\n",
       "     'id': '7077',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7083',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm.  We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games.',\n",
       "     'id': '7083',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7087',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a generalized Gibbs sampler algorithm for obtaining samples approximately distributed from a high-dimensional Gaussian distribution. Similarly to Hogwild methods, our approach does not target the original Gaussian distribution of interest, but an approximation to it. Contrary to Hogwild methods, a single parameter allows us to trade bias for variance. We show empirically that our method is very flexible and performs well compared to Hogwild-type algorithms.',\n",
       "     'id': '7087',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7088',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the question of fair clustering under the {\\\\em disparate impact} doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions---for instance a point may no longer be assigned to its nearest cluster center!  En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective.  We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms.  While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow.  We empirically demonstrate the \\\\emph{price of fairness} by quantifying the value of fair clustering on real-world datasets with sensitive attributes.',\n",
       "     'id': '7088',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7091',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most online optimization algorithms focus on one of two things: performing well in adversarial settings by adapting to unknown data parameters (such as Lipschitz constants), typically achieving $O(\\\\sqrt{T})$ regret, or performing well in stochastic settings where they can leverage some structure in the losses (such as strong convexity), typically achieving $O(\\\\log(T))$ regret. Algorithms that focus on the former problem hitherto achieved $O(\\\\sqrt{T})$ in the stochastic setting rather than $O(\\\\log(T))$. Here we introduce an online optimization algorithm that achieves $O(\\\\log^4(T))$ regret in a wide class of stochastic settings while gracefully degrading to the optimal $O(\\\\sqrt{T})$ regret in adversarial settings (up to logarithmic factors). Our algorithm does not require any prior knowledge about the data or tuning of parameters to achieve superior performance.',\n",
       "     'id': '7091',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7092',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Robots will eventually be part of every household.  It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. A descriptive sentence can provide a stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.  We propose a phrase-based captioning model trained with policy gradients, and design a critic that provides reward to the learner by conditioning on the human-provided feedback. We show  that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.',\n",
       "     'id': '7092',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7116',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.',\n",
       "     'id': '7116',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7118',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Aggregative games provide a rich abstraction to model strategic multi-agent interactions. We focus on learning local aggregative games, where the payoff of each player is a function of its own action and the aggregate behavior of its neighbors in a connected digraph. We show the existence of a pure strategy epsilon-Nash equilibrium in such games when the payoff functions are convex or sub-modular. We prove an information theoretic lower bound, in a value oracle model, on approximating the structure of the digraph with non-negative monotone sub-modular cost functions on the edge set cardinality. We also introduce gamma-aggregative games that generalize local aggregative games, and admit epsilon-Nash equilibrium that are stable with respect to small changes in some specified graph property. Moreover, we provide estimation algorithms for the game theoretic model that can meaningfully recover the underlying structure and payoff functions from real voting data.',\n",
       "     'id': '7118',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7121',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper presents a probabilistic-graphical model that can be used to infer characteristics of instantaneous brain activity by jointly analyzing spatial and temporal dependencies observed in electroencephalograms (EEG). Specifically, we describe a factor-graph-based model with customized factor-functions defined based on domain knowledge, to infer pathologic brain activity with the goal of identifying seizure-generating brain regions in epilepsy patients. We utilize an inference technique based on the graph-cut algorithm to exactly solve graph inference in polynomial time. We validate the model by using clinically collected intracranial EEG data from 29 epilepsy patients to show that the model correctly identifies seizure-generating brain regions. Our results indicate that our model outperforms two conventional approaches used for seizure-onset localization (5-7% better AUC: 0.72, 0.67, 0.65) and that the proposed inference technique provides 3-10% gain in AUC (0.72, 0.62, 0.69) compared to sampling-based alternatives.',\n",
       "     'id': '7121',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7122',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.',\n",
       "     'id': '7122',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7133',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.',\n",
       "     'id': '7133',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7138',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a \"two-player game\" between a generator and a discriminator.  Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence.   In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.',\n",
       "     'id': '7138',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7144',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its saddle point formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this ?dualing GAN? act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.',\n",
       "     'id': '7144',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7151',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.',\n",
       "     'id': '7151',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7152',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects.   In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a trained environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several strong baselines.',\n",
       "     'id': '7152',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7173',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both  multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales  and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character based language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where  we improve state of the art results to  1.19 and 1.25 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on  Hutter Prize Wikipedia outperforming  the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture,  and thus can be flexibly applied to different tasks.',\n",
       "     'id': '7173',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7200',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Hiererachical clustering, that is computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is a fundamental problem in data analysis. Although hierarchical clustering has mostly been studied through procedures such as linkage algorithms, or top-down heuristics, rather than as optimization problems, recently Dasgupta [1] proposed an objective function for hierarchical clustering and initiated a line of work developing algorithms that explicitly optimize an objective (see also [2, 3, 4]). In this paper, we consider a fairly general random graph model for hierarchical clustering, called the hierarchical stochastic blockmodel (HSBM), and show that in certain regimes the SVD approach of McSherry [5] combined with specific linkage methods results in a clustering that give an O(1)-approximation to Dasgupta?s cost function. We also show that an approach based on SDP relaxations for balanced cuts based on the work of Makarychev et al. [6], combined with the recursive sparsest cut algorithm of Dasgupta, yields an O(1) approximation in slightly larger regimes and also in the semi-random setting, where an adversary may remove edges from the random graph generated according to an HSBM. Finally, we report empirical evaluation on synthetic and real-world  data showing that our proposed SVD-based method does indeed achieve a better cost than other widely-used heurstics and also results in a better classification accuracy when the underlying problem was that of multi-class classification.',\n",
       "     'id': '7200',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7203',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.',\n",
       "     'id': '7203',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7204',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized \"spectral complexity\": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the MNIST and CIFAR10 datasets, with both original and random labels;  the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.',\n",
       "     'id': '7204',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7220',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Machine learning is now being used to make crucial decisions about people\\'s lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal \"world\" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.',\n",
       "     'id': '7220',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7232',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a solid harmonic wavelet scattering representation, invariant  to rigid motion and stable to deformations, for regression and classification  of 2D and 3D signals. Solid harmonic wavelets are computed by multiplying solid  harmonic functions with Gaussian windows dilated at different scales. Invariant  scattering coefficients are obtained by cascading such wavelet transforms with  the complex modulus nonlinearity. We study an application of solid harmonic  scattering invariants to the estimation of quantum molecular energies, which  are also invariant to rigid motion and stable with respect to deformations. A multilinear regression  over scattering invariants provides close to state of the art results over  small and large databases of organic molecules.',\n",
       "     'id': '7232',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7236',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the Frank-Wolfe (FW) method for constrained convex optimization, and we show that this classical technique can be interpreted from a different perspective: FW emerges as the computation of an equilibrium (saddle point) of a special convex-concave zero sum game. This saddle-point trick relies on the existence of no-regret online learning to both generate a sequence of iterates but also to provide a proof of convergence through vanishing regret. We show that our stated equivalence has several nice properties, as it exhibits a modularity that gives rise to various old and new algorithms. We explore a few such resulting methods, and provide experimental results to demonstrate correctness and efficiency.',\n",
       "     'id': '7236',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7238',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of $N$ points in $d$ dimensions generated by an unknown mixture of $k$ spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians  accurately if they are sufficiently separated. Assuming each pair of centers are $C\\\\sigma$ distant with $C=\\\\Omega((k\\\\log k)^{1/4}\\\\sigma)$ and where $\\\\sigma^2$ is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians \\\\citep{vempalawang}. For finite samples, we show that a bias term based on the initial estimate decreases at $O(1/{\\\\rm poly}(N))$ rate while variance decreases at nearly optimal rate of $\\\\sigma^2 d/N$. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal $d\\\\cdot k$ while space complexity of our algorithm is $O(dk\\\\log k)$.  In addition to the bias and variance terms which tend to $0$, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an \\\\emph{approximation error} that cannot be avoided. However, by using a streaming version of the classical \\\\emph{(soft-thresholding-based)} EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to $0$ for $N\\\\rightarrow \\\\infty$.\",\n",
       "     'id': '7238',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7246',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network.  Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities.  We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects.  We demonstrate that the learned representations are useful for next-step prediction.',\n",
       "     'id': '7246',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7247',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.',\n",
       "     'id': '7247',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7248',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortised variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.',\n",
       "     'id': '7248',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7253',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we  want. When designing the reward, we might think of some specific scenarios (driving on clean roads), and make sure that the reward will lead to the right behavior in \\\\emph{those} scenarios. Inevitably, agents encounter \\\\emph{new} scenarios (snowy roads), and optimizing the reward can lead to undesired behavior (driving too fast). Our insight in this work is that reward functions are merely \\\\emph{observations} about what the designer \\\\emph{actually} wants, and that they should be interpreted in the context in which they were designed. We introduce \\\\emph{Inverse Reward Design} (IRD) as the problem of inferring the true reward based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach takes a step towards alleviating negative side effects and preventing reward hacking.\",\n",
       "     'id': '7253',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7259',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.',\n",
       "     'id': '7259',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7263',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Our goal is to extract meaningful transformations from raw images, such as varying the thickness of lines in handwriting or the lighting in a portrait. We propose an unsupervised approach to learn such transformations by attempting to reconstruct an image from a linear combination of transformations of its nearest neighbors.  On handwritten digits and celebrity portraits, we show that even with linear transformations, our method generates visually high-quality modified images.  Moreover, since our method is semiparametric and does not model the data distribution, the learned transformations extrapolate off the training data and can be applied to new types of images.',\n",
       "     'id': '7263',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7266',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Matrix factorization (MF) is one of the most popular techniques for product recommendation, but is known to suffer from serious cold-start problems. Item cold-start problems are particularly acute in settings such as Tweet recommendation where new items arrive continuously. In this paper, we present a meta-learning strategy to address item cold-start when new items arrive continuously. We propose two deep neural network architectures that implement our meta-learning strategy. The first architecture learns a linear classifier whose weights are determined by the item history while the second architecture learns a neural network whose biases are instead adjusted. We evaluate our techniques on the real-world problem of Tweet recommendation. On production data at Twitter, we demonstrate that our proposed techniques significantly beat the MF baseline and also outperform production models for Tweet recommendation.',\n",
       "     'id': '7266',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7268',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.',\n",
       "     'id': '7268',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7277',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of learning causal structures with latent variables using interventions. Our objective is not only to learn the causal graph between the observed variables, but to locate unobserved variables that could confound the relationship between observables. Our approach is stage-wise: We first learn the observable graph, i.e., the induced graph between observable variables. Next we learn the existence and location of the latent variables given the observable graph. We propose an efficient randomized algorithm that can learn the observable graph using O(d\\\\log^2 n) interventions where d is the degree of the graph. We further propose an efficient deterministic variant which uses O(log n + l) interventions, where l is the longest directed path in the graph. Next, we propose an algorithm that uses only O(d^2 log n) interventions that can learn the latents between both non-adjacent and adjacent variables. While a naive baseline approach would require O(n^2) interventions, our combined algorithm can learn the causal graph with latents using O(d log^2 n + d^2 log (n)) interventions.',\n",
       "     'id': '7277',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4621',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences.',\n",
       "     'id': '4621',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4637',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by \\\\emph{multiple} latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden.  This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (\\\\emph{i.e.}, third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on $k \\\\times k$ matrices, where $k$ is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.',\n",
       "     'id': '4637',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4639',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study the average case performance of multi-task Gaussian process (GP)   regression as captured in the learning curve, i.e.\\\\ the average Bayes error   for a chosen task versus the total number of examples $n$ for all   tasks. For GP covariances that are the product of an   input-dependent covariance function and a free-form inter-task   covariance matrix, we   show that accurate approximations for the learning curve can be   obtained for an arbitrary number of tasks $T$.  We use   these to study the asymptotic learning behaviour for large   $n$. Surprisingly, multi-task learning can be asymptotically essentially   useless: examples from other tasks only help when the   degree of inter-task correlation, $\\\\rho$, is near its maximal value   $\\\\rho=1$. This effect is most extreme for learning of smooth target   functions as described by e.g.\\\\ squared exponential kernels. We also   demonstrate that when learning {\\\\em many} tasks, the learning curves   separate into an initial phase, where the Bayes error on each task   is reduced down to a plateau value by ``collective learning''    even though most tasks have not seen examples,   and a final decay that occurs only once the number of examples is   proportional to the number of tasks.\",\n",
       "     'id': '4639',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4642',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as Rmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.\",\n",
       "     'id': '4642',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4651',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.',\n",
       "     'id': '4651',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4653',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.',\n",
       "     'id': '4653',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4655',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.',\n",
       "     'id': '4655',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4660',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for   providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions   on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the   underlying i.i.d.\\\\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference   with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model.',\n",
       "     'id': '4660',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4664',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.',\n",
       "     'id': '4664',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4677',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the  gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.',\n",
       "     'id': '4677',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4680',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset.',\n",
       "     'id': '4680',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4681',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics.',\n",
       "     'id': '4681',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4682',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity.',\n",
       "     'id': '4682',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4687',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent work in unsupervised feature learning and deep learning has shown that  being able to train large models can dramatically improve performance.  In this  paper, we consider the problem of training a deep network with billions of  parameters using tens of thousands of CPU cores.  We have developed a  software framework called DistBelief that can utilize computing clusters  with thousands of machines to train large models.  Within this framework, we  have developed two algorithms for large-scale distributed training: (i) Downpour  SGD, an asynchronous stochastic gradient descent procedure supporting a  large number of model replicas, and (ii) Sandblaster, a framework that supports  for a variety of distributed batch optimization procedures, including a distributed  implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both  increase the scale and speed of deep network training.  We have successfully  used our system to train a deep network 100x larger than previously reported in  the literature, and achieves state-of-the-art performance on ImageNet, a visual  object recognition task with 16 million images and 21k categories.  We show that  these same techniques dramatically accelerate the training of a more modestly  sized deep network for a commercial speech recognition service. Although we  focus on and report performance of these methods as applied to training large  neural networks, the underlying algorithms are applicable to any gradient-based  machine learning algorithm.',\n",
       "     'id': '4687',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4708',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency.',\n",
       "     'id': '4708',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4720',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking.',\n",
       "     'id': '4720',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4721',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications. The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects.',\n",
       "     'id': '4721',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4722',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback.\",\n",
       "     'id': '4722',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4723',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.',\n",
       "     'id': '4723',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4725',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise.',\n",
       "     'id': '4725',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4728',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\\\\order(N^{-1}+(N/m)^{-2})$. Whenever $m \\\\le \\\\sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\\\\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset.',\n",
       "     'id': '4728',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4741',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures  depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment {\\\\em biological} neuron membranes, we use a special type of deep {\\\\em artificial} neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a $512 \\\\times 512 \\\\times 30$ stack with known ground truth, and tested on a stack of the same  size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \\\\emph{rand error}, \\\\emph{warping error} and \\\\emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer.',\n",
       "     'id': '4741',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4742',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a \\'subunit\\' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (\"convolutional\") copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the \\'subunits\\' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency.',\n",
       "     'id': '4742',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4744',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations.',\n",
       "     'id': '4744',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4746',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages.',\n",
       "     'id': '4746',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4759',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function.',\n",
       "     'id': '4759',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4768',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph.',\n",
       "     'id': '4768',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4769',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Unsupervised joint alignment of images has been demonstrated to   improve performance on recognition tasks such as face verification.   Such alignment reduces undesired variability due to factors such as   pose, while only requiring weak supervision in the form of poorly   aligned examples.  However, prior work on unsupervised alignment of   complex, real world images has required the careful selection of   feature representation based on hand-crafted image descriptors, in   order to achieve an appropriate, smooth optimization landscape.    In this paper, we instead propose a novel combination of   unsupervised joint alignment with unsupervised feature learning.   Specifically, we incorporate deep learning into the {\\\\em congealing}   alignment framework.  Through deep learning, we obtain features that   can represent the image at differing resolutions based on network   depth, and that are tuned to the statistics of the specific data   being aligned.  In addition, we modify the learning algorithm for   the restricted Boltzmann machine by incorporating a group sparsity   penalty, leading to a topographic organization on the learned   filters and improving subsequent alignment results.    We apply our method to the Labeled Faces in the Wild database   (LFW). Using the aligned images produced by our proposed   unsupervised algorithm, we achieve a significantly higher accuracy   in face verification than obtained using the original face images,   prior work in unsupervised alignment, and prior work in supervised   alignment.  We also match the accuracy for the best available, but   unpublished method.',\n",
       "     'id': '4769',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4780',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task.',\n",
       "     'id': '4780',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4795',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as a ''viewer'' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of the distances between the viewer and the cluster members. Two important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.\",\n",
       "     'id': '4795',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4819',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning.\",\n",
       "     'id': '4819',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4832',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models.',\n",
       "     'id': '4832',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4838',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.',\n",
       "     'id': '4838',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4849',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\\\\y$ can be modeled with a prior model $p(\\\\y)$ and the relations between data and target variables are estimated through $p(\\\\y)$ and a set of uncorresponded data $\\\\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\\\\t$ that maximizes the log likelihood of $f_\\\\t(\\\\x)$ on a uncorresponded training set with regards to $p(\\\\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video.',\n",
       "     'id': '4849',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4850',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods.',\n",
       "     'id': '4850',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4858',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of $k$-anonymity to the $b$-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results confirm improved utility on benchmark and social data-sets.',\n",
       "     'id': '4858',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4862',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Finite-State Transducers (FST) are a standard tool for modeling paired input-output sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. presented a spectral algorithm for learning FST from samples of aligned input-output sequences.  In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as finding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identifiability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently.',\n",
       "     'id': '4862',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4865',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for {\\\\em subspace clustering}. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of Self-Expressiveness\\'\\'. The main difference is that SSC minimizes the vector $\\\\ell_1$ norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank,  we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the \"Self-Expressiveness Property\\'\\' and \"Graph Connectivity\\'\\' at the same time.\"',\n",
       "     'id': '4865',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4872',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on first passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function.',\n",
       "     'id': '4872',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4873',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Shannon's entropy is a basic quantity in information theory, and a  fundamental building block for the analysis of neural codes.   Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable   attention in statistics and theoretical neuroscience.  However,  neural responses have characteristic statistical structure that   generic entropy estimators fail to exploit.  For example, existing  Bayesian entropy estimators make the naive assumption that all spike   words are equally likely a priori, which makes for an  inefficient allocation of prior probability mass in cases where   spikes are sparse.  Here we develop Bayesian estimators for the  entropy of binary spike trains using priors designed to flexibly   exploit the statistical structure of simultaneously-recorded spike  responses.  We define two prior distributions over spike words using   mixtures of Dirichlet distributions centered on simple parametric  models.  The parametric model captures high-level statistical   features of the data, such as the average spike count in a spike  word, which allows the posterior over entropy to concentrate more   rapidly than with standard estimators (e.g., in cases where the  probability of spiking differs strongly from 0.5). Conversely, the   Dirichlet distributions assign prior mass to distributions far from  the parametric model, ensuring consistent estimates for arbitrary   distributions.  We devise a compact representation of the data and  prior that allow for computationally efficient implementations of   Bayesian least squares and empirical Bayes entropy estimators with  large numbers of neurons.  We apply these estimators to simulated   and real neural data and show that they substantially outperform  traditional methods.\",\n",
       "     'id': '4873',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4878',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out\\'\\' neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.\"',\n",
       "     'id': '4878',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4884',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Distributions over exchangeable matrices with infinitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.',\n",
       "     'id': '4884',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4885',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose an approximate inference algorithm for continuous time Gaussian-Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for the discrete time terms and (2) variational updates for the continuous time term. We introduce corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.',\n",
       "     'id': '4885',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4886',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are  illustrated by simulated examples.',\n",
       "     'id': '4886',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4894',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes.\",\n",
       "     'id': '4894',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4904',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We establish theoretical results concerning all local optima of various regularized M-estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss function satisfies restricted strong convexity and the penalty function satisfies suitable regularity conditions, any local optimum of the composite objective function lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models; regression in generalized linear models using nonconvex regularizers such as SCAD and MCP; and graph and inverse covariance matrix estimation. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision epsilon in log(1/epsilon) iterations, which is the fastest possible rate of any first-order method. We provide a variety of simulations to illustrate the sharpness of our theoretical predictions.',\n",
       "     'id': '4904',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4916',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required cannot be directly extended except in a few restricted cases. In this paper, we present efficient approximate DPP sampling schemes based on Nystrom and random Fourier feature approximations that apply to a wide range of kernel functions. We demonstrate the utility of continuous DPPs in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.',\n",
       "     'id': '4916',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4920',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new online learning algorithm that extends the exponentiated gradient to infinite dimensional spaces. Our analysis shows that the algorithm is implicitly able to estimate the $L_2$ norm of the unknown competitor, $U$, achieving a regret bound of the order of $O(U \\\\log (U T+1))\\\\sqrt{T})$, instead of the standard $O((U^2 +1) \\\\sqrt{T})$, achievable without knowing $U$. For this analysis, we introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness. Through a lower bound, we also show that the algorithm is optimal up to $\\\\sqrt{\\\\log T}$ term for linear and Lipschitz losses.',\n",
       "     'id': '4920',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4921',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency implies bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage --orthogonal complement shrinkage-- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.',\n",
       "     'id': '4921',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4923',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20,000 variables. In this paper, we develop an algorithm BigQUIC, which can solve 1 million dimensional l1-regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of specific components. In spite of these modifications,  we are able to theoretically analyze our procedure and show that BigQUIC can achieve super-linear or even quadratic convergence rates.',\n",
       "     'id': '4923',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4924',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Multiple hypothesis testing is a significant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while being simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non parametric method of estimating the FWER for a given ? threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we observe that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub?sampled ? on the order of 0.5% ? matrix completion methods. Thus, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated FWER. Our valuations on four different neuroimaging datasets show that a computational speedup factor of roughly 50? can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated ? threshold is also recovered faithfully, and is stable.',\n",
       "     'id': '4924',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4928',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the  information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of  non-totally randomized trees such as Random Forests and Extra-Trees.',\n",
       "     'id': '4928',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4930',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In the high-dimensional regression model a response variable is linearly related to $p$ covariates,  but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active'  (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying  the active covariates.  A popular approach is to estimate the regression coefficients through the Lasso ($\\\\ell_1$-regularized least squares).  This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set.   We formulate `generalized irrepresentability condition' (GIC), an assumption that is substantially weaker than  irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.\",\n",
       "     'id': '4930',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4934',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'It is a common practice to approximate complicated\\'\\' functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends  the linearity of the proximal map. The new approximation is justified using a recent convex analysis tool---proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims.\"',\n",
       "     'id': '4934',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4935',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difficulties that entail sophisticated algorithms. Our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso.',\n",
       "     'id': '4935',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4938',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].',\n",
       "     'id': '4938',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4942',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we are interested in the development of efficient algorithms  for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information.   We  cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds.  We first examine  a two stages exploration-exploitation based algorithm which first approximates  the stochastic objectives by sampling  and  then solves a constrained stochastic optimization problem by projected gradient method. This method  attains a  suboptimal  convergence rate  even under  strong assumption on the objectives. Our second approach is an efficient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method in constrained optimization and attains  the optimal convergence rate of $[O(1/ \\\\sqrt{T})]$ in high probability for general Lipschitz continuous objectives.',\n",
       "     'id': '4942',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4944',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem.  It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional.  We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space.  A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner.  The algorithm scales efficiently to approximately one million features.  State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.',\n",
       "     'id': '4944',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4949',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Max-product ?belief propagation? (BP) is a popular distributed heuristic for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution ? namely, given a tight LP, can we design a ?good? BP algorithm.  In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most significant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efficient BP-based heuristic for the MWM problem, which consists of making sequential, ?cutting plane?, modifications to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems.',\n",
       "     'id': '4949',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4952',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to find the function?s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploit all the x values tried and all the f(x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further confirm that Gaussian Processes provide a general and unified theoretical account to explain passive and active function learning and search in humans.',\n",
       "     'id': '4952',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4958',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.',\n",
       "     'id': '4958',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4961',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneficial to group the parameters for more efficient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with stripped Beta approximation (Gibbs_SBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). Gibbs_SBA's performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs_SBA also generalize better than the models learned by MLE on real-world Senate voting data.\",\n",
       "     'id': '4961',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4971',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal $\\\\gamma$-discounted optimal policy. We consider two variations of PI: Howard's PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard's PI terminates after at most  $ O \\\\left( \\\\frac{ n m}{1-\\\\gamma} \\\\log \\\\left( \\\\frac{1}{1-\\\\gamma} \\\\right)\\\\right) $ iterations, improving by a factor $O(\\\\log n)$ a result by Hansen et al. (2013), while Simplex-PI terminates after at most $ O \\\\left(  \\\\frac{n^2 m}{1-\\\\gamma} \\\\log \\\\left( \\\\frac{1}{1-\\\\gamma} \\\\right)\\\\right) $ iterations, improving by a factor $O(\\\\log n)$ a result by Ye (2011). Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor~$\\\\gamma$: given a measure of the maximal transient time $\\\\tau_t$ and the maximal time $\\\\tau_r$ to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most $ \\\\tilde O \\\\left( n^3 m^2 \\\\tau_t \\\\tau_r \\\\right) $ iterations. This generalizes a recent result for deterministic MDPs by Post & Ye (2012), in which $\\\\tau_t \\\\le n$ and $\\\\tau_r \\\\le n$. We explain why similar results seem hard to derive for Howard's PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that Simplex-PI and Howard's PI terminate after at most  $ \\\\tilde O(nm (\\\\tau_t+\\\\tau_r))$ iterations.\",\n",
       "     'id': '4971',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4975',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves $O(\\\\sqrt{T\\\\log|\\\\Pi|}+\\\\log|\\\\Pi|)$ regret with respect to a comparison set of policies $\\\\Pi$.  The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efficiently and the comparison set $\\\\Pi$ has polynomial size, this algorithm is efficient.  We also consider the episodic adversarial online shortest path problem.  Here, in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a fixed policy for selecting paths. This problem is a special case of the online MDP problem. For randomly chosen graphs and adversarial losses, this problem can be efficiently solved. We show that it also can be efficiently solved for adversarial graphs and randomly chosen losses.  When both graphs and losses are adversarially chosen, we present an efficient algorithm whose regret scales linearly with the number of distinct graphs.  Finally, we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difficult problem that has been used to design efficient cryptographic schemes.',\n",
       "     'id': '4975',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4985',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new class of structured Schatten norms for tensors that includes two recently  proposed norms (overlapped\\'\\' and \"latent\\'\\') for convex-optimization-based  tensor decomposition. Based on the properties of the structured Schatten norms, we mathematically analyze the performance of \"latent\\'\\' approach for tensor decomposition, which was empirically found to perform better than the \"overlapped\\'\\' approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structures Schatten norms, which is also interesting in the general context of structured sparsity. We confirm through  numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.  \"',\n",
       "     'id': '4985',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4998',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents' types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classification of agents' types using agent-level data. We focus on applications involving data on agents' ranking over alternatives, and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach.\",\n",
       "     'id': '4998',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5001',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation. For further justification, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.',\n",
       "     'id': '5001',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5005',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an Expectation-Maximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion.',\n",
       "     'id': '5005',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5006',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems.  In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to ?share? signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure.',\n",
       "     'id': '5006',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5008',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time.  In addition, we show that this framework can be extended to sampling from cardinality-constrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.',\n",
       "     'id': '5008',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5012',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We provide a general technique for making online learning algorithms differentially private, in both the full information and bandit settings. Our technique applies to algorithms that aim to minimize a \\\\emph{convex} loss function which is a sum of smaller convex loss terms, one for each data point. We modify the popular \\\\emph{mirror descent} approach, or rather a variant called \\\\emph{follow the approximate leader}.    The technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work.  In many cases, our algorithms (in both settings) matching the dependence on the input length, $T$, of the \\\\emph{optimal nonprivate} regret bounds up to logarithmic factors in $T$. Our algorithms require logarithmic space and update time.',\n",
       "     'id': '5012',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5022',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised. Unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and RBMs, because they involve a reconstruction step where the whole input vector is predicted from the current feature values. An algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling  scheme stochastically selecting which input elements to actually reconstruct during training for each particular example.  To generalize this idea to RBMs, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme. We show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks (20 Newsgroups and RCV1), while keeping computational cost linear in the number of non-zeros.',\n",
       "     'id': '5022',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5028',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities. The goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships. Previous models suffer from weak interaction between entities or simple linear projection of the vector space. We address these problems by introducing a neural tensor network (NTN) model which allow the entities and relations to interact multiplicatively. Additionally, we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name, giving an additional dimension of similarity by which entities can share statistical strength. We assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.',\n",
       "     'id': '5028',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5046',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An increasing number of applications require processing of signals defined on weighted graphs. While wavelets provide a flexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less flexible -- they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed. This paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals. Our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network. Particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks. The training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders. After training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph. Improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data.',\n",
       "     'id': '5046',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5056',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identified. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms.',\n",
       "     'id': '5056',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5061',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparamet- ric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-the- art. Via exploratory data analysis?using data with partial ground truth as well as two novel data sets?we find several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) de- tecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using mul- tiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain.',\n",
       "     'id': '5061',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5067',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Markov Decision Processes (MDPs) are extremely useful for modeling and solving sequential decision making problems. Graph-based MDPs provide a compact representation for MDPs with large numbers of random variables. However, the complexity of exactly solving a graph-based MDP usually grows exponentially in the number of variables, which limits their application. We present a new variational framework to describe and solve the planning problem of MDPs, and derive both exact and approximate planning algorithms. In particular, by exploiting the graph structure of graph-based MDPs, we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions, then solved by minimizing a Kullback-Leibler (KL) divergence. The KL divergence is optimized using the belief propagation algorithm, with complexity exponential in only the cluster size of the graph. Experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies.',\n",
       "     'id': '5067',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5070',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often  provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex non-Gaussian factors, there is still a significant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model e.g., a neural network or random forest) to map EP message inputs to EP message outputs.  We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising.',\n",
       "     'id': '5070',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5073',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we theoretically study the problem of binary classification in the presence of random classification noise --- the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is \\\\emph{class-conditional} --- the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence --- methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88\\\\% accuracy even when 40\\\\% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.',\n",
       "     'id': '5073',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4174',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as defined by the recently introduced ?Stable Spline kernel?. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.',\n",
       "     'id': '4174',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4175',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.',\n",
       "     'id': '4175',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4176',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of -the-art active learning approaches.',\n",
       "     'id': '4176',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4179',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Estimating 3D pose from monocular images is a highly ambiguous problem. Physical constraints can be exploited to restrict the space of feasible configurations. In this paper we propose an approach to constraining the prediction of a discriminative predictor. We first show that the mean prediction of a Gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples. We then show how, by performing a change of variables, a GP can be forced to satisfy quadratic constraints. As evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.',\n",
       "     'id': '4179',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4191',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others. We propose a new way of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret. In particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods.',\n",
       "     'id': '4191',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4196',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Principal Components Analysis~(PCA) is often used as a feature extraction procedure. Given a matrix $X \\\\in \\\\mathbb{R}^{n \\\\times d}$, whose rows represent $n$ data points with respect to $d$ features, the top $k$ right singular vectors of $X$ (the so-called \\\\textit{eigenfeatures}), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a \\\\textit{small} number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while \\\\emph{provably} achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.',\n",
       "     'id': '4196',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4199',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Knowledge-based support vector machines (KBSVMs) incorporate advice from domain experts, which can improve generalization significantly. A major limitation that has not been fully addressed occurs when the expert advice is imperfect, which can lead to poorer models. We propose a model that extends KBSVMs and is able to not only learn from data and advice, but also simultaneously improve the advice. The proposed approach is particularly effective for knowledge discovery in domains with few labeled examples. The proposed model contains bilinear constraints, and is solved using two iterative approaches: successive linear programming and a constrained concave-convex approach. Experimental results demonstrate that these algorithms yield useful refinements to expert advice, as well as improve the performance of the learning algorithm overall.',\n",
       "     'id': '4199',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4214',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'State-of-the-art statistical methods in neuroscience have enabled us to fit mathematical models to experimental data and subsequently to infer the dynamics of hidden parameters underlying the observable phenomena. Here, we develop a Bayesian method for inferring the time-varying mean and variance of the synaptic input, along with the dynamics of each ion channel from a single voltage trace of a neuron. An estimation problem may be formulated on the basis of the state-space model with prior distributions that penalize large fluctuations in these parameters. After optimizing the hyperparameters by maximizing the marginal likelihood, the state-space model provides the time-varying parameters of the input signals and the ion channel states. The proposed method is tested not only on the simulated data from the Hodgkin-Huxley type models but also on experimental data obtained from a cortical slice in vitro.',\n",
       "     'id': '4214',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4219',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In this paper, we derive a method to refine a Bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering. At each step, the expert executes an evidence gathering test, which suggests the test's relative diagnostic value. We demonstrate that consistency with an expert's test selection leads to non-convex constraints on the model parameters.  We incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods. Gibbs sampling, stochastic hill climbing and greedy search algorithms are proposed to find a MAP estimate that takes into account test ordering constraints and any data available. We demonstrate our approach on diagnostic sessions from a manufacturing scenario.\",\n",
       "     'id': '4219',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4226',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model?s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for offices, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms.',\n",
       "     'id': '4226',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4231',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Non-negative data are commonly encountered in numerous fields, making non-negative least squares regression (NNLS) a frequently used tool.   At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern  high-dimensional linear models. Even in this setting - unlike first intuition may suggest - we show that for a broad class of designs, NNLS is resistant to overfitting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming L1-regularization.   Since NNLS also circumvents the delicate choice of a regularization parameter, our findings suggest that NNLS may be the method of choice.',\n",
       "     'id': '4231',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4240',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present theoretical and empirical results for a framework  that combines the benefits of apprenticeship and autonomous reinforcement learning.  Our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment.  The first change is replacing previously used Mistake Bound model learners with a recently proposed framework that melds the  KWIK and Mistake Bound supervised learning protocols.  The second change is introducing a communication of expected utility from the student to the teacher.   The resulting system only uses teacher traces when the agent needs to learn concepts it cannot efficiently learn on its own.',\n",
       "     'id': '4240',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4244',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling.  It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation.  In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well.  In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case.  Furthermore, we show that we can efficiently project on to this convex set using only samples generated from the system.  The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods.',\n",
       "     'id': '4244',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4247',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information. The  main application of our results is to the development of distributed  minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to  asynchrony. Our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible. In  application to distributed optimization, we show $n$-node architectures whose optimization error in stochastic problems---in spite of asynchronous delays---scales asymptotically as $\\\\order(1 / \\\\sqrt{nT})$, which is known to be optimal even in the absence of delays.',\n",
       "     'id': '4247',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4255',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A new Le ?vy process prior is proposed for an uncountable collection of covariate- dependent feature-learning measures; the model is called the kernel beta process (KBP). Available covariates are handled efficiently via the kernel construction, with covariates assumed observed with each data sample (?customer?), and latent covariates learned for each feature (?dish?). Each customer selects dishes from an infinite buffet, in a manner analogous to the beta process, with the added constraint that a customer first decides probabilistically whether to ?consider? a dish, based on the distance in covariate space between the customer and dish. If a customer does consider a particular dish, that dish is then selected probabilistically as in the beta process. The beta process is recovered as a limiting case of the KBP. An efficient Gibbs sampler is developed for computations, and state-of-the-art results are presented for image processing and music analysis tasks.',\n",
       "     'id': '4255',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4256',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We address the challenging task of decoupling material properties from lighting properties given a single image. In the last two decades virtually all works have concentrated on exploiting edge information to address this problem. We take a different route by introducing a new prior on reflectance, that models reflectance values as being drawn from a sparse set of basis colors. This results in a Random Field model with global, latent variables (basis colors) and pixel-accurate output reflectance values. We show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information. Finally, we present competitive results by integrating an additional edge model. We believe that our approach is a solid starting point for future development in this domain.',\n",
       "     'id': '4256',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4266',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The L_1 regularized Gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program.  In contrast to other state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and also present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when  compared to other state-of-the-art methods.\",\n",
       "     'id': '4266',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4272',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recently, Mahoney and Orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph Laplacian exactly solve certain regularized Semi-Definite Programs (SDPs). In this paper, we extend that result by providing a statistical interpretation of their approximation procedure. Our interpretation will be analogous to the manner in which l2-regularized or l1-regularized l2 regression (often called Ridge regression and Lasso regression, respectively) can be interpreted in terms of a Gaussian prior or a Laplace prior, respectively, on the coefficient vector of the regression problem. Our framework will imply that the solutions to the Mahoney-Orecchia regularized SDP can be interpreted as regularized estimates of the pseudoinverse of the graph Laplacian. Conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based PageRank procedure for computing an approximation to the first nontrivial eigenvector of the graph Laplacian. Empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm.',\n",
       "     'id': '4272',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4282',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field, and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data.',\n",
       "     'id': '4282',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4286',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of Bayesian inference for continuous time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times. We propose exact inference and sampling methodologies for two specific cases where the discontinuous dynamics is given by a Poisson process and a two-state Markovian switch. We test the methodology on simulated data, and apply it to two real data sets in finance and systems biology. Our experimental results show that the approach leads to valid inferences and non-trivial insights.',\n",
       "     'id': '4286',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4289',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neurons in the neocortex code and compute as part of a locally interconnected population. Large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data. What statistical structure best describes the concurrent spiking of cells within  a local network? We argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling. We test this claim by comparing  a latent dynamical model with realistic spiking observations to coupled generalised  linear spike-response models (GLMs) using cortical recordings. We find that the latent dynamical approach outperforms the GLM in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately. We also compare models whose observations models are either derived from a Gaussian or point-process models, finding that the non-Gaussian model provides slightly  better goodness-of-fit and more realistic population spike counts.',\n",
       "     'id': '4289',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4290',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of Omega(d^2 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.',\n",
       "     'id': '4290',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4293',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer.  Unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded \"local receptive fields\" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality).  In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods.  Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric.  This approach  allows us to harness the advantages of local receptive fields (such  as improved scalability, and reduced data requirements) when we do  not know how to specify such receptive fields by hand or where our  unsupervised training algorithm has no obvious generalization to a  topographic setting.  We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.',\n",
       "     'id': '4293',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4297',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Applications such as robot control and wireless communication require planning under uncertainty. Partially observable Markov decision processes (POMDPs) plan policies for single agents under uncertainty and their decentralized versions (DEC-POMDPs) find a policy for multiple agents. The policy in infinite-horizon POMDP and DEC-POMDP problems has been represented as finite state controllers (FSCs). We introduce a novel class of periodic FSCs, composed of layers connected only to the previous and next layer. Our periodic FSC method finds a deterministic finite-horizon policy and converts it to an initial periodic infinite-horizon policy. This policy is optimized by a new infinite-horizon algorithm to yield deterministic periodic policies, and by a new expectation maximization algorithm to yield stochastic periodic policies. Our method yields better results than earlier planning methods and can compute larger solutions than with regular FSCs.',\n",
       "     'id': '4297',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4308',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Learning problems such as logistic regression are typically formulated as pure  optimization problems defined on some loss function. We argue that this view  ignores the fact that the loss function depends on stochastically generated data  which in turn determines an intrinsic scale of precision for statistical estimation.  By considering the statistical properties of the update variables used during  the optimization (e.g. gradients), we can construct frequentist hypothesis tests  to determine the reliability of these updates. We utilize subsets of the data  for computing updates, and use the hypothesis tests for determining when the  batch-size needs to be increased. This provides computational benefits and avoids  overfitting by stopping when the batch-size has become equal to size of the full  dataset. Moreover, the proposed algorithms depend on a single interpretable  parameter ? the probability for an update to be in the wrong direction ? which is  set to a single value across all algorithms and datasets. In this paper, we illustrate  these ideas on three L1 regularized coordinate algorithms: L1 -regularized L2 -loss  SVMs, L1 -regularized logistic regression, and the Lasso, but we emphasize that  the underlying methods are much more generally applicable.',\n",
       "     'id': '4308',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4310',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Classical Boosting algorithms, such as AdaBoost, build a strong classifier without concern about the computational cost.  Some applications, in particular in computer vision, may involve up to millions of training examples and features.  In such contexts, the training time may become prohibitive.  Several methods exist to accelerate training, typically either by sampling the features, or the examples, used to train the weak learners.  Even if those methods can precisely quantify the speed improvement they deliver, they offer no guarantee of being more efficient than any other, given the same amount of time.    This paper aims at shading some light on this problem, i.e. given a fixed amount of time, for a particular problem, which strategy is optimal in order to reduce the training loss the most.  We apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction.  Experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.',\n",
       "     'id': '4310',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4320',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Extensive evidence suggests that items are not encoded independently in visual short-term memory (VSTM). However, previous research has not quantitatively considered how the encoding of an item influences the encoding of other items. Here, we model the dependencies among VSTM representations using a multivariate Gaussian distribution with a stimulus-dependent mean and covariance matrix. We report the results of an experiment designed to determine the specific form of the stimulus-dependence of the mean and the covariance matrix. We find that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items' feature values, similar to a Gaussian process with a distance-dependent, stationary kernel function. We further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses.\",\n",
       "     'id': '4320',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4331',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The efficient coding hypothesis holds that neural receptive fields are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism's lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive field properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive field properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive field plasticity during an organism's lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, first proposed by Mountcastle (1978), that a qualitatively similar learning algorithm acts throughout primary sensory cortices.\",\n",
       "     'id': '4331',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4334',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classification. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse filtering, a simple new algorithm which is efficient and only has one hyperparameter, the number of features to learn.  In contrast to most other feature learning methods, sparse filtering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function -- the sparsity of L2-normalized features -- which can easily be implemented in a few lines of MATLAB code. Sparse filtering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse filtering on natural images, object classification (STL-10), and phone classification (TIMIT), and show that our method works well on a range of different modalities.',\n",
       "     'id': '4334',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4335',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit $\\\\ell_2$ norm, incoherent columns or features.  But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications).  In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows.  Sparse penalized regression models are analyzed with the purpose of finding, to the extent possible, regimes of dictionary invariant performance.  In particular, a Type II Bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the $\\\\ell_1$ norm, especially in areas where existing theoretical recovery guarantees no longer hold.  This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions.',\n",
       "     'id': '4335',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4337',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination  pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide  experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.',\n",
       "     'id': '4337',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4346',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.',\n",
       "     'id': '4346',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4347',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world benefits from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Specifically, a category hierarchy is utilized to properly define loss function and select common set of features for  related categories. An efficient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach.',\n",
       "     'id': '4347',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4348',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm. In fact, for our application, minimum norm can have a running time of about O(n^7 ) (O(n^5 ) oracle calls). We therefore propose a fast approximate method to minimize arbitrary submodular functions. For a large sub-class of submodular functions, the algorithm is exact. Other submodular functions are iteratively approximated by tight submodular upper bounds, and then repeatedly optimized. We show theoretical properties, and empirical results suggest significant speedups over minimum norm while retaining higher accuracies.',\n",
       "     'id': '4348',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4353',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Given a set V of n vectors in d-dimensional space, we provide an efficient method  for computing quality upper and lower bounds of the Euclidean distances between  a pair of the vectors in V . For this purpose, we define a distance measure, called  the MS-distance, by using the mean and the standard deviation values of vectors in  V . Once we compute the mean and the standard deviation values of vectors in V in  O(dn) time, the MS-distance between them provides upper and lower bounds of  Euclidean distance between a pair of vectors in V in constant time. Furthermore,  these bounds can be refined further such that they converge monotonically to the  exact Euclidean distance within d refinement steps. We also provide an analysis on  a random sequence of refinement steps which can justify why MS-distance should  be refined to provide very tight bounds in a few steps of a typical sequence. The  MS-distance can be used to various problems where the Euclidean distance is used  to measure the proximity or similarity between objects. We provide experimental  results on the nearest and the farthest neighbor searches.',\n",
       "     'id': '4353',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4355',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-the-art models from both schools of thought. First we find the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-and-Fire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the firing probability given a deterministic membrane potential. We find a mathematical expression for this link-function and test the ability of the GLM to predict the firing probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we find that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models.',\n",
       "     'id': '4355',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4360',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many clustering problems, we have access to multiple views of the data each  of which could be individually used for clustering. Exploiting information from  multiple views, one can hope to find a clustering that is more accurate than the  ones obtained using the individual views. Since the true clustering would assign  a point to the same cluster irrespective of the view, we can approach this problem  by looking for clusterings that are consistent across the views, i.e., corresponding  data points in each view should have same cluster membership. We propose a  spectral clustering framework that achieves this goal by co-regularizing the clustering  hypotheses, and propose two co-regularization schemes to accomplish this.  Experimental comparisons with a number of baselines on two synthetic and three  real-world datasets establish the efficacy of our proposed approaches.',\n",
       "     'id': '4360',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4374',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning. However, this expressivity is detrimental to the tractability of inference, when done at the propositional level.  To solve this problem, various lifted inference algorithms have been proposed that reason at the first-order level, about groups of objects as a whole. Despite the existence of various lifted inference approaches, there are currently no completeness results about these algorithms.  The key contribution of this paper is that we introduce a formal definition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models.  We then show how to obtain a completeness result using a first-order knowledge compilation approach for theories of formulae containing up to two logical variables.',\n",
       "     'id': '4374',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4390',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine  learning tasks.  Several researchers have recently proposed schemes  to parallelize SGD, but all require  performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms,  and implementation that SGD can be implemented *without any locking*. We present an update scheme called Hogwild which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild achieves a nearly optimal rate of convergence.  We demonstrate experimentally that Hogwild outperforms alternative schemes that use locking by an order of magnitude.\",\n",
       "     'id': '4390',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4398',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper studies the problem of semi-supervised learning from the vector field perspective. Many of the existing work use the graph Laplacian to ensure the smoothness of the prediction function on the data manifold. However, beyond smoothness, it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems. To achieve this goal, we show that the second order smoothness measures the linearity of the function, and the gradient field of a linear function has to be a parallel vector field. Consequently, we propose to find a function which minimizes the empirical error, and simultaneously requires its gradient field to be as parallel as possible. We give a continuous objective function on the manifold and discuss how to discretize it by using random points. The discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently. The experimental results have demonstrated the effectiveness of our proposed approach.',\n",
       "     'id': '4398',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4417',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer?s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.',\n",
       "     'id': '4417',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4421',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Log-linear models are widely used probability models for statistical pattern recognition. Typically, log-linear models are trained according to a convex criterion. In recent years, the interest in log-linear models has greatly increased. The optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. Different optimization algorithms have been evaluated empirically in many papers. In this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. We verify our findings on two handwriting tasks. By making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.',\n",
       "     'id': '4421',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4434',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity $O(n^3)$  of the original ADM based method to $O(rn^2)$, where $r$ and $n$ are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.',\n",
       "     'id': '4434',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4448',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a novel Adaptive Markov Chain Monte Carlo algorithm to  compute the partition function. In particular, we show how to  accelerate a flat histogram sampling technique by significantly  reducing the number of ``null moves'' in the chain, while maintaining  asymptotic convergence properties. Our experiments show that our  method converges quickly to highly accurate solutions on a range of  benchmark instances, outperforming other state-of-the-art methods such  as IJGP, TRW, and Gibbs sampling both in run-time and accuracy. We  also show how obtaining a so-called density of states distribution  allows for efficient weight learning in Markov Logic theories.\",\n",
       "     'id': '4448',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4453',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.',\n",
       "     'id': '4453',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4456',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'When used to learn high dimensional parametric probabilistic models, the clas- sical maximum likelihood (ML) learning often suffers from computational in- tractability, which motivates the active developments of non-ML learning meth- ods. Yet, because of their divergent motivations and forms, the objective func- tions of many non-ML learning methods are seemingly unrelated, and there lacks a unified framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learn- ing methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score match- ing [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be unified under the minimum KL con- traction framework with different choices of the KL contraction operators.',\n",
       "     'id': '4456',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4464',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The performance of Markov chain Monte Carlo methods is often sensitive to the scaling and correlations between the random variables of interest. An important source of information about the local correlation and scale is given by the Hessian matrix of the target distribution, but this is often either computationally expensive or infeasible. In this paper we propose MCMC samplers that make use of quasi-Newton approximations from the optimization literature, that approximate the Hessian of the target distribution from previous samples and gradients generated by the sampler. A key issue is that MCMC samplers that depend on the history of previous states are in general not valid. We address this problem by using limited memory quasi-Newton methods, which depend only on a fixed window of previous samples. On several real world datasets, we show that the quasi-Newton sampler is a more effective sampler than standard Hamiltonian Monte Carlo at a fraction of the cost of MCMC methods that require higher-order derivatives.',\n",
       "     'id': '4464',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4465',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose an online prediction version of submodular set cover with connections to ranking and repeated active learning.  In each round, the learning algorithm chooses a sequence of items.  The algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function: the number of items needed, when items are selected in order of the chosen sequence, to achieve a coverage constraint.  We develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight.  Our proposed algorithm is readily extended to a setting where multiple functions are revealed at each round and to bandit and contextual bandit settings.',\n",
       "     'id': '4465',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4469',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers--annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators---with the spammers having a score close to zero and the good annotators having a high score close to one.',\n",
       "     'id': '4469',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4475',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper addresses the problem of minimizing a convex, Lipschitz  function $f$ over a convex, compact set $X$ under a stochastic bandit feedback model. In this model, the algorithm is allowed to  observe noisy realizations of the function value $f(x)$ at any query  point $x \\\\in X$. We demonstrate a generalization of the  ellipsoid algorithm that incurs $O(\\\\poly(d)\\\\sqrt{T})$ regret. Since any algorithm has regret at least $\\\\Omega(\\\\sqrt{T})$  on this problem, our algorithm is optimal in terms of the scaling  with $T$.',\n",
       "     'id': '4475',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4480',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We derive algorithms for generalised tensor factorisation (GTF) by building upon the well-established theory of Generalised Linear Models. Our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework, derived for a broad class of exponential family distributions including special cases such as Tweedie's distributions corresponding to $\\\\beta$-divergences. By bounding the step size of the Fisher Scoring iteration of the GLM, we obtain general updates for real data and multiplicative updates for non-negative data. The GTF framework is, then extended easily to address the problems when multiple observed tensors are factorised simultaneously. We illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem.\",\n",
       "     'id': '4480',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4481',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe a novel technique for feature combination in the bag-of-words model of image classification. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classification problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to find discriminative combinations of cues and the resulting vocabulary of portmanteau words is compact, has the cue binding property, and supports individual weighting of cues in the final image representation. State-of-the-art results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, significantly more complex approaches to multi-cue image representation',\n",
       "     'id': '4481',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4487',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process defined over the joint context-action space, and develop CGP-UCB, an intuitive upper-confidence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context.',\n",
       "     'id': '4487',\n",
       "     'year': '2011'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4492',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Given $\\\\alpha,\\\\epsilon$, we study the time complexity   required to improperly learn a halfspace with misclassification   error rate of at most $(1+\\\\alpha)\\\\,L^*_\\\\gamma + \\\\epsilon$, where   $L^*_\\\\gamma$ is the optimal $\\\\gamma$-margin error rate. For $\\\\alpha   = 1/\\\\gamma$, polynomial time and sample complexity is achievable   using the hinge-loss. For $\\\\alpha = 0$, \\\\cite{ShalevShSr11} showed   that $\\\\poly(1/\\\\gamma)$ time is impossible, while learning is   possible in time $\\\\exp(\\\\tilde{O}(1/\\\\gamma))$.  An immediate   question, which this paper tackles, is what is achievable if $\\\\alpha   \\\\in (0,1/\\\\gamma)$.  We derive positive results interpolating between   the polynomial time for $\\\\alpha = 1/\\\\gamma$ and the exponential   time for $\\\\alpha=0$. In particular, we show that there are cases in   which $\\\\alpha = o(1/\\\\gamma)$ but the problem is still solvable in   polynomial time. Our results naturally extend to the adversarial   online learning model and to the PAC learning with malicious noise   model.',\n",
       "     'id': '4492',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4499',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data.',\n",
       "     'id': '4499',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4508',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multiclass classification problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification. We give a ''goodness'' criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using ''good'' similarity functions. We demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs.\",\n",
       "     'id': '4508',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4510',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We  show how binary classification methods developed to work on i.i.d. data can be  used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series.  Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solving  these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods.  Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.',\n",
       "     'id': '4510',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4515',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes.\",\n",
       "     'id': '4515',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4518',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes.',\n",
       "     'id': '4518',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4520',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods.',\n",
       "     'id': '4520',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4523',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification.',\n",
       "     'id': '4523',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4533',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task.',\n",
       "     'id': '4533',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4552',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Active learning can substantially improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron's receptive field (RF) in real time. Bayesian active learning methods maintain a posterior distribution over the RF, and select stimuli to maximally reduce posterior entropy on each time step.  However, existing methods tend to rely on simple Gaussian priors, and do not exploit uncertainty at the level of hyperparameters when determining an optimal stimulus.  This uncertainty can play a substantial role in RF characterization, particularly when RFs are smooth, sparse, or local in space and time.  In this paper, we describe a novel framework for active learning under hierarchical, conditionally Gaussian priors.  Our algorithm uses sequential Markov Chain Monte Carlo sampling (''particle filtering'' with MCMC) over hyperparameters to construct a mixture-of-Gaussians representation of the RF posterior, and selects optimal stimuli using an approximate infomax criterion.  The core elements of this algorithm are parallelizable, making it computationally efficient for real-time experiments.  We apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive field estimates from very limited data, even with a small number of hyperparameter samples.\",\n",
       "     'id': '4552',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4556',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\\\cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.\",\n",
       "     'id': '4556',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4557',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We consider sequential prediction algorithms that are given the predictions from a set of models as inputs. If the nature of the data is changing over time in that different models predict well on different segments of the data, then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior (kind of like a weak restart). However, what if the favored models in each segment are from a small subset, i.e. the data is likely to be predicted well by models that predicted well before? Curiously, fitting such ''sparse composite models'' is achieved by mixing in a bit of all the past posteriors. This self-referential updating method is rather peculiar, but it is efficient and gives superior performance on many natural data sets. Also it is important because it introduces a long-term memory: any model that has done well in the past can be recovered quickly. While Bayesian interpretations can be found for mixing in a bit of the initial prior, no Bayesian interpretation is known for mixing in past posteriors.  We build atop the ''specialist'' framework from the online learning literature to give the Mixing Past Posteriors update a proper Bayesian foundation. We apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound.\",\n",
       "     'id': '4557',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4558',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach  compared to standard methods.',\n",
       "     'id': '4558',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4559',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a multi-task learning approach to jointly estimate the means of multiple independent data sets. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the single-task averages. We derive the optimal amount of regularization, and show that it can be effectively estimated. Simulations and real data experiments demonstrate that MTA  both maximum likelihood and James-Stein estimators, and that our approach to estimating the amount of regularization rivals cross-validation in performance but is more computationally efficient.',\n",
       "     'id': '4559',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4560',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many applications, one has information, e.g., labels that are  provided in a semi-supervised manner, about a specific target region of a  large data set, and one wants to perform machine learning and data analysis  tasks nearby that pre-specified target region.   Locally-biased problems of this sort are particularly challenging for  popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities. In this paper, we address this issue by providing a methodology to construct  semi-supervised eigenvectors of a graph Laplacian, and we illustrate  how these locally-biased eigenvectors can be used to perform  locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized  directions of maximum variance, conditioned on being well-correlated with an  input seed set of nodes that is assumed to be provided in a semi-supervised  manner. We also provide several empirical examples demonstrating how these  semi-supervised eigenvectors can be used to perform locally-biased learning.',\n",
       "     'id': '4560',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4573',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a scalable algorithm for posterior inference of overlapping communities in large networks.  Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel. It naturally interleaves subsampling the network with estimating its community structure.  We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.',\n",
       "     'id': '4573',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4575',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.',\n",
       "     'id': '4575',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4577',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Determinantal point processes (DPPs) have recently been proposed as   computationally efficient probabilistic models of diverse sets for a   variety of applications, including document summarization, image   search, and pose estimation.  Many DPP inference operations,   including normalization and sampling, are tractable; however,   finding the most likely configuration (MAP), which is often required   in practice for decoding, is NP-hard, so we must resort to   approximate inference.  Because DPP probabilities are   log-submodular, greedy algorithms have been used in the past with   some empirical success; however, these methods only give   approximation guarantees in the special case of DPPs with monotone   kernels.  In this paper we propose a new algorithm for approximating   the MAP problem based on continuous techniques for submodular   function maximization.  Our method involves a novel continuous   relaxation of the log-probability function, which, in contrast to   the multilinear extension used for general submodular functions, can   be evaluated and differentiated exactly and efficiently.  We obtain   a practical algorithm with a 1/4-approximation guarantee for a   general class of non-monotone DPPs.  Our algorithm also extends to   MAP inference under complex polytope constraints, making it possible   to combine DPPs with Markov random fields, weighted matchings, and   other models.  We demonstrate that our approach outperforms greedy   methods on both synthetic and real-world data.',\n",
       "     'id': '4577',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4583',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models.',\n",
       "     'id': '4583',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4587',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets.',\n",
       "     'id': '4587',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4589',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.   Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set.',\n",
       "     'id': '4589',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4595',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems. The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device.',\n",
       "     'id': '4595',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4601',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem. Comparisons with the method implemented in the QUIC software package are presented.',\n",
       "     'id': '4601',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4608',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure.',\n",
       "     'id': '4608',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4612',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.',\n",
       "     'id': '4612',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4613',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.',\n",
       "     'id': '4613',\n",
       "     'year': '2012'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3721',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper addresses the problem of noisy Generalized Binary Search (GBS).  GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries.  At each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search.  GBS is used in many applications, including fault testing, machine diagnostics, disease diagnosis, job scheduling, image processing, computer vision, and active learning. In most of these cases, the responses to queries can be noisy.  Past work has provided a partial characterization of GBS, but existing noise-tolerant versions of GBS are suboptimal in terms of sample complexity.  This paper presents the first optimal algorithm for noisy GBS and demonstrates its application to learning multidimensional threshold functions.',\n",
       "     'id': '3721',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3736',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from  $\\\\a$-mixing processes. To illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d. case.',\n",
       "     'id': '3736',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3740',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a {\\\\it conditional renewal} (CR) model for neural spike trains. This model captures both real and rescaled-time effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with $\\\\kappa \\\\neq1$), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.',\n",
       "     'id': '3740',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3749',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.',\n",
       "     'id': '3749',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3753',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'An algorithm is presented for online learning of rotations. The proposed algorithm involves matrix exponentiated gradient updates and is motivated by the Von Neumann divergence. The additive updates are skew-symmetric matrices with trace zero which comprise the Lie algebra of the rotation group. The orthogonality and unit determinant of the matrix  parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact Lie groups. The stability and the computational complexity of the algorithm are discussed.',\n",
       "     'id': '3753',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3755',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The standard assumption of identically distributed training and test data can be violated when an adversary can exercise some control over the generation of the test data. In a prediction game, a learner produces a predictive model while an adversary may alter the distribution of input data. We study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic. We identify conditions under which the prediction game has a unique Nash equilibrium, and derive algorithms that will find the equilibrial prediction models. In a case study, we explore properties of Nash-equilibrial prediction models for email spam filtering empirically.',\n",
       "     'id': '3755',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3765',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The estimation of high-dimensional parametric models requires imposing some structure on the models, for instance that they be sparse, or that matrix structured parameters have low rank. A general approach for such structured parametric model estimation is to use regularized M-estimation procedures, which regularize a loss function that measures goodness of fit of the parameters to the data with some regularization function that encourages the assumed structure. In this paper, we aim to provide a unified analysis of such regularized M-estimation procedures. In particular, we report the convergence rates of such estimators in any metric norm. Using just our main theorem, we are able to rederive some of the many existing results, but also obtain a wide range of novel convergence rates results. Our analysis also identifies key properties of loss and regularization functions such as restricted strong convexity, and decomposability, that ensure the corresponding regularized M-estimators have good convergence rates.',\n",
       "     'id': '3765',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3771',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity.  Rademacher complexity measures a learners ability to fit random data, and can be used to bound the learners true error based on the observed training sample error.  We first review the definition of Rademacher complexity and its generalization bound.  We then describe a learning the noise\" procedure to experimentally measure human Rademacher complexities.  The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overfitting in human learning.  Finally, we discuss the potential applications of human Rademacher complexity in cognitive science.\"',\n",
       "     'id': '3771',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3774',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development, and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.',\n",
       "     'id': '3774',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3788',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. Furthermore, the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts. We also use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors; actually the speedup is almost linearly scalable with the number of multiprocessors available. The proposed partitioning scheme and data streaming can be easily ported to many other models in machine learning.',\n",
       "     'id': '3788',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3789',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe an algorithm for learning bilinear SVMs. Bilinear classifiers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classifier yields a separable filter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classifiers. We also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks. Bilinear classifiers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difficult problems of people detection in video sequences and action classification of video sequences, achieving state-of-the-art results in both.',\n",
       "     'id': '3789',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3792',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the  Bernoulli graph model on data of the software projects AspectJ and Rhino.',\n",
       "     'id': '3792',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3799',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program -- the `smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. Second, experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude.',\n",
       "     'id': '3799',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3815',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences in the features used is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.',\n",
       "     'id': '3815',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3817',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., $\\\\ell_1$-regularizer). Gradient descent methods, though highly scalable and easy to implement, are known to converge slowly on these problems. In this paper, we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple but powerful algorithm.',\n",
       "     'id': '3817',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3821',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard ell_p-regularized objective functions currently used, such as ridge regression and ell_p-regularized boosting, are obtained from a relaxation of the KL divergence between the quasi uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and AdaBoost.',\n",
       "     'id': '3821',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3825',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms.  We provide the first approximation algorithm which solves stochastic games to within $\\\\epsilon$ relative error of the optimal game-theoretic solution, in time polynomial in $1/\\\\epsilon$. Our algorithm extends Murrays and Gordon?s (2007) modified Bellman equation which determines the \\\\emph{set} of all possible achievable utilities; this provides us a truly general framework for multi-agent learning. Further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.',\n",
       "     'id': '3825',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3830',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Score functions induced by generative models extract fixed-dimension feature vectors from different-length data observations by subsuming the process of data generation, projecting them in highly informative spaces called score spaces. In this way, standard discriminative classifiers are proved to achieve higher performances than a solely generative or discriminative approach. In this paper, we present a novel score space that exploits the free energy associated to a generative model through a score function. This function aims at capturing both the uncertainty of the model learning and ``local compliance  of data observations with respect to the generative process. Theoretical justifications and convincing comparative classification results on various generative models prove the goodness of the proposed strategy.',\n",
       "     'id': '3830',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3833',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional \\\\emph{factorial} prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.',\n",
       "     'id': '3833',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3847',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we study the problem of learning a low-dimensional (sparse)  distance matrix.  We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is  non-convex.  We then show that it can be equivalently formulated  as a convex saddle (min-max) problem. From this saddle representation, we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function. This smooth optimization approach has an optimal convergence rate of $O(1 /\\\\ell^2)$ for smooth problems where $\\\\ell$ is the iteration number. Finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.',\n",
       "     'id': '3847',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3849',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Existing models of categorization typically represent to-be-classified items as points in a multidimensional space. While from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions, and have a strong preference to generalize along the axes of these dimensions, but not diagonally\". What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. Our model can be viewed as a type of transformed Dirichlet process mixture model, where it is the learning of the base distribution of the Dirichlet process which allows dimensional generalization.The learning behaviour of our model captures the developmental shift from roughly \"isotropic\" for children to the axis-aligned generalization that adults show.\"',\n",
       "     'id': '3849',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3852',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices.  It turns out that  when the \\\\emph{cluster assumption} holds, that is, when the high density regions are sufficiently separated by  low density valleys, each high density area corresponds to a unique representative eigenvector. Linear combination of such eigenvectors (or, more precisely, of their Nystrom extensions) provide good candidates for good classification functions. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data  with Lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis. Importantly, the sparsity appears naturally from the  cluster assumption. Experimental results on a number  of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).',\n",
       "     'id': '3852',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3858',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we develop an efficient moments-based permutation test approach to improve the system?s efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation.  Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency.',\n",
       "     'id': '3858',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3863',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of learning probabilistic models for complex relational structures between various types of objects.  A model can help us ``understand a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true.  Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have better predictive performance on large data sets.  We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework.  Inference is fully Bayesian but scales well to large data sets.  The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.',\n",
       "     'id': '3863',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3867',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we present a novel approach to learn directed acyclic graphs (DAG) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison. We require identifiability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods.',\n",
       "     'id': '3867',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3868',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1.  A single-hidden-layer neural network of this kind of model achieves 1.5% error on MNIST.  We also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models.  This pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells.  With this pretraining, the same single-hidden-layer model achieves better generalization error, even though the pretraining sample distribution is very different from the fine-tuning distribution.  To implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.',\n",
       "     'id': '3868',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3870',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Across a wide range of cognitive tasks, recent experience in?uences behavior. For example, when individuals repeatedly perform a simple two-alternative forced-choice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g. Jones & Sieck, 2003; Mozer, Kinoshita, & Shettel, 2007; Yu & Cohen, 2008). The Dynamic Belief Model (DBM) (Yu & Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that ?rst-order statistics (base rates) also in?uence sequential effects. We propose a model that learns both ?rst- and second-order sequence properties, each according to the basic principles of the DBM but under a uni?ed inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious ?ts to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Dal Martello, Sahm, & Spillmann, 2005) and electrophysiological studies (Jentzsch & Sommer, 2002), supporting the psychological and neurobiological reality of its two components.',\n",
       "     'id': '3870',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3873',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We adapt a probabilistic latent variable model, namely GaP (Gamma-Poisson), to ad targeting in the contexts of sponsored search (SS) and behaviorally targeted (BT) display advertising. We also approach the important problem of ad positional bias by formulating a one-latent-dimension GaP factorization. Learning from click-through data is intrinsically large scale, even more so for ads. We scale up the algorithm to terabytes of real-world SS and BT data that contains hundreds of millions of users and hundreds of thousands of features, by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality. Specifically, we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems, through the SS and BT implementations, respectively. Finally, we report the experimental results using Yahoos vast datasets, and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy. For BT in particular, the ROC area achieved by GaP is exceeding 0.95, while one prior approach using Poisson regression yielded 0.83. For computational performance, we compare a single-node sparse implementation with a parallel implementation using Hadoop MapReduce, the results are counterintuitive yet quite interesting. We therefore provide insights into the underlying principles of large-scale learning.',\n",
       "     'id': '3873',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3879',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations. This framework can include the permutation-level ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modifications, the methods can work significantly better than their original versions.',\n",
       "     'id': '3879',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3888',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Online learning algorithms have impressive convergence properties   when it comes to risk minimization and convex games on very large   problems. However, they are inherently sequential in their design   which prevents them from taking advantage of modern multi-core   architectures. In this paper we prove that online learning with   delayed updates converges well, thereby facilitating parallel online   learning.',\n",
       "     'id': '3888',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3894',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We establish an excess risk bound of O(H R_n^2 + sqrt{H L*} R_n) for ERM with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = sqrt{R/n}, this translates to a learning rate of ? O(RH/n) in the separable (L* = 0) case and O(RH/n + sqrt{L* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.',\n",
       "     'id': '3894',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3909',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually meaningful\" segments in the ensemble. Knowledge about any specific objects and surfaces present in the image is not available. The selection of image regions occupied by objects is formalized as the maximum-weight independent set (MWIS) problem. MWIS is the heaviest subset of mutually non-adjacent nodes of an attributed graph. We construct such a graph from all segments in the ensemble. Then, MWIS selects maximally distinctive segments that together partition the image. A new MWIS algorithm is presented. The algorithm seeks a solution directly in the discrete domain, instead of relaxing MWIS to a continuous problem, as common in previous work. It iteratively finds a candidate discrete solution of the Taylor series expansion of the original MWIS objective function around the previous solution. The algorithm is shown to converge to a maximum. Our empirical evaluation on the benchmark Berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results.\"',\n",
       "     'id': '3909',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3910',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.',\n",
       "     'id': '3910',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3927',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the confidence region of parameters abound. We propose a decision criterion based on *distributional robustness*: the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible (i.e., it agrees with the a-priori information). We show that finding the optimal distributionally robust policy can be reduced to a standard robust MDP where the parameters belong to a single uncertainty set, hence it can be computed in polynomial time under mild technical conditions.',\n",
       "     'id': '3927',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3930',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend sparse coding to learn interpretable spatio-temporal primitives of human motion.  We cast the problem of learning spatio-temporal primitives as a tensor factorization problem  and introduce constraints to learn interpretable primitives. In particular, we use group norms over those tensors, diagonal constraints on the activations as well as smoothness constraints that are inherent to human motion.   We demonstrate the effectiveness of our approach to learn interpretable representations  of human motion from motion capture data, and show that our approach outperforms  recently developed matching pursuit and  sparse coding algorithms.',\n",
       "     'id': '3930',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3933',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lovasz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.',\n",
       "     'id': '3933',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3946',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.',\n",
       "     'id': '3946',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3952',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.',\n",
       "     'id': '3952',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3961',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations. Recent developments have shown ways to use multiple data sets, provided they originate from identical experiments. We present the MCI-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments. It is fast, reliable and produces very clear and easily interpretable output. It is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models. We test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output. The method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.',\n",
       "     'id': '3961',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3963',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents-a task that is difficult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (such as hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.',\n",
       "     'id': '3963',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3966',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include $K$-fold cross-validation ($K$-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all competing procedures.',\n",
       "     'id': '3966',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3978',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start proving a bound   independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with particular structures such as bipartite graphs or grids.  Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% of the optimal) on MRFs  with large variable-disjoint cycles (MRFs in which all cycles are variable-disjoint, namely that they do not share any edge and in which each cycle contains at least 20 variables).',\n",
       "     'id': '3978',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3984',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Many statistical $M$-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer.  We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $d$ to grow with (and possibly exceed) the sample size $n$.  This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie classical optimization analysis.  We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models.  Under these conditions, our theory guarantees that Nesterov's first-order method~\\\\cite{Nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter $\\\\theta^*$ and the optimal solution $\\\\widehat{\\\\theta}$.  This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates.  Our analysis applies to a wide range of $M$-estimators and statistical models, including sparse linear regression using Lasso ($\\\\ell_1$-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization.  Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.\",\n",
       "     'id': '3984',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3986',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a fast online solver for large scale maximum-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows. We apply the algorithm to optimize tier arrangement of over 80 Million web pages on a layered set of caches to serve an incoming query stream optimally. We provide an empirical demonstration of the effectiveness of our method on real query-pages data.',\n",
       "     'id': '3986',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3988',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint ?2,1-norm minimization on both loss function and regularization. The ?2,1-norm based loss function is robust to outliers in data points and the ?2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method.',\n",
       "     'id': '3988',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3989',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Applications of Brain-Machine-Interfaces typically estimate user intent based on biological signals that are under voluntary control. For example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity. To solve such problems it is necessary to integrate obtained information over time. To do so, state of the art approaches typically use a probabilistic model of how the state, e.g. position and velocity of the arm, evolves over time ? a so-called trajectory model. We wanted to further develop this approach using two intuitive insights: (1) At any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user. (2) The user may want to produce movements at varying speeds. We thus use a generative model with a trajectory model incorporating these insights. Approximate inference on that generative model is implemented using a mixture of extended Kalman filters. We find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.',\n",
       "     'id': '3989',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3990',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efficiently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces for the purpose of classification, such as integrating Yahoo! and DMOZ web directories.',\n",
       "     'id': '3990',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3992',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.',\n",
       "     'id': '3992',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3993',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique to account for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. We illustrate the utility of this framework for inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to PageRank. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.',\n",
       "     'id': '3993',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3995',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe an accelerated hardware neuron being capable of emulating the adap-tive exponential integrate-and-fire neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulation and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.',\n",
       "     'id': '3995',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4002',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efficient exact inference and exact parameter learning. At the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup',\n",
       "     'id': '4002',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In order to overcome such limitations we contribute threefold: (i) we introduce a taxonomy of camera shakes, (ii) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from Hirsch et al.~and a fast algorithm for single image blind deconvolution for space-invariant filters from Cho and Lee to introduce a method for blind deconvolution for space-variant blur. And (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake.',\n",
       "     'id': '4007',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4011',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Arithmetic circuits (ACs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the first ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (AC2-F) is faster and usually more accurate than loopy belief propagation, mean field, and Gibbs sampling; another (AC2-G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines.',\n",
       "     'id': '4011',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4017',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a general framework to online learning for   classification problems with time-varying potential functions in the   adversarial setting. This framework allows to design and prove   relative mistake bounds for any generic loss function. The mistake   bounds can be specialized for the hinge loss, allowing to recover   and improve the bounds of known online classification   algorithms. By optimizing the general bound we derive a new online   classification algorithm, called NAROW, that hybridly uses adaptive- and fixed- second order   information. We analyze the properties of the algorithm and   illustrate its performance using synthetic dataset.',\n",
       "     'id': '4017',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4023',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efficient mean field approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.',\n",
       "     'id': '4023',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4025',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study a setting in which Poisson processes generate sequences of decision-making events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service.',\n",
       "     'id': '4025',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4031',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, Rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10x10 Battleship and Partially Observable PacMan, with approximately 10^18 and 10^56 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.\",\n",
       "     'id': '4031',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4036',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems (Karzanov et al., 1991; Jerrum et al., 2001; Wilson, 2004), theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods.  Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning.  Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (Siepel et al., 2004).   Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models (Wainwright et al., 2008); unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments.   We propose a new framework that extends variational inference to a wide range of combinatorial spaces.  Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm.   We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset (Thompson et al., 1999).',\n",
       "     'id': '4036',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4050',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from V1 with a 96-electrode Utah\" array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron\\'s response, in addition to the neuron\\'s receptive field transfer function. We also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states. This work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions.\"',\n",
       "     'id': '4050',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4054',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.',\n",
       "     'id': '4054',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4059',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We first embed each brain into a functional map that reflects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.',\n",
       "     'id': '4059',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4108',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.',\n",
       "     'id': '4108',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4132',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.',\n",
       "     'id': '4132',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4137',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the $\\\\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.',\n",
       "     'id': '4137',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4140',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We combine random forest (RF) and conditional random field (CRF) into a new computational framework, called random forest random field (RF)^2. Inference of (RF)^2 uses the Swendsen-Wang cut algorithm, characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)^2. (RF)^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)^2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.',\n",
       "     'id': '4140',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4142',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We discuss an online learning framework in which the agent is allowed to say ``I don't know'' as well as making incorrect predictions on given examples. We analyze the trade off between saying ``I don't know'' and making mistakes. If the number of don't know predictions is forced to be zero, the model reduces to the well-known mistake-bound model introduced by Littlestone [Lit88]. On the other hand, if no mistakes are allowed, the model reduces to KWIK framework introduced by Li et. al. [LLW08]. We propose a general, though inefficient, algorithm for general finite concept classes that minimizes the number of don't-know predictions if a certain number of mistakes are allowed. We then present specific polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators.\",\n",
       "     'id': '4142',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4149',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.',\n",
       "     'id': '4149',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4157',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.',\n",
       "     'id': '4157',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4159',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper we consider the fundamental problem of semi-supervised kernel function learning. We propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. We introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the $k$-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.',\n",
       "     'id': '4159',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4163',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel method for multitask learning (MTL) based on {\\\\it manifold regularization}: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common {\\\\it linear} subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is fixed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efficient and easy to implement. We show the efficacy of our method on several datasets.',\n",
       "     'id': '4163',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4165',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Computing a {\\\\em maximum a posteriori} (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a finite mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. We experiment on the real-world protein design dataset and show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM achieves a solution quality within $95$\\\\% of optimal for most instances and is often an order-of-magnitude faster than MPLP.\",\n",
       "     'id': '4165',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4166',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large, or even infinite. We pro- pose a new optimistic, UCB-like, algorithm for non-linearly parameterized bandit problems using the Generalized Linear Model (GLM) framework. We analyze the regret of the proposed algorithm, termed GLM-UCB, obtaining results similar to those recently proved in the literature for the linear regression case. The analysis also highlights a key difficulty of the non-linear case which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice, we provide an asymptotic argument leading to significantly faster convergence. Simulation studies on real data sets illustrate the performance and the robustness of the proposed GLM-UCB approach.',\n",
       "     'id': '4166',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4167',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Optimal control entails combining probabilities and utilities. However, for most practical problems probability densities can be represented only approximately. Choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields.',\n",
       "     'id': '4167',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '4169',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5\\\\%, which is superior to all published results on speaker-independent TIMIT to date.',\n",
       "     'id': '4169',\n",
       "     'year': '2010'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3275',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.',\n",
       "     'id': '3275',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3278',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely appled in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a ``bag-of-words''. It is also critical to properly design ``words'' and ?documents? when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structure among visual words that are essential for solving many vision problems. The spatial information is not encoded in the value of visual words but in the design of documents. Instead of knowing the partition of words into documents \\\\textit{a priori}, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA.\",\n",
       "     'id': '3278',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3292',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 10^4 possible faults from 10^5 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a variational approximation, a mean-field approximation, and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours.',\n",
       "     'id': '3292',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3294',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics. This ``eigenmodel'' generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.\",\n",
       "     'id': '3294',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3295',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier. Most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration. However, single instance selection systems are unable to exploit a parallelized labeler when one is available. Recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration, guided by some heuristic scores. In this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables. The optimization is formuated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account. Although the objective is not convex, we can manipulate a quasi-Newton method to obtain a good local solution. Our empirical studies on UCI datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.',\n",
       "     'id': '3295',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3297',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The control of high-dimensional, continuous, non-linear systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP) are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper, we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing effectively with problems with (at least) 34 state and 14 action dimensions.',\n",
       "     'id': '3297',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3304',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.',\n",
       "     'id': '3304',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3350',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We combine two threads of research on approximate dynamic programming: random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function. This combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states. Our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics. In this paper we show that we can now solve problems we couldn't solve previously with regular grid-based approaches.\",\n",
       "     'id': '3350',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3372',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Automatic relevance determination (ARD), and the closely-related sparse Bayesian learning (SBL) framework, are effective tools for pruning large numbers of irrelevant features. However, popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties. This paper furnishes an alternative means of optimizing a general ARD cost function using an auxiliary function that can naturally be solved using a series of re-weighted L1 problems. The result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods. The analysis also leads to additional insights into the behavior of previous ARD updates as well as the ARD cost function. For example, the standard fixed-point updates of MacKay (1992) are shown to be iteratively solving a particular min-max problem, although they are not guaranteed to lead to a stationary point. The analysis also reveals that ARD is exactly equivalent to performing MAP estimation using a particular feature- and noise-dependent \\\\textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection. In particular, it provides a tighter approximation to the L0 quasi-norm sparsity measure than the L1 norm. Overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.',\n",
       "     'id': '3372',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3384',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse. We present a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efficacy of our method on a microarray dataset containing diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects.',\n",
       "     'id': '3384',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3387',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We address the problem of estimating the ratio of two probability density functions (a.k.a.~the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efficient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches.',\n",
       "     'id': '3387',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3389',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions. The definitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classifier is trained on the resulting sense-specific images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classification experiments show that our dictionary-based approach outperforms baseline methods.',\n",
       "     'id': '3389',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3392',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the following instance of transfer learning: given a pair of regression problems, suppose that the regression coefficients share a partially common support, parameterized by the overlap fraction $\\\\overlap$ between the two supports. This set-up suggests the use of $1, \\\\infty$-regularized linear regression for recovering the support sets of both regression vectors. Our main contribution is to provide a sharp characterization of the sample complexity of this $1,\\\\infty$ relaxation, exactly pinning down the minimal sample size $n$ required for joint support recovery as a function of the model dimension $\\\\pdim$, support size $\\\\spindex$ and overlap $\\\\overlap \\\\in [0,1]$. For measurement matrices drawn from standard Gaussian ensembles, we prove that the joint $1,\\\\infty$-regularized method undergoes a phase transition characterized by order parameter $\\\\orpar(\\\\numobs, \\\\pdim, \\\\spindex, \\\\overlap) = \\\\numobs{(4 - 3 \\\\overlap) s \\\\log(p-(2-\\\\overlap)s)}$. More precisely, the probability of successfully recovering both supports converges to $1$ for scalings such that $\\\\orpar > 1$, and converges to $0$ to scalings for which $\\\\orpar < 1$. An implication of this threshold is that use of $1, \\\\infty$-regularization leads to gains in sample complexity if the overlap parameter is large enough ($\\\\overlap > 2/3$), but performs worse than a naive approach if $\\\\overlap < 2/3$. We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. Thus, our results illustrate both the benefits and dangers associated with block-$1,\\\\infty$ regularization in high-dimensional inference.',\n",
       "     'id': '3392',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3393',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise (i.e., heteroscedasticity) varies spatially. Unfortunately, it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. In this paper, we focus on nonparametric regression and introduce a Bayesian formulation that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efficient (suitable for large data sets), requires no sampling, automatically rejects outliers and has only one prior to be specified. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian Processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law.',\n",
       "     'id': '3393',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3399',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overfitting, but is also built on the desirable discriminative laws resemble those of the M$^3$N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M$^3$N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task.',\n",
       "     'id': '3399',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3402',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for `local ranking', in the sense that the ranking model is defined on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two specific information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.\",\n",
       "     'id': '3402',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3406',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Accurate and efficient inference in evolutionary trees is a central problem in computational biology. Realistic models require tracking insertions and deletions along the phylogenetic tree, making inference challenging. We propose new sampling techniques that speed up inference and improve the quality of the samples. We compare our method to previous approaches and show performance improvement on metrics evaluating multiple sequence alignment and reconstruction of ancestral sequences.',\n",
       "     'id': '3406',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3434',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are acquired at fixed intervals whereas the events causing data artifacts may occur at any time and have durations that may be significantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables effective detection of artifacts and accurate estimation of the underlying blood pressure values.',\n",
       "     'id': '3434',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3438',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Clustering stability is an increasingly popular family of methods for performing model selection in data clustering. The basic idea is that the chosen model should be stable under perturbation or resampling of the data. Despite being reasonably effective in practice, these methods are not well understood theoretically, and present some difficulties. In particular, when the data is assumed to be sampled from an underlying distribution, the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases. This raises a potentially serious practical difficulty with these methods, because it means there might be some hard-to-compute sample size, beyond which clustering stability estimators 'break down' and become unreliable in detecting the most stable model. Namely, all models will be relatively stable, with differences in their stability measures depending mostly on random and meaningless sampling artifacts. In this paper, we provide a set of general sufficient conditions, which ensure the reliability of clustering stability estimators in the large sample regime. In contrast to previous work, which concentrated on specific toy distributions or specific idealized clustering frameworks, here we make no such assumptions. We then exemplify how these conditions apply to several important families of clustering algorithms, such as maximum likelihood clustering, certain types of kernel clustering, and centroid-based clustering with any Bregman divergence. In addition, we explicitly derive the non-trivial asymptotic behavior of these estimators, for any framework satisfying our conditions. This can help us understand what is considered a 'stable' model by these estimators, at least for large enough samples.\",\n",
       "     'id': '3438',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3441',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new family of linear time algorithms based on sufficient statistics for string comparison with mismatches under the string kernels framework. Our algorithms improve theoretical complexity bounds of existing approaches while scaling well with respect to the sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets with loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classification, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with significantly reduced running times.',\n",
       "     'id': '3441',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3459',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world. In this paper, we formally define an infinite sequence of nested beliefs about the state of the world at the current time $t$ and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efficient filtering in a range of multi-agent domains.',\n",
       "     'id': '3459',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3466',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the epsilon-insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we briefly discuss a trade-off in epsilon between sparsity and accuracy if the SVM is used to estimate the conditional median.',\n",
       "     'id': '3466',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3475',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Current on-line learning algorithms for predicting the labelling of a graph have an important limitation in the case of large diameter graphs; the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this problem with an efficient algorithm which achieves a logarithmic mistake bound. Furthermore, current algorithms are optimised for data which exhibits cluster-structure; we give an additional algorithm which performs well locally in the presence of cluster structure and on large diameter graphs.',\n",
       "     'id': '3475',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3481',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (~ 2mm cube) of brain tissue. Our model, which we call the SpAM V1 model, is based on the reasonable assumption that fMRI measurements reflect the (possibly nonlinearly) pooled, rectified output of a large population of simple and complex cells in V1. It has a hierarchical filtering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called ???pooled-complex??? cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the SpAM V1 model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive fields, frequency tuning and orientation tuning curves of the SpAM V1 model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the SpAM V1 model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities.',\n",
       "     'id': '3481',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3484',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in HazanKaKaAg06. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions.',\n",
       "     'id': '3484',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3489',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper presents the first data-dependent generalization bounds for non-i.i.d. settings based on the notion of Rademacher complexity. Our bounds extend to the non-i.i.d. case existing Rademacher complexity bounds derived for the i.i.d. setting. These bounds provide a strict generalization of the ones found in the i.i.d. case, and can also be used within the standard i.i.d. scenario. They apply to the standard scenario of beta-mixing stationary sequences examined in many previous studies of non-i.i.d. settings and benefit form the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from finite samples and lead to tighter bounds.',\n",
       "     'id': '3489',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3505',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"EEG connectivity measures could provide a new type of feature space for inferring a subject's intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the gamma-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions.\",\n",
       "     'id': '3505',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3506',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random field (MRF) methods. Moreover, we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean field theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difficulties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in MRF approaches with even hundreds of parameters.',\n",
       "     'id': '3506',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3520',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efficient methods, i.e., Semi-Infinite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method.',\n",
       "     'id': '3520',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3525',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditional-independence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable.',\n",
       "     'id': '3525',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3526',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: (1) Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. (2) Convex relaxation such as $L_1$-regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-$L_1$ regularization. Our performance bound shows that the procedure is superior to the standard $L_1$ convex relaxation for learning sparse targets. Experiments confirm the effectiveness of this method on some simulation and real data.',\n",
       "     'id': '3526',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3527',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a new, massively parallel architecture for accelerating machine learning algorithms, based on arrays of variable-resolution arithmetic vector processing elements (VPE). Groups of VPEs operate in SIMD (single instruction multiple data) mode, and each group is connected to an independent memory bank. In this way memory bandwidth scales with the number of VPE, and the main data flows are local, keeping power dissipation low. With 256 VPEs, implemented on two FPGA (field programmable gate array) chips, we obtain a sustained speed of 19 GMACS (billion multiply-accumulate per sec.) for SVM training, and 86 GMACS for SVM classification. This performance is more than an order of magnitude higher than that of any FPGA implementation reported so far. The speed on one FPGA is similar to the fastest speeds published on a Graphics Processor for the MNIST problem, despite a clock rate of the FPGA that is six times lower. High performance at low clock rates makes this massively parallel architecture particularly attractive for embedded applications, where low power dissipation is critical. Tests with Convolutional Neural Networks and other learning algorithms are under way now.',\n",
       "     'id': '3527',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3529',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are simply performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a Gaussian process model of human function learning that combines the strengths of both approaches.',\n",
       "     'id': '3529',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3536',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden multinomial unit that represents the cluster labels. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data.',\n",
       "     'id': '3536',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3548',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models are often used because these models are well understood and there are well-known methods to fit them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities.',\n",
       "     'id': '3548',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3553',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network.',\n",
       "     'id': '3553',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3562',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surpris- ingly impressive performance improvements over traditional one-sided (row) clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation (e.g., non-negative matrix factorization) formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms.',\n",
       "     'id': '3562',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3567',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.',\n",
       "     'id': '3567',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3568',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one. We demonstrate the effectiveness of our ``treewidth-friendly'' method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth.\",\n",
       "     'id': '3568',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3570',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems.',\n",
       "     'id': '3570',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3572',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Conditional Random Sampling (CRS) was originally proposed for efficiently computing pairwise ($l_2$, $l_1$) distances, in static, large-scale, and sparse data sets such as text and Web data. It was previously presented using a heuristic argument. This study extends CRS to handle dynamic or streaming data, which much better reflect the real-world situation than assuming static data. Compared with other known sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a significant advantage in that it is ``one-sketch-for-all.'' In particular, we demonstrate that CRS can be applied to efficiently compute the $l_p$ distance and the Hilbertian metrics, both are popular in machine learning. Although a fully rigorous analysis of CRS is difficult, we prove that, with a simple modification, CRS is rigorous at least for an important application of computing Hamming norms. A generic estimator and an approximate variance formula are provided and tested on various applications, for computing Hamming norms, Hamming distances, and $\\\\chi^2$ distances.\",\n",
       "     'id': '3572',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3586',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory.',\n",
       "     'id': '3586',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3587',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects. This unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We confirm this prediction with an experiment.',\n",
       "     'id': '3587',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3589',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Classical Game Theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in Economic games of human subjects. We investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a Partially Observable Markov Decision Process model that incorporates Game Theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investment game. We invert the generative process for a recognition model that is used to classify 200 subjects playing an Investor-Trustee game against randomly matched opponents.',\n",
       "     'id': '3589',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3591',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The machine learning problem of classifier design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the specification of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classification problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classifier design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations.',\n",
       "     'id': '3591',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3594',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We consider the problem of binary classification where the classifier may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow's rule, is defined by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classifier, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efficiently. We finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions.\",\n",
       "     'id': '3594',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3605',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider a generalization of stochastic bandit problems where the set of arms, X, is allowed to be a generic topological space. We constraint the mean-payoff function with a dissimilarity function over X in a way that is more general than Lipschitz. We construct an arm selection policy whose regret improves upon previous result for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally H?lder with a known exponent, then the expected regret is bounded up to a logarithmic factor by $n$, i.e., the rate of the growth of the regret is independent of the dimension of the space. Moreover, we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider.',\n",
       "     'id': '3605',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3610',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum and hippocampus CA3. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between firing and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points.',\n",
       "     'id': '3610',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3614',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its far-reaching application, there is almost no work on applying stochastic approximation to learning problems with constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization.',\n",
       "     'id': '3614',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3626',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov decision process, target policy, and exciting behavior policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d.\\\\ policy-evaluation setting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L_2 norm. Our analysis proves that its expected update is in the direction of the gradient, assuring convergence under the usual stochastic approximation conditions to the same least-squares solution as found by the LSTD, but without its quadratic computational complexity. GTD is online and incremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods.',\n",
       "     'id': '3626',\n",
       "     'year': '2008'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3629',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'While many advances have already been made on the topic of hierarchical classi-  ?cation learning, we take a step back and examine how a hierarchical classi?ca-  tion problem should be formally de?ned. We pay particular attention to the fact  that many arbitrary decisions go into the design of the the label taxonomy that  is provided with the training data, and that this taxonomy is often unbalanced.  We correct this problem by using the data distribution to calibrate the hierarchical  classi?cation loss function. This distribution-based correction must be done with  care, to avoid introducing unmanagable statstical dependencies into the learning  problem. This leads us off the beaten path of binomial-type estimation and into  the uncharted waters of geometric-type estimation. We present a new calibrated  de?nition of statistical risk for hierarchical classi?cation, an unbiased geometric  estimator for this risk, and a new algorithmic reduction from hierarchical classi?-  cation to cost-sensitive classi?cation.',\n",
       "     'id': '3629',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3631',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using Fisher Information. Here, we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the Jensen-Shannon information to study neural population codes. We first explore the relationship between minimum discrimination error, Jensen-Shannon Information and Fisher Information and show that the discrimination framework is more informative about the coding accuracy than Fisher Information as it defines an error for any pair of possible stimuli. In particular, it includes Fisher Information as a special case. Second, we use the framework to study population codes of angular variables. Specifically, we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows. That is, for long time window we use the common Gaussian noise approximation. To address the case of short time windows we analyze the Ising model with identical noise correlation structure. In this way, we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.',\n",
       "     'id': '3631',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3636',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system.',\n",
       "     'id': '3636',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3640',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by first principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning.  We experiment with manifold embeddings as the reconstructed observable state-space of an off-line, model-based reinforcement learning approach to control. We demonstrate the embedding of a system changes as a result of learning and that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system.  We apply this approach in simulation to learn a neurostimulation policy that is more efficient in treating epilepsy than conventional policies.  We then demonstrate the learned policy completely suppressing seizures in real-world neurostimulation experiments on actual animal brain slices.',\n",
       "     'id': '3640',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3644',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Modern machine learning-based approaches to computer vision require very large databases of labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector). While the collection of these large databases is becoming a bottleneck, new Internet-based services that allow labelers from around the world to be easily hired and managed provide a promising solution.  However, using these services to label large databases brings with it new theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used ``Majority Vote heuristic for inferring image labels, and is robust to both adversarial and noisy labelers.',\n",
       "     'id': '3644',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3645',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data.  In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data.  We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reflects known properties of OPMs, and a noise covariance adjusted to the data.  The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements.  By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations,  pinwheel locations or -counts.  Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and provides the basis for decoding studies.  We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex.',\n",
       "     'id': '3645',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3652',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points.  We show that in $\\\\R^d$, $d \\\\geq 2$, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function.  We also contrast the method with the Laplacian Eigenvector method, and discuss the ``smoothness assumptions associated with this alternate method.',\n",
       "     'id': '3652',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3670',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We extend Dyna planning architecture for policy evaluation and control in two significant aspects. First, we introduce a multi-step Dyna planning that projects the simulated state/feature many steps into the future. Our multi-step Dyna is based on a multi-step model, which we call the {\\\\em $\\\\lambda$-model}. The $\\\\lambda$-model interpolates between the one-step model and an infinite-step model, and can be learned efficiently online. Second, we use for Dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run. Experimental results show that Dyna using the multi-step model evaluates a policy faster than using single-step models; Dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms; further, multi-step Dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step Dyna algorithms.',\n",
       "     'id': '3670',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3681',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity.  While typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient, globally-convergent reweighted $\\\\ell_1$ minimization procedure.  The first method under consideration arises from the sparse Bayesian learning (SBL) framework.  Although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the Lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity profile, there will always exist cases where it does better.  These results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions.  We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests.  For both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted $\\\\ell_1$-norm algorithms in handling more general sparse estimation problems involving classification, group feature selection, and non-negativity constraints.  As a byproduct of this development, a rigorous reformulation of sparse Bayesian classification (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-defined objective function.',\n",
       "     'id': '3681',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3684',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory.',\n",
       "     'id': '3684',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3685',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a Bayesian nonparametric approach to relating multiple time series via a set of latent, dynamical behaviors.  Using a beta process prior, we allow data-driven selection of the size of this set, as well as the pattern with which behaviors are shared among time series.  Via the Indian buffet process representation of the beta process predictive distributions, we develop an exact Markov chain Monte Carlo inference method.  In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals.  We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising unsupervised segmentation of visual motion capture data.',\n",
       "     'id': '3685',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3686',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'When used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. Empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. We propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. We also develop a theory that motivates the algorithm.',\n",
       "     'id': '3686',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3692',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. It analyzes this problem in the case of regression and the kernel ridge regression algorithm. It examines the corresponding learning kernel optimization problem, shows how that minimax problem can be reduced to a simpler minimization problem, and proves that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.',\n",
       "     'id': '3692',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3697',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Given $n$ noisy samples with $p$ dimensions, where $n \\\\ll p$, we show that the multi-stage thresholding procedures can accurately estimate a sparse vector $\\\\beta \\\\in \\\\R^p$ in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very significant values of $s$, which is the number of non-zero elements in the true parameter $\\\\beta$. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if $X$ obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the Gauss-Dantzig selector (Cand\\\\{e}s-Tao 07) achieves the $\\\\ell_2$ loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.',\n",
       "     'id': '3697',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3702',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.',\n",
       "     'id': '3702',\n",
       "     'year': '2009'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3163',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'It is known that determinining whether a DEC-POMDP, namely, a cooperative partially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how cooperation affected that complexity. We show that, for competitive POSGs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class NEXP with an oracle for NP.',\n",
       "     'id': '3163',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3190',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a model that leverages the millions of clicks received by web search engines, to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.',\n",
       "     'id': '3190',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3212',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.',\n",
       "     'id': '3212',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3239',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are de- signed for specific learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. We also illustrate their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.',\n",
       "     'id': '3239',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3250',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts. Thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and epsilon-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.',\n",
       "     'id': '3250',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3259',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables. The distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution. We prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used. We illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts. Finally, we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.\",\n",
       "     'id': '3259',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '3265',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade earning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classifier can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate significant performance advantages.',\n",
       "     'id': '3265',\n",
       "     'year': '2007'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5197',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach we develop a generalization of the Max-Path search algorithm, which allows us to efficiently search over a structured space of multiple spatio-temporal paths, while also allowing to incorporate context information into the model. Instead of using spatial annotations, in the form of bounding boxes, to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, we show how our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.',\n",
       "     'id': '5197',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5206',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'One approach to computer object recognition and modeling the brain\\'s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D affine transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformation-invariance, we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identity-preserving transformations. The model\\'s wiring can be learned from videos of transforming objects---or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically confirm theoretical predictions for the case of 2D affine transformations. Next, we apply the model to non-affine transformations: as expected, it performs well on face verification tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter transformations\\'\\' which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical findings, we tested the same model on face verification benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig and a new dataset we gathered---achieving strong performance in these highly unconstrained cases as well.\"',\n",
       "     'id': '5206',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5208',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time. We describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy. Our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade. Our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of Vector Quantization levels, and by choosing to rescore windows or not. Our method can be directly plugged into any recognition system that relies on linear templates. We demonstrate our method to speed up the original Exemplar SVM detector [1] by an order of magnitude and Deformable Part models [2] by two orders of magnitude with no loss of accuracy.',\n",
       "     'id': '5208',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5211',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'All the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task. In this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks. Specifically, we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it. For regression problems, we extend the kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods.',\n",
       "     'id': '5211',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5212',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However, expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed, additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modifications. Furthermore, the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies, but can have different relevant features and model coefficients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data.',\n",
       "     'id': '5212',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5214',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing  empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives consistently better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n'th order bottom sample margin.\",\n",
       "     'id': '5214',\n",
       "     'year': '2013'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5232',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time. In this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.',\n",
       "     'id': '5232',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5240',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first {\\\\em fast} oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel {\\\\em without} explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.',\n",
       "     'id': '5240',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5249',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, \\\\emph{weighted} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).',\n",
       "     'id': '5249',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5258',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this work we introduce a new fast incremental gradient method SAGA, in the spirit of SAG, SDCA, MISO and SVRG. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.',\n",
       "     'id': '5258',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5259',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.',\n",
       "     'id': '5259',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5268',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.',\n",
       "     'id': '5268',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5271',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.',\n",
       "     'id': '5271',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5276',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.\",\n",
       "     'id': '5276',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5278',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter lambda. We use a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization constraints, such as l1, l2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the MNIST and rectangles dataset significantly.',\n",
       "     'id': '5278',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5281',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.',\n",
       "     'id': '5281',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5282',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256x256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.',\n",
       "     'id': '5282',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5284',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.',\n",
       "     'id': '5284',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5296',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.',\n",
       "     'id': '5296',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5301',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (1) discovering topic prevalence over time, and (2) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling, based on the forward-filtering backward-sampling algorithm. Moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.',\n",
       "     'id': '5301',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5308',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity be- tween subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.',\n",
       "     'id': '5308',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5316',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonparanormal blossoms\", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then \"grown\" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.\"',\n",
       "     'id': '5316',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5318',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE ``breaks down'' under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the $\\\\ell_1$-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.\",\n",
       "     'id': '5318',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5320',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we address the problem of learning the structure of Gaussian chain graph models in a high-dimensional space. Chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges. While the problem of sparse structure learning has been studied extensively for Gaussian graphical models and more recently for conditional Gaussian graphical models (CGGMs), there has been little previous work on the structure recovery of Gaussian chain graph models. We consider linear regression models and a re-parameterization of the linear regression models using CGGMs as building blocks of chain graph models. We argue that when the goal is to recover model structures, there are many advantages of using CGGMs as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning. We demonstrate our approach on simulated and genomic datasets.',\n",
       "     'id': '5320',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5321',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time (Iwata and Orlin 2009), however these algorithms are not practical. In 1976, Wolfe proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For general submodular functions, the Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance. Despite its good practical performance, theoretically very little is known about Wolfe's minimum norm algorithm -- to our knowledge the only result is an exponential time analysis due to Wolfe himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns a O(1/t)-approximate solution to the min-norm point. We also prove a robust version of Fujishige's theorem which shows that an O(1/n^2)-approximate solution to the min-norm point problem implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for submodular function minimization. In particular, we show that the min-norm point algorithm solves SFM in O(n^7F^2)-time, where $F$ is an upper bound on the maximum change a single element can cause in the function value.\",\n",
       "     'id': '5321',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5324',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.',\n",
       "     'id': '5324',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5325',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.',\n",
       "     'id': '5325',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5328',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the `median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.\",\n",
       "     'id': '5328',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5330',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the ``online'' setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).\",\n",
       "     'id': '5330',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5333',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.',\n",
       "     'id': '5333',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5336',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.\",\n",
       "     'id': '5336',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5345',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.',\n",
       "     'id': '5345',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5346',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\",\n",
       "     'id': '5346',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5354',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given metric distance function over the label set. Popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.',\n",
       "     'id': '5354',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5358',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The task of reconstructing a matrix given a sample of observed entries is known as the \\\\emph{matrix completion problem}. Such a consideration arises in a wide variety of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here, we investigate the case where the observations take a finite numbers of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (non-necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.',\n",
       "     'id': '5358',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5359',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of ``public'' users who are willing to share their preferences openly, and a large set of ``private'' users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.\",\n",
       "     'id': '5359',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5361',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cram\\\\'er-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cram\\\\'er-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.\",\n",
       "     'id': '5361',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5368',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing, with probability $1-\\\\delta$, an $\\\\epsilon$-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the optimism in the face of uncertainty\" principle. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.\"',\n",
       "     'id': '5368',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5373',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.',\n",
       "     'id': '5373',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5375',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.',\n",
       "     'id': '5375',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5377',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.',\n",
       "     'id': '5377',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5384',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that our proximal quasi-Newton algorithm is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.',\n",
       "     'id': '5384',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5389',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent input noise\" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.\"',\n",
       "     'id': '5389',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5393',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of {\\\\em adversarial} workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of {\\\\em sophisticated} adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.',\n",
       "     'id': '5393',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5398',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a general framework for regularization based on group majorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to lie in the convex hull of this orbit (the orbitope). Common regularizers are recovered as particular cases, and a connection is revealed between the recent sorted 1 -norm and the hyperoctahedral group. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.',\n",
       "     'id': '5398',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5400',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.',\n",
       "     'id': '5400',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5417',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation. This applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples. Such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects. Likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation. By assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the Fourier domain that can eliminate most redundancies. It can leverage off-the-shelf solvers with no modification (e.g. libsvm), and train several pose classifiers simultaneously at no extra cost. Our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.',\n",
       "     'id': '5417',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5418',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at',\n",
       "     'id': '5418',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5422',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.\",\n",
       "     'id': '5422',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5423',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.',\n",
       "     'id': '5423',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5430',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a new provable method for robust PCA, where the task is to recover a low-rank matrix, which is corrupted with sparse perturbations. Our method consists of simple alternating projections onto the set of low rank and sparse matrices with intermediate de-noising steps. We prove correct recovery of the low rank and sparse components under tight recovery conditions, which match those for the state-of-art convex relaxation techniques. Our method is extremely simple to implement and has low computational complexity. For a $m \\\\times n$ input matrix (say m \\\\geq n), our method has O(r^2 mn\\\\log(1/\\\\epsilon)) running time, where $r$ is the rank of the low-rank component and $\\\\epsilon$ is the accuracy. In contrast, the convex relaxation methods have a running time O(mn^2/\\\\epsilon), which is not scalable to large problem instances. Our running time nearly matches that of the usual PCA (i.e. non robust), which is O(rmn\\\\log (1/\\\\epsilon)). Thus, we achieve ``best of both the worlds'', viz low computational complexity and provable recovery for robust PCA. Our analysis represents one of the few instances of global convergence guarantees for non-convex methods.\",\n",
       "     'id': '5430',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5432',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F1 on real piano audio.\",\n",
       "     'id': '5432',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5435',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\\\\em{disagreement-based active learning}}, which has a high label requirement, and {\\\\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions -- a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.',\n",
       "     'id': '5435',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5441',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel $p$. In many problems, a good approximation of $p$ is not needed. For instance, if from one state-action pair $(s,a)$, one can only transit to states with the same value, learning $p(\\\\cdot|s,a)$ accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) we call the {\\\\em distribution-norm}. The distribution-norm w.r.t.~a measure $\\\\nu$ is defined on zero $\\\\nu$-mean functions $f$ by the standard variation of $f$ with respect to $\\\\nu$. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the generic but loose $||\\\\cdot||_1$ concentration inequalities used in most previous analysis of RL algorithms, to benefit from this new hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.',\n",
       "     'id': '5441',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5446',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied. We propose a policy-search-based approach coupled with predictive modelling to address this problem. This approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable. Our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.',\n",
       "     'id': '5446',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5447',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the gamma-discounted infinite horizon performance loss by a factor of 1/(1-gamma) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.',\n",
       "     'id': '5447',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5452',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/wildBootstrap.',\n",
       "     'id': '5452',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5453',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.',\n",
       "     'id': '5453',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5471',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a technique for decomposing the parameter learning problem in Bayesian networks into independent learning problems. Our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset. We show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time. We explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.',\n",
       "     'id': '5471',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5472',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.',\n",
       "     'id': '5472',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5478',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach.',\n",
       "     'id': '5478',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5481',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding ?pseudo landmark points? to the classical Nystr?om kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystr?om kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%).',\n",
       "     'id': '5481',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5490',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O(1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.',\n",
       "     'id': '5490',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5491',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many machine learning problems can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the trade off space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.',\n",
       "     'id': '5491',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5492',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise MRFs and Determinantal Point Processes. In particular, we present L-Field, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.',\n",
       "     'id': '5492',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5493',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate the problem of stochastic network design in bidirected trees. In this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges. Actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources. Our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (FPTAS), that is, an algorithm that can find (1??)-optimal solutions for any problem instance in time polynomial in the input size and 1/?. Our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.',\n",
       "     'id': '5493',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5494',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a model-based excessive gap technique to analyze first-order primal- dual methods for constrained convex minimization. As a result, we construct first- order primal-dual methods with optimal convergence rates on the primal objec- tive residual and the primal feasibility gap of their iterates separately. Through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented Lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.',\n",
       "     'id': '5494',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5496',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.',\n",
       "     'id': '5496',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5503',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.',\n",
       "     'id': '5503',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5504',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms. Recently, Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the `true' posterior class probability) is available to a learning algorithm. In this work, we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitable `estimate' of the class probability, and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (TPR) and true negative rate (TNR), and for which the Bayes optimal classifier is the class probability function thresholded suitably. We use this template to derive consistency results for plug-in algorithms for the F-measure and for the geometric mean of TPR and precision; to our knowledge, these are the first such results for these measures. In addition, for continuous distributions, we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of TPR and TNR. Experimental results confirm our theoretical findings.\",\n",
       "     'id': '5504',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5508',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a theoretical analysis of F-measures for binary, multiclass and multilabel classification. These performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation, we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on F-measures, which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.',\n",
       "     'id': '5508',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5523',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.',\n",
       "     'id': '5523',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5526',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the art approach.',\n",
       "     'id': '5526',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5527',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of \\\\emph{adaptive feedback} naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the \\\\emph{blinded multi-armed bandit}, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.\",\n",
       "     'id': '5527',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5533',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading $(\\\\delta,\\\\rho)$-modes of the underlying distributions. A point is defined to be a $(\\\\delta,\\\\rho)$-mode if it is a local optimum of the density within a $\\\\delta$-neighborhood under metric $\\\\rho$. As we increase the ``scale'' parameter $\\\\delta$, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the $(\\\\delta,\\\\rho)$-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.\",\n",
       "     'id': '5533',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5535',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization ? in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs. We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erd?s-R?nyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results. Our work exploits a structural characterization of essential graphs by Andersson et al. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.',\n",
       "     'id': '5535',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5537',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.',\n",
       "     'id': '5537',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5540',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials. The work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial. It shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability. We furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.\",\n",
       "     'id': '5540',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5541',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people?s category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for probabilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.',\n",
       "     'id': '5541',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5542',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.',\n",
       "     'id': '5542',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5566',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure. We present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring. This new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data. The proposed formulation also allows to handle non-uniform blur with a straightforward extension. Experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method. One surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.',\n",
       "     'id': '5566',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5576',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of $\\\\ell_1$ penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing $\\\\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for $\\\\ell_1$-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.',\n",
       "     'id': '5576',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5578',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we develop a family of algorithms for optimizing superposition-structured? or ?dirty? statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.\"',\n",
       "     'id': '5578',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5581',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Life-logging video streams, financial time series, and Twitter tweets are a few examples of high-dimensional signals over practically unbounded time. We consider the problem of computing optimal segmentation of such signals by k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal. The coreset enables fast further analysis such as automatic summarization and analysis of such signals. A coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task -- in our case, segmentation of the stream. We show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points. More precisely, we construct a representation of size O(klog n /eps^2) that provides a (1+eps)-approximation for the sum of squared distances to any given k-piecewise linear function. Moreover, such coresets can be constructed in a parallel streaming approach. Our results rely on a novel eduction of statistical estimations to problems in computational geometry. We empirically evaluate our algorithms on very large synthetic and real data sets from GPS, video and financial domains, using 255 machines in Amazon cloud.',\n",
       "     'id': '5581',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5584',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is {\\\\it offline}, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is {\\\\it online}, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.',\n",
       "     'id': '5584',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5585',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents? voluntary investment decisions when facing potential direct risk and transfer risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.',\n",
       "     'id': '5585',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5594',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.',\n",
       "     'id': '5594',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5600',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However, the $\\\\ell_0$ or $\\\\ell_1$-norm sparsity constraint on the representation coefficients adopted in many DL methods makes the training and testing phases time consuming. We propose a new discriminative DL framework, namely projective dictionary pair learning (DPL), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods, the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.',\n",
       "     'id': '5600',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5604',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.',\n",
       "     'id': '5604',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5606',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set $U$ is the collection of all strings containing some string $u \\\\in U$ as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.',\n",
       "     'id': '5606',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5610',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.\",\n",
       "     'id': '5610',\n",
       "     'year': '2014'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5636',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.',\n",
       "     'id': '5636',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5638',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.',\n",
       "     'id': '5638',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5640',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.',\n",
       "     'id': '5640',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5641',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: \\\\url{http://idl.baidu.com/FM-IQA.html}.',\n",
       "     'id': '5641',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5644',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to  place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors,  ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground.  Our experiments  show significant performance gains over existing RGB and RGB-D object proposal methods  on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.',\n",
       "     'id': '5644',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5650',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The dynamics of simple decisions are well understood and modeled as a class of random walk models (e.g. Laming, 1968; Ratcliff, 1978; Busemeyer and Townsend, 1993; Usher and McClelland, 2001; Bogacz et al., 2006). However, most real-life decisions include a rich and dynamically-changing influence of additional information we call context. In this work, we describe a computational theory of decision making under dynamically shifting context. We show how the model generalizes the dominant existing model of fixed-context decision making (Ratcliff, 1978) and can be built up from a weighted combination of fixed-context decisions evolving simultaneously. We also show how the model generalizes re- cent work on the control of attention in the Flanker task (Yu et al., 2009). Finally, we show how the model recovers qualitative data patterns in another task of longstanding psychological interest, the AX Continuous Performance Test (Servan-Schreiber et al., 1996), using the same model parameters.',\n",
       "     'id': '5650',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5652',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets.  Therefore, a key step in understanding neural systems is to reliably distinguish  cell types.  An important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive.  Here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array.  We use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations). These classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (ON vs. OFF).  We then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm.  This can result in accurate, fully automated methods for cell type classification.',\n",
       "     'id': '5652',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5661',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.',\n",
       "     'id': '5661',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5666',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes (SGVB) with global model parameters. Regular SGVB estimators rely on sampling of parameters once per minibatch of data, and have variance that is constant w.r.t. the minibatch size. The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence.We find an important connection with regularization by dropout: the original Gaussian dropout objective corresponds to SGVB with local noise, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose \\\\emph{variational dropout}, a generalization of Gaussian dropout, but with a more flexibly parameterized posterior, often leading to better generalization. The method is demonstrated through several experiments.',\n",
       "     'id': '5666',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5668',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.',\n",
       "     'id': '5668',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5674',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs  at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of Ihler and McAllester (2009) at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.',\n",
       "     'id': '5674',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5678',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution. This algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution. This is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a Rao-Blackwellized estimate that has low variance. Our method works efficiently for both continuous and discrete random variables. Furthermore, the proposed algorithm has interesting similarities with Gibbs sampling but at the same time, unlike Gibbs sampling, can be trivially parallelized.',\n",
       "     'id': '5678',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5679',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.\",\n",
       "     'id': '5679',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5683',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'For weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is. Even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred. Here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely. In this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision. Our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference. These efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.',\n",
       "     'id': '5683',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5686',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.',\n",
       "     'id': '5686',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5689',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Causal structure learning from time series data is a major scientific challenge.  Existing algorithms assume that measurements occur sufficiently quickly; more precisely, they assume that the system and measurement timescales are approximately equal. In many scientific domains, however, measurements occur at a significantly slower rate than the underlying system changes.  Moreover, the size of the mismatch between timescales is often unknown. This paper provides three distinct causal structure learning algorithms, all of which discover all dynamic graphs that could explain the observed measurement data as arising from undersampling at some rate. That is, these algorithms all learn causal structure without assuming any particular relation between the measurement and system timescales; they are thus rate-agnostic. We apply these algorithms to data from simulations. The results provide insight into the challenge of undersampling.',\n",
       "     'id': '5689',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5691',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability. In this work, we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories, a class of problems that includes MAP inference in Markov Logic and similar statistical-relational languages. We introduce term symmetries, which are induced by an evidence set and extend to symmetries over a relational theory. We provide the important special case of term equivalent symmetries, showing that such symmetries can be found in low-degree polynomial time. We show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain. We demonstrate the effectiveness of these techniques through experiments in two relational domains. We also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning.',\n",
       "     'id': '5691',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5696',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$ iterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.',\n",
       "     'id': '5696',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5699',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with L1 and L1/L2 norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.',\n",
       "     'id': '5699',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5702',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Mixture modeling is a general technique for making any simple model more expressive through weighted combination.  This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models.  However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees.  Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist.  In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem.  We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra.  This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation.  Simulations show good empirical performance on several models.',\n",
       "     'id': '5702',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5704',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank $r$ reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank $r$ of a large $n\\\\times m$ matrix from $C(r)r\\\\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.',\n",
       "     'id': '5704',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5708',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received. The Stochastic Block-Model (SBM) is a generative model for graphs with communities for which, because of its simplicity, the theoretical understanding has advanced fast in recent years. In particular, there have been various results showing that simple versions of spectralclustering using the Normalized Laplacian of the graph can recoverthe communities almost perfectly with high probability. Here we show that essentially the same algorithm used for the SBM and for its extension called Degree-Corrected SBM, works on a wider class of Block-Models, which we call Preference Frame Models, with essentially the same guarantees. Moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.',\n",
       "     'id': '5708',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5716',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'One approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance.By extending the notion of \\\\emph{statistical leverage scores} to the setting of kernel ridge regression, we are able to identify a sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the \\\\emph{effective dimensionality} of the problem.  This latter quantity is often much smaller than previous bounds that depend on the \\\\emph{maximal degrees of freedom}. We give an empirical evidence supporting this fact. Our second contribution is to present a fast algorithm to quickly compute coarse approximations to thesescores in time linear in the number of samples. More precisely, the running time of the algorithm is $O(np^2)$ with $p$ only depending on the trace of the kernel matrix and the regularization parameter. This is obtained via a variant of squared length sampling that we adapt to the kernel setting. Lastly, we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem.',\n",
       "     'id': '5716',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5717',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we useour new analysis in three ways: (1) we derive convergence rates for the convex case (Hogwild) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called Buckwild, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.\",\n",
       "     'id': '5717',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5718',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This poster has been moved from Monday #86 to Thursday #101.\\n\\nStochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD).The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.',\n",
       "     'id': '5718',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5723',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently.   The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler substeps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence.  We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence.  We rigorously analyze our methods, and identify convergence rates.  Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.',\n",
       "     'id': '5723',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5724',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This paper establishes a statistical versus computational trade-offfor solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the {\\\\em Sparse Principal Component Analysis} (Sparse PCA) problem, and the family of {\\\\em Sum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension $p$, a planted $k$-sparse unit vector can be {\\\\em in principle} detected using only $n \\\\approx k\\\\log p$ (Gaussian or Bernoulli) samples, but all {\\\\em efficient} (polynomial time) algorithms known require $n \\\\approx k^2 $ samples. It was also known that this quadratic gap cannot be improved by the the most basic {\\\\em semi-definite} (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or ``pseudo-expectations'') for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.\",\n",
       "     'id': '5724',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5725',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm which converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.',\n",
       "     'id': '5725',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5726',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of  regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squaresestimation comes without any tuning parameter and may hence be preferred due to its simplicity.',\n",
       "     'id': '5726',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5733',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing and completion. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.',\n",
       "     'id': '5733',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5735',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko, Martinsson, and Tropp, randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for *any* matrix, independently of singular value gaps. After ~O(1/epsilon) iterations, it gives a low-rank approximation within (1+epsilon) of optimal for spectral norm error.We give the first provable runtime improvement on Simultaneous Iteration: a randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just ~O(1/sqrt(epsilon)) iterations and performs substantially better experimentally. Our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice.Furthermore, while it is a simple accuracy benchmark, even (1+epsilon) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods.',\n",
       "     'id': '5735',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5744',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Submodular and supermodular functions have found wide applicability in machine learning, capturing  notions such as diversity and regularity, respectively. These notions have deep consequences for optimization, and the problem of (approximately) optimizing submodular functions has received much attention. However, beyond optimization, these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference. Prominent, well-studied special cases include Ising models and determinantal point processes, but the general class of log-submodular and log-supermodular models is much richer and little studied. In this paper, we investigate the use of Markov chain Monte Carlo sampling to perform approximate inference in general log-submodular and log-supermodular models. In particular, we consider a simple Gibbs sampling procedure, and establish two sufficient conditions, the first guaranteeing polynomial-time, and the second fast (O(nlogn)) mixing. We also evaluate the efficiency of the Gibbs sampler on three examples of such models, and compare against a recently proposed variational approach.',\n",
       "     'id': '5744',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5746',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Variational inference is an efficient, popular heuristic used in the context of latent variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. Our initializations are natural, one of them being used in LDA-c, the mostpopular implementation of variational inference.In addition to providing intuition into why this heuristic might work in practice, the multiplicative, rather than additive nature of the variational inference updates forces us to usenon-standard proof arguments, which we believe might be of general theoretical interest.',\n",
       "     'id': '5746',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5747',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U-statistic of interest. We establish convergence rate bounds of O(1 / t) and O(log t / t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.',\n",
       "     'id': '5747',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5779',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper formulates the search for a set of bounding boxes (as needed in object proposal generation) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image. Since the number of possible bounding boxes in an image is very large $O(#pixels^2)$, even a single linear scan to perform the greedy augmentation for submodular maximization is intractable. Thus, we formulate the greedy augmentation step as a Branch-and-Bound scheme. In order to speed up repeated application of B\\\\&B, we propose a novel generalization of Minoux?s ?lazy greedy? algorithm to the B\\\\&B tree. Theoretically, our proposed formulation provides a new understanding to the problem, and contains classic heuristic approaches such as Sliding Window+Non-Maximal Suppression (NMS) and and Efficient Subwindow Search (ESS) as special cases. Empirically, we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure.',\n",
       "     'id': '5779',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5784',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9?, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13?, from 138 million to 10.3 million, again with no loss of accuracy.',\n",
       "     'id': '5784',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5790',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Neural population activity often exhibits rich variability. This variability is thought to arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as  from modulations of neural firing properties on long time-scales, often referred to as non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a  hierarchical dynamics model that is able to capture inter-trial modulations in firing rates, as well as neural population dynamics. We derive an algorithm for Bayesian Laplace propagation for fast posterior inference, and demonstrate that our model provides a better account of the structure of neural firing than existing stationary dynamics models, when applied to neural population recordings from primary visual cortex.',\n",
       "     'id': '5790',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5797',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights.  We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization.  Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.',\n",
       "     'id': '5797',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5800',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlinespartitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real world datasets demonstrate the benefit of our method.',\n",
       "     'id': '5800',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5807',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose the ?-return as an alternative to the ?-return currently used by the TD(?) family of algorithms. The benefit of the ?-return is that it accounts for the correlation of different length returns. Because it is difficult to compute exactly, we suggest one way of approximating the ?-return. We provide empirical studies that suggest that it is superior to the ?-return and ?-return for a variety of problems.',\n",
       "     'id': '5807',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5813',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points.We propose a simple, computationally efficient, and memory friendly algorithm based on the ``doubly stochastic gradients'' to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the \\\\emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate $\\\\Otil(1/t)$ to the global optimum, even for the top $k$ eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.\",\n",
       "     'id': '5813',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5829',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient \\\\underline{H}ybrid \\\\underline{O}ptimization algorithm for \\\\underline{NO}n convex \\\\underline{R}egularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2)  We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.',\n",
       "     'id': '5829',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5831',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose CombEXP, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.',\n",
       "     'id': '5831',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5832',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Elicitation is the study of statistics or properties which are computable via empirical risk minimization.  While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question---all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable.  Specifically, what is the minimum number of regression parameters needed to compute the property?Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation.  We establish several general results and techniques for proving upper and lower bounds on elicitation complexity.  These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.',\n",
       "     'id': '5832',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5833',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study the performance of standard online learning algorithms when the feedback is delayed by an adversary. We show that \\\\texttt{online-gradient-descent} and \\\\texttt{follow-the-perturbed-leader} achieve regret $O(\\\\sqrt{D})$ in the delayed setting, where $D$ is the sum of delays of each round's feedback. This bound collapses to an optimal $O(\\\\sqrt{T})$ bound in the usual setting of no delays (where $D = T$).  Our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback, making adjustments to the analysis and not to the algorithms themselves.  Our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model.\",\n",
       "     'id': '5833',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5837',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the alpha-divergence. Small experiments show that the proposed estimator attains comparable performance to the MLE with drastically lower computational cost.',\n",
       "     'id': '5837',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5838',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree structured sparsity over the features, where each node encodes a group of features. It has been applied successfully in many real-world applications. However, with extremely large feature dimensions, solving TGL remains a significant challenge due to its highly complicated regularizer. In this paper, we propose a novel Multi-Layer Feature reduction method (MLFre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes. By a novel hierarchical projection algorithm, MLFre is able to test the nodes independently from any of their ancestor nodes. Moreover, we can integrate MLFre---that has a low computational cost---with any existing solvers. Experiments on both synthetic and real data sets demonstrate that the speedup gained by MLFre can be orders of magnitude.',\n",
       "     'id': '5838',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5839',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Given samples from an unknown  distribution, p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has receivedtremendous attention in Statistics, albeit focusing onasymptotic analysis, as well as in Computer Science, wherethe emphasis has been on small sample size and computationalcomplexity. Nevertheless, even for basic classes ofdistributions such as monotone, log-concave, unimodal, and monotone hazard rate, the optimal sample complexity is unknown.We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem:Given samplesfrom an unknown distribution p, and a known distribution q, are p and q close in Chi^2-distance, or far in total variation distance?The optimality of all testers is established by providing matching lower bounds. Finally, a necessary building block for our tester and important byproduct of our work are the first known computationally efficient proper learners for discretelog-concave and monotone hazard rate distributions. We exhibit the efficacy of our testers via experimental analysis.',\n",
       "     'id': '5839',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5842',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of $\\\\widetilde{O}(T^{5/6})$, while the best known lower bound is $\\\\Omega(T^{1/2})$. Many attemptshave been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case, the best known algorithm guarantees a regret of $\\\\widetilde{O}(T^{2/3})$. We present an efficient algorithm for the banditsmooth convex optimization problem that guarantees a regret of $\\\\widetilde{O}(T^{5/8})$. Our result rules out an $\\\\Omega(T^{2/3})$ lower bound and takes a significant step towards the resolution of this open problem.',\n",
       "     'id': '5842',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5847',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation reaches a competitive 18.6\\\\% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18\\\\% PER in single utterances and 20\\\\% in 10-times longer (repeated) utterances.  Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6\\\\% level.',\n",
       "     'id': '5847',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5849',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization.  Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN.  The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data.  Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.',\n",
       "     'id': '5849',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5850',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.',\n",
       "     'id': '5850',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5856',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations, often after a single trial. While the mechanism for rapid path planning is unknown, the CA3 region in the hippocampus plays an important role, and emerging evidence suggests that place cell activity during hippocampal preplay periods may trace out future goal-directed trajectories. Here, we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment, based only on the mapped representation of the goal. We show that this representation can be implemented in a neural attractor network model, resulting in bump--like activity profiles resembling those of the CA3 region of hippocampus. Neurons tend to locally excite neurons with similar place field centers, while inhibiting other neurons with distant place field centers, such that stable bumps of activity can form at arbitrary locations in the environment. The network is initialized to represent a point in the environment, then weakly stimulated with an input corresponding to an arbitrary goal location. We show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location. Indeed, in networks with large place fields, we show that the network properties cause the bump to move smoothly from its initial location to the goal, around obstacles or walls. Our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.',\n",
       "     'id': '5856',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5860',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an ?on-the-job? setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets-- named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning.',\n",
       "     'id': '5860',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5864',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a classifier for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier end-to-end. Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.\",\n",
       "     'id': '5864',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5868',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation.  The system, called NEXT,  provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments.  The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.',\n",
       "     'id': '5868',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5869',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the task of building compact deep learning pipelines suitable for deploymenton storage and power constrained mobile devices. We propose a uni-fied framework to learn a broad family of structured parameter matrices that arecharacterized by the notion of low displacement rank. Our structured transformsadmit fast function and gradient evaluation, and span a rich range of parametersharing configurations whose statistical modeling capacity can be explicitly tunedalong a continuum from structured to unstructured. Experimental results showthat these transforms can significantly accelerate inference and forward/backwardpasses during training, and offer superior accuracy-compactness-speed tradeoffsin comparison to a number of existing techniques. In keyword spotting applicationsin mobile speech recognition, our methods are much more effective thanstandard linear low-rank bottleneck layers and nearly retain the performance ofstate of the art models, while providing more than 3.5-fold compression.',\n",
       "     'id': '5869',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5871',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.',\n",
       "     'id': '5871',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5881',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models.  The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic.  For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.',\n",
       "     'id': '5881',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5882',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Existing inverse reinforcement learning (IRL) algorithms have assumed each expert?s demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts? behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model, an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert?s demonstrated trajectories. As a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL, which is, interestingly, a reduced variant of our approach.',\n",
       "     'id': '5882',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5891',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial.  Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise.  In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices.  We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework.  We show how previous continuous-dynamic samplers can be trivially reinvented in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC).  Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.',\n",
       "     'id': '5891',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5893',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal  running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH (Andoni-Indyk-Nguyen-Razenshteyn 2014) (Andoni-Razenshteyn 2015)), our algorithm is also practical, improving upon the well-studied hyperplane LSH (Charikar 2002) in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.',\n",
       "     'id': '5893',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5898',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent  approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the  number of tasks as an eigendecomposition is required in each step. Using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.',\n",
       "     'id': '5898',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5901',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the following multi-component sparse PCA problem:given a set of data points, we seek to extract a small number of sparse components with \\\\emph{disjoint} supports that jointly capture the maximum possible variance.Such components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but this greedy procedure is suboptimal.We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to $1$ from the optimal.Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem.Its complexity grows as a low order polynomial in the ambient dimension of the input data, but exponentially in its rank.However, it can be effectively applied on a low-dimensional sketch of the input data.We evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.',\n",
       "     'id': '5901',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5902',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a unifying generalization of the Lov?sz theta function, and the associated geometric embedding, for graphs with weights on both nodes and edges. We show how it can be computed exactly by semidefinite programming, and how to approximate it using SVM computations. We show how the theta function can be interpreted as a measure of diversity in graphs and use this idea, and the graph embedding in algorithms for Max-Cut, correlation clustering and document summarization, all of which are well represented as problems on weighted graphs.',\n",
       "     'id': '5902',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5904',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Bayesian networks are a popular representation of asymmetric (for example causal) relationships between random variables. Markov random fields (MRFs) are a complementary model of symmetric relationships used in computer vision, spatial modeling, and social and gene expression networks.  A chain graph model under the Lauritzen-Wermuth-Frydenberg interpretation (hereafter a chain graph model) generalizes both Bayesian networks and MRFs, and can represent asymmetric and symmetric relationships together.As in other graphical models, the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model.  One recent approach to the study of marginal graphical models is to consider a well-behaved supermodel.  Such a supermodel of marginals of Bayesian networks, defined only by conditional independences, and termed the ordinary Markov model, was studied at length in (Evans and Richardson, 2014).In this paper, we show that special mixed graphs which we call segregated graphs can be associated, via a Markov property, with supermodels of a marginal of chain graphs defined only by conditional independences.  Special features of segregated graphs imply the existence of a very natural factorization for these supermodels, and imply many existing results on the chain graph model, and ordinary Markov model carry over.  Our results suggest that segregated graphs define an analogue of the ordinary Markov model for marginals of chain graph models.',\n",
       "     'id': '5904',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5906',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The stochastic block model (SBM) has recently gathered significant attention due to new threshold phenomena. However, most developments rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in Abbe-Sandon FOCS15. In the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees. This lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees, and the model parameters are learned efficiently. For the logarithmic degree regime, this is further enhanced into a fully agnostic algorithm that achieves the CH-limit for exact recovery in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the SBM.',\n",
       "     'id': '5906',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5920',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.',\n",
       "     'id': '5920',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5925',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take `away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that has been successfully applied in practice: FW with away steps, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence under a weaker condition than strong convexity. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of the `condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.\",\n",
       "     'id': '5925',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5928',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants.  For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.',\n",
       "     'id': '5928',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5937',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.',\n",
       "     'id': '5937',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5940',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We study matrix completion problem with side information.  Side information has been considered in several matrix completion applications, and is generally shown to be useful empirically.  Recently, Xu et al. studied the effect of side information for matrix completion under a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features.  However, since in reality most given features are noisy or even weakly informative, how to develop a general model to handle general feature set, and how much the noisy features can help matrix recovery in theory, is still an important issue to investigate. In this paper, we propose a novel model that balances between features and observations simultaneously, enabling us to leverage feature information yet to be robust to feature noise.  Moreover, we study the effectof general features in theory, and show that by using our model, the sample complexity can still be lower than matrix completion as long as features are sufficiently informative. This result provides a theoretical insight of usefulness for general side information.  Finally, we consider synthetic data and two real applications - relationship prediction and semi-supervised clustering, showing that our model outperforms other methods for matrix completion with features both in theory and practice.',\n",
       "     'id': '5940',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5957',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We present the Mind the Gap Model (MGM), an approach for interpretable feature extraction and selection.  By placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence.  It also maintains or improves performance when compared to related approaches.  We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.\",\n",
       "     'id': '5957',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5959',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags.',\n",
       "     'id': '5959',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5962',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Subunit models provide a powerful yet parsimonious description of  neural spike responses to complex stimuli. They can be expressed by  a cascade of two linear-nonlinear (LN) stages, with the first linear  stage defined by convolution with one or more filters.  Recent  interest in such models has surged due to their biological  plausibility and accuracy for characterizing early sensory  responses. However, fitting subunit models poses a difficult  computational challenge due to the expense of evaluating the  log-likelihood and the ubiquity of local optima.  Here we address  this problem by forging a theoretical connection between  spike-triggered covariance analysis and nonlinear subunit models.  Specifically, we show that a ''convolutional'' decomposition of the  spike-triggered average (STA) and covariance (STC) provides an  asymptotically efficient estimator for the subunit model under  certain technical conditions. We also prove the identifiability of  such convolutional decomposition under mild assumptions.  Our  moment-based methods outperform highly regularized versions of the  GQM on neural data from macaque primary visual cortex, and achieves  nearly the same prediction performance as the full  maximum-likelihood estimator, yet with substantially lower cost.\",\n",
       "     'id': '5962',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5963',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm.On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique of deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.RFN package for GPU/CPU is available at http://www.bioinf.jku.at/software/rfn.\",\n",
       "     'id': '5963',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5964',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.',\n",
       "     'id': '5964',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5966',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Multi-output Gaussian processes provide a convenient framework for multi-task problems.  An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels.  Recently, Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework.  In this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (CSM) kernel.  This new, flexible kernel represents both the power and phase relationship between multiple observation channels.  We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model, where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel.  Results are presented for measured multi-region electrophysiological data.',\n",
       "     'id': '5966',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5970',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.',\n",
       "     'id': '5970',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5974',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs in order to identify the direct causes and effects of a target variable. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in one target variable (such as class labels). We propose a new local causal discovery algorithm, called Causal Markov Blanket (CMB), to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.',\n",
       "     'id': '5974',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '5979',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'By making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services. However, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users. Two central but less explored questions are how to recommend the most desirable item \\\\emph{at the right moment}, and how to predict \\\\emph{the next returning time} of a user to a service. To address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs. We show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains $O(1 / \\\\epsilon)$ convergence rate, scales up to problems with millions of user-item pairs and thousands of millions of temporal events. Compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation questions. Finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.',\n",
       "     'id': '5979',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6003',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Both learning and inference tasks on Bayesian networks are NP-hard in general. Bounded tree-width Bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however, while inference on bounded tree-width networks is tractable, the learning problem remains NP-hard even for tree-width~2. In this paper, we propose bounded vertex cover number Bayesian networks as an alternative to bounded tree-width networks. In particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound $k$, in contrast to the general and bounded tree-width cases; on the other hand, we also show that learning problem is W[1]-hard in parameter $k$. Furthermore, we give an alternative way to learn bounded vertex cover number Bayesian networks using integer linear programming (ILP),  and show this is feasible in practice.',\n",
       "     'id': '6003',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6015',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Within a statistical learning setting,  we propose and study an iterative regularization algorithm for least squares defined by  an incremental gradient method.   In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and  prove strong universal consistency, i.e.  almost sure convergence of the risk, as well as  sharp finite sample bounds for the iterates. Our  results are a step towards understanding the effect of multiple epochs in  stochastic gradient techniques in machine learning and rely  on  integrating  statistical and optimizationresults.',\n",
       "     'id': '6015',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6023',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results.  Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T) without requiring such assumptions.  Our results offer the best of both worlds: O(K log T) bounds without restrictive assumptions.',\n",
       "     'id': '6023',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6025',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretic results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.',\n",
       "     'id': '6025',\n",
       "     'year': '2015'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6039',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks?  This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model?s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.',\n",
       "     'id': '6039',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6041',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Many information systems use tags and keywords to describe and annotate content. These allow for efficient organization and categorization of items, as well as facilitate relevant search queries. As such, the selected set of tags for an item can have a considerable effect on the volume of traffic that eventually reaches an item.  In tagging systems where tags are exclusively chosen by an item's owner, who in turn is interested in maximizing traffic, a principled approach for assigning tags can prove valuable. In this paper we introduce the problem of optimal tagging, where the task is to choose a subset of tags for a new item such that the probability of browsing users reaching that item is maximized.  We formulate the problem by modeling traffic using a Markov chain, and asking how transitions in this chain should be modified to maximize traffic into a certain state of interest. The resulting optimization problem involves maximizing a certain function over subsets, under a cardinality constraint.  We show that the optimization problem is NP-hard, but has a (1-1/e)-approximation via a simple greedy algorithm due to monotonicity and submodularity. Furthermore, the structure of the problem allows for an efficient computation of the greedy step. To demonstrate the effectiveness of our method, we perform experiments on three tagging datasets, and show that the greedy algorithm outperforms other baselines.\",\n",
       "     'id': '6041',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6042',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.',\n",
       "     'id': '6042',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6044',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A sampling-based optimization method for quadratic functions is   proposed. Our method approximately solves the following   $n$-dimensional quadratic minimization problem in constant time,   which is independent of $n$:   $z^*=\\\\min_{\\\\bv \\\\in \\\\bbR^n}\\\\bracket{\\\\bv}{A \\\\bv} +   n\\\\bracket{\\\\bv}{\\\\diag(\\\\bd)\\\\bv} + n\\\\bracket{\\\\bb}{\\\\bv}$,   where $A \\\\in \\\\bbR^{n \\\\times n}$ is a matrix and $\\\\bd,\\\\bb \\\\in \\\\bbR^n$   are vectors. Our theoretical analysis specifies the number of   samples $k(\\\\delta, \\\\epsilon)$ such that the approximated solution   $z$ satisfies $|z - z^*| = O(\\\\epsilon n^2)$ with probability   $1-\\\\delta$. The empirical performance (accuracy and runtime) is   positively confirmed by numerical experiments.',\n",
       "     'id': '6044',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6048',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima \\\\--- all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with \\\\textit{arbitrary} initialization in polynomial time.',\n",
       "     'id': '6048',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6083',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"This paper presents a dynamical system based on the Poisson-Gamma construction for sequentially observed multivariate count data.  Inherent to the model is a novel Bayesian nonparametric prior that ties and shrinks parameters in a powerful way. We develop theory about the model's infinite limit and its steady-state.  The model's inductive bias is demonstrated on a variety of real-world datasets where it is shown to learn interpretable structure and have superior predictive performance.\",\n",
       "     'id': '6083',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6105',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a ?teacher? algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.',\n",
       "     'id': '6105',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6109',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.',\n",
       "     'id': '6109',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6115',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when the feasible set is a polytope, and the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: i) large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration ii) the worst case convergence rate depends unfavorably on the dimension In this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular, both memory and computation overheads are only linear in the dimension, and in addition, in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous works, with a linear dependence on the number of non-zeros in the optimal solution At the heart of our method, and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence that shows that our method delivers state-of-the-art performance.',\n",
       "     'id': '6115',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6119',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes. In this paper, we consider an alternate model that treats individual opinions as spins in an Ising system at dynamic equilibrium. We formalize the \\\\textit{Ising influence maximization} problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field. Under the mean-field (MF) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the MF magnetization is concave and our algorithm converges to a global optimum. We apply our algorithm on random and real-world networks, demonstrating, remarkably, that the MF optimal external fields (i.e., the external fields which maximize the MF magnetization) exhibit a phase transition from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures. We also establish a number of novel results about the structure of steady-states in the ferromagnetic MF Ising model on general graphs, which are of independent interest.',\n",
       "     'id': '6119',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6125',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: Our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.',\n",
       "     'id': '6125',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6127',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Finally, numerical evidence supports the effectiveness of our method in real-world problems.',\n",
       "     'id': '6127',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6132',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models.  It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it.  These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime.  Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs.',\n",
       "     'id': '6132',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6133',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1+eps)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms; it is satisfied even by the vanilla Hedge forecaster. Our results improve upon recent work of Syrgkanis et al. in a number of ways. We require only that players observe payoffs under other players\\' realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and show convergence under bandit feedback. Finally, we improve upon the speed of convergence by a factor of n, the number of players. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work. Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of Lykouris et al. in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved. In the bandit setting we present a new algorithm which provides a \"small loss\"-type bound with improved dependence on the number of actions in utility settings, and is both simple and efficient. This result may be of independent interest.',\n",
       "     'id': '6133',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6137',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis uses techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of arms pulled.',\n",
       "     'id': '6137',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6140',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain \"correctness\" guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.',\n",
       "     'id': '6140',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6145',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.',\n",
       "     'id': '6145',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6149',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider the \\\\emph{Threshold Bandit} setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \\\\emph{threshold value}. The learner selects one of $K$ actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the \\\\emph{uncensored} and \\\\emph{censored} case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically.',\n",
       "     'id': '6149',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6151',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'How does our motor system solve the problem of anticipatory control in spite of a wide spectrum of response dynamics from different musculo-skeletal systems, transport delays as well as response latencies throughout the central nervous system? To a great extent, our highly-skilled motor responses are a result of a reactive feedback system, originating in the brain-stem and spinal cord, combined with a feed-forward anticipatory system, that is adaptively fine-tuned by sensory experience and originates in the cerebellum. Based on that interaction we design the counterfactual predictive control (CFPC) architecture, an anticipatory adaptive motor control scheme in which a feed-forward module, based on the cerebellum, steers an error feedback controller with counterfactual error signals. Those are signals that trigger reactions as actual errors would, but that do not code for any current of forthcoming errors. In order to determine the optimal learning strategy, we derive a novel learning rule for the feed-forward module that involves an eligibility trace and operates at the synaptic level. In particular, our eligibility trace provides a mechanism beyond co-incidence detection in that it convolves a history of prior synaptic inputs with error signals. In the context of cerebellar physiology, this solution implies that Purkinje cell synapses should generate eligibility traces using a forward model of the system being controlled. From an engineering perspective, CFPC provides a general-purpose anticipatory control architecture equipped with a learning rule that exploits the full dynamics of the closed-loop system.',\n",
       "     'id': '6151',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6163',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.',\n",
       "     'id': '6163',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6165',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\\\bm{108}\\\\times$ and $\\\\bm{17.7}\\\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.',\n",
       "     'id': '6165',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6174',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-$?sqrt{n}$ convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e.,  when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces.',\n",
       "     'id': '6174',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6201',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show how to estimate a model?s test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently compute gradients of the estimated error and hence perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as conditional random fields.',\n",
       "     'id': '6201',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6202',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.',\n",
       "     'id': '6202',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6213',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We analyze the learning  properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that    for  a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds  can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on  a unifying approach, encompassing both batch and stochastic gradient methods as special cases.',\n",
       "     'id': '6213',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6218',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speeded behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes.      We have conducted a large-scale psychophysics study to assess the correlation between computational models and human behavioral responses on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three state-of-the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages.       Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed the complexity of those used by human participants during rapid categorization.',\n",
       "     'id': '6218',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6219',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Inference in Markov random fields subject to consistency structure is a fundamental problem that arises in many real-life applications. In order to enforce consistency, classical approaches utilize consistency potentials or encode constraints over feasible instances. Unfortunately this comes at the price of a serious computational bottleneck. In this paper we suggest to tackle consistency by incorporating constraints on beliefs. This permits derivation of a closed-form message-passing algorithm which we refer to as the Constraints Based Convex Belief Propagation (CBCBP). Experiments show that CBCBP outperforms the standard approach while being at least an order of magnitude faster.',\n",
       "     'id': '6219',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6223',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In this paper, we address the problems of identifying linear structural equation models and discovering the constraints they imply. We first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification and constraint discovery power. Finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.',\n",
       "     'id': '6223',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6239',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced. The proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing. Moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm. Numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time.',\n",
       "     'id': '6239',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6252',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Principal components analysis~(PCA) is the optimal linear  encoder of data. Sparse linear encoders (e.g., sparse PCA) produce more interpretable features that  can promote better generalization. (\\\\rn{1}) Given a level of sparsity, what is the best approximation to PCA?  (\\\\rn{2}) Are there efficient algorithms which can achieve this optimal  combinatorial tradeoff? We answer both questions by  providing the first polynomial-time algorithms to construct \\\\emph{optimal} sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data.',\n",
       "     'id': '6252',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6255',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment-matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.',\n",
       "     'id': '6255',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6256',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.',\n",
       "     'id': '6256',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6261',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Our method aims at reasoning over natural language questions and visual images. Given a natural language question about an image, our model updates the question representation iteratively by selecting image regions relevant to the query and learns to give the correct answer. Our model contains several reasoning layers, exploiting complex visual relations in the visual question answering (VQA) task. The proposed network is end-to-end trainable through back-propagation, where its weights are initialized using pre-trained convolutional neural network (CNN) and gated recurrent unit (GRU). Our method is evaluated on challenging datasets of COCO-QA and VQA and yields state-of-the-art performance.',\n",
       "     'id': '6261',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6265',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features describing the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods, this model does not require the low-rank condition on the model parameter matrix. We prove that when the side features can span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is $O(\\\\log N)$ where $N$ is the size of the matrix. When the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an $\\\\epsilon$-recovery with $O(\\\\log N)$ sample complexity, and maintains a $\\\\O(N^{3/2})$ rate similar to classfic methods with no side information. An efficient linearized Lagrangian algorithm is developed with a strong guarantee of convergence. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets.',\n",
       "     'id': '6265',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6271',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements.  With noisy measurements we show all local minima are very close to a global optimum.  Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\\\\em  from random initialization}.',\n",
       "     'id': '6271',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6272',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to overfitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low--rank parameters.  Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that  the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a $\\\\log$ factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain--regions and cognitive neuroscience terms.',\n",
       "     'id': '6272',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6281',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.',\n",
       "     'id': '6281',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6290',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages, WebPPL and Stan, and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in WebPPL and Stan.',\n",
       "     'id': '6290',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6296',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of N items. They have recently gained prominence in several applications that rely on diverse subsets. However, their applicability to large problems is still limited due to O(N^3) complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.',\n",
       "     'id': '6296',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6307',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment.  In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.',\n",
       "     'id': '6307',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6314',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Unsupervised learning of structured predictors has been a long standing pursuit in machine learning.  Recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred.  Aside from being nonconvex, it also requires the demanding inference of normalization.  In this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally.  We further expand its applicability by resorting to a weaker form of inference---maximum a-posteriori.  The flexibility of the model is demonstrated on two structures based on total unimodularity---graph matching and linear chain.  Experimental results confirm the promise of the method.',\n",
       "     'id': '6314',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6318',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM).  In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP  is a deterministic method, which is typically fast,  empirically very successful, however in general lacking control of accuracy over loopy graphs.  In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e.,  we provide a way to compensate for BP errors via a consecutive BP-aware MCMC.  Our framework is based on the Loop Calculus (LC) approach  which allows to express the BP error  as a sum of weighted generalized loops. Although the full series is computationally intractable,  it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we, first, propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models.  Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then  design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme  for approximating the full series.  The main novelty underlying our design  is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon  the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC  outperforms both direct MCMC and bare BP schemes.',\n",
       "     'id': '6318',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6331',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.\",\n",
       "     'id': '6331',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6352',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an $\\\\ell_1$-norm constraint. By \"structured\", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning  work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models.',\n",
       "     'id': '6352',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6353',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.',\n",
       "     'id': '6353',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6356',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice.',\n",
       "     'id': '6356',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6358',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.',\n",
       "     'id': '6358',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6368',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.',\n",
       "     'id': '6368',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6378',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Learning accurate prior knowledge of natural images is of great importance for single image super-resolution (SR). Existing SR methods either learn the prior from the low/high-resolution patch pairs or estimate the prior models from the input low-resolution (LR) image. Specifically, high-frequency details are learned in the former methods. Though effective, they are heuristic and have limitations in dealing with blurred LR images; while the latter suffers from the limitations of frequency aliasing. In this paper, we propose to combine those two lines of ideas for image super-resolution. More specifically, the parametric sparse prior of the desirable high-resolution (HR) image patches are learned from both the input low-resolution (LR) image and a training image dataset. With the learned sparse priors, the sparse codes and thus the HR image patches can be accurately recovered by solving a sparse coding problem. Experimental results show that the proposed SR method outperforms existing state-of-the-art methods in terms of both subjective and objective image qualities.',\n",
       "     'id': '6378',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6385',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 82.2% to 87.8% and from 88% accuracy to 95% accuracy on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.',\n",
       "     'id': '6385',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6388',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties.  First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise.  Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells.  Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes.  These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.\",\n",
       "     'id': '6388',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6391',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free  methods in imitating complex behaviors in large, high-dimensional environments.\",\n",
       "     'id': '6391',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6401',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points $N$, namely $O(\\\\sqrt{N})$. The second algorithm illustrates how the classical mistake bound of $O(\\\\frac{1}{\\\\gamma^2})$ can be further improved to $O(\\\\frac{1}{\\\\sqrt{\\\\gamma}})$ through quantum means, where $\\\\gamma$ denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.',\n",
       "     'id': '6401',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6404',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores (r(44) = ?0.82 ? 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = ?0.19 ? 0.13, p = 0.0774) in forced fixation search; and we extend foveation to other clutter models showing stronger correlations in all cases. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. Code for building peripheral representations is  available.',\n",
       "     'id': '6404',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6406',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of  under-sampled data in k-space, and accelerating the data acquisition in MRI.  To improve the current MRI system in reconstruction accuracy and computational speed,  in this paper, we propose a novel deep architecture, dubbed ADMM-Net.  ADMM-Net is defined over a data flow graph, which is derived from the iterative  procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the  training data for CS-based reconstruction task. Experiments on MRI image  reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction  accuracies with fast computational speed.',\n",
       "     'id': '6406',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6407',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"In this paper, we develop a novel {\\\\bf ho}moto{\\\\bf p}y  {\\\\bf s}moothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and  a smooth term or  a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is $O(1/\\\\epsilon)$ without any assumption on the strong convexity. In this work, we will show that the proposed  HOPS achieved a lower iteration complexity of $\\\\tilde O(1/\\\\epsilon^{1-\\\\theta})$ with $\\\\theta\\\\in(0,1]$ capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption.  The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and $\\\\ell_1$ norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods.\",\n",
       "     'id': '6407',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6412',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The recursive teaching dimension (RTD) of a concept class $C \\\\subseteq \\\\{0, 1\\\\}^n$, introduced by Zilles et al. [ZLHZ11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of $C$ in the recursive teaching model. In this paper, we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension.  Given a concept class $C \\\\subseteq \\\\{0, 1\\\\}^n$ with $VCD(C) = d$, we first show that $RTD(C)$ is at most $d 2^{d+1}$. This is the first upper bound for $RTD(C)$ that depends only on $VCD(C)$, independent of the size of the concept class $|C|$ and its~domain size $n$. Before our work, the best known upper bound for $RTD(C)$ is $O(d 2^d \\\\log \\\\log |C|)$, obtained by Moran et al. [MSWY15]. We remove the $\\\\log \\\\log |C|$ factor.  We also improve the lower bound on the worst-case ratio of $RTD(C)$ to $VCD(C)$. We present a family of classes $\\\\{ C_k \\\\}_{k \\\\ge 1}$ with $VCD(C_k) = 3k$ and $RTD(C_k)=5k$, which implies that the ratio of $RTD(C)$ to $VCD(C)$ in the worst case can be as large as $5/3$. Before our work, the largest ratio known was $3/2$ as obtained by Kuhlmann [Kuh99]. Since then, no finite concept class $C$ has been known to satisfy $RTD(C) > (3/2) VCD(C)$.',\n",
       "     'id': '6412',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6414',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to -- i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information.  These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on  several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.',\n",
       "     'id': '6414',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6419',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We investigate the statistical performance and computational efficiency of the  alternating minimization procedure for nonparametric tensor learning. Tensor modeling has been widely used for capturing the higher order relations between  multimodal data sources. In addition to a linear model,  a nonlinear tensor model has been received much attention recently because of its high flexibility. We consider  an alternating minimization procedure for  a general nonlinear model where the true function  consists of components in a reproducing kernel Hilbert space (RKHS). In this paper, we show that the alternating minimization method achieves linear convergence as an optimization algorithm  and that the generalization error of the resultant estimator yields the minimax optimality. We apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.',\n",
       "     'id': '6419',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6423',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.',\n",
       "     'id': '6423',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6426',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective.  Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra.  We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.',\n",
       "     'id': '6426',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6427',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.\",\n",
       "     'id': '6427',\n",
       "     'year': '2016'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6881',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Computing partition function is the most important statistical inference task arising in applications of Graphical Models (GM). Since it is computationally intractable, approximate  methods have been used in practice, where mean-field  (MF) and belief propagation (BP) are arguably the most popular and successful approaches of a variational type. In this paper, we propose two new variational schemes, coined Gauged-MF (G-MF) and Gauged-BP (G-BP), improving MF and BP, respectively. Both provide lower bounds for the partition function by utilizing the so-called gauge transformation which modifies factors of GM while keeping the partition function invariant. Moreover, we prove that both G-MF and G-BP are exact for GMs with a single loop of a special structure, even though the bare MF and BP perform badly in this case. Our extensive experiments indeed confirm that the proposed algorithms outperform and generalize MF and BP.',\n",
       "     'id': '6881',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6883',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Calcium imaging is a prominent technology in neuroscience research which allows for simultaneous recording of large numbers of neurons in awake animals. Automated extraction of neurons and their temporal activity in imaging datasets is an important step in the path to producing neuroscience results. However, nearly all imaging datasets typically contain gross contaminating sources which could be contributed by the technology used, or the underlying biological tissue. Although attempts were made to better extract neural signals in limited gross contamination scenarios, there has been no effort to address contamination in full generality through statistical estimation. In this work, we proceed in a new direction and propose to extract cells and their activity using robust estimation. We derive a minimax optimal robust loss based on a simple abstraction of calcium imaging data, and also find a simple and practical optimization routine for this loss with provably fast convergence. We use our proposed robust loss in a matrix factorization framework to extract the neurons and their temporal activity in calcium imaging datasets. We demonstrate the superiority of our robust estimation approach over existing methods on both simulated and real datasets.',\n",
       "     'id': '6883',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6893',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.',\n",
       "     'id': '6893',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6898',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate ``where'' and ``what'' processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object.    This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets: pedestrian tracking on the KTH activity recognition dataset and the more difficult KITTI object tracking dataset.\",\n",
       "     'id': '6898',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6899',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"The paper addresses the classical network tomography problem of inferring local traffic given origin-destination observations. Focussing on large complex public transportation systems, we build a scalable model that exploits input-output information to estimate the unobserved link/station loads and the users path preferences. Based on the reconstruction of the users' travel time distribution, the model is flexible enough to capture possible different path-choice strategies and correlations between users travelling on similar paths at similar times. The corresponding likelihood function is intractable for medium or large-scale networks and we propose two distinct strategies, namely the exact maximum-likelihood inference of an approximate but tractable model and the variational inference of the original intractable model. As an application of our approach, we consider the emblematic case of the London Underground network, where a tap-in/tap-out system tracks the start/exit time and location of all journeys in a day. A set of synthetic simulations and real data provided by Transport For London are used to validate and test the model on the predictions of observable and unobservable quantities.\",\n",
       "     'id': '6899',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6908',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.',\n",
       "     'id': '6908',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6910',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance.  Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program.  This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model.  The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm.  While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner.  In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model.  To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors as inputs, we show that if the network response can be described using a maximum number of $s$ non-zero weights per node, these weights can be learned from $O(s\\\\log N)$ samples.',\n",
       "     'id': '6910',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6913',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual $\\\\|T_* v_\\\\pi - v_\\\\pi\\\\|_{1,\\\\nu}$ over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.',\n",
       "     'id': '6913',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6915',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness, or add prohibitive amounts of noise. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.',\n",
       "     'id': '6915',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6921',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.',\n",
       "     'id': '6921',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6923',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images.  But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution.  To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise.  Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise.  In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption.  On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.',\n",
       "     'id': '6923',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6932',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.',\n",
       "     'id': '6932',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6935',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.',\n",
       "     'id': '6935',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6940',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Population activity measurement by calcium imaging can be combined with cellular resolution optogenetic activity perturbations to enable the mapping of neural connectivity in vivo. This requires accurate inference of perturbed and unperturbed neural activity from calcium imaging measurements, which are noisy and indirect, and can also be contaminated by photostimulation artifacts. We have developed a new fully Bayesian approach to jointly inferring spiking activity and neural connectivity from in vivo all-optical perturbation experiments. In contrast to standard approaches that perform spike inference and analysis in two separate maximum-likelihood phases, our joint model is able to propagate uncertainty in spike inference to the inference of connectivity and vice versa. We use the framework of variational autoencoders to model spiking activity using discrete latent variables, low-dimensional latent common input, and sparse spike-and-slab generalized linear coupling between neurons. Additionally, we model two properties of the optogenetic perturbation: off-target photostimulation and photostimulation transients. Using this model, we were able to fit models on 30 minutes of data in just 10 minutes. We performed an all-optical circuit mapping experiment in primary visual cortex of the awake mouse, and use our approach to predict neural connectivity between excitatory neurons in layer 2/3. Predicted connectivity is sparse and consistent with known correlations with stimulus tuning, spontaneous correlation and distance.',\n",
       "     'id': '6940',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6942',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of ?what? and ?where?.  Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space.  Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations ? a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.\",\n",
       "     'id': '6942',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6945',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sums  problems. First, we show that perhaps surprisingly, the finite sum structure, by itself, is not sufficient for obtaining a complexity bound of $\\\\tilde{\\\\cO}((n+L/\\\\mu)\\\\ln(1/\\\\epsilon))$ for $L$-smooth and $\\\\mu$-strongly convex finite sums - one must also know exactly which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sums algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' complexity bound of $\\\\tilde{\\\\cO}((n+\\\\sqrt{n L/\\\\mu})\\\\ln(1/\\\\epsilon))$, unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing $L$-smooth and non-strongly convex finite sums, the optimal complexity bound is $\\\\tilde{\\\\cO}(n+L/\\\\epsilon)$, assuming that (on average) the same update rule is used for any iteration, and $\\\\tilde{\\\\cO}(n+\\\\sqrt{nL/\\\\epsilon})$, otherwise.\",\n",
       "     'id': '6945',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6947',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The problem of selecting the best $k$-element subset from a universe is involved in many applications. While previous studies assumed a noise-free environment or a noisy monotone submodular objective function, this paper considers a more realistic and general situation where the evaluation of a subset is a noisy monotone function (not necessarily submodular), with both multiplicative and additive noises. To understand the impact of the noise, we firstly show the approximation ratio of the greedy algorithm and POSS, two powerful algorithms for noise-free subset selection, in the noisy environments. We then propose to incorporate a noise-aware strategy into POSS, resulting in the new PONSS algorithm. We prove that PONSS can achieve a better approximation ratio under some assumption such as i.i.d. noise distribution. The empirical results on influence maximization and sparse regression problems show the superior performance of PONSS.',\n",
       "     'id': '6947',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6956',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that ``all local optima are (approximately) global optima'', and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult.   In this paper, we analyze the optimization landscape of the random over-complete  tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant $\\\\epsilon > 0$, among the set of points with function values $(1+\\\\epsilon)$-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem.   Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.\",\n",
       "     'id': '6956',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6957',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they  take into account different data modalities,  such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.',\n",
       "     'id': '6957',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6958',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"Cell assemblies, originally proposed by Donald Hebb (1949), are subsets of neurons firing in a temporally coordinated way that gives rise to repeated motifs supposed to underly neural representations and information processing. Although Hebb's original proposal dates back many decades, the detection of assemblies and their role in coding is still an open and current research topic, partly because simultaneous recordings from large populations of neurons became feasible only relatively recently. Most current and easy-to-apply computational techniques focus on the identification of strictly synchronously spiking neurons. In this paper we propose a new algorithm, based on sparse convolutional coding, for detecting recurrent motifs of arbitrary structure up to a given length. Testing of our algorithm on synthetically generated datasets shows that it outperforms established methods and accurately identifies the temporal structure of embedded assemblies, even when these contain overlapping neurons or when strong background noise is present. Moreover, exploratory analysis of experimental datasets from hippocampal slices and cortical neuron cultures have provided promising results.\",\n",
       "     'id': '6958',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6966',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We establish the consistency of an algorithm of Mondrian Forests~\\\\cite{lakshminarayanan2014mondrianforests,lakshminarayanan2016mondrianuncertainty}, a randomized classification algorithm that can be implemented online.  First, we amend the original Mondrian Forest algorithm proposed in~\\\\cite{lakshminarayanan2014mondrianforests}, that considers a \\\\emph{fixed} lifetime parameter.  Indeed, the fact that this parameter is fixed actually hinders statistical consistency of the original procedure.  Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters $\\\\lambda_n$, and uses an alternative updating rule, allowing to work also in an online fashion.   Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results~\\\\cite{arlot2014purf_bias} to an \\\\emph{arbitrary dimension}.',\n",
       "     'id': '6966',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6967',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Analysis of efficiency of outcomes in game theoretic settings has been a main item of study at the intersection of economics and computer science. The notion of the price of anarchy takes a worst-case stance to efficiency analysis, considering instance independent guarantees of efficiency. We propose a data-dependent analog of the price of anarchy that refines this worst-case assuming access to samples of strategic behavior. We focus on auction settings, where the latter is non-trivial due to the private information held by participants. Our approach to bounding the efficiency from data is robust to statistical errors and mis-specification. Unlike traditional econometrics, which seek to learn the private information of players from observed behavior and then analyze properties of the outcome, we directly quantify the inefficiency without going through the private information. We apply our approach to datasets from a sponsored search auction system and find empirical results that are a significant improvement over bounds from worst-case analysis.',\n",
       "     'id': '6967',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6973',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We give the first algorithm for kernel Nystrom approximation that runs in linear time in the number of training points and is provably accurate for all kernel matrices, without dependence on regularity or incoherence conditions. The algorithm projects the kernel onto a set of s landmark points sampled by their ridge leverage scores, requiring just O(ns) kernel evaluations and O(ns^2) additional runtime. While leverage score sampling has long been known to give strong theoretical guarantees for Nystrom approximation, by employing a fast recursive sampling scheme, our algorithm is the first to make the approach scalable. Empirically we show that it finds more accurate kernel approximations in less time than popular techniques such as classic Nystrom approximation and the random Fourier features method.',\n",
       "     'id': '6973',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6981',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory.',\n",
       "     'id': '6981',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6993',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].',\n",
       "     'id': '6993',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6996',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.',\n",
       "     'id': '6996',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '6998',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP$\\\\subseteq$BPP, and only an exponential-time sublinear-regret algorithm has been found. In this paper, we introduce mild assumptions to solve the problem. Under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.',\n",
       "     'id': '6998',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7003',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks.  Examples include AdaGrad, RMSProp, and Adam. We show that for simple over-parameterized problems, adaptive methods often find drastically different solutions than vanilla stochastic gradient descent (SGD).  We construct an illustrative binary classification problem where the data is linearly separable, SGD achieves zero test error, and AdaGrad and Adam attain test errors arbitrarily close to 1/2.  We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.',\n",
       "     'id': '7003',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7007',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"There has been a resurgence of interest in multiagent reinforcement learning (MARL), due partly to the recent success of deep neural networks. The simplest form of MARL is independent reinforcement learning (InRL), where each agent treats all of its experience as part of its (non stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe a meta-algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game theoretic analysis to compute meta-strategies for policy selection. The meta-algorithm generalizes previous algorithms such as InRL, iterated best response, double oracle, and fictitious play. Then, we propose a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in three partially observable settings: gridworld coordination problems, emergent language games, and poker.\",\n",
       "     'id': '7007',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7008',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'This paper considers a number of related inverse filtering problems for hidden Markov models (HMMs). In particular, given a sequence of state posteriors and the system dynamics; i) estimate the corresponding sequence of observations, ii) estimate the observation likelihoods, and iii) jointly estimate the observation likelihoods and the observation sequence. We show how to avoid a computationally expensive mixed integer linear program (MILP) by exploiting the algebraic structure of the HMM filter using simple linear algebra operations, and provide conditions for when the quantities can be uniquely reconstructed. We also propose a solution to the more general case where the posteriors are noisily observed. Finally, the proposed inverse filtering algorithms are evaluated on real-world polysomnographic data used for automatic sleep segmentation.',\n",
       "     'id': '7008',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7012',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.',\n",
       "     'id': '7012',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7013',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"We propose a generic algorithmic building block to accelerate training of  machine learning models on heterogeneous compute systems. Our scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique is built upon novel theoretical insights regarding primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on a large-scale dataset exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.\",\n",
       "     'id': '7013',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7017',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': \"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.\",\n",
       "     'id': '7017',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7023',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.',\n",
       "     'id': '7023',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7026',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We propose a novel method to {\\\\it directly} learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems.   The proposed training objective, which we derive via principled variational methods, encourages the transition operator to \"walk back\" (prefer to revert its steps) in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution. Source Code:http://github.com/anirudh9119/walkback_nips17',\n",
       "     'id': '7026',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7027',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to optimally deal with straggling workers. The proposed strategy, named as \\\\emph{polynomial codes}, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. This is the first code that achieves the optimal utilization of redundancy for tolerating stragglers or failures in distributed matrix multiplication. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently.   Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics including computation latency and communication load.  Moreover, we extend this code to distributed convolution and show its order-wise optimality.',\n",
       "     'id': '7027',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   {'_id': '7029',\n",
       "    '_index': 'nips_papers',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'abstract': 'Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through  simulations on real-world federated datasets.',\n",
       "     'id': '7029',\n",
       "     'year': '2017'},\n",
       "    '_type': 'document'},\n",
       "   ...],\n",
       "  'max_score': 1.0,\n",
       "  'total': 3808},\n",
       " 'timed_out': False,\n",
       " 'took': 246}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_list = []\n",
    "tot_num_docs = es.count(index = 'nips_papers', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must_not': {\n",
    "                'match': { 'abstract': 'Abstract Missing' }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})['count']\n",
    "\n",
    "res = es.search(index = 'nips_papers', body = {\n",
    "    '_source': ['id', 'year', 'abstract'],\n",
    "    'size': tot_num_docs,\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must_not': {\n",
    "                'match': {\n",
    "                    'abstract': 'Abstract Missing'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': 'We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \\\\cite{KlampflETAL:07b}. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis {(PCA)} with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals $X$ that are related or are not related to some additional target signal $Y_T$. In a biological interpretation, this target signal $Y_T$ (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.',\n",
       "  'id': '3168',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.',\n",
       "  'id': '3172',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Independent component analysis (ICA) is a powerful method to decouple signals. Most of the algorithms performing ICA do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution. Moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations. In this paper, we are interested in understanding the neural mechanism responsible for solving ICA. We present an online learning rule that exploits delayed correlations in the input. This rule performs ICA by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based Hebbian learning rule.',\n",
       "  'id': '3174',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship, we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering, as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.',\n",
       "  'id': '3176',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We present Epoch-Greedy, an algorithm for multi-armed bandits with observable side information. Epoch-Greedy has the following properties: No knowledge of a time horizon $T$ is necessary. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. The regret scales as $O(T^{2/3} S^{1/3})$ or better (sometimes, much better). Here $S$ is the complexity term in a sample complexity bound for standard supervised learning.',\n",
       "  'id': '3178',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.',\n",
       "  'id': '3200',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.',\n",
       "  'id': '3204',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'On-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e. the movement of the pen, is recorded directly. However, the raw data can be difficult to interpret because each letter is spread over many pen locations. As a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as HMMs. In this paper we describe a system capable of directly transcribing raw on-line handwriting data. The system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark HMM in both cases.',\n",
       "  'id': '3213',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'When training and test samples follow different input distributions (i.e., the situation called \\\\emph{covariate shift}), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the \\\\emph{importance} (i.e., the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.',\n",
       "  'id': '3248',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Imaging neuroscience links human behavior to aspects of brain biology in ever-increasing datasets. Existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks. However, testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations. We therefore propose to blend representation modelling and task classification into a unified statistical learning problem. A multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder. We show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset, as well as better generalization to other datasets.',\n",
       "  'id': '5646',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.',\n",
       "  'id': '5647',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs).We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality.  This representation also enables a new form of stochastic regularization by randomized modification of resolution.  We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.  Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.',\n",
       "  'id': '5649',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.In this paper we explore the benefit of using recurrent neural networks to model student learning.This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.We show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.These results suggest a promising new line of research for knowledge tracing.',\n",
       "  'id': '5654',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose an exploratory approach to statistical model criticism using maximum mean discrepancy (MMD) two sample tests. Typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model. MMD two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy. We demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on. We then apply the procedure to real data where the models being assessed are restricted Boltzmann machines, deep belief networks and Gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.',\n",
       "  'id': '5657',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'In user-facing applications, displaying calibrated confidence measures---probabilities that correspond to true frequency---can be as important as obtaining high accuracy. We are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis. Structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output. We extend the notion of calibration so as to handle various subtleties pertaining to the structured setting,  and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest. We explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.',\n",
       "  'id': '5658',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of  CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.',\n",
       "  'id': '5664',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.',\n",
       "  'id': '5669',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Stochastic search algorithms are general black-box optimizers. Due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search. Yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely. To alleviate these problems, we introduce a new surrogate-based stochastic search approach. We learn simple, quadratic surrogate models of the objective function. As the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models. The algorithm can be misled by an inaccurate optimum introduced by the surrogate. Instead, we use information theoretic constraints to bound the `distance' between the new and old data distribution while maximizing the objective function. Additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence. We compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task.The proposed method considerably outperforms the existing approaches.\",\n",
       "  'id': '5672',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both  based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than  competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.',\n",
       "  'id': '5685',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We design an online algorithm to classify the vertices of a graph.   Underpinning the algorithm is the probability distribution of an  Ising model isomorphic to the graph.   Each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far.  Computing these classifications is unfortunately based on a $\\\\#P$-complete problem.  This motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework.    Our algorithm is optimal when the graph is a tree matching the prior results in [1].For a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound. The algorithm is efficient as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.',\n",
       "  'id': '5690',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call Submodular Partitioning. These problems generalize purely robust instances of the problem, namely max-min submodular fair allocation (SFA) and \\\\emph{min-max submodular load balancing} (SLB), and also average-case instances, that is the submodular welfare problem (SWP) and submodular multiway partition (SMP). While the robust versions have been studied in the theory community, existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This contrasts the average case instances, where most of the algorithms are scalable.  In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.',\n",
       "  'id': '5706',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We investigate the problem of learning an unknown probability distribution over a discrete population from random samples. Our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing Differential Privacy to the individuals of the population.We describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families. Our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient - both in terms of sample size and running time - as their non-private counterparts. We complement our theoretical guarantees with an experimental evaluation. Our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set.',\n",
       "  'id': '5713',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose a robust portfolio optimization approach based on quantile statistics. The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data. The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.',\n",
       "  'id': '5714',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem.In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy.This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs.The Counterfactual Risk Minimization (CRM) principle offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator.We show that this conventional estimator suffers from apropensity overfitting problem when used for learning over complex hypothesis spaces.We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem.This naturally gives rise to a new learning algorithm -- Normalized Policy Optimizer for Exponential Models (Norm-POEM) --for structured output prediction using linear rules.We evaluate the empirical effectiveness of Norm-POEM on severalmulti-label classification problems, finding that it consistently outperforms the conventional estimator.',\n",
       "  'id': '5748',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs)when the number of observations is much larger than the number of coefficients (n > > p > > 1). In this regime, optimization algorithms can immensely benefit fromapproximate second order information.We propose an alternative way of constructing the curvature information by formulatingit as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling andeigenvalue thresholding.Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the case where the rows of the design matrix are i.i.d. samples with bounded support.We show that the convergence has two phases, aquadratic phase followed by a linear phase. Finally,we empirically demonstrate that our algorithm achieves the highest performancecompared to various algorithms on several datasets.',\n",
       "  'id': '5750',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour,  whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matchingexpectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.',\n",
       "  'id': '5756',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local-approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale datasets settings. However, EP has a crucial limitation in this context: the number approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP).  Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.',\n",
       "  'id': '5760',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -- swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions.',\n",
       "  'id': '5764',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to fit key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.',\n",
       "  'id': '5780',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.',\n",
       "  'id': '5782',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce an unsupervised learning algorithmthat combines probabilistic modeling with solver-based techniques for program synthesis.We apply our techniques to both a visual learning domain and a language learning problem,showing that our algorithm can learn many visual concepts from only a few examplesand that it can recover some English inflectional morphology.Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures,and a technique for applying program synthesis tools to noisy data.',\n",
       "  'id': '5785',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'In simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus. Basic statistical considerations state that the reliability of the stimulus information, i.e., the amount of noise in the samples, should be taken into account when the decision is made. However, for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability. We here show that even the basic drift diffusion model, which has frequently been used to explain experimental findings in perceptual decision making, implicitly relies on estimates of stimulus reliability. We then show that only those variants of the drift diffusion model which allow stimulus-specific reliabilities are consistent with neurophysiological findings. Our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds.',\n",
       "  'id': '5789',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that ex- ploits the so far unused ?geometry? in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADA-grad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.',\n",
       "  'id': '5795',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We analyze in this paper a random feature map based on a  theory of invariance (\\\\emph{I-theory}) introduced in \\\\cite{AnselmiLRMTP13}. More specifically, a group invariant  signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a  set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting',\n",
       "  'id': '5798',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.',\n",
       "  'id': '5801',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We develop \\\\textit{parallel predictive entropy search} (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a \\\\textit{batch} of points which will maximize the information gain about the global maximizer of the objective.  Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.',\n",
       "  'id': '5804',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"We present a new algorithm for community detection. The algorithm uses random walks to embed the graph in a space of measures, after which a modification of $k$-means in that space is applied. The algorithm is therefore fast and easily parallelizable. We evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks,  and find its performance to be better or at least as good as previously known  algorithms. We also prove a linear time (in number of edges) guarantee for the algorithm on a $p,q$-stochastic block model with  where $p \\\\geq c\\\\cdot N^{-\\\\half +  \\\\epsilon}$ and $p-q \\\\geq c' \\\\sqrt{p N^{-\\\\half +  \\\\epsilon} \\\\log N}$.\",\n",
       "  'id': '5808',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Determinantal point processes (DPPs) are point process models thatnaturally encode diversity between the points of agiven realization, through a positive definite kernel $K$. DPPs possess desirable properties, such as exactsampling or analyticity of the moments, but learning the parameters ofkernel $K$ through likelihood-based inference is notstraightforward. First, the kernel that appears in thelikelihood is not $K$, but another kernel $L$ related to $K$ throughan often intractable spectral decomposition. This issue is typically bypassed in machine learning bydirectly parametrizing the kernel $L$, at the price of someinterpretability of the model parameters. We follow this approachhere. Second, the likelihood has an intractable normalizingconstant, which takes the form of large determinant in the case of aDPP over a finite set of objects, and the form of a Fredholm determinant in thecase of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood ofa DPP, both for finite and continuous domains. Unlike previous work, our bounds arecheap to evaluate since they do not rely on approximating the spectrumof a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variationalinference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs.',\n",
       "  'id': '5810',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template. The algorithmic instances of our framework are universal since they can automatically adapt to the unknown Holder continuity degree and constant within the dual formulation. They are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each Holder smoothness degree. In contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function. We instead leverage computationally cheaper, Fenchel-type operators, which are the main workhorses of the generalized conditional gradient (GCG)-type methods. In contrast to the GCG-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.',\n",
       "  'id': '5826',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We present an algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems (CSP), via a common generalization in terms of random bipartite graphs. Our algorithm matches up to a constant factor the best-known bounds  for the number of edges (or constraints) needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is significantly better than both spectral and SDP-based approaches.The main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted CSP.  Here our algorithm succeeds at a significantly lower density than the spectral approaches, surpassing a barrier based on the spectral norm of a random matrix.Other significant features of the algorithm and analysis include (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) the algorithm can be implemented statistically, i.e., with very limited access to the input distribution (iii) the algorithm is extremely simple to implement and runs in linear time, and thus is practical even for very large instances.',\n",
       "  'id': '5835',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. We use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.',\n",
       "  'id': '5836',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle. In particular, for a given function $x \\\\to f(x)$ we consider optimization when one is given access to absolute error oracles that return values in [f(x) - \\\\epsilon,f(x)+\\\\epsilon] or relative error oracles that return value in [(1+\\\\epsilon)f(x), (1 +\\\\epsilon)f (x)], for some \\\\epsilon larger than 0. We show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.',\n",
       "  'id': '5841',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.',\n",
       "  'id': '5846',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Convolutional Neural Networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner. In this work we introduce a new learnable module, theSpatial Transformer, which explicitly allows the spatial manipulation ofdata within the network. This differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself,without any extra training supervision or modification to the optimisation process. We show that the useof spatial transformers results in models which learn invariance to translation,scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.',\n",
       "  'id': '5854',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient. For the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets.  For the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses.  Our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging. To demonstrate, we trained a sparsely connected network that runs on the TrueNorth chip using the MNIST dataset. With a high performance network (ensemble of $64$), we achieve $99.42\\\\%$ accuracy at $121 \\\\mu$J per image, and with a high efficiency network (ensemble of $1$) we achieve $92.7\\\\%$ accuracy at $0.408 \\\\mu$J per image.',\n",
       "  'id': '5862',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance.',\n",
       "  'id': '5863',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence.Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines,because the number of target classes in eachstep of the output depends on the length of the input, which is variable.Problems such as sorting variable sized sequences, and various combinatorialoptimization problems belong to this class.  Our model solvesthe problem of variable size output dictionaries using a recently proposedmechanism of neural attention. It differs from the previous attentionattempts in that, instead of using attention to blend hidden units of anencoder to a context vector at each decoder step, it uses attention asa pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).We show Ptr-Nets can be used to learn approximate solutions to threechallenging geometric problems -- finding planar convex hulls, computingDelaunay triangulations, and the planar Travelling Salesman Problem-- using training examples alone. Ptr-Nets not only improve oversequence-to-sequence with input attention, butalso allow us to generalize to variable size output dictionaries.We show that the learnt models generalize beyond the maximum lengthsthey were trained on. We hope our results on these taskswill encourage a broader exploration of neural learning for discreteproblems.',\n",
       "  'id': '5866',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.',\n",
       "  'id': '5885',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (Scott et al, 2013). A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC, achieving near-ideal speedup in some instances.',\n",
       "  'id': '5888',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.\",\n",
       "  'id': '5890',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Counterfactual Regret Minimization (CFR) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games. CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set.We introduce an improvement to CFR that prunes any path of play in the tree, and its descendants, that has negative regret. It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration. The new algorithm maintains CFR's convergence guarantees while making iterations significantly faster---even if previously known pruning techniques are used in the comparison. This improvement carries over to CFR+, a recent variant of CFR. Experiments show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game.\",\n",
       "  'id': '5910',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Expectation Propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP's convergence with respect to the true posterior. In particular, we show that EP converges at a rate of $O(n^{-2})$ for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.\",\n",
       "  'id': '5912',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting -- maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extraround of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation.  On the empirical side, the recent work in (Gu et al. 2012) and (Gu et al. 2014) (on active linear and logistic regression) shows the promise of this approach.',\n",
       "  'id': '5918',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We study the restless bandit associated with an extremely simple scalar Kalman filter model in discrete time. Under certain assumptions, we prove that the problem is {\\\\it indexable} in the sense that the {\\\\it Whittle index} is a non-decreasing function of the relevant belief state. In spite of the long history of this problem, this appears to be the first such proof. We use results about {\\\\it Schur-convexity} and {\\\\it mechanical words}, which are particularbinary strings intimately related to {\\\\it palindromes}.',\n",
       "  'id': '5922',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.',\n",
       "  'id': '5923',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Computing the MAP assignment in graphical models is generally intractable.  As a result, for discrete graphical models, the MAP problem is often approximated using linear programming relaxations.  Much research has focused on characterizing when these LP relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog.  In this work, we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight.  We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models.  We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.',\n",
       "  'id': '5932',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.',\n",
       "  'id': '5938',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss? SLN-robustness.',\n",
       "  'id': '5941',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials. Meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit l2 norm. The question we address in this work is: if we know a priori that data is so normalized, can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting, and introduce a new approximation paradigm, Spherical Random Fourier (SRF) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work, SRF features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy.',\n",
       "  'id': '5943',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.',\n",
       "  'id': '5947',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Training deep feature hierarchies to solve supervised learning tasks has achieving state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabelednatural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing a latent variables that are non-deterministic functions of the input into the network architecture.',\n",
       "  'id': '5951',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches make training and prediction tractable by assuming that the training label matrix is low-rank and hence the effective number of labels can be reduced by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies or scale to large problems as the low rank assumption is violated in most real world applications.This paper develops the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world as well as benchmark data sets and compare our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embeddings (by as much as 35%) as well as trees (by as much as 6%). SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.',\n",
       "  'id': '5969',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N-body methods developed for nonlinear embeddings, yielding linear-time iterations.\",\n",
       "  'id': '5972',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.',\n",
       "  'id': '5982',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Recently, there has been growing interest in lifting MAP inference algorithms for Markov logic networks (MLNs). A key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the MLN and these symmetries can be detected using lifted inference rules. Unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries. This is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the MLN, and search for solutions in the much larger propositional space. In this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding. Our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable. We show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space MAP inference algorithms. Our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.',\n",
       "  'id': '5987',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible.This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.',\n",
       "  'id': '5988',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition. We present an efficient computational algorithm that modifies Leurgans' algoirthm for tensor factorization. Our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction. We use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor. We delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm. We validate our algorithm with numerical experiments.\",\n",
       "  'id': '6017',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We investigate the robust PCA problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming Principal Component Pursuit (PCP). In contrast to previous studies that assume the support of the error matrix is generated by uniform Bernoulli sampling, we allow non-uniform sampling, i.e., entries of the low-rank matrix are corrupted by errors with unequal probabilities. We characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix, under which correct matrix decomposition by PCP is guaranteed. Such a refined analysis of robust PCA captures how robust each entry of the low rank matrix combats error corruption. In order to deal with non-uniform error corruption, our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies.',\n",
       "  'id': '6018',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result.  For instance,  a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.',\n",
       "  'id': '6019',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements. The proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially.In recent years, various methods are proposed for compressive phase retrieval, but they have suboptimal sample complexity or lack robustness guarantees. The main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target. Given a set of underdetermined measurements, there is a standard framework for recovering a sparse matrix, and a standard framework for recovering a low-rank matrix. However, a general, efficient method for recovering a jointly sparse and low-rank matrix has remained elusive.Deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace, then the low-rank and sparse structures of the target signal can be effectively decoupled. We show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is $\\\\mathsf{O}(k\\\\,\\\\log\\\\frac{d}{k})$, where $k$ and $d$ denote the sparsity level and the dimension of the input signal. We also evaluate the algorithm through numerical simulation.',\n",
       "  'id': '6021',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms. In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by {\\\\em any} norm regularization.We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain \\\\textit{\\\\modified}~complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably including the recently proposed spectral $k$-support norm.',\n",
       "  'id': '6022',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We consider Empirical Risk Minimization (ERM) in the context of stochastic optimization with exp-concave and smooth losses---a general optimization framework that captures several important learning problems including linear and logistic regression, learning SVMs with the squared hinge-loss, portfolio selection and more. In this setting, we establish the first evidence that ERM is able to attain fast generalization rates, and show that the expected loss of the ERM solution in $d$ dimensions converges to the optimal expected loss in a rate of $d/n$. This rate matches existing lower bounds up to constants and improves by a $\\\\log{n}$ factor upon the state-of-the-art, which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.',\n",
       "  'id': '6034',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, and prove this class includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness comparing with existing heuristics on common benchmark datasets.',\n",
       "  'id': '6038',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times. By considering a novel problem formulation?the minimization of a sum of piecewise functions?we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization. This result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization. In empirical comparisons, we study the scalability of our methods. We find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.',\n",
       "  'id': '6043',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models.  However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer.  This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties.  Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians.  This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.',\n",
       "  'id': '6047',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.',\n",
       "  'id': '6049',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In many applications such as advertisement placement or automated dialog systems, an intelligent system optimizes performance over a sequence of interactions with each user. Such tasks often involve many states and potentially time-dependent transition dynamics, and can be modeled well as episodic Markov decision processes (MDPs). In this paper, we present a PAC algorithm for reinforcement learning in episodic finite MDPs with time-dependent transitions that acts epsilon-optimal in all but O(S A H^3  / epsilon^2 log(1 / delta)) episodes. Our algorithm has a polynomial computational complexity, and our sample complexity bound accounts for the fact that we may only be able to approximately solve the internal planning problems. In addition, our PAC sample complexity bound has only linear dependency on the number of states S and actions A and strictly improves previous bounds with S^2 dependency in this setting. Compared against other methods for infinite horizon reinforcement learning with linear state space sample complexity our method has much lower dependency on the (effective) horizon. Indeed, our bound is optimal up to a factor of H.',\n",
       "  'id': '6052',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We introduce the Forget-me-not Process, an efficient, non-parametric meta-algorithm for online probabilistic sequence prediction for piecewise stationary, repeating sources. Our method works by taking a Bayesian approach to partition a stream of data into postulated task-specific segments, while simultaneously building a model for each task. We provide regret guarantees with respect to piecewise stationary data sources under the logarithmic loss, and validate the method empirically across a range of sequence prediction and task identification problems.',\n",
       "  'id': '6055',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We formalize notions of robustness for composite estimators via the notion of a breakdown point.  A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.',\n",
       "  'id': '6056',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider the problem of recovering a signal observed in Gaussian noise. If the set of signals is convex and compact, and can be specified beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk. However, when the set is unspecified, designing an estimator that is blind to the hidden structure of the signal remains a challenging problem. We propose a new family of estimators to recover signals observed in Gaussian noise. Instead of specifying the set where the signal lives, we assume the existence of a well-performing linear estimator. Proposed estimators enjoy exact oracle inequalities and can be efficiently computed through convex optimization. We present several numerical illustrations that show the potential of the approach.',\n",
       "  'id': '6063',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features.  Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism.  We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations.  In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.',\n",
       "  'id': '6067',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.',\n",
       "  'id': '6068',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.',\n",
       "  'id': '6071',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. We focus on the most biased coin problem, asking how many total coin flips are required to identify a ``heavy\\'\\' coin from an infinite bag containing both ``heavy\\'\\' coins with mean $\\\\theta_1 \\\\in (0,1)$, and ``light\" coins with mean $\\\\theta_0 \\\\in (0,\\\\theta_1)$, where heavy coins are drawn from the bag with proportion $\\\\alpha \\\\in (0,1/2)$. When $\\\\alpha,\\\\theta_0,\\\\theta_1$ are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters $\\\\theta_0,\\\\theta_1,\\\\alpha$, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples.  In characterizing this gap between adaptive and nonadaptive strategies,  we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.',\n",
       "  'id': '6072',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Many problems in real-world applications involve predicting continuous-valued random variables that are statistically related.  In this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the   dependencies between continuous output variables.  We show that  inference in our  model using proximal methods can be efficiently solved as a feed-foward pass of a special  type of  deep recurrent neural network. We demonstrate the  effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.',\n",
       "  'id': '6074',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ?-ball around the observed data, which is only correct in the limit ??0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ??0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.',\n",
       "  'id': '6084',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability.  In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as ?share of voice?, and ?buyer surplus?. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.',\n",
       "  'id': '6085',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We introduce the Multiple Quantile Graphical Model (MQGM), which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models.  The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others.  Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates.  We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model. We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers.  We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate. Lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data.',\n",
       "  'id': '6092',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i)  the envelope function $\\\\sup_{f \\\\in \\\\mathcal{F}}|\\\\ell \\\\circ f|$, where $\\\\ell$ is the loss function and $\\\\mathcal{F}$ is the hypothesis class, exists and is $L^r$-integrable, and (ii) $\\\\ell$ satisfies the multi-scale Bernstein's condition on $\\\\mathcal{F}$. Under these assumptions, we prove that learning rate faster than $O(n^{-1/2})$ can be obtained and, depending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily close to $O(n^{-1})$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.\",\n",
       "  'id': '6104',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.',\n",
       "  'id': '6107',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We introduce the framework of {\\\\em blind regression} motivated by {\\\\em matrix completion} for recommendation systems: given $m$ users, $n$ movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user $u$ and movie $i$ have features $x_1(u)$ and $x_2(i)$ respectively, and their corresponding rating $y(u,i)$ is a noisy measurement of $f(x_1(u), x_2(i))$ for some unknown function $f$. In contrast with classical regression, the features $x = (x_1(u), x_2(i))$ are not observed, making it challenging to apply standard regression methods to  predict the unobserved ratings.  Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least $\\\\max(m^{-1+\\\\delta},n^{-1/2+\\\\delta})$ with $\\\\delta > 0$, we prove that the expected fraction of our estimates with error greater than $\\\\epsilon$ is less than $\\\\gamma^2 / \\\\epsilon^2$ plus a polynomially decaying term, where $\\\\gamma^2$ is the variance of the additive entry-wise noise term.  Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods.\",\n",
       "  'id': '6108',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Over the last years, many variations of the quadratic k-means clustering procedure have been proposed, all aiming to robustify the performance of the algorithm in the presence of outliers. In general terms, two main approaches have been developed: one based on penalized regularization methods, and one based on trimming functions. In this work, we present a theoretical analysis of the robustness and consistency properties of a variant of the classical quadratic k-means algorithm, the robust k-means, which borrows ideas from outlier detection in regression. We show that two outliers in a dataset are enough to breakdown this clustering procedure. However, if we focus on ?well-structured? datasets, then robust k-means can recover the underlying cluster structure in spite of the outliers. Finally, we show that, with slight modifications, the most general non-asymptotic results for consistency of quadratic k-means remain valid for this robust variant.',\n",
       "  'id': '6126',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The algorithmic advancement of synchronizing maps is important in order to solve a wide range of practice problems  with possible large-scale dataset. In this paper, we provide theoretical justifications for spectral techniques for the map synchronization problem, i.e., it takes as input a collection of objects and noisy maps estimated between pairs of objects, and outputs clean maps between all pairs of objects. We show that a simple normalized spectral method that projects the blocks of the top eigenvectors of a data matrix to the map space leads to surprisingly good results. As the noise is modelled naturally as random permutation matrix, this algorithm NormSpecSync leads to competing theoretical guarantees as state-of-the-art convex optimization techniques, yet it is much more efficient. We demonstrate the usefulness of our algorithm in a couple of applications, where it is optimal in both complexity and exactness among existing methods.',\n",
       "  'id': '6128',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs.',\n",
       "  'id': '6129',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In neuroscience, the similarity matrix of neural activity patterns in response to different sensory stimuli or under different cognitive states reflects the structure of neural representational space. Existing methods derive point estimations of neural activity patterns from noisy neural imaging data, and the similarity is calculated from these point estimations. We show that this approach translates structured noise from estimated patterns into spurious bias structure in the resulting similarity matrix, which is especially severe when signal-to-noise ratio is low and experimental conditions cannot be fully randomized in a cognitive task. We propose an alternative Bayesian framework for computing representational similarity in which we treat the covariance structure of neural activity patterns as a hyper-parameter in a generative model of the neural data, and directly estimate this covariance structure from imaging data while marginalizing over the unknown activity patterns. Converting the estimated covariance structure into a correlation matrix offers a much less biased estimate of neural representational similarity. Our method can also simultaneously estimate a signal-to-noise map that informs where the learned representational structure is supported more strongly, and the learned covariance matrix can be used as a structured prior to constrain Bayesian estimation of neural activity patterns. Our code is freely available in Brain Imaging Analysis Kit (Brainiak) (https://github.com/IntelPNI/brainiak), a python toolkit for brain imaging analysis.',\n",
       "  'id': '6131',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy.  We demonstrate the proposed approaches on four different link prediction tasks.',\n",
       "  'id': '6144',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.',\n",
       "  'id': '6148',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice.  These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices.  We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator.  The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.',\n",
       "  'id': '6159',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.',\n",
       "  'id': '6172',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Longitudinal data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of longitudinal EHR data. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled longitudinal data. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.',\n",
       "  'id': '6177',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard ``deep learning'' datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.\",\n",
       "  'id': '6184',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.',\n",
       "  'id': '6191',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Linkages are essentially determined by similarity measures that may be derived from multiple perspectives. For example, spatial linkages are usually generated based on localities of heterogeneous data, whereas semantic linkages can come from various properties, such as different physical meanings behind social relations. Many existing metric learning models focus on spatial linkages, but leave the rich semantic factors unconsidered. Similarities based on these models are usually overdetermined on linkages. We propose a Unified Multi-Metric Learning (UM2L) framework to exploit multiple types of metrics. In UM2L, a type of combination operator is introduced for distance characterization from multiple perspectives, and thus can introduce flexibilities for representing and utilizing both spatial and semantic linkages. Besides, we propose a uniform solver for UM2L which is guaranteed to converge. Extensive experiments on diverse applications exhibit the superior classification performance and comprehensibility of UM2L. Visualization results also validate its ability on physical meanings discovery.',\n",
       "  'id': '6192',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.\",\n",
       "  'id': '6194',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Utilizing the structure of a probabilistic model can significantly increase its learning speed. Motivated by several recent applications, in particular bigram models in language processing, we consider learning low-rank conditional probability matrices under expected KL-risk. This choice makes smoothing, that is the careful handling of low-probability elements, paramount. We derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk. We then derive sample-complexity bounds for the global minimizer of the penalized risk and show that it is within a small factor of the optimal sample complexity. This framework generalizes to more sophisticated smoothing techniques, including absolute-discounting.',\n",
       "  'id': '6199',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1% test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.',\n",
       "  'id': '6211',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.',\n",
       "  'id': '6214',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We introduce a general simple structural design called ?Multiplicative Integration? (MI) to improve recurrent neural networks (RNNs). MI changes the way of how the information flow gets integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.',\n",
       "  'id': '6215',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We introduce a theoretical and algorithmic framework for multi-way graph partitioning that relies on a multiplicative cut-based objective. We refer to this objective as the Product Cut. We provide a detailed investigation of the mathematical properties of this objective and an effective algorithm for its optimization. The proposed model has strong mathematical underpinnings, and the corresponding algorithm achieves state-of-the-art performance on benchmark data sets.',\n",
       "  'id': '6226',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward  neural networks can be trained globally optimal with a linear convergence rate. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one- and two hidden layer networks. Our experiments confirms that these models are already rich enough to achieve good performance on a series of real-world datasets.',\n",
       "  'id': '6238',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider cooperative multi-agent consensus optimization problems over an undirected network of agents, where only those agents connected by an edge can directly communicate. The objective is to minimize the sum of agent-specific composite convex functions over agent-specific private conic constraint sets; hence, the optimal consensus decision should lie in the intersection of these private sets. We provide convergence rates in sub-optimality, infeasibility and consensus violation; examine the effect of underlying network topology on the convergence rates of the proposed decentralized algorithms; and show how to extend these methods to handle time-varying communication networks.',\n",
       "  'id': '6242',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We study the sampling-based planning problem in Markov decision processes (MDPs) that we can access only through a generative model, usually referred to as Monte-Carlo planning. Our objective is to return a good estimate of the optimal value function at any state while minimizing the number of calls to the generative model, i.e. the sample complexity. We propose a new algorithm, TrailBlazer, able to handle MDPs with a finite or an infinite number of transitions from state-action to next states. TrailBlazer is an adaptive algorithm that exploits possible structures of the MDP by exploring only a subset of states reachable by following near-optimal policies. We provide bounds on its sample complexity that depend on a measure of the quantity of near-optimal states. The algorithm behavior can be considered as an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). Finally, another appealing feature of TrailBlazer is that it is simple to implement and computationally efficient.',\n",
       "  'id': '6253',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Semi-supervised clustering algorithms have been proposed to identify data clusters that align with user perceived ones via the aid of side information such as seeds or pairwise constrains. However, traditional side information is mostly at the instance level and subject to the sampling bias, where non-randomly sampled instances in the supervision can mislead the algorithms to wrong clusters. In this paper, we propose learning from the feature-level supervision. We show that this kind of supervision can be easily obtained in the form of perception vectors in many applications. Then we present novel algorithms, called Perception Embedded (PE) clustering, that exploit the perception vectors as well as traditional side information to find clusters perceived by the user. Extensive experiments are conducted on real datasets and the results demonstrate the effectiveness of PE empirically.',\n",
       "  'id': '6260',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\",\n",
       "  'id': '6264',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.',\n",
       "  'id': '6275',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks. Finally, a neural network implementation of end-to-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.',\n",
       "  'id': '6278',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In this paper, we propose a multi-step inertial Forward--Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the scheme with the help of the Kurdyka??ojasiewicz property. Then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on a few problems arising from statistics and machine learning.',\n",
       "  'id': '6285',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called \\\\emph{Split LBI}. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some $\\\\ell_2$ error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking.',\n",
       "  'id': '6288',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.',\n",
       "  'id': '6289',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Complex networks play an important role in a plethora of disciplines in natural sciences. Cleaning up noisy observed networks, poses an important challenge in network analysis Existing methods utilize labeled data to alleviate the noise effect in the network.  However, labeled data is usually expensive to collect while unlabeled data can be gathered cheaply. In this paper, we propose an optimization framework to mine useful structures from noisy networks in an unsupervised manner. The key feature of our optimization framework is its ability to utilize local structures as well as global patterns in the network. We extend our method to incorporate multi-resolution networks in order to add further resistance to high-levels of noise. We also generalize our framework to utilize partial labels to enhance the performance. We specifically focus our method on multi-resolution Hi-C data by recovering clusters of genomic regions that co-localize in 3D space. Additionally, we use Capture-C-generated partial labels to further denoise the Hi-C network. We empirically demonstrate the effectiveness of our framework in denoising the network and improving community detection results.',\n",
       "  'id': '6291',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the sum of these vectors doesn't exceed the budget in each dimension. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits  (linContextual),  bandits with knapsacks (BwK), and the online stochastic packing problem (OSPP). We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem, where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through  an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases.\",\n",
       "  'id': '6292',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications. These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset. However, the high variance inherent in these noisy gradients degrades performance and leads to slower mixing. In this paper, we present techniques for reducing variance in stochastic gradient Langevin dynamics, yielding novel  stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics. This is complemented by impressive empirical results obtained on  a variety of real world datasets, and on four different machine learning tasks (regression, classification, independent component analysis and mixture modeling). These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods.',\n",
       "  'id': '6293',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization  of a least square criterion under a multilinear rank constraint, a difficult  non convex problem.  HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR computes accurate solutions while being computationally very competitive.',\n",
       "  'id': '6302',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. Current RNN models are ill suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors which generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences.   The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information.  It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes.',\n",
       "  'id': '6310',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Experiments reveal that in the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas, where visual and vestibular cues are integrated to infer heading direction, there are two types of neurons with roughly the same number. One is ?congruent? cells, whose preferred heading directions are similar in response to visual and vestibular cues; and the other is ?opposite? cells, whose preferred heading directions are nearly ?opposite? (with an offset of 180 degree) in response to visual vs. vestibular cues. Congruent neurons are known to be responsible for cue integration, but the computational role of opposite neurons remains largely unknown. Here, we propose that opposite neurons may serve to encode the disparity information between cues necessary for multisensory segregation. We build a computational model composed of two reciprocally coupled modules, MSTd and VIP, and each module consists of groups of congruent and opposite neurons. In the model, congruent neurons in two modules are reciprocally connected with each other in the congruent manner, whereas opposite neurons are reciprocally connected in the opposite manner. Mimicking the experimental protocol, our model reproduces the characteristics of congruent and opposite neurons, and demonstrates that in each module, the sisters of congruent and opposite neurons can jointly achieve optimal multisensory information integration and segregation. This study sheds light on our understanding of how the brain implements optimal multisensory integration and segregation concurrently in a distributed manner.',\n",
       "  'id': '6317',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and capture into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.',\n",
       "  'id': '6323',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with $M \\\\geq 3$ components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro (2007). Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least $1-e^{-\\\\Omega(M)}$. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.',\n",
       "  'id': '6324',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We study the cost function for  hierarchical clusterings introduced by [Dasgupta, 2015]  where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [Dasgupta, 2015] that a top-down algorithm  returns a hierarchical clustering of cost at most \\\\(O\\\\left(\\\\alpha_n \\\\log n\\\\right)\\\\) times the cost of the optimal hierarchical clustering, where \\\\(\\\\alpha_n\\\\) is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani,  the top down algorithm returns a hierarchical clustering of cost at most  \\\\(O\\\\left(\\\\log^{3/2} n\\\\right)\\\\) times the cost of the optimal solution. We improve this by giving an \\\\(O(\\\\log{n})\\\\)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by  using the idea of \\\\emph{sphere growing} which has been extensively used in the context of graph  partitioning. We also prove that our algorithm returns an \\\\(O(\\\\log{n})\\\\)-approximate  hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2015]. Experiments show that the hierarchies found by using the ILP formulation as well  as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with an inapproximability result for this problem, namely that no polynomial sized LP or SDP can be used to obtain a constant factor approximation for this problem.',\n",
       "  'id': '6325',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results. Here we introduce KiloSort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering. KiloSort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved. Unlike previous algorithms that compress the data with PCA, KiloSort operates on the raw data which allows it to construct a more accurate model of the waveforms. Processing times are faster than in previous algorithms thanks to batch-based optimization on GPUs. We compare KiloSort to an established algorithm and show favorable performance, at much reduced processing times. A novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings.',\n",
       "  'id': '6326',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.\",\n",
       "  'id': '6332',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.',\n",
       "  'id': '6334',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present a new boosting algorithm for the key scenario of binary classification with abstention where the algorithm can abstain from predicting the label of a point, at the price of a fixed cost.  At each round, our algorithm selects a pair of functions, a base predictor and a base abstention function.  We define convex upper bounds for the natural loss function associated to this problem, which we prove to be calibrated with respect to the Bayes solution. Our algorithm benefits from general margin-based learning guarantees which we derive for ensembles of pairs of base predictor and abstention functions, in terms of the Rademacher complexities of the corresponding function classes.  We give convergence guarantees for our algorithm along with a linear-time weak-learning algorithm for abstention stumps. We also report the results of several experiments suggesting that our algorithm provides a significant improvement in practice over two confidence-based algorithms.',\n",
       "  'id': '6336',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein?s identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.',\n",
       "  'id': '6338',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In this work, we propose an infinite restricted Boltzmann machine (RBM), whose maximum likelihood estimation (MLE) corresponds to a constrained convex optimization.  We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity.  As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization. The resulting model can also be used as an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.',\n",
       "  'id': '6342',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Several works have shown that deep CNN classifiers can be easily transferred across datasets, e.g. the transfer of a CNN trained to recognize objects on ImageNET to an object detector on Pascal VOC. Less clear, however, is the ability of CNNs to transfer knowledge across tasks. A common example of such transfer is the problem of scene classification that should leverage localized object detections to recognize holistic visual concepts. While this problem is currently addressed with Fisher vector representations, these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern CNNs. It is argued that this is mostly due to the reliance on a model, the Gaussian mixture of diagonal covariances, which has a very limited ability to capture the second order statistics of CNN features. This problem is addressed by the adoption of a better model, the mixture of factor analyzers (MFA), which approximates the non-linear data manifold by a collection of local subspaces. The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an image representation for holistic image classifiers. Extensive experiments show that the MFA-FS has state of the art performance for object-to-scene transfer and this transfer actually  outperforms the training of a scene CNN from a large scene dataset. The two representations are also shown to be complementary, in the sense that their combination outperforms each of the representations by itself. When combined, they produce a state of the art scene classifier.',\n",
       "  'id': '6343',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Bayesian nonparametric  methods based on the Dirichlet process (DP), gamma process and beta process, have proven effective in capturing aspects of various datasets arising in machine learning.  However, it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior. As such there is now considerable interest in models based on the Stable Processs (SP), Generalized Gamma process (GGP) and Stable-beta process (SBP). These models present new challenges in terms of practical statistical implementation. In analogy to tractable processes such as the finite-dimensional Dirichlet process, we describe a class of random processes, we call iid finite-dimensional BFRY processes, that enables one to begin to develop efficient posterior inference algorithms such as variational Bayes that readily scale to massive datasets. For illustrative purposes, we describe a simple variational Bayes algorithm for normalized SP mixture models, and demonstrate its usefulness with experiments on synthetic and real-world datasets.',\n",
       "  'id': '6348',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We introduce the study of fairness in multi-armed bandit problems. Our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm?s uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on ?chained? confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.',\n",
       "  'id': '6355',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider tractable representations of probability distributions and the polytime operations they support.  In particular, we consider a recently proposed arithmetic circuit representation, the Probabilistic Sentential Decision Diagram (PSDD).  We show that PSDD supports a polytime multiplication operator, while they do not support a polytime operator for summing-out variables.  A polytime multiplication operator make PSDDs suitable for a broader class of applications compared to arithmetic circuits, which do not in general support multiplication.  As one example, we show that PSDD multiplication leads to a very simple but effective compilation algorithm for probabilistic graphical models: represent each model factor as a PSDD, and then multiply them.',\n",
       "  'id': '6363',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms. For the {\\\\it detection} problem in symmetric SBMs, Decelle et al.\\\\ conjectured that the so-called Kesten-Stigum (KS) threshold can be achieved efficiently. This was proved for two communities, but remained open from three communities. We prove this conjecture here, obtaining a more general result that applies to arbitrary SBMs with linear size communities. The developed algorithm is a linearized acyclic belief propagation (ABP) algorithm, which mitigates the effects of cycles while provably achieving the KS threshold in $O(n \\\\ln n)$ time. This extends prior methods by achieving universally the KS threshold while reducing or preserving the computational complexity. ABP is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in Krzakala et al., and extending results from Bordenave et al.',\n",
       "  'id': '6365',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.',\n",
       "  'id': '6374',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.',\n",
       "  'id': '6380',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary?solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.',\n",
       "  'id': '6382',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.\",\n",
       "  'id': '6383',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of convolutional neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.',\n",
       "  'id': '6387',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure, and limited progress has been made on matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector based on sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures such as Gaussian widths of suitable sets associated with the structure of the underlying true matrix. Further, we derive general bounds on these geometric measures for structures characterized by unitarily invariant norms, a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development.',\n",
       "  'id': '6394',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL),  finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.',\n",
       "  'id': '6395',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We propose a new oracle-based algorithm, BISTRO+, for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order $O((KT)^{\\\\frac{2}{3}}(\\\\log N)^{\\\\frac{1}{3}})$, where $K$ is the number of actions, $T$ is the number of iterations, and $N$ is the number of baseline policies. Our result is the first to break the $O(T^{\\\\frac{3}{4}})$ barrier achieved by recent algorithms, which was left as a major open problem. Our analysis employs the recent relaxation framework of (Rakhlin and Sridharan, ICML'16).\",\n",
       "  'id': '6400',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Non-negative matrix factorization is a popular tool for  decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.',\n",
       "  'id': '6417',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables.  To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages.  We present applications of our method to a number of tasks including engineering design and parameter optimization.',\n",
       "  'id': '6421',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Many applications of machine learning involve structured output with large domain, where learning of structured predictor is prohibitive due to repetitive calls to expensive inference oracle. In this work, we show that, by decomposing training of Structural Support Vector Machine (SVM) into a series of multiclass SVM problems connected through messages, one can replace expensive structured oracle with Factorwise Maximization Oracle (FMO) that allows efficient implementation of complexity sublinear to the factor domain. A Greedy Direction Method of Multiplier (GDMM) algorithm is proposed to exploit sparsity of messages which guarantees $\\\\epsilon$ sub-optimality after $O(log(1/\\\\epsilon))$ passes of FMO calls. We conduct experiments on chain-structured problems and fully-connected problems of large output domains. The proposed approach is orders-of-magnitude faster than the state-of-the-art training algorithms for Structural SVM.',\n",
       "  'id': '6422',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables. Our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression. We give numerical results to illustrate these tools on simulated data and on health record data.',\n",
       "  'id': '6437',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.\",\n",
       "  'id': '6441',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model  remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.',\n",
       "  'id': '6451',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called ?Bayesian optimization? only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.',\n",
       "  'id': '6452',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly ? rather than exponentially? with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy.',\n",
       "  'id': '6453',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are positively curved. In this paper we ask whether there are other \"lucky\" settings when FTL achieves sublinear, \"small\" regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of  the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polyhedral domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL.',\n",
       "  'id': '6455',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Despite the success of kernel-based nonparametric methods, kernel selection still requires considerable expertise, and is often described as a ?black art.? We present a sophisticated method for automatically searching for an appropriate kernel from an infinite space of potential choices. Previous efforts in this direction have focused on traversing a kernel grammar, only examining the data via computation of marginal likelihood. Our proposed search method is based on Bayesian optimization in model space, where we reason about model evidence as a function to be maximized. We explicitly reason about the data distribution and how it induces similarity between potential model choices in terms of the explanations they can offer for observed data. In this light, we construct a novel kernel between models to explain a given dataset. Our method is capable of finding a model that explains a given dataset well without any human assistance, often with fewer computations of model evidence than previous approaches, a claim we demonstrate empirically.',\n",
       "  'id': '6466',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In stochastic convex optimization the goal is to minimize a convex function $F(x) \\\\doteq \\\\E_{f\\\\sim D}[f(x)]$ over a convex set $\\\\K \\\\subset \\\\R^d$ where $D$ is some unknown distribution and each $f(\\\\cdot)$ in the support of $D$ is convex over $\\\\K$. The optimization is based on i.i.d.~samples $f^1,f^2,\\\\ldots,f^n$ from $D$. A common approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \\\\doteq \\\\frac{1}{n}\\\\sum_{i\\\\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\\\\K$. We demonstrate that in the standard $\\\\ell_p/\\\\ell_q$ setting of Lipschitz-bounded functions over a $\\\\K$ of bounded radius, ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\\\\Omega(\\\\log d)$ dependence proved for $\\\\ell_2/\\\\ell_2$ setting in (Shalev-Shwartz et al.  2009). In stark contrast, these problems can be solved using dimension-independent number of samples for $\\\\ell_2/\\\\ell_2$ setting and $\\\\log d$ dependence for $\\\\ell_1/\\\\ell_\\\\infty$ setting using other approaches. We also demonstrate that for a more general class of range-bounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.',\n",
       "  'id': '6467',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference.  However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than  the recent state-of-the-art incremental solutions to variational sparse GPR.',\n",
       "  'id': '6473',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.',\n",
       "  'id': '6474',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"Matching users to the right items at the right time is a fundamental task in recommendation systems. As users interact with different items over time, users' and items' feature may evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature. To learn parameters, we design an efficient convex optimization algorithm with a novel low rank space sharing constraints. Extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.\",\n",
       "  'id': '6480',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on $\\\\R^d$ and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its $U$-statistic variant, which are usually employed in applications.',\n",
       "  'id': '6483',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We present a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition. These are the tightest margin bounds known for both standard multi-class and general structured prediction problems.  Our guarantees are expressed in terms of a data-dependent complexity measure, \\\\emph{factor graph complexity}, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets, and a sparsity measure for features and graphs. Our proof techniques include generalizations of Talagrand's contraction lemma that can be of independent interest. We further extend our theory by leveraging the principle of Voted Risk Minimization (VRM) and show that learning is possible even with complex factor graphs.  We present new learning bounds for this advanced setting, which we use to devise two new algorithms, \\\\emph{Voted Conditional Random Field} (VCRF) and \\\\emph{Voted Structured Boosting} (StructBoost). These algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees. We also report the results of experiments with VCRF on several datasets to validate our theory.\",\n",
       "  'id': '6485',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset ?Depth in the Wild? consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.',\n",
       "  'id': '6489',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned  regularizations suppress down the eigenvalues associated with localized  eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the  theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.',\n",
       "  'id': '6491',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse nonnegative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed:  more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. The algorithm enables real-time simultaneous deconvolution of $O(10^5)$ traces of whole-brain zebrafish imaging data on a laptop.',\n",
       "  'id': '6505',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [1] proved the first gap-free convergence result using the block Krylov method, Shamir [2] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [3] provided the fastest $O(\\\\mathsf{nnz}(A) + \\\\mathsf{poly}(1/\\\\varepsilon))$-time algorithm using alternating minimization.\\n\\nIn this paper, we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [1], and the first accelerated and stochastic method outperforming [2]. In the $O(\\\\mathsf{nnz}(A) + \\\\mathsf{poly}(1/\\\\varepsilon))$ running-time regime, LazySVD outperforms [3] in certain parameter regimes without even using alternating minimization.',\n",
       "  'id': '6507',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability $1-\\\\alpha$. Although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by $\\\\alpha$. In this paper, we characterize the effect of $\\\\alpha$ by establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model. For small $\\\\alpha$, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly, we also show that this gap narrows as $\\\\alpha$ increases. In other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy.',\n",
       "  'id': '6518',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right---similar to why we study the human brain---and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization, which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network. The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).',\n",
       "  'id': '6519',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In many cases of network analysis, it is more attractive to study how a network varies under  different conditions than an individual static network. We propose a novel graphical model, namely Latent Differential Graph Model, where the networks under two different conditions are represented by two semiparametric elliptical distributions respectively, and the variation of these two networks (i.e., differential graph) is characterized by the difference between their latent precision matrices. We propose an estimator for the differential graph based on quasi likelihood maximization with nonconvex regularization. We show that our estimator attains a faster statistical rate in parameter estimation than the state-of-the-art methods, and enjoys oracle property under mild conditions. Thorough experiments on both synthetic and real world data support our theory.',\n",
       "  'id': '6529',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'A key problem in structured output prediction is enabling direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient method that incorporates task reward into maximum likelihood training. We establish a connection between maximum likelihood and regularized expected reward, showing that they are approximately equivalent in the vicinity of the optimal solution. Then we show how maximum likelihood can be generalized by optimizing the conditional probability of auxiliary outputs that are sampled proportional to their exponentiated scaled rewards. We apply this framework to optimize edit distance in the output space, by sampling from edited targets. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over maximum likelihood baseline by simply sampling from target output augmentations.',\n",
       "  'id': '6547',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Asynchronous parallel optimization received substantial successes and extensive attention recently. One of core theoretical questions is how much speedup (or benefit) the asynchronous parallelization can bring to us. This paper provides a comprehensive and generic analysis to study the speedup property for a broad range of asynchronous parallel stochastic algorithms from the zeroth order to the first order methods. Our result recovers or improves existing analysis on special cases, provides more insights for understanding the asynchronous parallel behaviors, and suggests a novel asynchronous parallel zeroth order method for the first time. Our experiments provide novel applications of the proposed asynchronous parallel zeroth order method on hyper parameter tuning and model blending problems.',\n",
       "  'id': '6551',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach which models future frames in a probabilistic manner. Our proposed method is therefore able to synthesize multiple possible next frames using the same model. Solving this challenging problem involves low- and high-level image and motion understanding for successful image synthesis. Here, we propose a novel network structure, namely a Cross Convolutional Network, that encodes images as feature maps and motion information as convolutional kernels to aid in synthesizing future frames. In experiments, our model performs well on both synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold video data. We show that our model can also be applied to tasks such as visual analogy-making, and present analysis of the learned network representations.',\n",
       "  'id': '6552',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate $\\\\eta(t)$, and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights $w(t)$. Using a Lyapunov argument, we give sufficient conditions on $\\\\eta$ and $w$ to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme. We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases.',\n",
       "  'id': '6553',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.',\n",
       "  'id': '6560',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We study the problem of completing a binary matrix in an online learning setting. On each trial we predict a matrix entry and then receive the true entry. We propose a Matrix Exponentiated Gradient algorithm [1] to solve this problem. We provide a mistake bound for the algorithm, which scales with the margin complexity [2, 3] of the underlying matrix. The bound suggests an interpretation where each row of the matrix is a prediction task over a finite set of objects, the columns. Using this we show that the algorithm makes a number of mistakes which is comparable up to a logarithmic factor to the number of mistakes made by the Kernel Perceptron with an optimal kernel in hindsight. We discuss applications of the algorithm to predicting as well as the best biclustering and to the problem of predicting the labeling of a graph without knowing the graph in advance.',\n",
       "  'id': '6567',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.\",\n",
       "  'id': '6569',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in community detection under the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities. For such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on SBM parameters for exact recoverability. Our results exploit the tradeoffs among the various parameters of heterogenous SBM and provide recovery guarantees for many new interesting SBM configurations.',\n",
       "  'id': '6574',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.',\n",
       "  'id': '6594',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes S2FO2 and S2RU of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules.',\n",
       "  'id': '6603',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present Cyclades, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. Cyclades is asynchronous during model updates, and requires no memory locking mechanisms, similar to Hogwild!-type algorithms. Unlike Hogwild!, Cyclades introduces no conflicts during parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms.  Due to its inherent cache locality and conflict-free nature,  our multi-core implementation of Cyclades consistently outperforms Hogwild!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to Hogwild!, and up to 5\\\\times gains over asynchronous implementations of variance reduction algorithms.',\n",
       "  'id': '6604',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII (12.5% relative improvement) and HMDB (RGB) datasets. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.',\n",
       "  'id': '6609',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints.   In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.',\n",
       "  'id': '6611',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Synthesizing realistic profile faces is promising for more efficiently  training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively.\",\n",
       "  'id': '6612',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper studies the numerical computation of integrals, representing estimates or predictions, over the output $f(x)$ of a computational model with respect to a distribution $p(\\\\mathrm{d}x)$ over uncertain inputs $x$ to the model. For the functional cardiac models that motivate this work, neither $f$ nor $p$ possess a closed-form expression and evaluation of either requires $\\\\approx$ 100 CPU hours, precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem, with a joint model for both the a priori unknown function $f$ and the a priori unknown distribution $p$. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account, in a statistically principled manner, for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment.',\n",
       "  'id': '6616',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM).   We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits.   Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.',\n",
       "  'id': '6631',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called \"calibration function\" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.',\n",
       "  'id': '6634',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce \\'safe\\' and generic responses like \"I don\\'t know\", \"I can\\'t tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users.   Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, a RNN is augmented with a sequence of GS samplers, which coupled with the straight-through gradient estimator enables end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). The source code can be downloaded from https://github.com/jiasenlu/visDial.pytorch',\n",
       "  'id': '6635',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Convolutional neural networks (CNNs) have recently been applied to the optical flow estimation problem. As training the CNNs requires sufficiently large ground truth training data, existing approaches resort to synthetic, unrealistic datasets. On the other hand, unsupervised methods are capable of leveraging real-world videos for training where the ground truth flow fields are not available. These methods, however, rely on the fundamental assumptions of brightness constancy and spatial smoothness priors which do not hold near motion boundaries. In this paper, we propose to exploit unlabeled videos for semi-supervised learning of optical flow with a Generative Adversarial Network. Our key insight is that the adversarial loss can capture the structural patterns of flow warp errors without making explicit assumptions. Extensive experiments on benchmark datasets demonstrate that the proposed semi-supervised algorithm performs favorably against purely supervised and semi-supervised learning schemes.',\n",
       "  'id': '6639',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In this paper, we study the {\\\\em pooled data} problem of identifying the labels associated with a large collection of items, based on a sequence of pooled tests revealing the counts of each label within the pool.  In the noiseless setting, we identify an exact asymptotic threshold on the required number of tests with optimal decoding, and prove a {\\\\em phase transition} between complete success and complete failure.  In addition, we present a novel {\\\\em noisy} variation of the problem, and provide an information-theoretic framework for characterizing the required number of tests for general random noise models.  Our results reveal that noise can make the problem considerably more difficult, with strict increases in the scaling laws even at low noise levels.  Finally, we demonstrate similar behavior in an {\\\\em approximate recovery} setting, where a given number of errors is allowed in the decoded labels.',\n",
       "  'id': '6641',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process ($\\\\Gamma$P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal $\\\\Gamma$P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the $\\\\Gamma$P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed.\",\n",
       "  'id': '6643',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In this work, we introduce the average top-$k$ (\\\\atk) loss as a new ensemble loss for supervised learning. The \\\\atk loss provides a natural generalization of the two widely used ensemble losses, namely the average loss and the maximum loss. Furthermore, the \\\\atk loss combines the advantages of them and can alleviate their corresponding drawbacks to better adapt to different data distributions. We show that the \\\\atk loss affords an intuitive interpretation that reduces the penalty of continuous and convex individual losses on correctly classified data. The \\\\atk loss can lead to convex optimization problems that can be solved effectively with conventional sub-gradient based method. We further study the Statistical Learning Theory of \\\\matk by establishing its classification calibration and statistical consistency of \\\\matk which provide useful insights on the practical choice of the  parameter $k$. We demonstrate the applicability of \\\\matk learning combined with different individual loss functions for binary and multi-class classification and regression using synthetic and real datasets.',\n",
       "  'id': '6653',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the penalty is a separable sum of support functions, is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra's algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.\",\n",
       "  'id': '6655',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Recently, there has been a growing research interest in the analysis of dynamic regret, which measures the performance of an online learner against a sequence of local minimizers. By exploiting the strong convexity, previous studies have shown that the dynamic regret can be upper bounded by the path-length of the comparator sequence. In this paper, we illustrate that the dynamic regret can be further improved by allowing the learner to query the gradient of the function multiple times, and meanwhile the strong convexity can be weakened to other non-degenerate conditions. Specifically, we introduce the squared path-length, which could be much smaller than the path-length, as a new regularity of the comparator sequence. When multiple gradients are accessible to the learner, we first demonstrate that the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. We then extend our theoretical guarantee to functions that are semi-strongly convex or self-concordant. To the best of our knowledge, this is the first time that semi-strong convexity and self-concordance are utilized to tighten the dynamic regret.',\n",
       "  'id': '6675',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In this paper we introduce a natural image prior that directly represents a Gaussian-smoothed version of the natural image distribution. We include our prior in a formulation of image restoration as a Bayes estimator that also allows us to solve noise-blind image restoration problems. We show that the gradient of our prior corresponds to the mean-shift vector on the natural image distribution. In addition, we learn the mean-shift vector field using denoising autoencoders, and use it in a gradient descent approach to perform Bayes risk minimization. We demonstrate competitive results for noise-blind deblurring, super-resolution, and demosaicing.',\n",
       "  'id': '6678',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e^{-t})) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.',\n",
       "  'id': '6679',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In this paper we consider the problem of minimizing a submodular function from training data. Submodular functions can be efficiently minimized and are conse- quently heavily applied in machine learning. There are many cases, however, in which we do not know the function we aim to optimize, but rather have access to training data that is used to learn the function. In this paper we consider the question of whether submodular functions can be minimized in such cases. We show that even learnable submodular functions cannot be minimized within any non-trivial approximation when given access to polynomially-many samples. Specifically, we show that there is a class of submodular functions with range in [0, 1] such that, despite being PAC-learnable and minimizable in polynomial-time, no algorithm can obtain an approximation strictly better than 1/2 ? o(1) using polynomially-many samples drawn from any distribution. Furthermore, we show that this bound is tight using a trivial algorithm that obtains an approximation of 1/2.',\n",
       "  'id': '6683',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.',\n",
       "  'id': '6684',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.',\n",
       "  'id': '6692',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.',\n",
       "  'id': '6700',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications, e.g., neuroscience, genetics, systems biology, etc. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce the Union of Intersections (UoI) method, a flexible, modular, and scalable framework for enhanced model selection and estimation. The method performs model selection and model estimation through intersection and union operations, respectively. We show that UoI can satisfy the bi-criteria of low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm ($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as the accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the $UoI_{L1Logistic}$ and $UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields.',\n",
       "  'id': '6708',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The {\\\\L}ojasiewicz inequality shows that H\\\\\"olderian error bounds on the minimum of convex optimization problems hold almost generically. Here, we clarify results of \\\\citet{Nemi85} who show that H\\\\\"olderian error bounds directly controls the performance of restart schemes. The constants quantifying error bounds are of course unobservable, but we show that optimal restart strategies are robust, and searching for the best scheme only increases the complexity by a logarithmic factor compared to the optimal bound. Overall then, restart schemes generically accelerate accelerated methods.',\n",
       "  'id': '6712',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy.  Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training.  We showcase this method for two challenging applications: Image compression and neural network compression.  While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.',\n",
       "  'id': '6714',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We introduce a hierarchical model which allows to estimate a group-average piecewise-geodesic trajectory in the Riemannian space of measurements and individual variability. This model falls into the well defined mixed-effect models. The subject-specific trajectories are defined through spatial and temporal transformations of the group-average piecewise-geodesic path, component by component. Thus we can apply our model to a wide variety of situations. Due to the non-linearity of the model, we use the Stochastic Approximation Expectation-Maximization algorithm to estimate the model parameters. Experiments on synthetic data validate this choice. The model is then applied to the metastatic renal cancer chemotherapy monitoring: we run estimations on RECIST scores of treated patients and estimate the time they escape from the treatment. Experiments highlight the role of the different parameters on the response to treatment.',\n",
       "  'id': '6715',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We study combinatorial multi-armed bandit with probabilistically triggered arms (CMAB-T) and semi-bandit feedback. We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of 1/p*, where p* is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing a triggering probability modulated (TPM) bounded smoothness condition into the influence maximization bandit and combinatorial cascading bandit satisfy this TPM condition. As a result, we completely remove the factor of 1/p* from the regret bounds, achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally, we provide lower bound results showing that the factor 1/p* is unavoidable for general CMAB-T problems, suggesting that the TPM condition is crucial in removing this factor.',\n",
       "  'id': '6716',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\\\\tilde{O}(D\\\\sqrt{SAT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$, when $T\\\\ge S^5A$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result improves over the best previously known upper bound of $\\\\tilde{O}(DS\\\\sqrt{AT})$ achieved by any algorithm in this setting, and matches the dependence on $S$ in the established lower bound of $\\\\Omega(\\\\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.',\n",
       "  'id': '6718',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Several important applications, such as streaming PCA and semidefinite programming, involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates.  Because of storage limitations, it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nystr?m approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix.  Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.',\n",
       "  'id': '6722',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Consider a dataset where data is collected on multiple features of multiple individuals over multiple times. This type of data can be represented as a three dimensional individual/feature/time tensor and has become increasingly prominent in various areas of science. The tensor biclustering problem computes a subset of individuals and a subset of features whose signal trajectories over time lie in a low-dimensional subspace, modeling similarity among the signal trajectories while allowing different scalings across different individuals or different features. We study the information-theoretic limit of this problem under a generative model. Moreover, we propose an efficient spectral algorithm to solve the tensor biclustering problem and analyze its achievability bound in an asymptotic regime. Finally, we show the efficiency of our proposed method in several synthetic and real datasets.',\n",
       "  'id': '6730',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Adaptive stimulus selection methods in neuroscience have primarily focused on maximizing the firing rate of a single recorded neuron. When recording from a population of neurons, it is usually not possible to find a single stimulus that maximizes the firing rates of all neurons. This motivates optimizing an objective function that takes into account the responses of all recorded neurons together. We propose ?Adept,? an adaptive stimulus selection method that can optimize population objective functions. In simulations, we first confirmed that population objective functions elicited more diverse stimulus responses than single-neuron objective functions. Then, we tested Adept in a closed-loop electrophysiological experiment in which population activity was recorded from macaque V4, a cortical area known for mid-level visual processing. To predict neural responses, we used the outputs of a deep convolutional neural network model as feature embeddings. Images chosen by Adept elicited mean neural responses that were 20% larger than those for randomly-chosen natural images, and also evoked a larger diversity of neural responses. Such adaptive stimulus selection methods can facilitate experiments that involve neurons far from the sensory periphery, for which it is often unclear which stimuli to present.',\n",
       "  'id': '6738',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an unbiased estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to state-of-arts.',\n",
       "  'id': '6762',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'A new model, named as deep dynamic poisson factorization model, is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks, representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution, and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.',\n",
       "  'id': '6764',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning, in which the state of the art is unbiased PU learning. However, if its model is very flexible, empirical risks on training data will go negative, and we will suffer from serious overfitting. In this paper, we propose a non-negative risk estimator for PU learning: when getting minimized, it is more robust against overfitting, and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover, we analyze the bias, consistency, and mean-squared-error reduction of the proposed risk estimator, and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.',\n",
       "  'id': '6765',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight $\\\\ell_\\\\infty$ estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in $\\\\ell_\\\\infty$ error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model.',\n",
       "  'id': '6766',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Answering \"What if?\" questions is important in many domains. For example, would a patient\\'s disease progression slow down if I were to give them a dose of drug A? Ideally, we answer our question using an experiment, but this is not always possible (e.g., it may be unethical). As an alternative, we can use non-experimental data to learn models that make counterfactual predictions of what we would observe had we run an experiment. In this paper, we propose the counterfactual GP, a counterfactual model of continuous-time trajectories (time series) under sequences of actions taken in continuous-time. We develop our model within the potential outcomes framework of Neyman and Rubin. The counterfactual GP is trained using a joint maximum likelihood objective that adjusts for dependencies between observed actions and outcomes in the training data. We report two sets of experimental results using the counterfactual GP. The first shows that it can be used to learn the natural progression (i.e. untreated progression) of biomarker trajectories from observational data. In the second, we show how the CGP can be used for medical decision support by learning counterfactual models of renal health under different types of dialysis.',\n",
       "  'id': '6767',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to its excellent scalability properties. A fundamental barrier when parallelizing SGD is the high bandwidth cost of communicating gradient updates between nodes; consequently, several lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always guarantee convergence, and it is not clear whether they can be improved.  In this paper, we propose Quantized SGD (QSGD), a family of compression schemes for gradient updates which provides convergence guarantees. QSGD allows the user to smoothly trade off \\\\emph{communication bandwidth} and \\\\emph{convergence time}: nodes can adjust the number of bits sent per iteration, at the cost of possibly higher variance. We show that this trade-off is inherent, in the sense that improving it past some threshold would violate  information-theoretic lower bounds. QSGD guarantees convergence for convex and non-convex objectives,  under asynchrony, and can be extended to stochastic variance-reduced techniques.   When applied to  training deep neural networks for image classification and  automated speech recognition, QSGD leads to significant reductions in  end-to-end training time. For example, on 16GPUs, we can train the ResNet152  network to full accuracy on ImageNet 1.8x faster than the full-precision  variant.',\n",
       "  'id': '6768',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.',\n",
       "  'id': '6769',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.\",\n",
       "  'id': '6775',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Computational models in fields such as computational neuroscience  are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning.  Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.\",\n",
       "  'id': '6780',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types, and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community.  We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.',\n",
       "  'id': '6782',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Adversarial training has proved to be competitive against supervised learning methods on computer vision tasks. However, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing the existence of information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques. We also show that supervised training of our adversarial model produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques.',\n",
       "  'id': '6791',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Key to multitask learning is exploiting the relationships between different tasks to improve prediction performance. Most previous methods have focused on the case where tasks relations can be modeled as linear operators and regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related is often restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that our algorithm can be efficiently implemented and study its generalization properties, proving universal consistency and learning rates. Our theoretical analysis highlights the benefits of non-linear multitask learning over learning the tasks independently. Encouraging experimental results show the benefits of the proposed method in practice.',\n",
       "  'id': '6794',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form $\\\\vct{x}\\\\mapsto \\\\max(0,\\\\langle \\\\vct{w},\\\\vct{x}\\\\rangle)$ with $\\\\vct{w}\\\\in\\\\R^d$ denoting the weight vector.  We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialized at $\\\\vct{0}$, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.',\n",
       "  'id': '6796',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Technological breakthroughs allow us to collect data with increasing spatio-temporal resolution from complex interaction systems. The combination of high-resolution observations, expressive dynamic models, and efficient machine learning algorithms can lead to crucial insights into complex interaction dynamics and the functions of these systems. In this paper, we formulate the dynamics of a complex interacting network as a stochastic process driven by a sequence of events, and develop expectation propagation algorithms to make inferences from noisy observations. To avoid getting stuck at a local optimum, we formulate the problem of minimizing Bethe free energy as a constrained primal problem and take advantage of the concavity of dual problem in the feasible domain of dual variables guaranteed by duality theorem. Our expectation propagation algorithms demonstrate better performance in inferring the interaction dynamics in complex transportation networks than competing models such as particle filter, extended Kalman filter, and deep neural networks.',\n",
       "  'id': '6798',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present a data-efficient reinforcement learning method for continuous state-action systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.',\n",
       "  'id': '6799',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Neural networks have a smooth initial inductive bias, such that small changes in input do not lead to large changes in output. However, in reinforcement learning domains with sparse rewards, value functions have non-smooth structure with a characteristic asymmetric discontinuity whenever rewards arrive. We propose a mechanism that learns an interpolation between a direct value estimate and a projected value estimate computed from the encountered reward and the previous estimate. This reduces the need to learn about discontinuities, and thus improves the value function approximation. Furthermore, as the interpolation is learned and state-dependent, our method can deal with heterogeneous observability. We demonstrate that this one change leads to significant improvements on multiple Atari games, when applied to the state-of-the-art A3C algorithm.',\n",
       "  'id': '6807',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error -- TreeHist and Bitstogram. In both algorithms, server running time is $\\\\tilde O(n)$ and user running time is $\\\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $\\\\tilde O(n^{5/2})$ server time and $\\\\tilde O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for the use of such algorithms in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.\",\n",
       "  'id': '6823',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion.  To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.',\n",
       "  'id': '6831',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world data sets.',\n",
       "  'id': '6835',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space of the network, as defined in Daniely, Frostig and Singer. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two.  As corollaries, it follows that for neural networks of any depth between 2 and log(n), SGD is guaranteed to learn, in polynomial time, constant degree polynomials with polynomially bounded coefficients. Likewise, it follows  that SGD on large enough networks can learn any continuous function (not in polynomial time), complementing classical expressivity results.',\n",
       "  'id': '6836',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We study the problem of interactively learning a binary classifier using noisy labeling and pairwise comparison oracles, where the comparison oracle answers which one in the given two instances is more likely to be positive. Learning from such oracles has multiple applications where obtaining direct labels is harder but pairwise comparisons are easier, and the algorithm can leverage both types of oracles. In this paper, we attempt to characterize how the access to an easier comparison oracle helps in improving the label and total query complexity. We show that the comparison oracle reduces the learning problem to that of learning a threshold function. We then present an algorithm that interactively queries the label and comparison oracles and we characterize its query complexity under Tsybakov and adversarial noise conditions for the comparison and labeling oracles. Our lower bounds show that our label and total query complexity is almost optimal.',\n",
       "  'id': '6837',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Neural networks have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.',\n",
       "  'id': '6838',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"In this paper, we develop an algorithm that approximates the residual error of Tucker decomposition, one of the most popular tensor decomposition methods, with a provable guarantee.  Given an order-$K$ tensor $X\\\\in\\\\mathbb{R}^{N_1\\\\times\\\\cdots\\\\times N_K}$, our algorithm randomly samples a constant number $s$ of indices for each mode and creates a ``mini'' tensor $\\\\tilde{X}\\\\in\\\\mathbb{R}^{s\\\\times\\\\cdots\\\\times s}$, whose elements are given by the intersection of the sampled indices on $X$.  Then, we show that the residual error of the Tucker decomposition of $\\\\tilde{X}$ is sufficiently close to that of $X$ with high probability.  This result implies that we can figure out how much we can fit a low-rank tensor to $X$ \\\\emph{in constant time}, regardless of the size of $X$. This is useful for guessing the favorable rank of Tucker decomposition.  Finally, we demonstrate how the sampling method works quickly and accurately using multiple real datasets.\",\n",
       "  'id': '6841',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider the problem of computing a restricted nonnegative matrix factorization (NMF) of an m\\\\times n matrix X.  Specifically, we seek a factorization X\\\\approx BC, where the k columns of B are a subset of those from X and C\\\\in\\\\Re_{\\\\geq 0}^{k\\\\times n}.  Equivalently, given the matrix X, consider the problem of finding a small subset, S, of the columns of X such that the conic hull of S \\\\eps-approximates the conic hull of the columns of X, i.e., the distance of every column of X to the conic hull of the columns of S should be at most an \\\\eps-fraction of the angular diameter of X.   If k is the size of the smallest \\\\eps-approximation, then we produce an O(k/\\\\eps^{2/3}) sized O(\\\\eps^{1/3})-approximation, yielding the first provable, polynomial time \\\\eps-approximation for this class of NMF problems, where also desirably the approximation is independent of n and m. Furthermore, we prove an approximate conic Carath?odory theorem, a general sparsity result, that shows that any column of X can be \\\\eps-approximated with an O(1/\\\\eps^2) sparse combination from S.   Our results are facilitated by a reduction to the problem of approximating convex hulls,  and we prove that both the convex and conic hull variants are d-sum-hard, resolving an open problem.  Finally, we provide experimental results for the convex and conic algorithms on a variety of feature selection tasks.',\n",
       "  'id': '6847',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.',\n",
       "  'id': '6851',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider the problem of estimating multiple related functions computed by weighted  automata~(WFA). We first present a natural notion of relatedness between WFAs by considering to which extent several WFAs can share a common underlying representation. We then introduce the model of vector-valued WFA which conveniently helps us formalize this notion of relatedness. Finally, we propose a spectral learning algorithm for vector-valued WFAs to tackle the multitask learning problem. By jointly learning multiple tasks in the form of a vector-valued WFA, our algorithm enforces the discovery of a representation space shared between tasks. The benefits of the proposed multitask approach are theoretically motivated and showcased through experiments on both synthetic and real world datasets.',\n",
       "  'id': '6852',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix $Y^TY$, $Y=f(WX)$, where $W$ is a random weight matrix, $X$ is a random data matrix, and $f$ is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature methods on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.',\n",
       "  'id': '6857',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.\",\n",
       "  'id': '6860',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Kernel machines as well as neural networks possess universal function approximation properties. Nevertheless in practice their ways of choosing the appropriate function class differ. Specifically neural networks learn a representation by adapting their basis functions to the data and the task at hand, while kernel methods typically use a basis that is not adapted during training. In this work, we contrast random features of approximated kernel machines with learned features of neural networks. Our analysis reveals how these random and adaptive basis functions affect the quality of learning. Furthermore, we present basis adaptation schemes that allow for a more compact representation, while retaining the generalization properties of kernel machines.',\n",
       "  'id': '6869',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%.',\n",
       "  'id': '6871',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.\",\n",
       "  'id': '5074',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.',\n",
       "  'id': '5082',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering  datasets with high-dimensional inputs.',\n",
       "  'id': '5088',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We consider design of linear projection measurements for a vector Poisson signal model. The projections are performed on the vector Poisson rate, $X\\\\in\\\\mathbb{R}_+^n$, and the observed data are a vector of counts, $Y\\\\in\\\\mathbb{Z}_+^m$. The projection matrix is designed by maximizing mutual information between $Y$ and $X$, $I(Y;X)$. When there is a latent class label $C\\\\in\\\\{1,\\\\dots,L\\\\}$ associated with $X$, we consider the mutual information with respect to $Y$ and $C$, $I(Y;C)$. New analytic expressions for the gradient of $I(Y;X)$ and $I(Y;C)$ are presented, with gradient performed with respect to the measurement matrix. Connections are made to the more widely studied Gaussian measurement model. Example results are presented for compressive topic modeling of a document corpora (word counting), and hyperspectral compressive sensing for chemical classification (photon counting).',\n",
       "  'id': '5091',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. and Amini et al. proposed variations on the algorithm that artificially inflate the node degrees for improved statistical performance.  The current paper extends the previous theoretical results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and  provides guidance on the choice of tuning parameter.  Moreover, our results show how the star shape\" in the eigenvectors--which are consistently observed in empirical networks--can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical model that allow for highly heterogeneous degrees.  Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.  \"',\n",
       "  'id': '5099',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of \\\\emph{Preconditioned Lasso} algorithms that pre-multiply $X$ and $y$ by matrices $P_X$, $P_y$ prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter $\\\\lambda$. In this paper we propose an agnostic, theoretical framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose $\\\\lambda$. We apply our framework to three Preconditioned Lasso instances and highlight when they will outperform the Lasso. Additionally, our theory offers insights into the fragilities of these algorithms to which we provide partial solutions.',\n",
       "  'id': '5104',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Many scientific data occur as sequences of multidimensional arrays called tensors.  How can hidden, evolving trends in such data be extracted while preserving the tensor structure?  The model that is traditionally used is the linear dynamical system (LDS), which treats the observation at each time slice as a vector.  In this paper, we propose the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters.  The MLDS models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent, low-dimensional tensors.  Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets.',\n",
       "  'id': '5117',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H. Instead of assuming a generative model for H, we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i,j) of interest. We do this, for all the cells of H simultaneously, without generating realizations but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with running time equal to the time required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.',\n",
       "  'id': '5118',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we first introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner.',\n",
       "  'id': '5120',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'In stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming (DP)-based solution schemes can be applied. If the conditional expectations in the DP recursions are estimated via kernel regression, however, the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions. The resulting data-driven DP scheme is asymptotically consistent and admits efficient computational solution when combined with parametric value function approximations. If training data is sparse, however, the estimated cost-to-go functions display a high variability and an optimistic bias, while the corresponding control policies perform poorly in out-of-sample tests. To mitigate these small sample effects, we propose a robust data-driven DP scheme, which replaces the expectations in the DP recursions with worst-case expectations over a set of distributions close to the best estimate. We show that the arising min-max problems in the DP recursions reduce to tractable conic programs. We also demonstrate that this robust algorithm dominates state-of-the-art benchmark algorithms in out-of-sample tests across several application domains.',\n",
       "  'id': '5123',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'In this paper we introduce a novel method that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. Our method can be regarded as a natural extension of the one-class SVM (OCSVM) algorithm that finds multiple parallel separating hyperplanes in a reproducing kernel Hilbert space. We call our method q-OCSVM, as it can be used to estimate $q$ quantiles of a high-dimensional distribution. For this purpose, we introduce a new global convex optimization program that finds all estimated sets at once and show that it can be solved efficiently. We prove the correctness of our method and present empirical results that demonstrate its superiority over existing methods.',\n",
       "  'id': '5125',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Stochastic And-Or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events. We present a unified formalization of stochastic And-Or grammars that is agnostic to the type of the data being modeled, and propose an unsupervised approach to learning the structures as well as the parameters of such grammars. Starting from a trivial initial grammar, our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar. In our empirical evaluation, we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.',\n",
       "  'id': '5126',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also used a visual Turing test\" to show that our model produces human-like performance on other conceptual tasks, including generating new examples and parsing.\"',\n",
       "  'id': '5128',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Robust PCA methods are typically based on batch optimization and have to load all the samples into memory. This prevents them from efficiently processing big data. In this paper, we develop  an Online Robust Principal Component Analysis (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the data size,  significantly enhancing the computation and storage efficiency. The proposed method is based on stochastic optimization of an equivalent reformulation of the batch RPCA method. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust  to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the OR-PCA over online PCA and batch RPCA methods.',\n",
       "  'id': '5131',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R?nyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.',\n",
       "  'id': '5138',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"In text analysis documents are represented as disorganized bags of words, models of count features are typically based on mixing a small number of topics \\\\cite{lda,sam}. Recently, it has been observed that for many text corpora documents evolve into one another in a smooth way, with some features dropping and new ones being introduced. The counting grid \\\\cite{cgUai} models this spatial metaphor literally: it is multidimensional grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid. The major drawback of this method is that it is essentially a mixture and all the content much be generated by a single contiguous area on the grid. This may be problematic especially for lower dimensional grids. In this paper, we overcome to this issue with the \\\\emph{Componential Counting Grid} which brings the componential nature of topic models to the basic counting grid. We also introduce a generative kernel based on the document's grid usage and a visualization strategy useful for understanding large text corpora. We evaluate our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.\",\n",
       "  'id': '5140',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'In this paper, we study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with perfect information and simultaneous moves. We present a general template of MCTS algorithms for these games, which can be instantiated by various selection methods. We formally prove that if a selection method is $\\\\epsilon$-Hannan consistent in a matrix game and satisfies additional requirements on exploration, then the MCTS algorithm eventually converges to an approximate Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this claim using regret matching and Exp3 as the selection methods on randomly generated and worst case games. We confirm the formal result and show that additional MCTS variants also converge to approximate NE on the evaluated games.',\n",
       "  'id': '5145',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror-Prox algorithm, prove an extension to Holder-smooth functions, and apply the results to saddle-point type problems. Second, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of O(log T / T). This addresses a question of Daskalakis et al, 2011. Further, we consider a partial information version of the problem. We then apply the results to approximate convex programming  and show a simple algorithm for the approximate Max-Flow problem.',\n",
       "  'id': '5147',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We consider energy minimization for undirected graphical models, also known as MAP-inference problem for Markov random fields. Although combinatorial methods, which return a provably optimal integral solution of the problem, made a big progress in the past decade, they are still typically unable to cope with large-scale datasets. On the other hand, large scale datasets are typically defined on sparse graphs, and convex relaxation methods, such as linear programming relaxations often provide good approximations to integral solutions.   We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation, our method confines application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve big problems.   We demonstrate the power of our approach on a computer vision energy minimization benchmark.',\n",
       "  'id': '5159',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"Inspired by a two-level theory that unifies agenda setting and ideological  framing, we propose supervised hierarchical latent Dirichlet allocation  (SHLDA) which jointly captures documents' multi-level topic structure and  their polar response variables. Our model extends the nested Chinese restaurant  process to discover a tree-structured topic hierarchy and uses both per-topic  hierarchical and per-word lexical regression parameters to model the response  variables. Experiments in a political domain and on sentiment analysis tasks  show that SHLDA improves predictive accuracy while adding a new dimension of  insight into how topics under discussion are framed.\",\n",
       "  'id': '5163',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the K-SVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We fit the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The flexibility of the block-based representation is reflected in the variability of the recovered cell shapes.',\n",
       "  'id': '5167',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models~(LFMs). FAB inference has not been applicable to models, including LFMs, without a specific condition on the Hesqsian matrix of a complete log-likelihood, which is required to derive a factorized information criterion\\'\\'~(FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models.  FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efficiency.\"',\n",
       "  'id': '5171',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary. In real-world environments, however, such changes often occur without warning or signal. Real-world data often come from generating models that are only locally stationary. In this paper, we present LoSST, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner. We show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and significantly better when it is only locally stationary.',\n",
       "  'id': '5172',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix. Our method, named ALICE, is applicable to the elliptical family. Computationally, we develop an efficient dual inexact iterative projection (${\\\\rm D_2}$P) algorithm based on the alternating direction method of multipliers (ADMM). Theoretically, we prove that the ALICE estimator achieves the parametric rate of convergence in both parameter estimation and model selection. Moreover, ALICE calibrates regularizations when estimating each column of the inverse covariance matrix. So it not only is asymptotically tuning free, but also achieves an improved finite sample performance. We present numerical simulations to support our theory, and a real data example to illustrate the effectiveness of the proposed estimator.',\n",
       "  'id': '5173',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Most provably efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\\\\tilde{O}(\\\\tau S \\\\sqrt{AT} )$ bound on the expected regret, where $T$ is time, $\\\\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.',\n",
       "  'id': '5185',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'A long term goal of Interactive Reinforcement Learning is to incorporate non-expert human feedback to solve complex tasks. State-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy. We compare Advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.',\n",
       "  'id': '5187',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD($\\\\lambda$)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of soft-greedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward any optimal policy, except in a certain pathological case. Consequently, in the context of approximations, the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.',\n",
       "  'id': '5188',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Human eye movements provide a rich source of information into the human visual processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood. This has precluded the development of reliable dynamic eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement annotations collected under the task constraints of action and context recognition. Our dataset is unique among eyetracking datasets for still images in terms of its large scale (over 1 million fixations, 9157 images), task control and action from a single image emphasis. Second, we introduce models to automatically discover areas of interest (AOI) and introduce novel dynamic consistency metrics, based on them. Our method can automatically determine the number and spatial support of the AOIs, in addition to their locations. Based on such encodings, we show that, on unconstrained read-world stimuli, task instructions have significant influence on visual behavior. Finally, we leverage our large scale dataset in conjunction with powerful machine learning techniques and computer vision features, to introduce novel dynamic eye movement prediction methods which learn task-sensitive reward functions from eye movement data and efficiently integrate these rewards to plan future saccades based on inverse optimal control. We show that the propose methodology achieves state of the art scanpath modeling results.',\n",
       "  'id': '5196',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multi-task learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state-of-the-art.',\n",
       "  'id': '5200',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We introduce a novel formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle infinitely many tasks parameterized by a continuous parameter. Our key finding is that, for a certain class of PTL problems, the path of optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks efficiently. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression, and demonstrate the usefulness of the proposed method experimentally in these scenarios.',\n",
       "  'id': '5213',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.',\n",
       "  'id': '5222',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE, based on the whole dataset jointly. We study the statistical properties of this framework, showing that the loss of efficiency compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the full-exponential-family-ness\" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of the KL and linear combination methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.\"',\n",
       "  'id': '5229',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources. Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios. In this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data. We demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients. Our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.',\n",
       "  'id': '5236',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the noise assumption\" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.\"',\n",
       "  'id': '5237',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.',\n",
       "  'id': '5241',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees. In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping. Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of $1/\\\\sqrt{t}$ within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level $s^*$, dimension $d$ and sample size $n$. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.',\n",
       "  'id': '5252',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'State of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs. A key facet of thesestatistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however, proximal optimization methods attain only a sublinear rate. In this paper, we investigate a novel variant of strong convexity, which we call Constant Nullspace Strong Convexity (CNSC), where we require that the objective function be strongly convex only over a constant subspace. As we show, the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method, when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.',\n",
       "  'id': '5257',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\\\\mathcal{M}$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold $\\\\mathcal{M}$ in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.',\n",
       "  'id': '5260',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We focus on the problem of maximum a posteriori (MAP) inference in Markov random fields with binary variables and pairwise interactions. For this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation, followed by randomized rounding. We develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases. In practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark MAP inference tasks.',\n",
       "  'id': '5261',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite ?push-pull? fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.',\n",
       "  'id': '5262',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures the temporal dynamics, effective network connectivity in large population recordings, and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a relatively large number of idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional latent dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations. Inference scales linearly in both population size and recording duration. We apply this model to multi-channel recordings from primary visual cortex and show that it accounts for a large number of individual neural receptive fields using a small number of nonlinear inputs and a low-dimensional dynamical model.',\n",
       "  'id': '5263',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time. Recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces. However, animals and robots are commonly confronted with unbounded environments. To address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric Bayesian models. Specifically, we demonstrate that the Chinese Restaurant Process (CRP) model is able to solve this problem and that an Empirical Bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.',\n",
       "  'id': '5266',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.',\n",
       "  'id': '5270',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. We apply convolutional neural networks (CNNs) to analyze and classify EEG data recorded within a rhythm perception study in Kigali, Rwanda which comprises 12 East African and 12 Western rhythmic stimuli ? each presented in a loop for 32 seconds to 13 participants. We investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures. Using CNNs, we are able to recognize individual rhythms from the EEG with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel. Aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.',\n",
       "  'id': '5272',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P). In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.',\n",
       "  'id': '5286',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In many situations we have some measurement of confidence on ``positiveness for a binary label. The ``positiveness\" is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called \\\\emph{expectation loss SVM} (e-SVM) that is devoted to the problems where only the ``positiveness\" instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.\"',\n",
       "  'id': '5287',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.',\n",
       "  'id': '5291',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.',\n",
       "  'id': '5295',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.',\n",
       "  'id': '5298',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. (2014) is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models (Mimno et al. 2011, Newman et al. 2010) and measures of model fitness (Mimno & Blei 2011) provide strong support that explicitly modeling word dependencies---as in APM---could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because $O(p^2)$ parameters must be estimated where $p$ is the number of words (Inouye et al. could only provide results for datasets with $p = 200$). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle $p = 10^4$ as an important step towards scaling to large datasets. In addition, Inouye et al. only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word brings to mind\" another word (Boyd-Graber et al. 2006)). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies. (MATLAB code available at: http://bigdata.ices.utexas.edu/software/apm/)\"',\n",
       "  'id': '5300',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.',\n",
       "  'id': '5305',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.',\n",
       "  'id': '5312',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs. In this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.',\n",
       "  'id': '5314',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing of univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters.',\n",
       "  'id': '5315',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov?s accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov?s scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov?s scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov?s scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.',\n",
       "  'id': '5322',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.',\n",
       "  'id': '5327',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We develop a biologically-plausible learning rule that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves. It achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide both proofs of convergence, and a close fit to experimental data on STDP.',\n",
       "  'id': '5337',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.',\n",
       "  'id': '5343',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We improve a recent gurantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.',\n",
       "  'id': '5355',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Coverage functions are an important class of discrete functions that capture laws of diminishing returns. In this paper, we propose a new problem of learning time-varying coverage functions which arise naturally from applications in social network analysis, machine learning, and algorithmic game theory. We develop a novel parametrization of the time-varying coverage function by illustrating the connections with counting processes. We present an efficient algorithm to learn the parameters by maximum likelihood estimation, and provide a rigorous theoretic analysis of its sample complexity. Empirical experiments from information diffusion in social network analysis demonstrate that with few assumptions about the underlying diffusion process, our method performs significantly better than existing approaches on both synthetic and real world data.',\n",
       "  'id': '5366',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.',\n",
       "  'id': '5376',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler\\'s objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward variation\" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.\"',\n",
       "  'id': '5378',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.',\n",
       "  'id': '5381',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.',\n",
       "  'id': '5383',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.',\n",
       "  'id': '5394',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'With the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing~(DGC) frameworks has attracted much attention from big data machine learning community. In DGC frameworks, the graph partitioning~(GP) strategy plays a key role to affect the performance, including the workload balance and communication cost. Typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes GP a challenging task. Recently, many methods have been proposed to solve the GP problem. However, the existing GP methods cannot achieve satisfactory performance for applications with power-law graphs. In this paper, we propose a novel vertex-cut method, called \\\\emph{degree-based hashing}~(DBH), for GP. DBH makes effective use of the skewed degree distributions for GP. We theoretically prove that DBH can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance. Furthermore, empirical results on several large power-law graphs also show that DBH can outperform the state of the art.',\n",
       "  'id': '5396',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Can we effectively learn a nonlinear representation in time comparable to linear learning? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.',\n",
       "  'id': '5397',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.',\n",
       "  'id': '5399',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We consider the problem of recovering the sparsest vector in a subspace $ \\\\mathcal{S} \\\\in \\\\mathbb{R}^p $ with $ \\\\text{dim}(\\\\mathcal{S})=n$. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing and machine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/ \\\\sqrt{n}$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\\\Omega(1)$. To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.',\n",
       "  'id': '5402',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.',\n",
       "  'id': '5404',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l_1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.',\n",
       "  'id': '5408',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability. In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.',\n",
       "  'id': '5412',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.',\n",
       "  'id': '5419',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.\",\n",
       "  'id': '5424',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.',\n",
       "  'id': '5429',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.',\n",
       "  'id': '5431',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Large Markov Decision Processes (MDPs) are usually solved using Approximate Dynamic Programming (ADP) methods such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API). The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) $T^*Q-Q$, where $T^*$ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning (RL) problem.',\n",
       "  'id': '5443',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Tensor factorizations have become popular methods for learning from multi-relational data. In this context, the rank of a factorization is an important parameter that determines runtime as well as generalization ability. To determine conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model for learning from latent and observable patterns in multi-relational data and present a scalable algorithm for computing the factorization. Experimentally, we show that the proposed approach does not only improve the predictive performance over pure latent variable methods but that it also reduces the required rank --- and therefore runtime and memory complexity --- significantly.',\n",
       "  'id': '5448',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.',\n",
       "  'id': '5449',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We present two new methods for inference in Gaussian process (GP) models with general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a Taylor series expansion or statistical linearization. We show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented Kalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.\",\n",
       "  'id': '5455',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\\\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the $G$-optimality criterion used in optimal experimental design.',\n",
       "  'id': '5460',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We consider the problem of learning sparse additive models, i.e., functions of the form: $f(\\\\vecx) = \\\\sum_{l \\\\in S} \\\\phi_{l}(x_l)$, $\\\\vecx \\\\in \\\\matR^d$ from point queries of $f$. Here $S$ is an unknown subset of coordinate variables with $\\\\abs{S} = k \\\\ll d$. Assuming $\\\\phi_l$'s to be smooth, we propose a set of points at which to sample $f$ and an efficient randomized algorithm that recovers a \\\\textit{uniform approximation} to each unknown $\\\\phi_l$. We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise -- either arbitrary but bounded, or stochastic -- on the performance of our algorithm.\",\n",
       "  'id': '5466',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications, existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty, we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix SNM problem. Finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.',\n",
       "  'id': '5476',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.',\n",
       "  'id': '5484',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The Jacobi metric introduced in mathematical physics can be used to analyze Hamiltonian Monte Carlo (HMC). In a geometrical setting, each step of HMC corresponds to a geodesic on a Riemannian manifold with a Jacobi metric. Our calculation of the sectional curvature of this HMC manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate Gaussian. We show that positive curvature can be used to prove theoretical concentration results for HMC Markov chains.',\n",
       "  'id': '5500',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty. It finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty. However, the Bayes-adaptive solution is typically intractable in domains with large or continuous state spaces. We present a tractable method for approximating the Bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises over belief space. Our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.',\n",
       "  'id': '5501',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost, together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI). We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.',\n",
       "  'id': '5506',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than $2\\\\sqrt{2}$ times the fully supervised case. These theoretical findings are also validated through experiments.',\n",
       "  'id': '5509',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We present the Convex Polytope Machine (CPM), a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition, text topic, and web security) demonstrate that the CPM trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.',\n",
       "  'id': '5511',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble?s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.',\n",
       "  'id': '5514',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.',\n",
       "  'id': '5516',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram -- the (power) magnitude of the short-time Fourier transform (STFT) -- has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.',\n",
       "  'id': '5522',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.',\n",
       "  'id': '5546',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture --- a deep recursive neural network (deep RNN) --- constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.',\n",
       "  'id': '5551',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.',\n",
       "  'id': '5555',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.',\n",
       "  'id': '5556',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.',\n",
       "  'id': '5557',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics. In this paper, we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically, we analytically derive the leading term of the VB free energy under an asymptotic setup, and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model, which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last.FM music data.',\n",
       "  'id': '5558',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. These methods are fast and easy to use, while being reasonably accurate. A difficulty remains in computation of the lower bound when the latent dimensionality $L$ is large. Even though the lower bound is concave for many models, its computation requires optimization over $O(L^2)$ variational parameters. Efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First, it maximizes a Lagrangian of the lower bound reducing the number of parameters to $O(N)$, where $N$ is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when the bound is not concave. Second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples and computes gradient efficiently. Overall, our approach avoids all direct computations of the covariance, only requiring its linear projections. Theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.',\n",
       "  'id': '5559',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan) but no proof was known. Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).',\n",
       "  'id': '5569',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests. This new criterion aims to maximize the worst performance of agents with consideration on the overall performance. We develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy. This game-theoretic approach formulates this fairness optimization as a two-player, zero-sum game and employs an iterative algorithm for finding a Nash equilibrium, corresponding to an optimal fairness policy. We scale up this approach by exploiting problem structure and value function approximation. Our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.',\n",
       "  'id': '5588',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables in order to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.',\n",
       "  'id': '5592',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.',\n",
       "  'id': '5608',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of $O(s\\\\log d/T)$ for $s$-sparse problems in $d$ dimensions in $T$ steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish $O(1/T)$ rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.',\n",
       "  'id': '5613',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a ``brain state,'' relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.\",\n",
       "  'id': '5624',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how the latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.',\n",
       "  'id': '5625',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.\",\n",
       "  'id': '5626',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations $X$ across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels $Y$ and adjusting conditional distributions $P(X|Y)$, such that $P(X)$ can be matched across domains. However, covariate shift assumes that the support of test $P(X)$ is contained in the support of training $P(X)$, i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for $P(Y)$. Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both $X$ and $Y$ by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.',\n",
       "  'id': '5632',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is in particular challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long- term dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability of disentangling latent data factors without using object class labels.',\n",
       "  'id': '5639',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.',\n",
       "  'id': '6880',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'MicroRNAs (miRNAs) are small non-coding ribonucleic acids (RNAs) which play key roles in post-transcriptional gene regulation. Direct identification of mature miRNAs is infeasible due to their short lengths, and researchers instead aim at identifying precursor miRNAs (pre-miRNAs). Many of the known pre-miRNAs have distinctive stem-loop secondary structure, and structure-based filtering is usually the first step to predict the possibility of a given sequence being a pre-miRNA. To identify new pre-miRNAs that often have non-canonical structure, however, we need to consider additional features other than structure. To obtain such additional characteristics, existing computational methods rely on manual feature extraction, which inevitably limits the efficiency, robustness, and generalization of computational identification. To address the limitations of existing approaches, we propose a pre-miRNA identification method that incorporates (1) a deep recurrent neural network (RNN) for automated feature learning and classification, (2) multimodal architecture for seamless integration of prior knowledge (secondary structure), (3) an attention mechanism for improving long-term dependence modeling, and (4) an RNN-based class activation mapping for highlighting the learned representations that can contrast pre-miRNAs and non-pre-miRNAs. In our experiments with recent benchmarks, the proposed approach outperformed the compared state-of-the-art alternatives in terms of various performance metrics.',\n",
       "  'id': '6882',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we determine whether $X \\\\independent Y \\\\vert Z$. We approach this by converting the conditional independence test into a classification problem.  This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks.  These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution $f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X \\\\independent Y \\\\vert Z.$ -- when given access only to i.i.d.  samples from the true joint distribution $f(x,y,z)$.  To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $f^{CI}$ in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d \\\\textit{near-independent} samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.',\n",
       "  'id': '6888',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We introduce a technique for augmenting neural text-to-speech (TTS) with low-dimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-of-the-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.',\n",
       "  'id': '6889',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (?generator?) and a task solving model (?solver?). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.',\n",
       "  'id': '6892',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We study minimax strategies for the online prediction problem with expert advice. It has been conjectured that a simple adversary strategy, called COMB, is near optimal in this game for any number of experts. Our results and new insights make progress in this direction by showing that, up to a small additive term, COMB is minimax optimal in the finite-time three expert problem. In addition, we provide for this setting a new near minimax optimal COMB-based learner. Prior to this work, in this problem, learners obtaining the optimal multiplicative constant in their regret rate were known only when $K=2$ or $K\\\\rightarrow\\\\infty$. We characterize, when $K=3$, the regret of the game scaling as $\\\\sqrt{8/(9\\\\pi)T}\\\\pm \\\\log(T)^2$ which gives for the first time the optimal constant in the leading ($\\\\sqrt{T}$) term of the regret.',\n",
       "  'id': '6896',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant  in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, i.e., leveraging  the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the H\\\\\"{o}lderian error bound (HEB) condition.   {\\\\it The key technique} for our development is a novel synthesis of  {\\\\it adaptive regularization and a conditional restarting scheme}, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic, PG\\'s convergence speed is essentially $o(\\\\frac{1}{t})$, where $t$ is the total number of iterations; (ii) if the objective function consists of an $\\\\ell_1$, $\\\\ell_\\\\infty$, $\\\\ell_{1,\\\\infty}$, or huber  norm regularization and a convex smooth piecewise quadratic loss (e.g., square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a {\\\\it faster linear convergence} than PG without any other assumptions  (e.g., restricted eigen-value condition).   It is notable that  our linear convergence results for the aforementioned problems  are global instead of local.  To the best of our knowledge, these improved results are first shown in this work.',\n",
       "  'id': '6903',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We study the generalization properties of ridge regression with random features in the statistical learning  framework. We show for the first time that $O(1/\\\\sqrt{n})$ learning bounds can be achieved with only  $O(\\\\sqrt{n}\\\\log n)$  random features rather than $O({n})$  as suggested by previous results. Further,  we prove  faster learning rates and show that they might require more random features, unless they are sampled according to  a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential  effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties.',\n",
       "  'id': '6914',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.',\n",
       "  'id': '6916',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present extensive experimental and theoretical support for the efficacy of recurrent highway networks (RHNs) and recurrent hypernetworks complimentary to the original works. Where the original RHN work primarily provides theoretical treatment of the subject, we demonstrate experimentally that RHNs benefit from far better gradient flow than LSTMs in addition to their improved task accuracy. The original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved--we consider these in depth and frame several feasible solutions that we believe will yield further gains in the future. We demonstrate that these approaches are complementary: by combining RHNs and hypernetworks, we make a significant improvement over current state-of-the-art character-level language modeling performance on Penn Treebank while relying on much simpler regularization. Finally, we argue for RHNs as a drop-in replacement for LSTMs (analogous to LSTMs for vanilla RNNs) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.',\n",
       "  'id': '6919',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.',\n",
       "  'id': '6927',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented.  In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus  allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while  fulfilling  any continuous and convex constraining criterion.',\n",
       "  'id': '6929',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Predicated on the increasing abundance of electronic health records, we investigate the problem of inferring individualized treatment effects using observational data. Stemming from the potential outcomes model, we propose a novel multi-task learning framework in which factual and counterfactual outcomes are modeled as the outputs of a function in a vector-valued reproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian method for learning the treatment effects using a multi-task Gaussian process (GP) with a linear coregionalization kernel as a prior over the vvRKHS. The Bayesian approach allows us to compute individualized measures of confidence in our estimates via pointwise credible intervals, which are crucial for realizing the full potential of precision medicine. The impact of selection bias is alleviated via a risk-based empirical Bayes method for adapting the multi-task GP prior, which jointly minimizes the empirical error in factual outcomes and the uncertainty in (unobserved) counterfactual outcomes. We conduct experiments on observational datasets for an interventional social program applied to premature infants, and a left ventricular assist device applied to cardiac patients wait-listed for a heart transplant. In both experiments, we show that our method significantly outperforms the state-of-the-art.',\n",
       "  'id': '6934',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary?a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout?s discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.',\n",
       "  'id': '6949',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.',\n",
       "  'id': '6950',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a  non-linear transformation between the joint feature/label space distributions of the two domain $\\\\ps$ and $\\\\pt$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target $\\\\pt^f=(X,f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.',\n",
       "  'id': '6963',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been  particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have  encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow  architecture.   In this paper we  identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data.  To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a  significant performance  boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.',\n",
       "  'id': '6968',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Influence maximization is the problem of selecting $k$ nodes in a social network to maximize their influence spread. The problem has been extensively studied but most works focus on the submodular influence diffusion models. In this paper, motivated by empirical evidences, we explore influence maximization in the non-submodular regime. In particular, we study the general threshold model in which a fraction of nodes have non-submodular threshold functions, but their threshold functions are closely upper- and lower-bounded by some submodular functions (we call them $\\\\varepsilon$-almost submodular). We first show a strong hardness result: there is no $1/n^{\\\\gamma/c}$ approximation for influence maximization (unless P = NP) for all networks with up to $n^{\\\\gamma}$ $\\\\varepsilon$-almost submodular nodes, where $\\\\gamma$ is in (0,1) and $c$ is a parameter depending on $\\\\varepsilon$. This indicates that influence maximization is still hard to approximate even though threshold functions are close to submodular. We then provide $(1-\\\\varepsilon)^{\\\\ell}(1-1/e)$ approximation algorithms when the number of $\\\\varepsilon$-almost submodular nodes is $\\\\ell$. Finally, we conduct experiments on a number of real-world datasets, and the results demonstrate that our approximation algorithms outperform other baseline algorithms.',\n",
       "  'id': '6970',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules.  When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.',\n",
       "  'id': '6975',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.',\n",
       "  'id': '6979',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.',\n",
       "  'id': '6988',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.',\n",
       "  'id': '6997',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can  be learnt either from simple synthetic 2D datasets or  from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.',\n",
       "  'id': '7010',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'A new method for learning variational autoencoders (VAEs) is developed, based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.',\n",
       "  'id': '7011',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Predicting the future from a sequence of video frames has been recently a sought after yet challenging task in the field of computer vision and machine learning. Although there have been efforts for tracking using motion trajectories and flow features, the complex problem of generating unseen frames has not been studied extensively. In this paper, we deal with this problem using convolutional models within a multi-stage Generative Adversarial Networks (GAN) framework. The proposed method uses two stages of GANs to generate a crisp and clear set of future frames. Although GANs have been used in the past for predicting the future, none of the works consider the relation between subsequent frames in the temporal dimension. Our main contribution lies in formulating two objective functions based on the Normalized Cross Correlation (NCC) and the Pairwise Contrastive Divergence (PCD) for solving this problem. This method, coupled with the traditional L1 loss, has been experimented with three real-world video datasets, viz. Sports-1M, UCF-101 and the KITTI. Performance analysis reveals superior results over the recent state-of-the-art methods.',\n",
       "  'id': '7014',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Decentralized (PO)MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity, recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDec-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing CDec-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this, we show how a particular decomposition of the approximate action-value function over agents leads to effective updates, and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches.',\n",
       "  'id': '7019',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmarks datasets.',\n",
       "  'id': '7020',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants -- using importance values defined by the complete gradient information which changes during optimization -- enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is  (i) provably the \\\\emph{best sampling} with respect to the given bounds,  (ii) always better than uniform sampling and fixed importance sampling and  (iii) can efficiently be computed -- in many applications  at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.',\n",
       "  'id': '7025',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Non-parametric multivariate density estimation faces strong statistical and computational bottlenecks, and the more practical approaches impose near-parametric assumptions on the form of the density functions. In this paper, we leverage recent developments to propose a class of non-parametric models which have very attractive computational and statistical properties. Our approach relies on the simple function space assumption that the conditional distribution of each variable conditioned on the other variables has a non-parametric exponential family form.',\n",
       "  'id': '7031',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We address the problem of setting the kernel bandwidth, epps, used by Manifold Learning algorithms to construct the graph Laplacian. Exploiting the connection between manifold geometry, represented by the Riemannian metric, and the Laplace-Beltrami operator, we set epps by optimizing the Laplacian's ability to preserve the geometry of the data. Experiments show that this principled approach is effective and robust\",\n",
       "  'id': '7032',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).',\n",
       "  'id': '7035',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This article introduces an energy-based model that is adversarial regarding data: it minimizes the energy for a given data distribution (the positive samples) while maximizing the energy for another given data distribution (the negative or unlabeled samples). The model is especially instantiated with autoencoders where the energy, represented by the reconstruction error, provides a general distance measure for unknown data. The resulting neural network thus learns to reconstruct data from the first distribution while crushing data from the second distribution. This solution can handle different problems such as Positive and Unlabeled (PU) learning or covariate shift, especially with imbalanced data. Using autoencoders allows handling a large variety of data, such as images, text or even dialogues. Our experiments show the flexibility of the proposed approach in dealing with different types of data in different settings: images with CIFAR-10 and CIFAR-100 (not-in-training setting), text with Amazon reviews (PU learning) and dialogues with Facebook bAbI (next response classification and dialogue completion).',\n",
       "  'id': '7041',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical examples of potential correlations.',\n",
       "  'id': '7044',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We formulate a supervised learning problem, referred to as continuous ranking, where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space X and the goal is to order all possible observations x in X by means of a scoring function s : X ? R so that s(X) and Y tend to increase or decrease together with highest probability. This problem generalizes bi/multi-partite ranking to a certain extent and the task of finding optimal scoring functions s(x) can be naturally cast as optimization of a dedicated functional cri- terion, called the IROC curve here, or as maximization of the Kendall ? related to the pair (s(X), Y ). From the theoretical side, we describe the optimal elements of this problem and provide statistical guarantees for empirical Kendall ? maximiza- tion under appropriate conditions for the class of scoring function candidates. We also propose a recursive statistical learning algorithm tailored to empirical IROC curve optimization and producing a piecewise constant scoring function that is fully described by an oriented binary tree. Preliminary numerical experiments highlight the difference in nature between regression and continuous ranking and provide strong empirical evidence of the performance of empirical optimizers of the criteria proposed.',\n",
       "  'id': '7046',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Suppose, we are given a set of $n$ elements to be clustered into $k$ (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, ``do two elements $u$ and $v$ belong to the same cluster?''. The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some  function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution $f_+$ when the underlying pair of elements belong to the same cluster, and from some $f_-$ otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from $\\\\Theta(nk)$ (no similarity matrix) to $O(\\\\frac{k^2\\\\log{n}}{\\\\cH^2(f_+\\\\|f_-)})$ where $\\\\cH^2$ denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an $O(\\\\log{n})$ factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of $k, f_+$ and $f_-$, and only depend logarithmically with $n$.\",\n",
       "  'id': '7054',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.',\n",
       "  'id': '7058',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose novel first-order stochastic approximation algorithms for canonical correlation analysis (CCA). Algorithms presented are instances of inexact matrix stochastic gradient (MSG) and inexact matrix exponentiated gradient (MEG), and achieve $\\\\epsilon$-suboptimality in the population objective in $\\\\operatorname{poly}(\\\\frac{1}{\\\\epsilon})$ iterations. We also consider practical variants of the proposed algorithms and compare them with other methods for CCA both theoretically and empirically.',\n",
       "  'id': '7063',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets.\",\n",
       "  'id': '7070',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"In this paper, we propose an accelerated first-order method for geodesically convex optimization, which is the generalization of the standard Nesterov's accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular, we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems, which show that our method improves the convergence rate from O((1-\\\\mu/L)^{k}) to O((1-\\\\sqrt{\\\\mu/L})^{k}). Moreover, our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k^{2}). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments.\",\n",
       "  'id': '7072',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, with almost 60% test coverage.',\n",
       "  'id': '7073',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The inverse covariance matrix provides considerable insight for understanding statistical models in the multivariate setting.  In particular, when the distribution over variables is assumed to be multivariate normal, the sparsity pattern in the inverse covariance matrix, commonly referred to as the precision matrix, corresponds to the adjacency matrix representation of the Gauss-Markov graph, which encodes conditional independence statements between variables.  Minimax results under the spectral norm have previously been established for covariance matrices, both sparse and banded, and for sparse precision matrices.  We establish minimax estimation bounds for estimating banded precision matrices under the spectral norm. Our results greatly improve upon the existing bounds; in particular, we find that the minimax rate for estimating banded precision matrices matches that of estimating banded covariance matrices.  The key insight in our analysis is that we are able to obtain barely-noisy estimates of $k \\\\times k$ subblocks of the precision matrix by inverting slightly wider blocks of the empirical covariance matrix along the diagonal.  Our theoretical results are complemented by experiments demonstrating the sharpness of our bounds.',\n",
       "  'id': '7074',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider the problem of recovering a signal x in R^n, from magnitude-only measurements, y_i = |a_i^T x|  for i={1,2...m}.  Also known as the phase retrieval problem, it is a fundamental challenge in nano-, bio- and astronomical imaging systems, astronomical imaging, and speech processing. The problem is ill-posed, and therefore additional assumptions on the signal and/or the measurements are necessary.  In this paper, we first study the case where the underlying signal x is s-sparse. We develop a novel recovery algorithm that we call Compressive Phase Retrieval with Alternating Minimization, or CoPRAM. Our algorithm is simple and can be obtained via a natural combination of the classical alternating minimization approach for phase retrieval, with the CoSaMP algorithm for sparse recovery. Despite its simplicity, we prove that our algorithm achieves a sample complexity of O(s^2 log n) with Gaussian samples, which matches the best known existing results. It also demonstrates linear convergence in theory and practice and requires no extra tuning parameters other than the signal sparsity level s.  We then consider the case where the underlying signal x arises from to structured sparsity models. We specifically examine the case of block-sparse signals with uniform block size of b and block sparsity k=s/b. For this problem, we design a recovery algorithm that we call Block CoPRAM that further reduces the sample complexity to O(ks log n). For sufficiently large block lengths of b=Theta(s), this bound equates to O(s log n). To our knowledge, this constitutes the first end-to-end linearly convergent algorithm for phase retrieval where the Gaussian sample complexity has a sub-quadratic dependence on the sparsity level of the signal.',\n",
       "  'id': '7077',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm.  We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games.',\n",
       "  'id': '7083',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose a generalized Gibbs sampler algorithm for obtaining samples approximately distributed from a high-dimensional Gaussian distribution. Similarly to Hogwild methods, our approach does not target the original Gaussian distribution of interest, but an approximation to it. Contrary to Hogwild methods, a single parameter allows us to trade bias for variance. We show empirically that our method is very flexible and performs well compared to Hogwild-type algorithms.',\n",
       "  'id': '7087',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We study the question of fair clustering under the {\\\\em disparate impact} doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions---for instance a point may no longer be assigned to its nearest cluster center!  En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective.  We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms.  While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow.  We empirically demonstrate the \\\\emph{price of fairness} by quantifying the value of fair clustering on real-world datasets with sensitive attributes.',\n",
       "  'id': '7088',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Most online optimization algorithms focus on one of two things: performing well in adversarial settings by adapting to unknown data parameters (such as Lipschitz constants), typically achieving $O(\\\\sqrt{T})$ regret, or performing well in stochastic settings where they can leverage some structure in the losses (such as strong convexity), typically achieving $O(\\\\log(T))$ regret. Algorithms that focus on the former problem hitherto achieved $O(\\\\sqrt{T})$ in the stochastic setting rather than $O(\\\\log(T))$. Here we introduce an online optimization algorithm that achieves $O(\\\\log^4(T))$ regret in a wide class of stochastic settings while gracefully degrading to the optimal $O(\\\\sqrt{T})$ regret in adversarial settings (up to logarithmic factors). Our algorithm does not require any prior knowledge about the data or tuning of parameters to achieve superior performance.',\n",
       "  'id': '7091',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Robots will eventually be part of every household.  It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. A descriptive sentence can provide a stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.  We propose a phrase-based captioning model trained with policy gradients, and design a critic that provides reward to the learner by conditioning on the human-provided feedback. We show  that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.',\n",
       "  'id': '7092',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.',\n",
       "  'id': '7116',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Aggregative games provide a rich abstraction to model strategic multi-agent interactions. We focus on learning local aggregative games, where the payoff of each player is a function of its own action and the aggregate behavior of its neighbors in a connected digraph. We show the existence of a pure strategy epsilon-Nash equilibrium in such games when the payoff functions are convex or sub-modular. We prove an information theoretic lower bound, in a value oracle model, on approximating the structure of the digraph with non-negative monotone sub-modular cost functions on the edge set cardinality. We also introduce gamma-aggregative games that generalize local aggregative games, and admit epsilon-Nash equilibrium that are stable with respect to small changes in some specified graph property. Moreover, we provide estimation algorithms for the game theoretic model that can meaningfully recover the underlying structure and payoff functions from real voting data.',\n",
       "  'id': '7118',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper presents a probabilistic-graphical model that can be used to infer characteristics of instantaneous brain activity by jointly analyzing spatial and temporal dependencies observed in electroencephalograms (EEG). Specifically, we describe a factor-graph-based model with customized factor-functions defined based on domain knowledge, to infer pathologic brain activity with the goal of identifying seizure-generating brain regions in epilepsy patients. We utilize an inference technique based on the graph-cut algorithm to exactly solve graph inference in polynomial time. We validate the model by using clinically collected intracranial EEG data from 29 epilepsy patients to show that the model correctly identifies seizure-generating brain regions. Our results indicate that our model outperforms two conventional approaches used for seizure-onset localization (5-7% better AUC: 0.72, 0.67, 0.65) and that the proposed inference technique provides 3-10% gain in AUC (0.72, 0.62, 0.69) compared to sampling-based alternatives.',\n",
       "  'id': '7121',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.',\n",
       "  'id': '7122',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.',\n",
       "  'id': '7133',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a \"two-player game\" between a generator and a discriminator.  Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence.   In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.',\n",
       "  'id': '7138',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its saddle point formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this ?dualing GAN? act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.',\n",
       "  'id': '7144',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.',\n",
       "  'id': '7151',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects.   In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a trained environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several strong baselines.',\n",
       "  'id': '7152',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both  multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales  and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character based language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where  we improve state of the art results to  1.19 and 1.25 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on  Hutter Prize Wikipedia outperforming  the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture,  and thus can be flexibly applied to different tasks.',\n",
       "  'id': '7173',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Hiererachical clustering, that is computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is a fundamental problem in data analysis. Although hierarchical clustering has mostly been studied through procedures such as linkage algorithms, or top-down heuristics, rather than as optimization problems, recently Dasgupta [1] proposed an objective function for hierarchical clustering and initiated a line of work developing algorithms that explicitly optimize an objective (see also [2, 3, 4]). In this paper, we consider a fairly general random graph model for hierarchical clustering, called the hierarchical stochastic blockmodel (HSBM), and show that in certain regimes the SVD approach of McSherry [5] combined with specific linkage methods results in a clustering that give an O(1)-approximation to Dasgupta?s cost function. We also show that an approach based on SDP relaxations for balanced cuts based on the work of Makarychev et al. [6], combined with the recursive sparsest cut algorithm of Dasgupta, yields an O(1) approximation in slightly larger regimes and also in the semi-random setting, where an adversary may remove edges from the random graph generated according to an HSBM. Finally, we report empirical evaluation on synthetic and real-world  data showing that our proposed SVD-based method does indeed achieve a better cost than other widely-used heurstics and also results in a better classification accuracy when the underlying problem was that of multi-class classification.',\n",
       "  'id': '7200',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.',\n",
       "  'id': '7203',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized \"spectral complexity\": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the MNIST and CIFAR10 datasets, with both original and random labels;  the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.',\n",
       "  'id': '7204',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Machine learning is now being used to make crucial decisions about people\\'s lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal \"world\" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.',\n",
       "  'id': '7220',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We introduce a solid harmonic wavelet scattering representation, invariant  to rigid motion and stable to deformations, for regression and classification  of 2D and 3D signals. Solid harmonic wavelets are computed by multiplying solid  harmonic functions with Gaussian windows dilated at different scales. Invariant  scattering coefficients are obtained by cascading such wavelet transforms with  the complex modulus nonlinearity. We study an application of solid harmonic  scattering invariants to the estimation of quantum molecular energies, which  are also invariant to rigid motion and stable with respect to deformations. A multilinear regression  over scattering invariants provides close to state of the art results over  small and large databases of organic molecules.',\n",
       "  'id': '7232',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider the Frank-Wolfe (FW) method for constrained convex optimization, and we show that this classical technique can be interpreted from a different perspective: FW emerges as the computation of an equilibrium (saddle point) of a special convex-concave zero sum game. This saddle-point trick relies on the existence of no-regret online learning to both generate a sequence of iterates but also to provide a proof of convergence through vanishing regret. We show that our stated equivalence has several nice properties, as it exhibits a modularity that gives rise to various old and new algorithms. We explore a few such resulting methods, and provide experimental results to demonstrate correctness and efficiency.',\n",
       "  'id': '7236',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of $N$ points in $d$ dimensions generated by an unknown mixture of $k$ spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians  accurately if they are sufficiently separated. Assuming each pair of centers are $C\\\\sigma$ distant with $C=\\\\Omega((k\\\\log k)^{1/4}\\\\sigma)$ and where $\\\\sigma^2$ is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians \\\\citep{vempalawang}. For finite samples, we show that a bias term based on the initial estimate decreases at $O(1/{\\\\rm poly}(N))$ rate while variance decreases at nearly optimal rate of $\\\\sigma^2 d/N$. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal $d\\\\cdot k$ while space complexity of our algorithm is $O(dk\\\\log k)$.  In addition to the bias and variance terms which tend to $0$, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an \\\\emph{approximation error} that cannot be avoided. However, by using a streaming version of the classical \\\\emph{(soft-thresholding-based)} EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to $0$ for $N\\\\rightarrow \\\\infty$.\",\n",
       "  'id': '7238',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network.  Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities.  We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects.  We demonstrate that the learned representations are useful for next-step prediction.',\n",
       "  'id': '7246',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.',\n",
       "  'id': '7247',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortised variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.',\n",
       "  'id': '7248',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we  want. When designing the reward, we might think of some specific scenarios (driving on clean roads), and make sure that the reward will lead to the right behavior in \\\\emph{those} scenarios. Inevitably, agents encounter \\\\emph{new} scenarios (snowy roads), and optimizing the reward can lead to undesired behavior (driving too fast). Our insight in this work is that reward functions are merely \\\\emph{observations} about what the designer \\\\emph{actually} wants, and that they should be interpreted in the context in which they were designed. We introduce \\\\emph{Inverse Reward Design} (IRD) as the problem of inferring the true reward based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach takes a step towards alleviating negative side effects and preventing reward hacking.\",\n",
       "  'id': '7253',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.',\n",
       "  'id': '7259',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Our goal is to extract meaningful transformations from raw images, such as varying the thickness of lines in handwriting or the lighting in a portrait. We propose an unsupervised approach to learn such transformations by attempting to reconstruct an image from a linear combination of transformations of its nearest neighbors.  On handwritten digits and celebrity portraits, we show that even with linear transformations, our method generates visually high-quality modified images.  Moreover, since our method is semiparametric and does not model the data distribution, the learned transformations extrapolate off the training data and can be applied to new types of images.',\n",
       "  'id': '7263',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Matrix factorization (MF) is one of the most popular techniques for product recommendation, but is known to suffer from serious cold-start problems. Item cold-start problems are particularly acute in settings such as Tweet recommendation where new items arrive continuously. In this paper, we present a meta-learning strategy to address item cold-start when new items arrive continuously. We propose two deep neural network architectures that implement our meta-learning strategy. The first architecture learns a linear classifier whose weights are determined by the item history while the second architecture learns a neural network whose biases are instead adjusted. We evaluate our techniques on the real-world problem of Tweet recommendation. On production data at Twitter, we demonstrate that our proposed techniques significantly beat the MF baseline and also outperform production models for Tweet recommendation.',\n",
       "  'id': '7266',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.',\n",
       "  'id': '7268',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider the problem of learning causal structures with latent variables using interventions. Our objective is not only to learn the causal graph between the observed variables, but to locate unobserved variables that could confound the relationship between observables. Our approach is stage-wise: We first learn the observable graph, i.e., the induced graph between observable variables. Next we learn the existence and location of the latent variables given the observable graph. We propose an efficient randomized algorithm that can learn the observable graph using O(d\\\\log^2 n) interventions where d is the degree of the graph. We further propose an efficient deterministic variant which uses O(log n + l) interventions, where l is the longest directed path in the graph. Next, we propose an algorithm that uses only O(d^2 log n) interventions that can learn the latents between both non-adjacent and adjacent variables. While a naive baseline approach would require O(n^2) interventions, our combined algorithm can learn the causal graph with latents using O(d log^2 n + d^2 log (n)) interventions.',\n",
       "  'id': '7277',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences.',\n",
       "  'id': '4621',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by \\\\emph{multiple} latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden.  This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (\\\\emph{i.e.}, third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on $k \\\\times k$ matrices, where $k$ is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.',\n",
       "  'id': '4637',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"We study the average case performance of multi-task Gaussian process (GP)   regression as captured in the learning curve, i.e.\\\\ the average Bayes error   for a chosen task versus the total number of examples $n$ for all   tasks. For GP covariances that are the product of an   input-dependent covariance function and a free-form inter-task   covariance matrix, we   show that accurate approximations for the learning curve can be   obtained for an arbitrary number of tasks $T$.  We use   these to study the asymptotic learning behaviour for large   $n$. Surprisingly, multi-task learning can be asymptotically essentially   useless: examples from other tasks only help when the   degree of inter-task correlation, $\\\\rho$, is near its maximal value   $\\\\rho=1$. This effect is most extreme for learning of smooth target   functions as described by e.g.\\\\ squared exponential kernels. We also   demonstrate that when learning {\\\\em many} tasks, the learning curves   separate into an initial phase, where the Bayes error on each task   is reduced down to a plateau value by ``collective learning''    even though most tasks have not seen examples,   and a final decay that occurs only once the number of examples is   proportional to the number of tasks.\",\n",
       "  'id': '4639',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as Rmax base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.\",\n",
       "  'id': '4642',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.',\n",
       "  'id': '4651',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.',\n",
       "  'id': '4653',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'This paper explores unsupervised learning of parsing models along two directions.  First, which models are identifiable from infinite data?  We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models.  Second, for identifiable models, how do we estimate the parameters efficiently?  EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.  We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.',\n",
       "  'id': '4655',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Probabilistic latent variable models are one of the cornerstones of machine learning.  They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.  Such models are useful for exploratory analysis and visualization, for building density models of data, and for   providing features that can be used for later discriminative tasks. A significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions   on internal parameters.  For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.  In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the   underlying i.i.d.\\\\ prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.  Using a kernel between probability distributions, we are able to define a DPP on probability measures.  We show how to perform MAP inference   with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model.',\n",
       "  'id': '4660',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.',\n",
       "  'id': '4664',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the  gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.',\n",
       "  'id': '4677',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We present an approach to detecting and analyzing the 3D configuration of objects in real-world images with heavy occlusion and clutter. We focus on the application of finding and analyzing cars. We do so with a two-stage model; the first stage reasons about 2D shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then refined by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset.',\n",
       "  'id': '4680',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck. To make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed. In the case of the retina, Srinivasan et al. (1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields. However, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear. Can such circuits implement predictive coding as well? Here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized Bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit. In response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction. This analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics.',\n",
       "  'id': '4681',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity.',\n",
       "  'id': '4682',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Recent work in unsupervised feature learning and deep learning has shown that  being able to train large models can dramatically improve performance.  In this  paper, we consider the problem of training a deep network with billions of  parameters using tens of thousands of CPU cores.  We have developed a  software framework called DistBelief that can utilize computing clusters  with thousands of machines to train large models.  Within this framework, we  have developed two algorithms for large-scale distributed training: (i) Downpour  SGD, an asynchronous stochastic gradient descent procedure supporting a  large number of model replicas, and (ii) Sandblaster, a framework that supports  for a variety of distributed batch optimization procedures, including a distributed  implementation of L-BFGS.  Downpour SGD and Sandblaster L-BFGS both  increase the scale and speed of deep network training.  We have successfully  used our system to train a deep network 100x larger than previously reported in  the literature, and achieves state-of-the-art performance on ImageNet, a visual  object recognition task with 16 million images and 21k categories.  We show that  these same techniques dramatically accelerate the training of a more modestly  sized deep network for a commercial speech recognition service. Although we  focus on and report performance of these methods as applied to training large  neural networks, the underlying algorithms are applicable to any gradient-based  machine learning algorithm.',\n",
       "  'id': '4687',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'This paper proposes a novel image representation called a Graphical Gaussian Vector, which is a counterpart of the codebook and local feature matching approaches. In our method, we model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efficiently represent the spatial relationship among local features. We consider the parameter of GMRF as a feature vector of the image. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers. Our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. As the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency.',\n",
       "  'id': '4708',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'There is no generally accepted way to define wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of O(n^p) complexity with small p for sparse signals (in contrast to the O(n^q n!) complexity typical of FFTs). We discuss potential applications in ranking, sparse approximation, and multi-object tracking.',\n",
       "  'id': '4720',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications. The domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects.',\n",
       "  'id': '4721',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"The interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback.\",\n",
       "  'id': '4722',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability. We introduce a novel message-passing algorithm called Density Propagation (DP) for estimating this function. We show that DP is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms. Further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function. For any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds. We conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.',\n",
       "  'id': '4723',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers. We formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error. We train locally linear classifiers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also show improved robustness to label noise.',\n",
       "  'id': '4725',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the $N$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.  We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\\\\order(N^{-1}+(N/m)^{-2})$. Whenever $m \\\\le \\\\sqrt{N}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $N$ samples.  The second algorithm is a novel method, based on an appropriate form of the bootstrap.  Requiring only a single round of communication, it has mean-squared error that decays as $\\\\order(N^{-1}+(N/m)^{-3})$, and so is more robust to the amount of parallelization. We complement our theoretical results with experiments on large-scale problems from the Microsoft Learning to Rank dataset.',\n",
       "  'id': '4728',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures  depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment {\\\\em biological} neuron membranes, we use a special type of deep {\\\\em artificial} neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a $512 \\\\times 512 \\\\times 30$ stack with known ground truth, and tested on a stack of the same  size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge.  Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \\\\emph{rand error}, \\\\emph{warping error} and \\\\emph{pixel error}.  For pixel error, our approach is the only one outperforming a second human observer.',\n",
       "  'id': '4741',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters. These filters cannot be found using spike-triggered averaging (STA), which estimates only a single filter. Other methods, like spike-triggered covariance (STC), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters. Rather, they provide a linear basis for the subspace in which the filters reside. Here, we define a \\'subunit\\' model as an LN-LN cascade, in which the first linear stage is restricted to a set of shifted (\"convolutional\") copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial LN elements as the \\'subunits\\' of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data. The method performs well for both simulated and real data (from primate V1), and the resulting model outperforms STA and STC in terms of both cross-validated accuracy and efficiency.',\n",
       "  'id': '4742',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities. While there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations.',\n",
       "  'id': '4744',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We propose a simple and novel framework for MCMC inference in continuous-time discrete-state systems with pure jump trajectories. We construct an exact MCMC sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.  The first step can be performed efficiently using properties of the Poisson process, while the second step can avail of discrete-time MCMC techniques based on the forward-backward algorithm. We compare our approach to particle MCMC and a uniformization-based sampler, and show its advantages.',\n",
       "  'id': '4746',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals. Because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (LGN) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons. We propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals. The stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> LGN) and its filter weights can be learned using Hebbian rules in a stage-wise sequential manner. Moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in LGN neurons and fruit fly second-order visual neurons. Therefore, the lattice filter model is a useful abstraction that may help unravel visual system function.',\n",
       "  'id': '4759',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph.',\n",
       "  'id': '4768',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Unsupervised joint alignment of images has been demonstrated to   improve performance on recognition tasks such as face verification.   Such alignment reduces undesired variability due to factors such as   pose, while only requiring weak supervision in the form of poorly   aligned examples.  However, prior work on unsupervised alignment of   complex, real world images has required the careful selection of   feature representation based on hand-crafted image descriptors, in   order to achieve an appropriate, smooth optimization landscape.    In this paper, we instead propose a novel combination of   unsupervised joint alignment with unsupervised feature learning.   Specifically, we incorporate deep learning into the {\\\\em congealing}   alignment framework.  Through deep learning, we obtain features that   can represent the image at differing resolutions based on network   depth, and that are tuned to the statistics of the specific data   being aligned.  In addition, we modify the learning algorithm for   the restricted Boltzmann machine by incorporating a group sparsity   penalty, leading to a topographic organization on the learned   filters and improving subsequent alignment results.    We apply our method to the Labeled Faces in the Wild database   (LFW). Using the aligned images produced by our proposed   unsupervised algorithm, we achieve a significantly higher accuracy   in face verification than obtained using the original face images,   prior work in unsupervised alignment, and prior work in supervised   alignment.  We also match the accuracy for the best available, but   unpublished method.',\n",
       "  'id': '4769',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy. The first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time. The coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.  When fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events. The model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task.',\n",
       "  'id': '4780',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as a ''viewer'' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of the distances between the viewer and the cluster members. Two important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.\",\n",
       "  'id': '4795',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"Linear chains and trees are basic building blocks in many applications of graphical models.  Although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.  Standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.  For this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.  This paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.  Improving worst-case performance is impossible. However, our method substantially speeds real-world, typical-case inference in chains and trees.  Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.  Our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning.\",\n",
       "  'id': '4819',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We present a new learning strategy based on an efficient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models.',\n",
       "  'id': '4832',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random fields (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.',\n",
       "  'id': '4838',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\\\\y$ can be modeled with a prior model $p(\\\\y)$ and the relations between data and target variables are estimated through $p(\\\\y)$ and a set of uncorresponded data $\\\\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\\\\t$ that maximizes the log likelihood of $f_\\\\t(\\\\x)$ on a uncorresponded training set with regards to $p(\\\\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video.',\n",
       "  'id': '4849',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We present a novel marginalized particle Gaussian process (MPGP) regression, which provides a fast, accurate online Bayesian filtering framework to model the latent function. Using a state space model established by the data construction procedure, our MPGP recursively filters out the estimation of hidden function values by a Gaussian mixture. Meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles. We demonstrate the estimated performance of our MPGP on both simulated and real large data sets. The results show that our MPGP is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse GP methods.',\n",
       "  'id': '4850',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of $k$-anonymity to the $b$-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results confirm improved utility on benchmark and social data-sets.',\n",
       "  'id': '4858',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Finite-State Transducers (FST) are a standard tool for modeling paired input-output sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. presented a spectral algorithm for learning FST from samples of aligned input-output sequences.  In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as finding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identifiability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently.',\n",
       "  'id': '4862',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Sparse Subspace Clustering (SSC) and Low-Rank Representation (LRR) are both considered as the state-of-the-art methods for {\\\\em subspace clustering}. The two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of Self-Expressiveness\\'\\'. The main difference is that SSC minimizes the vector $\\\\ell_1$ norm of the representation matrix to induce sparsity while LRR minimizes nuclear norm (aka trace norm) to promote a low-rank structure. Because the representation matrix is often simultaneously sparse and low-rank,  we propose a new algorithm, termed Low-Rank Sparse Subspace Clustering (LRSSC), by combining SSC and LRR, and develops theoretical guarantees of when the algorithm succeeds. The results reveal interesting insights into the strength and weakness of SSC and LRR and demonstrate how LRSSC can take the advantages of both methods in preserving the \"Self-Expressiveness Property\\'\\' and \"Graph Connectivity\\'\\' at the same time.\"',\n",
       "  'id': '4865',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on first passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function.',\n",
       "  'id': '4872',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"Shannon's entropy is a basic quantity in information theory, and a  fundamental building block for the analysis of neural codes.   Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable   attention in statistics and theoretical neuroscience.  However,  neural responses have characteristic statistical structure that   generic entropy estimators fail to exploit.  For example, existing  Bayesian entropy estimators make the naive assumption that all spike   words are equally likely a priori, which makes for an  inefficient allocation of prior probability mass in cases where   spikes are sparse.  Here we develop Bayesian estimators for the  entropy of binary spike trains using priors designed to flexibly   exploit the statistical structure of simultaneously-recorded spike  responses.  We define two prior distributions over spike words using   mixtures of Dirichlet distributions centered on simple parametric  models.  The parametric model captures high-level statistical   features of the data, such as the average spike count in a spike  word, which allows the posterior over entropy to concentrate more   rapidly than with standard estimators (e.g., in cases where the  probability of spiking differs strongly from 0.5). Conversely, the   Dirichlet distributions assign prior mass to distributions far from  the parametric model, ensuring consistent estimates for arbitrary   distributions.  We devise a compact representation of the data and  prior that allow for computationally efficient implementations of   Bayesian least squares and empirical Bayes entropy estimators with  large numbers of neurons.  We apply these estimators to simulated   and real neural data and show that they substantially outperform  traditional methods.\",\n",
       "  'id': '4873',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out\\'\\' neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.\"',\n",
       "  'id': '4878',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Distributions over exchangeable matrices with infinitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.',\n",
       "  'id': '4884',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We propose an approximate inference algorithm for continuous time Gaussian-Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for the discrete time terms and (2) variational updates for the continuous time term. We introduce corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.',\n",
       "  'id': '4885',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are  illustrated by simulated examples.',\n",
       "  'id': '4886',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes.\",\n",
       "  'id': '4894',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We establish theoretical results concerning all local optima of various regularized M-estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss function satisfies restricted strong convexity and the penalty function satisfies suitable regularity conditions, any local optimum of the composite objective function lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models; regression in generalized linear models using nonconvex regularizers such as SCAD and MCP; and graph and inverse covariance matrix estimation. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision epsilon in log(1/epsilon) iterations, which is the fastest possible rate of any first-order method. We provide a variety of simulations to illustrate the sharpness of our theoretical predictions.',\n",
       "  'id': '4904',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required cannot be directly extended except in a few restricted cases. In this paper, we present efficient approximate DPP sampling schemes based on Nystrom and random Fourier feature approximations that apply to a wide range of kernel functions. We demonstrate the utility of continuous DPPs in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.',\n",
       "  'id': '4916',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We present a new online learning algorithm that extends the exponentiated gradient to infinite dimensional spaces. Our analysis shows that the algorithm is implicitly able to estimate the $L_2$ norm of the unknown competitor, $U$, achieving a regret bound of the order of $O(U \\\\log (U T+1))\\\\sqrt{T})$, instead of the standard $O((U^2 +1) \\\\sqrt{T})$, achievable without knowing $U$. For this analysis, we introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness. Through a lower bound, we also show that the algorithm is optimal up to $\\\\sqrt{\\\\log T}$ term for linear and Lipschitz losses.',\n",
       "  'id': '4920',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency implies bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage --orthogonal complement shrinkage-- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.',\n",
       "  'id': '4921',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20,000 variables. In this paper, we develop an algorithm BigQUIC, which can solve 1 million dimensional l1-regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of specific components. In spite of these modifications,  we are able to theoretically analyze our procedure and show that BigQUIC can achieve super-linear or even quadratic convergence rates.',\n",
       "  'id': '4923',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Multiple hypothesis testing is a significant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while being simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non parametric method of estimating the FWER for a given ? threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we observe that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub?sampled ? on the order of 0.5% ? matrix completion methods. Thus, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated FWER. Our valuations on four different neuroimaging datasets show that a computational speedup factor of roughly 50? can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated ? threshold is also recovered faithfully, and is stable.',\n",
       "  'id': '4924',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the  information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of  non-totally randomized trees such as Random Forests and Extra-Trees.',\n",
       "  'id': '4928',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"In the high-dimensional regression model a response variable is linearly related to $p$ covariates,  but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active'  (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying  the active covariates.  A popular approach is to estimate the regression coefficients through the Lasso ($\\\\ell_1$-regularized least squares).  This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set.   We formulate `generalized irrepresentability condition' (GIC), an assumption that is substantially weaker than  irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.\",\n",
       "  'id': '4930',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'It is a common practice to approximate complicated\\'\\' functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends  the linearity of the proximal map. The new approximation is justified using a recent convex analysis tool---proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims.\"',\n",
       "  'id': '4934',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difficulties that entail sophisticated algorithms. Our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso.',\n",
       "  'id': '4935',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].',\n",
       "  'id': '4938',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'In this paper, we are interested in the development of efficient algorithms  for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information.   We  cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds.  We first examine  a two stages exploration-exploitation based algorithm which first approximates  the stochastic objectives by sampling  and  then solves a constrained stochastic optimization problem by projected gradient method. This method  attains a  suboptimal  convergence rate  even under  strong assumption on the objectives. Our second approach is an efficient primal-dual stochastic algorithm. It leverages on the theory of Lagrangian method in constrained optimization and attains  the optimal convergence rate of $[O(1/ \\\\sqrt{T})]$ in high probability for general Lipschitz continuous objectives.',\n",
       "  'id': '4942',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem.  It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional.  We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space.  A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner.  The algorithm scales efficiently to approximately one million features.  State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.',\n",
       "  'id': '4944',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Max-product ?belief propagation? (BP) is a popular distributed heuristic for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution ? namely, given a tight LP, can we design a ?good? BP algorithm.  In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most significant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efficient BP-based heuristic for the MWM problem, which consists of making sequential, ?cutting plane?, modifications to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems.',\n",
       "  'id': '4949',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to find the function?s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploit all the x values tried and all the f(x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further confirm that Gaussian Processes provide a general and unified theoretical account to explain passive and active function learning and search in humans.',\n",
       "  'id': '4952',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.',\n",
       "  'id': '4958',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"In large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters. In this situation, it is beneficial to group the parameters for more efficient learning. We show that even when the grouping is unknown, we can infer these parameter groups during learning via a Bayesian approach. We impose a Dirichlet process prior on the parameters. Posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a Metropolis-Hastings algorithm with auxiliary variables and a Gibbs sampling algorithm with stripped Beta approximation (Gibbs_SBA). Simulations show that both algorithms outperform conventional maximum likelihood estimation (MLE). Gibbs_SBA's performance is close to Gibbs sampling with exact likelihood calculation. Models learned with Gibbs_SBA also generalize better than the models learned by MLE on real-world Senate voting data.\",\n",
       "  'id': '4961',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal $\\\\gamma$-discounted optimal policy. We consider two variations of PI: Howard's PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard's PI terminates after at most  $ O \\\\left( \\\\frac{ n m}{1-\\\\gamma} \\\\log \\\\left( \\\\frac{1}{1-\\\\gamma} \\\\right)\\\\right) $ iterations, improving by a factor $O(\\\\log n)$ a result by Hansen et al. (2013), while Simplex-PI terminates after at most $ O \\\\left(  \\\\frac{n^2 m}{1-\\\\gamma} \\\\log \\\\left( \\\\frac{1}{1-\\\\gamma} \\\\right)\\\\right) $ iterations, improving by a factor $O(\\\\log n)$ a result by Ye (2011). Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor~$\\\\gamma$: given a measure of the maximal transient time $\\\\tau_t$ and the maximal time $\\\\tau_r$ to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most $ \\\\tilde O \\\\left( n^3 m^2 \\\\tau_t \\\\tau_r \\\\right) $ iterations. This generalizes a recent result for deterministic MDPs by Post & Ye (2012), in which $\\\\tau_t \\\\le n$ and $\\\\tau_r \\\\le n$. We explain why similar results seem hard to derive for Howard's PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that Simplex-PI and Howard's PI terminate after at most  $ \\\\tilde O(nm (\\\\tau_t+\\\\tau_r))$ iterations.\",\n",
       "  'id': '4971',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves $O(\\\\sqrt{T\\\\log|\\\\Pi|}+\\\\log|\\\\Pi|)$ regret with respect to a comparison set of policies $\\\\Pi$.  The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efficiently and the comparison set $\\\\Pi$ has polynomial size, this algorithm is efficient.  We also consider the episodic adversarial online shortest path problem.  Here, in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a fixed policy for selecting paths. This problem is a special case of the online MDP problem. For randomly chosen graphs and adversarial losses, this problem can be efficiently solved. We show that it also can be efficiently solved for adversarial graphs and randomly chosen losses.  When both graphs and losses are adversarially chosen, we present an efficient algorithm whose regret scales linearly with the number of distinct graphs.  Finally, we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difficult problem that has been used to design efficient cryptographic schemes.',\n",
       "  'id': '4975',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We propose a new class of structured Schatten norms for tensors that includes two recently  proposed norms (overlapped\\'\\' and \"latent\\'\\') for convex-optimization-based  tensor decomposition. Based on the properties of the structured Schatten norms, we mathematically analyze the performance of \"latent\\'\\' approach for tensor decomposition, which was empirically found to perform better than the \"overlapped\\'\\' approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structures Schatten norms, which is also interesting in the general context of structured sparsity. We confirm through  numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.  \"',\n",
       "  'id': '4985',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents' types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classification of agents' types using agent-level data. We focus on applications involving data on agents' ranking over alternatives, and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach.\",\n",
       "  'id': '4998',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation. For further justification, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.',\n",
       "  'id': '5001',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an Expectation-Maximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion.',\n",
       "  'id': '5005',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems.  In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to ?share? signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure.',\n",
       "  'id': '5006',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time.  In addition, we show that this framework can be extended to sampling from cardinality-constrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.',\n",
       "  'id': '5008',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We provide a general technique for making online learning algorithms differentially private, in both the full information and bandit settings. Our technique applies to algorithms that aim to minimize a \\\\emph{convex} loss function which is a sum of smaller convex loss terms, one for each data point. We modify the popular \\\\emph{mirror descent} approach, or rather a variant called \\\\emph{follow the approximate leader}.    The technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work.  In many cases, our algorithms (in both settings) matching the dependence on the input length, $T$, of the \\\\emph{optimal nonprivate} regret bounds up to logarithmic factors in $T$. Our algorithms require logarithmic space and update time.',\n",
       "  'id': '5012',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised. Unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and RBMs, because they involve a reconstruction step where the whole input vector is predicted from the current feature values. An algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling  scheme stochastically selecting which input elements to actually reconstruct during training for each particular example.  To generalize this idea to RBMs, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme. We show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks (20 Newsgroups and RCV1), while keeping computational cost linear in the number of non-zeros.',\n",
       "  'id': '5022',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'A common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities. The goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships. Previous models suffer from weak interaction between entities or simple linear projection of the vector space. We address these problems by introducing a neural tensor network (NTN) model which allow the entities and relations to interact multiplicatively. Additionally, we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name, giving an additional dimension of similarity by which entities can share statistical strength. We assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.',\n",
       "  'id': '5028',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'An increasing number of applications require processing of signals defined on weighted graphs. While wavelets provide a flexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less flexible -- they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed. This paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals. Our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network. Particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks. The training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders. After training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph. Improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data.',\n",
       "  'id': '5046',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We investigate a spiking neuron model of multisensory integration. Multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators. We demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes. We also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identified. We provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms.',\n",
       "  'id': '5056',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparamet- ric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-the- art. Via exploratory data analysis?using data with partial ground truth as well as two novel data sets?we find several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) de- tecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using mul- tiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain.',\n",
       "  'id': '5061',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Markov Decision Processes (MDPs) are extremely useful for modeling and solving sequential decision making problems. Graph-based MDPs provide a compact representation for MDPs with large numbers of random variables. However, the complexity of exactly solving a graph-based MDP usually grows exponentially in the number of variables, which limits their application. We present a new variational framework to describe and solve the planning problem of MDPs, and derive both exact and approximate planning algorithms. In particular, by exploiting the graph structure of graph-based MDPs, we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions, then solved by minimizing a Kullback-Leibler (KL) divergence. The KL divergence is optimized using the belief propagation algorithm, with complexity exponential in only the cluster size of the graph. Experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies.',\n",
       "  'id': '5067',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often  provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex non-Gaussian factors, there is still a significant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model e.g., a neural network or random forest) to map EP message inputs to EP message outputs.  We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising.',\n",
       "  'id': '5070',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'In this paper, we theoretically study the problem of binary classification in the presence of random classification noise --- the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is \\\\emph{class-conditional} --- the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence --- methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88\\\\% accuracy even when 40\\\\% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.',\n",
       "  'id': '5073',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as defined by the recently introduced ?Stable Spline kernel?. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.',\n",
       "  'id': '4174',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.',\n",
       "  'id': '4175',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of -the-art active learning approaches.',\n",
       "  'id': '4176',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Estimating 3D pose from monocular images is a highly ambiguous problem. Physical constraints can be exploited to restrict the space of feasible configurations. In this paper we propose an approach to constraining the prediction of a discriminative predictor. We first show that the mean prediction of a Gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples. We then show how, by performing a change of variables, a GP can be forced to satisfy quadratic constraints. As evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.',\n",
       "  'id': '4179',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others. We propose a new way of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret. In particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods.',\n",
       "  'id': '4191',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Principal Components Analysis~(PCA) is often used as a feature extraction procedure. Given a matrix $X \\\\in \\\\mathbb{R}^{n \\\\times d}$, whose rows represent $n$ data points with respect to $d$ features, the top $k$ right singular vectors of $X$ (the so-called \\\\textit{eigenfeatures}), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a \\\\textit{small} number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while \\\\emph{provably} achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.',\n",
       "  'id': '4196',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Knowledge-based support vector machines (KBSVMs) incorporate advice from domain experts, which can improve generalization significantly. A major limitation that has not been fully addressed occurs when the expert advice is imperfect, which can lead to poorer models. We propose a model that extends KBSVMs and is able to not only learn from data and advice, but also simultaneously improve the advice. The proposed approach is particularly effective for knowledge discovery in domains with few labeled examples. The proposed model contains bilinear constraints, and is solved using two iterative approaches: successive linear programming and a constrained concave-convex approach. Experimental results demonstrate that these algorithms yield useful refinements to expert advice, as well as improve the performance of the learning algorithm overall.',\n",
       "  'id': '4199',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'State-of-the-art statistical methods in neuroscience have enabled us to fit mathematical models to experimental data and subsequently to infer the dynamics of hidden parameters underlying the observable phenomena. Here, we develop a Bayesian method for inferring the time-varying mean and variance of the synaptic input, along with the dynamics of each ion channel from a single voltage trace of a neuron. An estimation problem may be formulated on the basis of the state-space model with prior distributions that penalize large fluctuations in these parameters. After optimizing the hyperparameters by maximizing the marginal likelihood, the state-space model provides the time-varying parameters of the input signals and the ion channel states. The proposed method is tested not only on the simulated data from the Hodgkin-Huxley type models but also on experimental data obtained from a cortical slice in vitro.',\n",
       "  'id': '4214',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"In this paper, we derive a method to refine a Bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering. At each step, the expert executes an evidence gathering test, which suggests the test's relative diagnostic value. We demonstrate that consistency with an expert's test selection leads to non-convex constraints on the model parameters.  We incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods. Gibbs sampling, stochastic hill climbing and greedy search algorithms are proposed to find a MAP estimate that takes into account test ordering constraints and any data available. We demonstrate our approach on diagnostic sessions from a manufacturing scenario.\",\n",
       "  'id': '4219',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model?s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for offices, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms.',\n",
       "  'id': '4226',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Non-negative data are commonly encountered in numerous fields, making non-negative least squares regression (NNLS) a frequently used tool.   At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern  high-dimensional linear models. Even in this setting - unlike first intuition may suggest - we show that for a broad class of designs, NNLS is resistant to overfitting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming L1-regularization.   Since NNLS also circumvents the delicate choice of a regularization parameter, our findings suggest that NNLS may be the method of choice.',\n",
       "  'id': '4231',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We present theoretical and empirical results for a framework  that combines the benefits of apprenticeship and autonomous reinforcement learning.  Our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment.  The first change is replacing previously used Mistake Bound model learners with a recently proposed framework that melds the  KWIK and Mistake Bound supervised learning protocols.  The second change is introducing a communication of expected utility from the student to the teacher.   The resulting system only uses teacher traces when the agent needs to learn concepts it cannot efficiently learn on its own.',\n",
       "  'id': '4240',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling.  It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation.  In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well.  In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case.  Furthermore, we show that we can efficiently project on to this convex set using only samples generated from the system.  The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods.',\n",
       "  'id': '4244',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information. The  main application of our results is to the development of distributed  minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to  asynchrony. Our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible. In  application to distributed optimization, we show $n$-node architectures whose optimization error in stochastic problems---in spite of asynchronous delays---scales asymptotically as $\\\\order(1 / \\\\sqrt{nT})$, which is known to be optimal even in the absence of delays.',\n",
       "  'id': '4247',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'A new Le ?vy process prior is proposed for an uncountable collection of covariate- dependent feature-learning measures; the model is called the kernel beta process (KBP). Available covariates are handled efficiently via the kernel construction, with covariates assumed observed with each data sample (?customer?), and latent covariates learned for each feature (?dish?). Each customer selects dishes from an infinite buffet, in a manner analogous to the beta process, with the added constraint that a customer first decides probabilistically whether to ?consider? a dish, based on the distance in covariate space between the customer and dish. If a customer does consider a particular dish, that dish is then selected probabilistically as in the beta process. The beta process is recovered as a limiting case of the KBP. An efficient Gibbs sampler is developed for computations, and state-of-the-art results are presented for image processing and music analysis tasks.',\n",
       "  'id': '4255',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We address the challenging task of decoupling material properties from lighting properties given a single image. In the last two decades virtually all works have concentrated on exploiting edge information to address this problem. We take a different route by introducing a new prior on reflectance, that models reflectance values as being drawn from a sparse set of basis colors. This results in a Random Field model with global, latent variables (basis colors) and pixel-accurate output reflectance values. We show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information. Finally, we present competitive results by integrating an additional edge model. We believe that our approach is a solid starting point for future development in this domain.',\n",
       "  'id': '4256',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"The L_1 regularized Gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program.  In contrast to other state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and also present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when  compared to other state-of-the-art methods.\",\n",
       "  'id': '4266',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Recently, Mahoney and Orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph Laplacian exactly solve certain regularized Semi-Definite Programs (SDPs). In this paper, we extend that result by providing a statistical interpretation of their approximation procedure. Our interpretation will be analogous to the manner in which l2-regularized or l1-regularized l2 regression (often called Ridge regression and Lasso regression, respectively) can be interpreted in terms of a Gaussian prior or a Laplace prior, respectively, on the coefficient vector of the regression problem. Our framework will imply that the solutions to the Mahoney-Orecchia regularized SDP can be interpreted as regularized estimates of the pseudoinverse of the graph Laplacian. Conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based PageRank procedure for computing an approximation to the first nontrivial eigenvector of the graph Laplacian. Empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm.',\n",
       "  'id': '4272',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field, and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data.',\n",
       "  'id': '4282',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We consider the problem of Bayesian inference for continuous time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times. We propose exact inference and sampling methodologies for two specific cases where the discontinuous dynamics is given by a Poisson process and a two-state Markovian switch. We test the methodology on simulated data, and apply it to two real data sets in finance and systems biology. Our experimental results show that the approach leads to valid inferences and non-trivial insights.',\n",
       "  'id': '4286',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Neurons in the neocortex code and compute as part of a locally interconnected population. Large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data. What statistical structure best describes the concurrent spiking of cells within  a local network? We argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling. We test this claim by comparing  a latent dynamical model with realistic spiking observations to coupled generalised  linear spike-response models (GLMs) using cortical recordings. We find that the latent dynamical approach outperforms the GLM in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately. We also compare models whose observations models are either derived from a Gaussian or point-process models, finding that the non-Gaussian model provides slightly  better goodness-of-fit and more realistic population spike counts.',\n",
       "  'id': '4289',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of Omega(d^2 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.',\n",
       "  'id': '4290',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer.  Unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded \"local receptive fields\" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality).  In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods.  Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric.  This approach  allows us to harness the advantages of local receptive fields (such  as improved scalability, and reduced data requirements) when we do  not know how to specify such receptive fields by hand or where our  unsupervised training algorithm has no obvious generalization to a  topographic setting.  We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.',\n",
       "  'id': '4293',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Applications such as robot control and wireless communication require planning under uncertainty. Partially observable Markov decision processes (POMDPs) plan policies for single agents under uncertainty and their decentralized versions (DEC-POMDPs) find a policy for multiple agents. The policy in infinite-horizon POMDP and DEC-POMDP problems has been represented as finite state controllers (FSCs). We introduce a novel class of periodic FSCs, composed of layers connected only to the previous and next layer. Our periodic FSC method finds a deterministic finite-horizon policy and converts it to an initial periodic infinite-horizon policy. This policy is optimized by a new infinite-horizon algorithm to yield deterministic periodic policies, and by a new expectation maximization algorithm to yield stochastic periodic policies. Our method yields better results than earlier planning methods and can compute larger solutions than with regular FSCs.',\n",
       "  'id': '4297',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Learning problems such as logistic regression are typically formulated as pure  optimization problems defined on some loss function. We argue that this view  ignores the fact that the loss function depends on stochastically generated data  which in turn determines an intrinsic scale of precision for statistical estimation.  By considering the statistical properties of the update variables used during  the optimization (e.g. gradients), we can construct frequentist hypothesis tests  to determine the reliability of these updates. We utilize subsets of the data  for computing updates, and use the hypothesis tests for determining when the  batch-size needs to be increased. This provides computational benefits and avoids  overfitting by stopping when the batch-size has become equal to size of the full  dataset. Moreover, the proposed algorithms depend on a single interpretable  parameter ? the probability for an update to be in the wrong direction ? which is  set to a single value across all algorithms and datasets. In this paper, we illustrate  these ideas on three L1 regularized coordinate algorithms: L1 -regularized L2 -loss  SVMs, L1 -regularized logistic regression, and the Lasso, but we emphasize that  the underlying methods are much more generally applicable.',\n",
       "  'id': '4308',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Classical Boosting algorithms, such as AdaBoost, build a strong classifier without concern about the computational cost.  Some applications, in particular in computer vision, may involve up to millions of training examples and features.  In such contexts, the training time may become prohibitive.  Several methods exist to accelerate training, typically either by sampling the features, or the examples, used to train the weak learners.  Even if those methods can precisely quantify the speed improvement they deliver, they offer no guarantee of being more efficient than any other, given the same amount of time.    This paper aims at shading some light on this problem, i.e. given a fixed amount of time, for a particular problem, which strategy is optimal in order to reduce the training loss the most.  We apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction.  Experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.',\n",
       "  'id': '4310',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"Extensive evidence suggests that items are not encoded independently in visual short-term memory (VSTM). However, previous research has not quantitatively considered how the encoding of an item influences the encoding of other items. Here, we model the dependencies among VSTM representations using a multivariate Gaussian distribution with a stimulus-dependent mean and covariance matrix. We report the results of an experiment designed to determine the specific form of the stimulus-dependence of the mean and the covariance matrix. We find that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items' feature values, similar to a Gaussian process with a distance-dependent, stationary kernel function. We further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses.\",\n",
       "  'id': '4320',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"The efficient coding hypothesis holds that neural receptive fields are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism's lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive field properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive field properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive field plasticity during an organism's lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, first proposed by Mountcastle (1978), that a qualitatively similar learning algorithm acts throughout primary sensory cortices.\",\n",
       "  'id': '4331',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classification. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse filtering, a simple new algorithm which is efficient and only has one hyperparameter, the number of features to learn.  In contrast to most other feature learning methods, sparse filtering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function -- the sparsity of L2-normalized features -- which can easily be implemented in a few lines of MATLAB code. Sparse filtering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse filtering on natural images, object classification (STL-10), and phone classification (TIMIT), and show that our method works well on a range of different modalities.',\n",
       "  'id': '4334',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit $\\\\ell_2$ norm, incoherent columns or features.  But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications).  In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows.  Sparse penalized regression models are analyzed with the purpose of finding, to the extent possible, regimes of dictionary invariant performance.  In particular, a Type II Bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the $\\\\ell_1$ norm, especially in areas where existing theoretical recovery guarantees no longer hold.  This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions.',\n",
       "  'id': '4335',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination  pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide  experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.',\n",
       "  'id': '4337',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.',\n",
       "  'id': '4346',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world benefits from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Specifically, a category hierarchy is utilized to properly define loss function and select common set of features for  related categories. An efficient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach.',\n",
       "  'id': '4347',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm. In fact, for our application, minimum norm can have a running time of about O(n^7 ) (O(n^5 ) oracle calls). We therefore propose a fast approximate method to minimize arbitrary submodular functions. For a large sub-class of submodular functions, the algorithm is exact. Other submodular functions are iteratively approximated by tight submodular upper bounds, and then repeatedly optimized. We show theoretical properties, and empirical results suggest significant speedups over minimum norm while retaining higher accuracies.',\n",
       "  'id': '4348',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Given a set V of n vectors in d-dimensional space, we provide an efficient method  for computing quality upper and lower bounds of the Euclidean distances between  a pair of the vectors in V . For this purpose, we define a distance measure, called  the MS-distance, by using the mean and the standard deviation values of vectors in  V . Once we compute the mean and the standard deviation values of vectors in V in  O(dn) time, the MS-distance between them provides upper and lower bounds of  Euclidean distance between a pair of vectors in V in constant time. Furthermore,  these bounds can be refined further such that they converge monotonically to the  exact Euclidean distance within d refinement steps. We also provide an analysis on  a random sequence of refinement steps which can justify why MS-distance should  be refined to provide very tight bounds in a few steps of a typical sequence. The  MS-distance can be used to various problems where the Euclidean distance is used  to measure the proximity or similarity between objects. We provide experimental  results on the nearest and the farthest neighbor searches.',\n",
       "  'id': '4353',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-the-art models from both schools of thought. First we find the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-and-Fire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the firing probability given a deterministic membrane potential. We find a mathematical expression for this link-function and test the ability of the GLM to predict the firing probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we find that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models.',\n",
       "  'id': '4355',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'In many clustering problems, we have access to multiple views of the data each  of which could be individually used for clustering. Exploiting information from  multiple views, one can hope to find a clustering that is more accurate than the  ones obtained using the individual views. Since the true clustering would assign  a point to the same cluster irrespective of the view, we can approach this problem  by looking for clusterings that are consistent across the views, i.e., corresponding  data points in each view should have same cluster membership. We propose a  spectral clustering framework that achieves this goal by co-regularizing the clustering  hypotheses, and propose two co-regularization schemes to accomplish this.  Experimental comparisons with a number of baselines on two synthetic and three  real-world datasets establish the efficacy of our proposed approaches.',\n",
       "  'id': '4360',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning. However, this expressivity is detrimental to the tractability of inference, when done at the propositional level.  To solve this problem, various lifted inference algorithms have been proposed that reason at the first-order level, about groups of objects as a whole. Despite the existence of various lifted inference approaches, there are currently no completeness results about these algorithms.  The key contribution of this paper is that we introduce a formal definition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models.  We then show how to obtain a completeness result using a first-order knowledge compilation approach for theories of formulae containing up to two logical variables.',\n",
       "  'id': '4374',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine  learning tasks.  Several researchers have recently proposed schemes  to parallelize SGD, but all require  performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms,  and implementation that SGD can be implemented *without any locking*. We present an update scheme called Hogwild which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild achieves a nearly optimal rate of convergence.  We demonstrate experimentally that Hogwild outperforms alternative schemes that use locking by an order of magnitude.\",\n",
       "  'id': '4390',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'This paper studies the problem of semi-supervised learning from the vector field perspective. Many of the existing work use the graph Laplacian to ensure the smoothness of the prediction function on the data manifold. However, beyond smoothness, it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems. To achieve this goal, we show that the second order smoothness measures the linearity of the function, and the gradient field of a linear function has to be a parallel vector field. Consequently, we propose to find a function which minimizes the empirical error, and simultaneously requires its gradient field to be as parallel as possible. We give a continuous objective function on the manifold and discuss how to discretize it by using random points. The discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently. The experimental results have demonstrated the effectiveness of our proposed approach.',\n",
       "  'id': '4398',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer?s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.',\n",
       "  'id': '4417',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Log-linear models are widely used probability models for statistical pattern recognition. Typically, log-linear models are trained according to a convex criterion. In recent years, the interest in log-linear models has greatly increased. The optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. Different optimization algorithms have been evaluated empirically in many papers. In this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. We verify our findings on two handwriting tasks. By making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.',\n",
       "  'id': '4421',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity $O(n^3)$  of the original ADM based method to $O(rn^2)$, where $r$ and $n$ are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.',\n",
       "  'id': '4434',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"We propose a novel Adaptive Markov Chain Monte Carlo algorithm to  compute the partition function. In particular, we show how to  accelerate a flat histogram sampling technique by significantly  reducing the number of ``null moves'' in the chain, while maintaining  asymptotic convergence properties. Our experiments show that our  method converges quickly to highly accurate solutions on a range of  benchmark instances, outperforming other state-of-the-art methods such  as IJGP, TRW, and Gibbs sampling both in run-time and accuracy. We  also show how obtaining a so-called density of states distribution  allows for efficient weight learning in Markov Logic theories.\",\n",
       "  'id': '4448',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.',\n",
       "  'id': '4453',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'When used to learn high dimensional parametric probabilistic models, the clas- sical maximum likelihood (ML) learning often suffers from computational in- tractability, which motivates the active developments of non-ML learning meth- ods. Yet, because of their divergent motivations and forms, the objective func- tions of many non-ML learning methods are seemingly unrelated, and there lacks a unified framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learn- ing methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score match- ing [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be unified under the minimum KL con- traction framework with different choices of the KL contraction operators.',\n",
       "  'id': '4456',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'The performance of Markov chain Monte Carlo methods is often sensitive to the scaling and correlations between the random variables of interest. An important source of information about the local correlation and scale is given by the Hessian matrix of the target distribution, but this is often either computationally expensive or infeasible. In this paper we propose MCMC samplers that make use of quasi-Newton approximations from the optimization literature, that approximate the Hessian of the target distribution from previous samples and gradients generated by the sampler. A key issue is that MCMC samplers that depend on the history of previous states are in general not valid. We address this problem by using limited memory quasi-Newton methods, which depend only on a fixed window of previous samples. On several real world datasets, we show that the quasi-Newton sampler is a more effective sampler than standard Hamiltonian Monte Carlo at a fraction of the cost of MCMC methods that require higher-order derivatives.',\n",
       "  'id': '4464',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We propose an online prediction version of submodular set cover with connections to ranking and repeated active learning.  In each round, the learning algorithm chooses a sequence of items.  The algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function: the number of items needed, when items are selected in order of the chosen sequence, to achieve a coverage constraint.  We develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight.  Our proposed algorithm is readily extended to a setting where multiple functions are revealed at each round and to bandit and contextual bandit settings.',\n",
       "  'id': '4465',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers--annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators---with the spammers having a score close to zero and the good annotators having a high score close to one.',\n",
       "  'id': '4469',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'This paper addresses the problem of minimizing a convex, Lipschitz  function $f$ over a convex, compact set $X$ under a stochastic bandit feedback model. In this model, the algorithm is allowed to  observe noisy realizations of the function value $f(x)$ at any query  point $x \\\\in X$. We demonstrate a generalization of the  ellipsoid algorithm that incurs $O(\\\\poly(d)\\\\sqrt{T})$ regret. Since any algorithm has regret at least $\\\\Omega(\\\\sqrt{T})$  on this problem, our algorithm is optimal in terms of the scaling  with $T$.',\n",
       "  'id': '4475',\n",
       "  'year': '2011'},\n",
       " {'abstract': \"We derive algorithms for generalised tensor factorisation (GTF) by building upon the well-established theory of Generalised Linear Models. Our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework, derived for a broad class of exponential family distributions including special cases such as Tweedie's distributions corresponding to $\\\\beta$-divergences. By bounding the step size of the Fisher Scoring iteration of the GLM, we obtain general updates for real data and multiplicative updates for non-negative data. The GTF framework is, then extended easily to address the problems when multiple observed tensors are factorised simultaneously. We illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem.\",\n",
       "  'id': '4480',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'We describe a novel technique for feature combination in the bag-of-words model of image classification. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classification problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to find discriminative combinations of cues and the resulting vocabulary of portmanteau words is compact, has the cue binding property, and supports individual weighting of cues in the final image representation. State-of-the-art results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, significantly more complex approaches to multi-cue image representation',\n",
       "  'id': '4481',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process defined over the joint context-action space, and develop CGP-UCB, an intuitive upper-confidence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context.',\n",
       "  'id': '4487',\n",
       "  'year': '2011'},\n",
       " {'abstract': 'Given $\\\\alpha,\\\\epsilon$, we study the time complexity   required to improperly learn a halfspace with misclassification   error rate of at most $(1+\\\\alpha)\\\\,L^*_\\\\gamma + \\\\epsilon$, where   $L^*_\\\\gamma$ is the optimal $\\\\gamma$-margin error rate. For $\\\\alpha   = 1/\\\\gamma$, polynomial time and sample complexity is achievable   using the hinge-loss. For $\\\\alpha = 0$, \\\\cite{ShalevShSr11} showed   that $\\\\poly(1/\\\\gamma)$ time is impossible, while learning is   possible in time $\\\\exp(\\\\tilde{O}(1/\\\\gamma))$.  An immediate   question, which this paper tackles, is what is achievable if $\\\\alpha   \\\\in (0,1/\\\\gamma)$.  We derive positive results interpolating between   the polynomial time for $\\\\alpha = 1/\\\\gamma$ and the exponential   time for $\\\\alpha=0$. In particular, we show that there are cases in   which $\\\\alpha = o(1/\\\\gamma)$ but the problem is still solvable in   polynomial time. Our results naturally extend to the adversarial   online learning model and to the PAC learning with malicious noise   model.',\n",
       "  'id': '4492',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We consider estimation of multiple high-dimensional Gaussian graphical models corresponding to a single set of nodes under several distinct conditions. We assume that most aspects of the networks are shared, but that there are some structured differences between them. Specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks. This corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes. We propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm. Our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data.',\n",
       "  'id': '4499',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"We address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multiclass classification problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification. We give a ''goodness'' criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using ''good'' similarity functions. We demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs.\",\n",
       "  'id': '4508',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We  show how binary classification methods developed to work on i.i.d. data can be  used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series.  Specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solving  these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods.  Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.',\n",
       "  'id': '4510',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"Two-alternative forced choice (2AFC) and Go/NoGo (GNG) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior. While GNG is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the Go response (higher hits and false alarm rates) in the GNG task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks. Existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (DDM; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the Go bias.  We postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of GNG: the NoGo response requires waiting until the response deadline, while a Go response immediately terminates the current trial. We show that a Bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.  The optimal decision policy is formally equivalent to a DDM with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline. The initial rise is due to the fading temporal advantage of choosing the Go response over the fixed-delay NoGo response. We show that fitting a simpler, fixed-threshold DDM to the optimal model reproduces the counterintuitive result of a higher threshold in GNG than 2AFC decision-making, previously observed in direct DDM fit to behavioral data [2], although such approximations cannot reproduce the Go bias. Thus, observed discrepancies between GNG and 2AFC decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes.\",\n",
       "  'id': '4515',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X = CX and some linear constraints.  The matrix C selects features, which are then used to compute a low-rank NMF of X.  A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes.',\n",
       "  'id': '4518',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'In hierarchical classification, the prediction paths may be required to always end at leaf nodes. This is called mandatory leaf node prediction (MLNP) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes. However, while there have been a lot of MLNP methods in hierarchical multiclass classification, performing MLNP in hierarchical multilabel classification is much more difficult. In this paper, we propose a novel MLNP algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and DAGs. We show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm. Moreover, this can be further extended to the minimization of the expected symmetric loss. Experiments are performed on a number of real-world data sets with tree- and DAG-structured label hierarchies. The  proposed method consistently outperforms other hierarchical and flat multilabel classification methods.',\n",
       "  'id': '4520',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification.',\n",
       "  'id': '4523',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Boolean satisfiability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science.  In practice, real-world SAT sentences are drawn from a distribution that may result in efficient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem.  In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space.  Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm.  Furthermore, we show that a simple perceptron-style learning rule will find an optimal SAT solver with a bounded number of training updates.  We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT.  Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware verification task.',\n",
       "  'id': '4533',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"Active learning can substantially improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron's receptive field (RF) in real time. Bayesian active learning methods maintain a posterior distribution over the RF, and select stimuli to maximally reduce posterior entropy on each time step.  However, existing methods tend to rely on simple Gaussian priors, and do not exploit uncertainty at the level of hyperparameters when determining an optimal stimulus.  This uncertainty can play a substantial role in RF characterization, particularly when RFs are smooth, sparse, or local in space and time.  In this paper, we describe a novel framework for active learning under hierarchical, conditionally Gaussian priors.  Our algorithm uses sequential Markov Chain Monte Carlo sampling (''particle filtering'' with MCMC) over hyperparameters to construct a mixture-of-Gaussians representation of the RF posterior, and selects optimal stimuli using an approximate infomax criterion.  The core elements of this algorithm are parallelizable, making it computationally efficient for real-time experiments.  We apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive field estimates from very limited data, even with a small number of hyperparameter samples.\",\n",
       "  'id': '4552',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"Users want natural language processing (NLP) systems to be both fast and accurate, but quality often comes at the cost of speed. The field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).  We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\\\cite{kay-1986}. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.  Moreover, it is not specifically tuned for the known reward function.  We propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.\",\n",
       "  'id': '4556',\n",
       "  'year': '2012'},\n",
       " {'abstract': \"We consider sequential prediction algorithms that are given the predictions from a set of models as inputs. If the nature of the data is changing over time in that different models predict well on different segments of the data, then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior (kind of like a weak restart). However, what if the favored models in each segment are from a small subset, i.e. the data is likely to be predicted well by models that predicted well before? Curiously, fitting such ''sparse composite models'' is achieved by mixing in a bit of all the past posteriors. This self-referential updating method is rather peculiar, but it is efficient and gives superior performance on many natural data sets. Also it is important because it introduces a long-term memory: any model that has done well in the past can be recovered quickly. While Bayesian interpretations can be found for mixing in a bit of the initial prior, no Bayesian interpretation is known for mixing in past posteriors.  We build atop the ''specialist'' framework from the online learning literature to give the Mixing Past Posteriors update a proper Bayesian foundation. We apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound.\",\n",
       "  'id': '4557',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Despite the variety of robust regression methods that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach  compared to standard methods.',\n",
       "  'id': '4558',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We present a multi-task learning approach to jointly estimate the means of multiple independent data sets. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the single-task averages. We derive the optimal amount of regularization, and show that it can be effectively estimated. Simulations and real data experiments demonstrate that MTA  both maximum likelihood and James-Stein estimators, and that our approach to estimating the amount of regularization rivals cross-validation in performance but is more computationally efficient.',\n",
       "  'id': '4559',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'In many applications, one has information, e.g., labels that are  provided in a semi-supervised manner, about a specific target region of a  large data set, and one wants to perform machine learning and data analysis  tasks nearby that pre-specified target region.   Locally-biased problems of this sort are particularly challenging for  popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities. In this paper, we address this issue by providing a methodology to construct  semi-supervised eigenvectors of a graph Laplacian, and we illustrate  how these locally-biased eigenvectors can be used to perform  locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized  directions of maximum variance, conditioned on being well-correlated with an  input seed set of nodes that is assumed to be provided in a semi-supervised  manner. We also provide several empirical examples demonstrating how these  semi-supervised eigenvectors can be used to perform locally-biased learning.',\n",
       "  'id': '4560',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We develop a scalable algorithm for posterior inference of overlapping communities in large networks.  Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel. It naturally interleaves subsampling the network with estimating its community structure.  We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.',\n",
       "  'id': '4573',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.',\n",
       "  'id': '4575',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Determinantal point processes (DPPs) have recently been proposed as   computationally efficient probabilistic models of diverse sets for a   variety of applications, including document summarization, image   search, and pose estimation.  Many DPP inference operations,   including normalization and sampling, are tractable; however,   finding the most likely configuration (MAP), which is often required   in practice for decoding, is NP-hard, so we must resort to   approximate inference.  Because DPP probabilities are   log-submodular, greedy algorithms have been used in the past with   some empirical success; however, these methods only give   approximation guarantees in the special case of DPPs with monotone   kernels.  In this paper we propose a new algorithm for approximating   the MAP problem based on continuous techniques for submodular   function maximization.  Our method involves a novel continuous   relaxation of the log-probability function, which, in contrast to   the multilinear extension used for general submodular functions, can   be evaluated and differentiated exactly and efficiently.  We obtain   a practical algorithm with a 1/4-approximation guarantee for a   general class of non-monotone DPPs.  Our algorithm also extends to   MAP inference under complex polytope constraints, making it possible   to combine DPPs with Markov random fields, weighted matchings, and   other models.  We demonstrate that our approach outperforms greedy   methods on both synthetic and real-world data.',\n",
       "  'id': '4577',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be specified in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models.',\n",
       "  'id': '4583',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets.',\n",
       "  'id': '4587',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.  Indeed, finite mixtures and infinite mixtures, relying on Dirichlet processes and modifications, have become a standard tool.  One important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.   Such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.  Redundancy can arise in the absence of a penalty on components placed close together even when a Bayesian approach is used to learn the number of components.  To solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.  We characterize this repulsive prior theoretically and propose a Markov chain Monte Carlo sampling algorithm for posterior computation.  The methods are illustrated using synthetic examples and an iris data set.',\n",
       "  'id': '4589',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'Memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems. The plasticity in these memristive devices, i.e. their resistance change, is defined by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO$_3$ memristive material. Based on this approach, we show STDP for the first time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device.',\n",
       "  'id': '4595',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context, and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem. Comparisons with the method implemented in the QUIC software package are presented.',\n",
       "  'id': '4601',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'This paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure.',\n",
       "  'id': '4608',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.',\n",
       "  'id': '4612',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Specifically, we take inspiration from the conditional mean-field recursive equations of the Replicated Softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.',\n",
       "  'id': '4613',\n",
       "  'year': '2012'},\n",
       " {'abstract': 'This paper addresses the problem of noisy Generalized Binary Search (GBS).  GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries.  At each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search.  GBS is used in many applications, including fault testing, machine diagnostics, disease diagnosis, job scheduling, image processing, computer vision, and active learning. In most of these cases, the responses to queries can be noisy.  Past work has provided a partial characterization of GBS, but existing noise-tolerant versions of GBS are suboptimal in terms of sample complexity.  This paper presents the first optimal algorithm for noisy GBS and demonstrates its application to learning multidimensional threshold functions.',\n",
       "  'id': '3721',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from  $\\\\a$-mixing processes. To illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d. case.',\n",
       "  'id': '3736',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a {\\\\it conditional renewal} (CR) model for neural spike trains. This model captures both real and rescaled-time effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with $\\\\kappa \\\\neq1$), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.',\n",
       "  'id': '3740',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.',\n",
       "  'id': '3749',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'An algorithm is presented for online learning of rotations. The proposed algorithm involves matrix exponentiated gradient updates and is motivated by the Von Neumann divergence. The additive updates are skew-symmetric matrices with trace zero which comprise the Lie algebra of the rotation group. The orthogonality and unit determinant of the matrix  parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact Lie groups. The stability and the computational complexity of the algorithm are discussed.',\n",
       "  'id': '3753',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'The standard assumption of identically distributed training and test data can be violated when an adversary can exercise some control over the generation of the test data. In a prediction game, a learner produces a predictive model while an adversary may alter the distribution of input data. We study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic. We identify conditions under which the prediction game has a unique Nash equilibrium, and derive algorithms that will find the equilibrial prediction models. In a case study, we explore properties of Nash-equilibrial prediction models for email spam filtering empirically.',\n",
       "  'id': '3755',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'The estimation of high-dimensional parametric models requires imposing some structure on the models, for instance that they be sparse, or that matrix structured parameters have low rank. A general approach for such structured parametric model estimation is to use regularized M-estimation procedures, which regularize a loss function that measures goodness of fit of the parameters to the data with some regularization function that encourages the assumed structure. In this paper, we aim to provide a unified analysis of such regularized M-estimation procedures. In particular, we report the convergence rates of such estimators in any metric norm. Using just our main theorem, we are able to rederive some of the many existing results, but also obtain a wide range of novel convergence rates results. Our analysis also identifies key properties of loss and regularization functions such as restricted strong convexity, and decomposability, that ensure the corresponding regularized M-estimators have good convergence rates.',\n",
       "  'id': '3765',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity.  Rademacher complexity measures a learners ability to fit random data, and can be used to bound the learners true error based on the observed training sample error.  We first review the definition of Rademacher complexity and its generalization bound.  We then describe a learning the noise\" procedure to experimentally measure human Rademacher complexities.  The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overfitting in human learning.  Finally, we discuss the potential applications of human Rademacher complexity in cognitive science.\"',\n",
       "  'id': '3771',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development, and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.',\n",
       "  'id': '3774',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. Furthermore, the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts. We also use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors; actually the speedup is almost linearly scalable with the number of multiprocessors available. The proposed partitioning scheme and data streaming can be easily ported to many other models in machine learning.',\n",
       "  'id': '3788',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We describe an algorithm for learning bilinear SVMs. Bilinear classifiers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classifier yields a separable filter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classifiers. We also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks. Bilinear classifiers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difficult problems of people detection in video sequences and action classification of video sequences, achieving state-of-the-art results in both.',\n",
       "  'id': '3789',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the  Bernoulli graph model on data of the software projects AspectJ and Rhino.',\n",
       "  'id': '3792',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program -- the `smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. Second, experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude.',\n",
       "  'id': '3799',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences in the features used is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.',\n",
       "  'id': '3815',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., $\\\\ell_1$-regularizer). Gradient descent methods, though highly scalable and easy to implement, are known to converge slowly on these problems. In this paper, we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple but powerful algorithm.',\n",
       "  'id': '3817',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard ell_p-regularized objective functions currently used, such as ridge regression and ell_p-regularized boosting, are obtained from a relaxation of the KL divergence between the quasi uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and AdaBoost.',\n",
       "  'id': '3821',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms.  We provide the first approximation algorithm which solves stochastic games to within $\\\\epsilon$ relative error of the optimal game-theoretic solution, in time polynomial in $1/\\\\epsilon$. Our algorithm extends Murrays and Gordon?s (2007) modified Bellman equation which determines the \\\\emph{set} of all possible achievable utilities; this provides us a truly general framework for multi-agent learning. Further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.',\n",
       "  'id': '3825',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Score functions induced by generative models extract fixed-dimension feature vectors from different-length data observations by subsuming the process of data generation, projecting them in highly informative spaces called score spaces. In this way, standard discriminative classifiers are proved to achieve higher performances than a solely generative or discriminative approach. In this paper, we present a novel score space that exploits the free energy associated to a generative model through a score function. This function aims at capturing both the uncertainty of the model learning and ``local compliance  of data observations with respect to the generative process. Theoretical justifications and convincing comparative classification results on various generative models prove the goodness of the proposed strategy.',\n",
       "  'id': '3830',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional \\\\emph{factorial} prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.',\n",
       "  'id': '3833',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'In this paper we study the problem of learning a low-dimensional (sparse)  distance matrix.  We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is  non-convex.  We then show that it can be equivalently formulated  as a convex saddle (min-max) problem. From this saddle representation, we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function. This smooth optimization approach has an optimal convergence rate of $O(1 /\\\\ell^2)$ for smooth problems where $\\\\ell$ is the iteration number. Finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.',\n",
       "  'id': '3847',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Existing models of categorization typically represent to-be-classified items as points in a multidimensional space. While from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions, and have a strong preference to generalize along the axes of these dimensions, but not diagonally\". What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. Our model can be viewed as a type of transformed Dirichlet process mixture model, where it is the learning of the base distribution of the Dirichlet process which allows dimensional generalization.The learning behaviour of our model captures the developmental shift from roughly \"isotropic\" for children to the axis-aligned generalization that adults show.\"',\n",
       "  'id': '3849',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices.  It turns out that  when the \\\\emph{cluster assumption} holds, that is, when the high density regions are sufficiently separated by  low density valleys, each high density area corresponds to a unique representative eigenvector. Linear combination of such eigenvectors (or, more precisely, of their Nystrom extensions) provide good candidates for good classification functions. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data  with Lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis. Importantly, the sparsity appears naturally from the  cluster assumption. Experimental results on a number  of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).',\n",
       "  'id': '3852',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'In this paper, we develop an efficient moments-based permutation test approach to improve the system?s efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation.  Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency.',\n",
       "  'id': '3858',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We consider the problem of learning probabilistic models for complex relational structures between various types of objects.  A model can help us ``understand a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true.  Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have better predictive performance on large data sets.  We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework.  Inference is fully Bayesian but scales well to large data sets.  The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.',\n",
       "  'id': '3863',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'In this paper we present a novel approach to learn directed acyclic graphs (DAG) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison. We require identifiability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods.',\n",
       "  'id': '3867',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1.  A single-hidden-layer neural network of this kind of model achieves 1.5% error on MNIST.  We also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models.  This pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells.  With this pretraining, the same single-hidden-layer model achieves better generalization error, even though the pretraining sample distribution is very different from the fine-tuning distribution.  To implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.',\n",
       "  'id': '3868',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Across a wide range of cognitive tasks, recent experience in?uences behavior. For example, when individuals repeatedly perform a simple two-alternative forced-choice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g. Jones & Sieck, 2003; Mozer, Kinoshita, & Shettel, 2007; Yu & Cohen, 2008). The Dynamic Belief Model (DBM) (Yu & Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that ?rst-order statistics (base rates) also in?uence sequential effects. We propose a model that learns both ?rst- and second-order sequence properties, each according to the basic principles of the DBM but under a uni?ed inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious ?ts to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Dal Martello, Sahm, & Spillmann, 2005) and electrophysiological studies (Jentzsch & Sommer, 2002), supporting the psychological and neurobiological reality of its two components.',\n",
       "  'id': '3870',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We adapt a probabilistic latent variable model, namely GaP (Gamma-Poisson), to ad targeting in the contexts of sponsored search (SS) and behaviorally targeted (BT) display advertising. We also approach the important problem of ad positional bias by formulating a one-latent-dimension GaP factorization. Learning from click-through data is intrinsically large scale, even more so for ads. We scale up the algorithm to terabytes of real-world SS and BT data that contains hundreds of millions of users and hundreds of thousands of features, by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality. Specifically, we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems, through the SS and BT implementations, respectively. Finally, we report the experimental results using Yahoos vast datasets, and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy. For BT in particular, the ROC area achieved by GaP is exceeding 0.95, while one prior approach using Poisson regression yielded 0.83. For computational performance, we compare a single-node sparse implementation with a parallel implementation using Hadoop MapReduce, the results are counterintuitive yet quite interesting. We therefore provide insights into the underlying principles of large-scale learning.',\n",
       "  'id': '3873',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations. This framework can include the permutation-level ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modifications, the methods can work significantly better than their original versions.',\n",
       "  'id': '3879',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Online learning algorithms have impressive convergence properties   when it comes to risk minimization and convex games on very large   problems. However, they are inherently sequential in their design   which prevents them from taking advantage of modern multi-core   architectures. In this paper we prove that online learning with   delayed updates converges well, thereby facilitating parallel online   learning.',\n",
       "  'id': '3888',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We establish an excess risk bound of O(H R_n^2 + sqrt{H L*} R_n) for ERM with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = sqrt{R/n}, this translates to a learning rate of ? O(RH/n) in the separable (L* = 0) case and O(RH/n + sqrt{L* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.',\n",
       "  'id': '3894',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually meaningful\" segments in the ensemble. Knowledge about any specific objects and surfaces present in the image is not available. The selection of image regions occupied by objects is formalized as the maximum-weight independent set (MWIS) problem. MWIS is the heaviest subset of mutually non-adjacent nodes of an attributed graph. We construct such a graph from all segments in the ensemble. Then, MWIS selects maximally distinctive segments that together partition the image. A new MWIS algorithm is presented. The algorithm seeks a solution directly in the discrete domain, instead of relaxing MWIS to a continuous problem, as common in previous work. It iteratively finds a candidate discrete solution of the Taylor series expansion of the original MWIS objective function around the previous solution. The algorithm is shown to converge to a maximum. Our empirical evaluation on the benchmark Berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results.\"',\n",
       "  'id': '3909',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.',\n",
       "  'id': '3910',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the confidence region of parameters abound. We propose a decision criterion based on *distributional robustness*: the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible (i.e., it agrees with the a-priori information). We show that finding the optimal distributionally robust policy can be reduced to a standard robust MDP where the parameters belong to a single uncertainty set, hence it can be computed in polynomial time under mild technical conditions.',\n",
       "  'id': '3927',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend sparse coding to learn interpretable spatio-temporal primitives of human motion.  We cast the problem of learning spatio-temporal primitives as a tensor factorization problem  and introduce constraints to learn interpretable primitives. In particular, we use group norms over those tensors, diagonal constraints on the activations as well as smoothness constraints that are inherent to human motion.   We demonstrate the effectiveness of our approach to learn interpretable representations  of human motion from motion capture data, and show that our approach outperforms  recently developed matching pursuit and  sparse coding algorithms.',\n",
       "  'id': '3930',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lovasz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.',\n",
       "  'id': '3933',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.',\n",
       "  'id': '3946',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.',\n",
       "  'id': '3952',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'A long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations. Recent developments have shown ways to use multiple data sets, provided they originate from identical experiments. We present the MCI-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments. It is fast, reliable and produces very clear and easily interpretable output. It is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models. We test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output. The method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.',\n",
       "  'id': '3961',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents-a task that is difficult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (such as hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.',\n",
       "  'id': '3963',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include $K$-fold cross-validation ($K$-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all competing procedures.',\n",
       "  'id': '3966',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start proving a bound   independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with particular structures such as bipartite graphs or grids.  Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% of the optimal) on MRFs  with large variable-disjoint cycles (MRFs in which all cycles are variable-disjoint, namely that they do not share any edge and in which each cycle contains at least 20 variables).',\n",
       "  'id': '3978',\n",
       "  'year': '2010'},\n",
       " {'abstract': \"Many statistical $M$-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer.  We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $d$ to grow with (and possibly exceed) the sample size $n$.  This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie classical optimization analysis.  We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models.  Under these conditions, our theory guarantees that Nesterov's first-order method~\\\\cite{Nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter $\\\\theta^*$ and the optimal solution $\\\\widehat{\\\\theta}$.  This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates.  Our analysis applies to a wide range of $M$-estimators and statistical models, including sparse linear regression using Lasso ($\\\\ell_1$-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization.  Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.\",\n",
       "  'id': '3984',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We present a fast online solver for large scale maximum-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows. We apply the algorithm to optimize tier arrangement of over 80 Million web pages on a layered set of caches to serve an incoming query stream optimally. We provide an empirical demonstration of the effectiveness of our method on real query-pages data.',\n",
       "  'id': '3986',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint ?2,1-norm minimization on both loss function and regularization. The ?2,1-norm based loss function is robust to outliers in data points and the ?2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method.',\n",
       "  'id': '3988',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Applications of Brain-Machine-Interfaces typically estimate user intent based on biological signals that are under voluntary control. For example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity. To solve such problems it is necessary to integrate obtained information over time. To do so, state of the art approaches typically use a probabilistic model of how the state, e.g. position and velocity of the arm, evolves over time ? a so-called trajectory model. We wanted to further develop this approach using two intuitive insights: (1) At any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user. (2) The user may want to produce movements at varying speeds. We thus use a generative model with a trajectory model incorporating these insights. Approximate inference on that generative model is implemented using a mixture of extended Kalman filters. We find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.',\n",
       "  'id': '3989',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efficiently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces for the purpose of classification, such as integrating Yahoo! and DMOZ web directories.',\n",
       "  'id': '3990',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.',\n",
       "  'id': '3992',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique to account for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. We illustrate the utility of this framework for inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to PageRank. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.',\n",
       "  'id': '3993',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We describe an accelerated hardware neuron being capable of emulating the adap-tive exponential integrate-and-fire neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulation and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.',\n",
       "  'id': '3995',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efficient exact inference and exact parameter learning. At the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup',\n",
       "  'id': '4002',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In order to overcome such limitations we contribute threefold: (i) we introduce a taxonomy of camera shakes, (ii) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from Hirsch et al.~and a fast algorithm for single image blind deconvolution for space-invariant filters from Cho and Lee to introduce a method for blind deconvolution for space-variant blur. And (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake.',\n",
       "  'id': '4007',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Arithmetic circuits (ACs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the first ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (AC2-F) is faster and usually more accurate than loopy belief propagation, mean field, and Gibbs sampling; another (AC2-G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines.',\n",
       "  'id': '4011',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We propose a general framework to online learning for   classification problems with time-varying potential functions in the   adversarial setting. This framework allows to design and prove   relative mistake bounds for any generic loss function. The mistake   bounds can be specialized for the hinge loss, allowing to recover   and improve the bounds of known online classification   algorithms. By optimizing the general bound we derive a new online   classification algorithm, called NAROW, that hybridly uses adaptive- and fixed- second order   information. We analyze the properties of the algorithm and   illustrate its performance using synthetic dataset.',\n",
       "  'id': '4017',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efficient mean field approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.',\n",
       "  'id': '4023',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We study a setting in which Poisson processes generate sequences of decision-making events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service.',\n",
       "  'id': '4025',\n",
       "  'year': '2010'},\n",
       " {'abstract': \"This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, Rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10x10 Battleship and Partially Observable PacMan, with approximately 10^18 and 10^56 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.\",\n",
       "  'id': '4031',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems (Karzanov et al., 1991; Jerrum et al., 2001; Wilson, 2004), theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods.  Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning.  Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (Siepel et al., 2004).   Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models (Wainwright et al., 2008); unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments.   We propose a new framework that extends variational inference to a wide range of combinatorial spaces.  Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm.   We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset (Thompson et al., 1999).',\n",
       "  'id': '4036',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from V1 with a 96-electrode Utah\" array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron\\'s response, in addition to the neuron\\'s receptive field transfer function. We also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states. This work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions.\"',\n",
       "  'id': '4050',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.',\n",
       "  'id': '4054',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We first embed each brain into a functional map that reflects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.',\n",
       "  'id': '4059',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.',\n",
       "  'id': '4108',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.',\n",
       "  'id': '4132',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the $\\\\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.',\n",
       "  'id': '4137',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We combine random forest (RF) and conditional random field (CRF) into a new computational framework, called random forest random field (RF)^2. Inference of (RF)^2 uses the Swendsen-Wang cut algorithm, characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)^2. (RF)^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)^2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.',\n",
       "  'id': '4140',\n",
       "  'year': '2010'},\n",
       " {'abstract': \"We discuss an online learning framework in which the agent is allowed to say ``I don't know'' as well as making incorrect predictions on given examples. We analyze the trade off between saying ``I don't know'' and making mistakes. If the number of don't know predictions is forced to be zero, the model reduces to the well-known mistake-bound model introduced by Littlestone [Lit88]. On the other hand, if no mistakes are allowed, the model reduces to KWIK framework introduced by Li et. al. [LLW08]. We propose a general, though inefficient, algorithm for general finite concept classes that minimizes the number of don't-know predictions if a certain number of mistakes are allowed. We then present specific polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators.\",\n",
       "  'id': '4142',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.',\n",
       "  'id': '4149',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.',\n",
       "  'id': '4157',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'In this paper we consider the fundamental problem of semi-supervised kernel function learning. We propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. We introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the $k$-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.',\n",
       "  'id': '4159',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We present a novel method for multitask learning (MTL) based on {\\\\it manifold regularization}: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common {\\\\it linear} subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is fixed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efficient and easy to implement. We show the efficacy of our method on several datasets.',\n",
       "  'id': '4163',\n",
       "  'year': '2010'},\n",
       " {'abstract': \"Computing a {\\\\em maximum a posteriori} (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a finite mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. We experiment on the real-world protein design dataset and show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM achieves a solution quality within $95$\\\\% of optimal for most instances and is often an order-of-magnitude faster than MPLP.\",\n",
       "  'id': '4165',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large, or even infinite. We pro- pose a new optimistic, UCB-like, algorithm for non-linearly parameterized bandit problems using the Generalized Linear Model (GLM) framework. We analyze the regret of the proposed algorithm, termed GLM-UCB, obtaining results similar to those recently proved in the literature for the linear regression case. The analysis also highlights a key difficulty of the non-linear case which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice, we provide an asymptotic argument leading to significantly faster convergence. Simulation studies on real data sets illustrate the performance and the robustness of the proposed GLM-UCB approach.',\n",
       "  'id': '4166',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Optimal control entails combining probabilities and utilities. However, for most practical problems probability densities can be represented only approximately. Choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields.',\n",
       "  'id': '4167',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5\\\\%, which is superior to all published results on speaker-independent TIMIT to date.',\n",
       "  'id': '4169',\n",
       "  'year': '2010'},\n",
       " {'abstract': 'We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.',\n",
       "  'id': '3275',\n",
       "  'year': '2007'},\n",
       " {'abstract': \"In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely appled in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a ``bag-of-words''. It is also critical to properly design ``words'' and ?documents? when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structure among visual words that are essential for solving many vision problems. The spatial information is not encoded in the value of visual words but in the design of documents. Instead of knowing the partition of words into documents \\\\textit{a priori}, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA.\",\n",
       "  'id': '3278',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 10^4 possible faults from 10^5 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a variational approximation, a mean-field approximation, and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours.',\n",
       "  'id': '3292',\n",
       "  'year': '2007'},\n",
       " {'abstract': \"This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics. This ``eigenmodel'' generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.\",\n",
       "  'id': '3294',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier. Most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration. However, single instance selection systems are unable to exploit a parallelized labeler when one is available. Recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration, guided by some heuristic scores. In this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables. The optimization is formuated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account. Although the objective is not convex, we can manipulate a quasi-Newton method to obtain a good local solution. Our empirical studies on UCI datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.',\n",
       "  'id': '3295',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'The control of high-dimensional, continuous, non-linear systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP) are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper, we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing effectively with problems with (at least) 34 state and 14 action dimensions.',\n",
       "  'id': '3297',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.',\n",
       "  'id': '3304',\n",
       "  'year': '2007'},\n",
       " {'abstract': \"We combine two threads of research on approximate dynamic programming: random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function. This combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states. Our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics. In this paper we show that we can now solve problems we couldn't solve previously with regular grid-based approaches.\",\n",
       "  'id': '3350',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Automatic relevance determination (ARD), and the closely-related sparse Bayesian learning (SBL) framework, are effective tools for pruning large numbers of irrelevant features. However, popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties. This paper furnishes an alternative means of optimizing a general ARD cost function using an auxiliary function that can naturally be solved using a series of re-weighted L1 problems. The result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods. The analysis also leads to additional insights into the behavior of previous ARD updates as well as the ARD cost function. For example, the standard fixed-point updates of MacKay (1992) are shown to be iteratively solving a particular min-max problem, although they are not guaranteed to lead to a stationary point. The analysis also reveals that ARD is exactly equivalent to performing MAP estimation using a particular feature- and noise-dependent \\\\textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection. In particular, it provides a tighter approximation to the L0 quasi-norm sparsity measure than the L1 norm. Overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.',\n",
       "  'id': '3372',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse. We present a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efficacy of our method on a microarray dataset containing diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects.',\n",
       "  'id': '3384',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We address the problem of estimating the ratio of two probability density functions (a.k.a.~the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efficient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches.',\n",
       "  'id': '3387',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions. The definitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classifier is trained on the resulting sense-specific images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classification experiments show that our dictionary-based approach outperforms baseline methods.',\n",
       "  'id': '3389',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We consider the following instance of transfer learning: given a pair of regression problems, suppose that the regression coefficients share a partially common support, parameterized by the overlap fraction $\\\\overlap$ between the two supports. This set-up suggests the use of $1, \\\\infty$-regularized linear regression for recovering the support sets of both regression vectors. Our main contribution is to provide a sharp characterization of the sample complexity of this $1,\\\\infty$ relaxation, exactly pinning down the minimal sample size $n$ required for joint support recovery as a function of the model dimension $\\\\pdim$, support size $\\\\spindex$ and overlap $\\\\overlap \\\\in [0,1]$. For measurement matrices drawn from standard Gaussian ensembles, we prove that the joint $1,\\\\infty$-regularized method undergoes a phase transition characterized by order parameter $\\\\orpar(\\\\numobs, \\\\pdim, \\\\spindex, \\\\overlap) = \\\\numobs{(4 - 3 \\\\overlap) s \\\\log(p-(2-\\\\overlap)s)}$. More precisely, the probability of successfully recovering both supports converges to $1$ for scalings such that $\\\\orpar > 1$, and converges to $0$ to scalings for which $\\\\orpar < 1$. An implication of this threshold is that use of $1, \\\\infty$-regularization leads to gains in sample complexity if the overlap parameter is large enough ($\\\\overlap > 2/3$), but performs worse than a naive approach if $\\\\overlap < 2/3$. We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. Thus, our results illustrate both the benefits and dangers associated with block-$1,\\\\infty$ regularization in high-dimensional inference.',\n",
       "  'id': '3392',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise (i.e., heteroscedasticity) varies spatially. Unfortunately, it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. In this paper, we focus on nonparametric regression and introduce a Bayesian formulation that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efficient (suitable for large data sets), requires no sampling, automatically rejects outliers and has only one prior to be specified. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian Processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law.',\n",
       "  'id': '3393',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overfitting, but is also built on the desirable discriminative laws resemble those of the M$^3$N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M$^3$N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task.',\n",
       "  'id': '3399',\n",
       "  'year': '2008'},\n",
       " {'abstract': \"This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for `local ranking', in the sense that the ranking model is defined on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two specific information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.\",\n",
       "  'id': '3402',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Accurate and efficient inference in evolutionary trees is a central problem in computational biology. Realistic models require tracking insertions and deletions along the phylogenetic tree, making inference challenging. We propose new sampling techniques that speed up inference and improve the quality of the samples. We compare our method to previous approaches and show performance improvement on metrics evaluating multiple sequence alignment and reconstruction of ancestral sequences.',\n",
       "  'id': '3406',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are acquired at fixed intervals whereas the events causing data artifacts may occur at any time and have durations that may be significantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables effective detection of artifacts and accurate estimation of the underlying blood pressure values.',\n",
       "  'id': '3434',\n",
       "  'year': '2008'},\n",
       " {'abstract': \"Clustering stability is an increasingly popular family of methods for performing model selection in data clustering. The basic idea is that the chosen model should be stable under perturbation or resampling of the data. Despite being reasonably effective in practice, these methods are not well understood theoretically, and present some difficulties. In particular, when the data is assumed to be sampled from an underlying distribution, the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases. This raises a potentially serious practical difficulty with these methods, because it means there might be some hard-to-compute sample size, beyond which clustering stability estimators 'break down' and become unreliable in detecting the most stable model. Namely, all models will be relatively stable, with differences in their stability measures depending mostly on random and meaningless sampling artifacts. In this paper, we provide a set of general sufficient conditions, which ensure the reliability of clustering stability estimators in the large sample regime. In contrast to previous work, which concentrated on specific toy distributions or specific idealized clustering frameworks, here we make no such assumptions. We then exemplify how these conditions apply to several important families of clustering algorithms, such as maximum likelihood clustering, certain types of kernel clustering, and centroid-based clustering with any Bregman divergence. In addition, we explicitly derive the non-trivial asymptotic behavior of these estimators, for any framework satisfying our conditions. This can help us understand what is considered a 'stable' model by these estimators, at least for large enough samples.\",\n",
       "  'id': '3438',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We present a new family of linear time algorithms based on sufficient statistics for string comparison with mismatches under the string kernels framework. Our algorithms improve theoretical complexity bounds of existing approaches while scaling well with respect to the sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets with loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classification, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with significantly reduced running times.',\n",
       "  'id': '3441',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world. In this paper, we formally define an infinite sequence of nested beliefs about the state of the world at the current time $t$ and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efficient filtering in a range of multi-agent domains.',\n",
       "  'id': '3459',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the epsilon-insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we briefly discuss a trade-off in epsilon between sparsity and accuracy if the SVM is used to estimate the conditional median.',\n",
       "  'id': '3466',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Current on-line learning algorithms for predicting the labelling of a graph have an important limitation in the case of large diameter graphs; the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this problem with an efficient algorithm which achieves a logarithmic mistake bound. Furthermore, current algorithms are optimised for data which exhibits cluster-structure; we give an additional algorithm which performs well locally in the presence of cluster structure and on large diameter graphs.',\n",
       "  'id': '3475',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (~ 2mm cube) of brain tissue. Our model, which we call the SpAM V1 model, is based on the reasonable assumption that fMRI measurements reflect the (possibly nonlinearly) pooled, rectified output of a large population of simple and complex cells in V1. It has a hierarchical filtering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called ???pooled-complex??? cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the SpAM V1 model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive fields, frequency tuning and orientation tuning curves of the SpAM V1 model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the SpAM V1 model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities.',\n",
       "  'id': '3481',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in HazanKaKaAg06. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions.',\n",
       "  'id': '3484',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'This paper presents the first data-dependent generalization bounds for non-i.i.d. settings based on the notion of Rademacher complexity. Our bounds extend to the non-i.i.d. case existing Rademacher complexity bounds derived for the i.i.d. setting. These bounds provide a strict generalization of the ones found in the i.i.d. case, and can also be used within the standard i.i.d. scenario. They apply to the standard scenario of beta-mixing stationary sequences examined in many previous studies of non-i.i.d. settings and benefit form the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from finite samples and lead to tighter bounds.',\n",
       "  'id': '3489',\n",
       "  'year': '2008'},\n",
       " {'abstract': \"EEG connectivity measures could provide a new type of feature space for inferring a subject's intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the gamma-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions.\",\n",
       "  'id': '3505',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random field (MRF) methods. Moreover, we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean field theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difficulties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in MRF approaches with even hundreds of parameters.',\n",
       "  'id': '3506',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efficient methods, i.e., Semi-Infinite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method.',\n",
       "  'id': '3520',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditional-independence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable.',\n",
       "  'id': '3525',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: (1) Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. (2) Convex relaxation such as $L_1$-regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-$L_1$ regularization. Our performance bound shows that the procedure is superior to the standard $L_1$ convex relaxation for learning sparse targets. Experiments confirm the effectiveness of this method on some simulation and real data.',\n",
       "  'id': '3526',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We present a new, massively parallel architecture for accelerating machine learning algorithms, based on arrays of variable-resolution arithmetic vector processing elements (VPE). Groups of VPEs operate in SIMD (single instruction multiple data) mode, and each group is connected to an independent memory bank. In this way memory bandwidth scales with the number of VPE, and the main data flows are local, keeping power dissipation low. With 256 VPEs, implemented on two FPGA (field programmable gate array) chips, we obtain a sustained speed of 19 GMACS (billion multiply-accumulate per sec.) for SVM training, and 86 GMACS for SVM classification. This performance is more than an order of magnitude higher than that of any FPGA implementation reported so far. The speed on one FPGA is similar to the fastest speeds published on a Graphics Processor for the MNIST problem, despite a clock rate of the FPGA that is six times lower. High performance at low clock rates makes this massively parallel architecture particularly attractive for embedded applications, where low power dissipation is critical. Tests with Convolutional Neural Networks and other learning algorithms are under way now.',\n",
       "  'id': '3527',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are simply performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a Gaussian process model of human function learning that combines the strengths of both approaches.',\n",
       "  'id': '3529',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden multinomial unit that represents the cluster labels. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data.',\n",
       "  'id': '3536',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models are often used because these models are well understood and there are well-known methods to fit them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities.',\n",
       "  'id': '3548',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network.',\n",
       "  'id': '3553',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surpris- ingly impressive performance improvements over traditional one-sided (row) clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation (e.g., non-negative matrix factorization) formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms.',\n",
       "  'id': '3562',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.',\n",
       "  'id': '3567',\n",
       "  'year': '2008'},\n",
       " {'abstract': \"With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one. We demonstrate the effectiveness of our ``treewidth-friendly'' method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth.\",\n",
       "  'id': '3568',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems.',\n",
       "  'id': '3570',\n",
       "  'year': '2008'},\n",
       " {'abstract': \"Conditional Random Sampling (CRS) was originally proposed for efficiently computing pairwise ($l_2$, $l_1$) distances, in static, large-scale, and sparse data sets such as text and Web data. It was previously presented using a heuristic argument. This study extends CRS to handle dynamic or streaming data, which much better reflect the real-world situation than assuming static data. Compared with other known sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a significant advantage in that it is ``one-sketch-for-all.'' In particular, we demonstrate that CRS can be applied to efficiently compute the $l_p$ distance and the Hilbertian metrics, both are popular in machine learning. Although a fully rigorous analysis of CRS is difficult, we prove that, with a simple modification, CRS is rigorous at least for an important application of computing Hamming norms. A generic estimator and an approximate variance formula are provided and tested on various applications, for computing Hamming norms, Hamming distances, and $\\\\chi^2$ distances.\",\n",
       "  'id': '3572',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory.',\n",
       "  'id': '3586',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects. This unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We confirm this prediction with an experiment.',\n",
       "  'id': '3587',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Classical Game Theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in Economic games of human subjects. We investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a Partially Observable Markov Decision Process model that incorporates Game Theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investment game. We invert the generative process for a recognition model that is used to classify 200 subjects playing an Investor-Trustee game against randomly matched opponents.',\n",
       "  'id': '3589',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'The machine learning problem of classifier design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the specification of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classification problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classifier design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations.',\n",
       "  'id': '3591',\n",
       "  'year': '2008'},\n",
       " {'abstract': \"We consider the problem of binary classification where the classifier may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow's rule, is defined by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classifier, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efficiently. We finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions.\",\n",
       "  'id': '3594',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We consider a generalization of stochastic bandit problems where the set of arms, X, is allowed to be a generic topological space. We constraint the mean-payoff function with a dissimilarity function over X in a way that is more general than Lipschitz. We construct an arm selection policy whose regret improves upon previous result for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally H?lder with a known exponent, then the expected regret is bounded up to a logarithmic factor by $n$, i.e., the rate of the growth of the regret is independent of the dimension of the space. Moreover, we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider.',\n",
       "  'id': '3605',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum and hippocampus CA3. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between firing and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points.',\n",
       "  'id': '3610',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its far-reaching application, there is almost no work on applying stochastic approximation to learning problems with constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization.',\n",
       "  'id': '3614',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov decision process, target policy, and exciting behavior policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d.\\\\ policy-evaluation setting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L_2 norm. Our analysis proves that its expected update is in the direction of the gradient, assuring convergence under the usual stochastic approximation conditions to the same least-squares solution as found by the LSTD, but without its quadratic computational complexity. GTD is online and incremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods.',\n",
       "  'id': '3626',\n",
       "  'year': '2008'},\n",
       " {'abstract': 'While many advances have already been made on the topic of hierarchical classi-  ?cation learning, we take a step back and examine how a hierarchical classi?ca-  tion problem should be formally de?ned. We pay particular attention to the fact  that many arbitrary decisions go into the design of the the label taxonomy that  is provided with the training data, and that this taxonomy is often unbalanced.  We correct this problem by using the data distribution to calibrate the hierarchical  classi?cation loss function. This distribution-based correction must be done with  care, to avoid introducing unmanagable statstical dependencies into the learning  problem. This leads us off the beaten path of binomial-type estimation and into  the uncharted waters of geometric-type estimation. We present a new calibrated  de?nition of statistical risk for hierarchical classi?cation, an unbiased geometric  estimator for this risk, and a new algorithmic reduction from hierarchical classi?-  cation to cost-sensitive classi?cation.',\n",
       "  'id': '3629',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'The relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using Fisher Information. Here, we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the Jensen-Shannon information to study neural population codes. We first explore the relationship between minimum discrimination error, Jensen-Shannon Information and Fisher Information and show that the discrimination framework is more informative about the coding accuracy than Fisher Information as it defines an error for any pair of possible stimuli. In particular, it includes Fisher Information as a special case. Second, we use the framework to study population codes of angular variables. Specifically, we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows. That is, for long time window we use the common Gaussian noise approximation. To address the case of short time windows we analyze the Ising model with identical noise correlation structure. In this way, we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.',\n",
       "  'id': '3631',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system.',\n",
       "  'id': '3636',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by first principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning.  We experiment with manifold embeddings as the reconstructed observable state-space of an off-line, model-based reinforcement learning approach to control. We demonstrate the embedding of a system changes as a result of learning and that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system.  We apply this approach in simulation to learn a neurostimulation policy that is more efficient in treating epilepsy than conventional policies.  We then demonstrate the learned policy completely suppressing seizures in real-world neurostimulation experiments on actual animal brain slices.',\n",
       "  'id': '3640',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Modern machine learning-based approaches to computer vision require very large databases of labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector). While the collection of these large databases is becoming a bottleneck, new Internet-based services that allow labelers from around the world to be easily hired and managed provide a promising solution.  However, using these services to label large databases brings with it new theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used ``Majority Vote heuristic for inferring image labels, and is robust to both adversarial and noisy labelers.',\n",
       "  'id': '3644',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data.  In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data.  We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reflects known properties of OPMs, and a noise covariance adjusted to the data.  The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements.  By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations,  pinwheel locations or -counts.  Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and provides the basis for decoding studies.  We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex.',\n",
       "  'id': '3645',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points.  We show that in $\\\\R^d$, $d \\\\geq 2$, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function.  We also contrast the method with the Laplacian Eigenvector method, and discuss the ``smoothness assumptions associated with this alternate method.',\n",
       "  'id': '3652',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We extend Dyna planning architecture for policy evaluation and control in two significant aspects. First, we introduce a multi-step Dyna planning that projects the simulated state/feature many steps into the future. Our multi-step Dyna is based on a multi-step model, which we call the {\\\\em $\\\\lambda$-model}. The $\\\\lambda$-model interpolates between the one-step model and an infinite-step model, and can be learned efficiently online. Second, we use for Dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run. Experimental results show that Dyna using the multi-step model evaluates a policy faster than using single-step models; Dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms; further, multi-step Dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step Dyna algorithms.',\n",
       "  'id': '3670',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity.  While typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient, globally-convergent reweighted $\\\\ell_1$ minimization procedure.  The first method under consideration arises from the sparse Bayesian learning (SBL) framework.  Although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the Lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity profile, there will always exist cases where it does better.  These results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions.  We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests.  For both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted $\\\\ell_1$-norm algorithms in handling more general sparse estimation problems involving classification, group feature selection, and non-negativity constraints.  As a byproduct of this development, a rigorous reformulation of sparse Bayesian classification (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-defined objective function.',\n",
       "  'id': '3681',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory.',\n",
       "  'id': '3684',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'We propose a Bayesian nonparametric approach to relating multiple time series via a set of latent, dynamical behaviors.  Using a beta process prior, we allow data-driven selection of the size of this set, as well as the pattern with which behaviors are shared among time series.  Via the Indian buffet process representation of the beta process predictive distributions, we develop an exact Markov chain Monte Carlo inference method.  In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals.  We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising unsupervised segmentation of visual motion capture data.',\n",
       "  'id': '3685',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'When used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. Empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. We propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. We also develop a theory that motivates the algorithm.',\n",
       "  'id': '3686',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. It analyzes this problem in the case of regression and the kernel ridge regression algorithm. It examines the corresponding learning kernel optimization problem, shows how that minimax problem can be reduced to a simpler minimization problem, and proves that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.',\n",
       "  'id': '3692',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'Given $n$ noisy samples with $p$ dimensions, where $n \\\\ll p$, we show that the multi-stage thresholding procedures can accurately estimate a sparse vector $\\\\beta \\\\in \\\\R^p$ in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very significant values of $s$, which is the number of non-zero elements in the true parameter $\\\\beta$. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if $X$ obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the Gauss-Dantzig selector (Cand\\\\{e}s-Tao 07) achieves the $\\\\ell_2$ loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.',\n",
       "  'id': '3697',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.',\n",
       "  'id': '3702',\n",
       "  'year': '2009'},\n",
       " {'abstract': 'It is known that determinining whether a DEC-POMDP, namely, a cooperative partially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how cooperation affected that complexity. We show that, for competitive POSGs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class NEXP with an oracle for NP.',\n",
       "  'id': '3163',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We propose a model that leverages the millions of clicks received by web search engines, to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.',\n",
       "  'id': '3190',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.',\n",
       "  'id': '3212',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are de- signed for specific learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. We also illustrate their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.',\n",
       "  'id': '3239',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts. Thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and epsilon-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.',\n",
       "  'id': '3250',\n",
       "  'year': '2007'},\n",
       " {'abstract': \"We describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables. The distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution. We prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used. We illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts. Finally, we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.\",\n",
       "  'id': '3259',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade earning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classifier can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate significant performance advantages.',\n",
       "  'id': '3265',\n",
       "  'year': '2007'},\n",
       " {'abstract': 'We propose a new weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach we develop a generalization of the Max-Path search algorithm, which allows us to efficiently search over a structured space of multiple spatio-temporal paths, while also allowing to incorporate context information into the model. Instead of using spatial annotations, in the form of bounding boxes, to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, we show how our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.',\n",
       "  'id': '5197',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'One approach to computer object recognition and modeling the brain\\'s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D affine transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformation-invariance, we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identity-preserving transformations. The model\\'s wiring can be learned from videos of transforming objects---or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically confirm theoretical predictions for the case of 2D affine transformations. Next, we apply the model to non-affine transformations: as expected, it performs well on face verification tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter transformations\\'\\' which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical findings, we tested the same model on face verification benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig and a new dataset we gathered---achieving strong performance in these highly unconstrained cases as well.\"',\n",
       "  'id': '5206',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time. We describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy. Our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade. Our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of Vector Quantization levels, and by choosing to rescore windows or not. Our method can be directly plugged into any recognition system that relies on linear templates. We demonstrate our method to speed up the original Exemplar SVM detector [1] by an order of magnitude and Deformable Part models [2] by two orders of magnitude with no loss of accuracy.',\n",
       "  'id': '5208',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'All the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task. In this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks. Specifically, we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific. By defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it. For regression problems, we extend the kernel regression to multi-task setting in a similar way to the classification case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods.',\n",
       "  'id': '5211',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'A probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction. Exact inference is intractable in this model. However, expectation propagation offers an approximate alternative. Because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed, additional data from a multi-task learning scenario are considered for induction. The same model can be used in this setting with few modifications. Furthermore, the assumptions made are less restrictive than in other multi-task methods: The different tasks must share feature selection dependencies, but can have different relevant features and model coefficients. Experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature. The experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data.',\n",
       "  'id': '5212',\n",
       "  'year': '2013'},\n",
       " {'abstract': \"We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing  empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives consistently better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n'th order bottom sample margin.\",\n",
       "  'id': '5214',\n",
       "  'year': '2013'},\n",
       " {'abstract': 'Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time. In this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.',\n",
       "  'id': '5232',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first {\\\\em fast} oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel {\\\\em without} explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.',\n",
       "  'id': '5240',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, \\\\emph{weighted} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).',\n",
       "  'id': '5249',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this work we introduce a new fast incremental gradient method SAGA, in the spirit of SAG, SDCA, MISO and SVRG. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.',\n",
       "  'id': '5258',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.',\n",
       "  'id': '5259',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.',\n",
       "  'id': '5268',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.',\n",
       "  'id': '5271',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.\",\n",
       "  'id': '5276',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter lambda. We use a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization constraints, such as l1, l2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the MNIST and rectangles dataset significantly.',\n",
       "  'id': '5278',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.',\n",
       "  'id': '5281',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256x256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.',\n",
       "  'id': '5282',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.',\n",
       "  'id': '5284',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.',\n",
       "  'id': '5296',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (1) discovering topic prevalence over time, and (2) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling, based on the forward-filtering backward-sampling algorithm. Moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.',\n",
       "  'id': '5301',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity be- tween subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.',\n",
       "  'id': '5308',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonparanormal blossoms\", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then \"grown\" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.\"',\n",
       "  'id': '5316',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE ``breaks down'' under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the $\\\\ell_1$-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.\",\n",
       "  'id': '5318',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we address the problem of learning the structure of Gaussian chain graph models in a high-dimensional space. Chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges. While the problem of sparse structure learning has been studied extensively for Gaussian graphical models and more recently for conditional Gaussian graphical models (CGGMs), there has been little previous work on the structure recovery of Gaussian chain graph models. We consider linear regression models and a re-parameterization of the linear regression models using CGGMs as building blocks of chain graph models. We argue that when the goal is to recover model structures, there are many advantages of using CGGMs as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning. We demonstrate our approach on simulated and genomic datasets.',\n",
       "  'id': '5320',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time (Iwata and Orlin 2009), however these algorithms are not practical. In 1976, Wolfe proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For general submodular functions, the Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance. Despite its good practical performance, theoretically very little is known about Wolfe's minimum norm algorithm -- to our knowledge the only result is an exponential time analysis due to Wolfe himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns a O(1/t)-approximate solution to the min-norm point. We also prove a robust version of Fujishige's theorem which shows that an O(1/n^2)-approximate solution to the min-norm point problem implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for submodular function minimization. In particular, we show that the min-norm point algorithm solves SFM in O(n^7F^2)-time, where $F$ is an upper bound on the maximum change a single element can cause in the function value.\",\n",
       "  'id': '5321',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.',\n",
       "  'id': '5324',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.',\n",
       "  'id': '5325',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the `median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.\",\n",
       "  'id': '5328',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the ``online'' setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).\",\n",
       "  'id': '5330',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.',\n",
       "  'id': '5333',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.\",\n",
       "  'id': '5336',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.',\n",
       "  'id': '5345',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\",\n",
       "  'id': '5346',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given metric distance function over the label set. Popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.',\n",
       "  'id': '5354',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The task of reconstructing a matrix given a sample of observed entries is known as the \\\\emph{matrix completion problem}. Such a consideration arises in a wide variety of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here, we investigate the case where the observations take a finite numbers of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (non-necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.',\n",
       "  'id': '5358',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of ``public'' users who are willing to share their preferences openly, and a large set of ``private'' users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.\",\n",
       "  'id': '5359',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cram\\\\'er-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cram\\\\'er-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.\",\n",
       "  'id': '5361',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We consider the problem of online planning in a Markov decision process with discounted rewards for any given initial state. We consider the PAC sample complexity problem of computing, with probability $1-\\\\delta$, an $\\\\epsilon$-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples). We design an algorithm, called StOP (for Stochastic-Optimistic Planning), based on the optimism in the face of uncertainty\" principle. StOP can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the MDP.\"',\n",
       "  'id': '5368',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.',\n",
       "  'id': '5373',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.',\n",
       "  'id': '5375',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.',\n",
       "  'id': '5377',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that our proximal quasi-Newton algorithm is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.',\n",
       "  'id': '5384',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent input noise\" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.\"',\n",
       "  'id': '5389',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of {\\\\em adversarial} workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of {\\\\em sophisticated} adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.',\n",
       "  'id': '5393',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a general framework for regularization based on group majorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to lie in the convex hull of this orbit (the orbitope). Common regularizers are recovered as particular cases, and a connection is revealed between the recent sorted 1 -norm and the hyperoctahedral group. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.',\n",
       "  'id': '5398',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.',\n",
       "  'id': '5400',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation. This applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples. Such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects. Likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation. By assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the Fourier domain that can eliminate most redundancies. It can leverage off-the-shelf solvers with no modification (e.g. libsvm), and train several pose classifiers simultaneously at no extra cost. Our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.',\n",
       "  'id': '5417',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at',\n",
       "  'id': '5418',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.\",\n",
       "  'id': '5422',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.',\n",
       "  'id': '5423',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We propose a new provable method for robust PCA, where the task is to recover a low-rank matrix, which is corrupted with sparse perturbations. Our method consists of simple alternating projections onto the set of low rank and sparse matrices with intermediate de-noising steps. We prove correct recovery of the low rank and sparse components under tight recovery conditions, which match those for the state-of-art convex relaxation techniques. Our method is extremely simple to implement and has low computational complexity. For a $m \\\\times n$ input matrix (say m \\\\geq n), our method has O(r^2 mn\\\\log(1/\\\\epsilon)) running time, where $r$ is the rank of the low-rank component and $\\\\epsilon$ is the accuracy. In contrast, the convex relaxation methods have a running time O(mn^2/\\\\epsilon), which is not scalable to large problem instances. Our running time nearly matches that of the usual PCA (i.e. non robust), which is O(rmn\\\\log (1/\\\\epsilon)). Thus, we achieve ``best of both the worlds'', viz low computational complexity and provable recovery for robust PCA. Our analysis represents one of the few instances of global convergence guarantees for non-convex methods.\",\n",
       "  'id': '5430',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F1 on real piano audio.\",\n",
       "  'id': '5432',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\\\\em{disagreement-based active learning}}, which has a high label requirement, and {\\\\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions -- a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.',\n",
       "  'id': '5435',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel $p$. In many problems, a good approximation of $p$ is not needed. For instance, if from one state-action pair $(s,a)$, one can only transit to states with the same value, learning $p(\\\\cdot|s,a)$ accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) we call the {\\\\em distribution-norm}. The distribution-norm w.r.t.~a measure $\\\\nu$ is defined on zero $\\\\nu$-mean functions $f$ by the standard variation of $f$ with respect to $\\\\nu$. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the generic but loose $||\\\\cdot||_1$ concentration inequalities used in most previous analysis of RL algorithms, to benefit from this new hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.',\n",
       "  'id': '5441',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied. We propose a policy-search-based approach coupled with predictive modelling to address this problem. This approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable. Our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.',\n",
       "  'id': '5446',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We describe how to use robust Markov decision processes for value function approximation with state aggregation. The robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration. This results in reducing the bounds on the gamma-discounted infinite horizon performance loss by a factor of 1/(1-gamma) while preserving polynomial-time computational complexity. Our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.',\n",
       "  'id': '5447',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/wildBootstrap.',\n",
       "  'id': '5452',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.',\n",
       "  'id': '5453',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We propose a technique for decomposing the parameter learning problem in Bayesian networks into independent learning problems. Our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset. We show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time. We explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.',\n",
       "  'id': '5471',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.',\n",
       "  'id': '5472',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach.',\n",
       "  'id': '5478',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding ?pseudo landmark points? to the classical Nystr?om kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystr?om kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%).',\n",
       "  'id': '5481',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O(1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.',\n",
       "  'id': '5490',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Many machine learning problems can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the trade off space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.',\n",
       "  'id': '5491',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise MRFs and Determinantal Point Processes. In particular, we present L-Field, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.',\n",
       "  'id': '5492',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We investigate the problem of stochastic network design in bidirected trees. In this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges. Actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources. Our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (FPTAS), that is, an algorithm that can find (1??)-optimal solutions for any problem instance in time polynomial in the input size and 1/?. Our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.',\n",
       "  'id': '5493',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We introduce a model-based excessive gap technique to analyze first-order primal- dual methods for constrained convex minimization. As a result, we construct first- order primal-dual methods with optimal convergence rates on the primal objec- tive residual and the primal feasibility gap of their iterates separately. Through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented Lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.',\n",
       "  'id': '5494',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.',\n",
       "  'id': '5496',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.',\n",
       "  'id': '5503',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms. Recently, Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the `true' posterior class probability) is available to a learning algorithm. In this work, we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitable `estimate' of the class probability, and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (TPR) and true negative rate (TNR), and for which the Bayes optimal classifier is the class probability function thresholded suitably. We use this template to derive consistency results for plug-in algorithms for the F-measure and for the geometric mean of TPR and precision; to our knowledge, these are the first such results for these measures. In addition, for continuous distributions, we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of TPR and TNR. Experimental results confirm our theoretical findings.\",\n",
       "  'id': '5504',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We present a theoretical analysis of F-measures for binary, multiclass and multilabel classification. These performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation, we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on F-measures, which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.',\n",
       "  'id': '5508',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.',\n",
       "  'id': '5523',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the art approach.',\n",
       "  'id': '5526',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of \\\\emph{adaptive feedback} naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the \\\\emph{blinded multi-armed bandit}, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.\",\n",
       "  'id': '5527',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading $(\\\\delta,\\\\rho)$-modes of the underlying distributions. A point is defined to be a $(\\\\delta,\\\\rho)$-mode if it is a local optimum of the density within a $\\\\delta$-neighborhood under metric $\\\\rho$. As we increase the ``scale'' parameter $\\\\delta$, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the $(\\\\delta,\\\\rho)$-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.\",\n",
       "  'id': '5533',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization ? in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs. We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erd?s-R?nyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results. Our work exploits a structural characterization of essential graphs by Andersson et al. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.',\n",
       "  'id': '5535',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.',\n",
       "  'id': '5537',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials. The work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial. It shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability. We furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.\",\n",
       "  'id': '5540',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people?s category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for probabilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.',\n",
       "  'id': '5541',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.',\n",
       "  'id': '5542',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure. We present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring. This new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data. The proposed formulation also allows to handle non-uniform blur with a straightforward extension. Experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method. One surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.',\n",
       "  'id': '5566',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of $\\\\ell_1$ penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing $\\\\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for $\\\\ell_1$-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.',\n",
       "  'id': '5576',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we develop a family of algorithms for optimizing superposition-structured? or ?dirty? statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.\"',\n",
       "  'id': '5578',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Life-logging video streams, financial time series, and Twitter tweets are a few examples of high-dimensional signals over practically unbounded time. We consider the problem of computing optimal segmentation of such signals by k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal. The coreset enables fast further analysis such as automatic summarization and analysis of such signals. A coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task -- in our case, segmentation of the stream. We show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points. More precisely, we construct a representation of size O(klog n /eps^2) that provides a (1+eps)-approximation for the sum of squared distances to any given k-piecewise linear function. Moreover, such coresets can be constructed in a parallel streaming approach. Our results rely on a novel eduction of statistical estimations to problems in computational geometry. We empirically evaluate our algorithms on very large synthetic and real data sets from GPS, video and financial domains, using 255 machines in Amazon cloud.',\n",
       "  'id': '5581',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is {\\\\it offline}, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is {\\\\it online}, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.',\n",
       "  'id': '5584',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents? voluntary investment decisions when facing potential direct risk and transfer risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.',\n",
       "  'id': '5585',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.',\n",
       "  'id': '5594',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However, the $\\\\ell_0$ or $\\\\ell_1$-norm sparsity constraint on the representation coefficients adopted in many DL methods makes the training and testing phases time consuming. We propose a new discriminative DL framework, namely projective dictionary pair learning (DPL), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods, the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.',\n",
       "  'id': '5600',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.',\n",
       "  'id': '5604',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set $U$ is the collection of all strings containing some string $u \\\\in U$ as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.',\n",
       "  'id': '5606',\n",
       "  'year': '2014'},\n",
       " {'abstract': \"Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.\",\n",
       "  'id': '5610',\n",
       "  'year': '2014'},\n",
       " {'abstract': 'Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.',\n",
       "  'id': '5636',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.',\n",
       "  'id': '5638',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.',\n",
       "  'id': '5640',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: \\\\url{http://idl.baidu.com/FM-IQA.html}.',\n",
       "  'id': '5641',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to  place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors,  ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground.  Our experiments  show significant performance gains over existing RGB and RGB-D object proposal methods  on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.',\n",
       "  'id': '5644',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The dynamics of simple decisions are well understood and modeled as a class of random walk models (e.g. Laming, 1968; Ratcliff, 1978; Busemeyer and Townsend, 1993; Usher and McClelland, 2001; Bogacz et al., 2006). However, most real-life decisions include a rich and dynamically-changing influence of additional information we call context. In this work, we describe a computational theory of decision making under dynamically shifting context. We show how the model generalizes the dominant existing model of fixed-context decision making (Ratcliff, 1978) and can be built up from a weighted combination of fixed-context decisions evolving simultaneously. We also show how the model generalizes re- cent work on the control of attention in the Flanker task (Yu et al., 2009). Finally, we show how the model recovers qualitative data patterns in another task of longstanding psychological interest, the AX Continuous Performance Test (Servan-Schreiber et al., 1996), using the same model parameters.',\n",
       "  'id': '5650',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets.  Therefore, a key step in understanding neural systems is to reliably distinguish  cell types.  An important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive.  Here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array.  We use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations). These classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (ON vs. OFF).  We then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm.  This can result in accurate, fully automated methods for cell type classification.',\n",
       "  'id': '5652',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.',\n",
       "  'id': '5661',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes (SGVB) with global model parameters. Regular SGVB estimators rely on sampling of parameters once per minibatch of data, and have variance that is constant w.r.t. the minibatch size. The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence.We find an important connection with regularization by dropout: the original Gaussian dropout objective corresponds to SGVB with local noise, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose \\\\emph{variational dropout}, a generalization of Gaussian dropout, but with a more flexibly parameterized posterior, often leading to better generalization. The method is demonstrated through several experiments.',\n",
       "  'id': '5666',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.',\n",
       "  'id': '5668',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs  at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of Ihler and McAllester (2009) at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.',\n",
       "  'id': '5674',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution. This algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution. This is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a Rao-Blackwellized estimate that has low variance. Our method works efficiently for both continuous and discrete random variables. Furthermore, the proposed algorithm has interesting similarities with Gibbs sampling but at the same time, unlike Gibbs sampling, can be trivially parallelized.',\n",
       "  'id': '5678',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.\",\n",
       "  'id': '5679',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'For weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is. Even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred. Here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely. In this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision. Our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference. These efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.',\n",
       "  'id': '5683',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.',\n",
       "  'id': '5686',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Causal structure learning from time series data is a major scientific challenge.  Existing algorithms assume that measurements occur sufficiently quickly; more precisely, they assume that the system and measurement timescales are approximately equal. In many scientific domains, however, measurements occur at a significantly slower rate than the underlying system changes.  Moreover, the size of the mismatch between timescales is often unknown. This paper provides three distinct causal structure learning algorithms, all of which discover all dynamic graphs that could explain the observed measurement data as arising from undersampling at some rate. That is, these algorithms all learn causal structure without assuming any particular relation between the measurement and system timescales; they are thus rate-agnostic. We apply these algorithms to data from simulations. The results provide insight into the challenge of undersampling.',\n",
       "  'id': '5689',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability. In this work, we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories, a class of problems that includes MAP inference in Markov Logic and similar statistical-relational languages. We introduce term symmetries, which are induced by an evidence set and extend to symmetries over a relational theory. We provide the important special case of term equivalent symmetries, showing that such symmetries can be found in low-degree polynomial time. We show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain. We demonstrate the effectiveness of these techniques through experiments in two relational domains. We also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning.',\n",
       "  'id': '5691',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$ iterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.',\n",
       "  'id': '5696',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with L1 and L1/L2 norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.',\n",
       "  'id': '5699',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Mixture modeling is a general technique for making any simple model more expressive through weighted combination.  This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models.  However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees.  Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist.  In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem.  We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra.  This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation.  Simulations show good empirical performance on several models.',\n",
       "  'id': '5702',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank $r$ reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank $r$ of a large $n\\\\times m$ matrix from $C(r)r\\\\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.',\n",
       "  'id': '5704',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received. The Stochastic Block-Model (SBM) is a generative model for graphs with communities for which, because of its simplicity, the theoretical understanding has advanced fast in recent years. In particular, there have been various results showing that simple versions of spectralclustering using the Normalized Laplacian of the graph can recoverthe communities almost perfectly with high probability. Here we show that essentially the same algorithm used for the SBM and for its extension called Degree-Corrected SBM, works on a wider class of Block-Models, which we call Preference Frame Models, with essentially the same guarantees. Moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.',\n",
       "  'id': '5708',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'One approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance.By extending the notion of \\\\emph{statistical leverage scores} to the setting of kernel ridge regression, we are able to identify a sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the \\\\emph{effective dimensionality} of the problem.  This latter quantity is often much smaller than previous bounds that depend on the \\\\emph{maximal degrees of freedom}. We give an empirical evidence supporting this fact. Our second contribution is to present a fast algorithm to quickly compute coarse approximations to thesescores in time linear in the number of samples. More precisely, the running time of the algorithm is $O(np^2)$ with $p$ only depending on the trace of the kernel matrix and the regularization parameter. This is obtained via a variant of squared length sampling that we adapt to the kernel setting. Lastly, we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem.',\n",
       "  'id': '5716',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we useour new analysis in three ways: (1) we derive convergence rates for the convex case (Hogwild) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called Buckwild, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.\",\n",
       "  'id': '5717',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This poster has been moved from Monday #86 to Thursday #101.\\n\\nStochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD).The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.',\n",
       "  'id': '5718',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently.   The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler substeps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence.  We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence.  We rigorously analyze our methods, and identify convergence rates.  Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.',\n",
       "  'id': '5723',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"This paper establishes a statistical versus computational trade-offfor solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the {\\\\em Sparse Principal Component Analysis} (Sparse PCA) problem, and the family of {\\\\em Sum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension $p$, a planted $k$-sparse unit vector can be {\\\\em in principle} detected using only $n \\\\approx k\\\\log p$ (Gaussian or Bernoulli) samples, but all {\\\\em efficient} (polynomial time) algorithms known require $n \\\\approx k^2 $ samples. It was also known that this quadratic gap cannot be improved by the the most basic {\\\\em semi-definite} (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or ``pseudo-expectations'') for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.\",\n",
       "  'id': '5724',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm which converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.',\n",
       "  'id': '5725',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of  regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squaresestimation comes without any tuning parameter and may hence be preferred due to its simplicity.',\n",
       "  'id': '5726',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing and completion. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.',\n",
       "  'id': '5733',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko, Martinsson, and Tropp, randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for *any* matrix, independently of singular value gaps. After ~O(1/epsilon) iterations, it gives a low-rank approximation within (1+epsilon) of optimal for spectral norm error.We give the first provable runtime improvement on Simultaneous Iteration: a randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just ~O(1/sqrt(epsilon)) iterations and performs substantially better experimentally. Our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice.Furthermore, while it is a simple accuracy benchmark, even (1+epsilon) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods.',\n",
       "  'id': '5735',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Submodular and supermodular functions have found wide applicability in machine learning, capturing  notions such as diversity and regularity, respectively. These notions have deep consequences for optimization, and the problem of (approximately) optimizing submodular functions has received much attention. However, beyond optimization, these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference. Prominent, well-studied special cases include Ising models and determinantal point processes, but the general class of log-submodular and log-supermodular models is much richer and little studied. In this paper, we investigate the use of Markov chain Monte Carlo sampling to perform approximate inference in general log-submodular and log-supermodular models. In particular, we consider a simple Gibbs sampling procedure, and establish two sufficient conditions, the first guaranteeing polynomial-time, and the second fast (O(nlogn)) mixing. We also evaluate the efficiency of the Gibbs sampler on three examples of such models, and compare against a recently proposed variational approach.',\n",
       "  'id': '5744',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Variational inference is an efficient, popular heuristic used in the context of latent variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. Our initializations are natural, one of them being used in LDA-c, the mostpopular implementation of variational inference.In addition to providing intuition into why this heuristic might work in practice, the multiplicative, rather than additive nature of the variational inference updates forces us to usenon-standard proof arguments, which we believe might be of general theoretical interest.',\n",
       "  'id': '5746',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U-statistic of interest. We establish convergence rate bounds of O(1 / t) and O(log t / t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.',\n",
       "  'id': '5747',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This paper formulates the search for a set of bounding boxes (as needed in object proposal generation) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image. Since the number of possible bounding boxes in an image is very large $O(#pixels^2)$, even a single linear scan to perform the greedy augmentation for submodular maximization is intractable. Thus, we formulate the greedy augmentation step as a Branch-and-Bound scheme. In order to speed up repeated application of B\\\\&B, we propose a novel generalization of Minoux?s ?lazy greedy? algorithm to the B\\\\&B tree. Theoretically, our proposed formulation provides a new understanding to the problem, and contains classic heuristic approaches such as Sliding Window+Non-Maximal Suppression (NMS) and and Efficient Subwindow Search (ESS) as special cases. Empirically, we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure.',\n",
       "  'id': '5779',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9?, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13?, from 138 million to 10.3 million, again with no loss of accuracy.',\n",
       "  'id': '5784',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Neural population activity often exhibits rich variability. This variability is thought to arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as  from modulations of neural firing properties on long time-scales, often referred to as non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a  hierarchical dynamics model that is able to capture inter-trial modulations in firing rates, as well as neural population dynamics. We derive an algorithm for Bayesian Laplace propagation for fast posterior inference, and demonstrate that our model provides a better account of the structure of neural firing than existing stationary dynamics models, when applied to neural population recordings from primary visual cortex.',\n",
       "  'id': '5790',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights.  We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization.  Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.',\n",
       "  'id': '5797',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlinespartitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and real world datasets demonstrate the benefit of our method.',\n",
       "  'id': '5800',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose the ?-return as an alternative to the ?-return currently used by the TD(?) family of algorithms. The benefit of the ?-return is that it accounts for the correlation of different length returns. Because it is difficult to compute exactly, we suggest one way of approximating the ?-return. We provide empirical studies that suggest that it is superior to the ?-return and ?-return for a variety of problems.',\n",
       "  'id': '5807',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points.We propose a simple, computationally efficient, and memory friendly algorithm based on the ``doubly stochastic gradients'' to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the \\\\emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate $\\\\Otil(1/t)$ to the global optimum, even for the top $k$ eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.\",\n",
       "  'id': '5813',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient \\\\underline{H}ybrid \\\\underline{O}ptimization algorithm for \\\\underline{NO}n convex \\\\underline{R}egularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2)  We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.',\n",
       "  'id': '5829',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose CombEXP, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.',\n",
       "  'id': '5831',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Elicitation is the study of statistics or properties which are computable via empirical risk minimization.  While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question---all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable.  Specifically, what is the minimum number of regression parameters needed to compute the property?Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation.  We establish several general results and techniques for proving upper and lower bounds on elicitation complexity.  These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.',\n",
       "  'id': '5832',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"We study the performance of standard online learning algorithms when the feedback is delayed by an adversary. We show that \\\\texttt{online-gradient-descent} and \\\\texttt{follow-the-perturbed-leader} achieve regret $O(\\\\sqrt{D})$ in the delayed setting, where $D$ is the sum of delays of each round's feedback. This bound collapses to an optimal $O(\\\\sqrt{T})$ bound in the usual setting of no delays (where $D = T$).  Our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback, making adjustments to the analysis and not to the algorithms themselves.  Our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model.\",\n",
       "  'id': '5833',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'In this paper, we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the alpha-divergence. Small experiments show that the proposed estimator attains comparable performance to the MLE with drastically lower computational cost.',\n",
       "  'id': '5837',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree structured sparsity over the features, where each node encodes a group of features. It has been applied successfully in many real-world applications. However, with extremely large feature dimensions, solving TGL remains a significant challenge due to its highly complicated regularizer. In this paper, we propose a novel Multi-Layer Feature reduction method (MLFre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes. By a novel hierarchical projection algorithm, MLFre is able to test the nodes independently from any of their ancestor nodes. Moreover, we can integrate MLFre---that has a low computational cost---with any existing solvers. Experiments on both synthetic and real data sets demonstrate that the speedup gained by MLFre can be orders of magnitude.',\n",
       "  'id': '5838',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Given samples from an unknown  distribution, p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has receivedtremendous attention in Statistics, albeit focusing onasymptotic analysis, as well as in Computer Science, wherethe emphasis has been on small sample size and computationalcomplexity. Nevertheless, even for basic classes ofdistributions such as monotone, log-concave, unimodal, and monotone hazard rate, the optimal sample complexity is unknown.We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem:Given samplesfrom an unknown distribution p, and a known distribution q, are p and q close in Chi^2-distance, or far in total variation distance?The optimality of all testers is established by providing matching lower bounds. Finally, a necessary building block for our tester and important byproduct of our work are the first known computationally efficient proper learners for discretelog-concave and monotone hazard rate distributions. We exhibit the efficacy of our testers via experimental analysis.',\n",
       "  'id': '5839',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of $\\\\widetilde{O}(T^{5/6})$, while the best known lower bound is $\\\\Omega(T^{1/2})$. Many attemptshave been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case, the best known algorithm guarantees a regret of $\\\\widetilde{O}(T^{2/3})$. We present an efficient algorithm for the banditsmooth convex optimization problem that guarantees a regret of $\\\\widetilde{O}(T^{5/8})$. Our result rules out an $\\\\Omega(T^{2/3})$ lower bound and takes a significant step towards the resolution of this open problem.',\n",
       "  'id': '5842',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation reaches a competitive 18.6\\\\% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18\\\\% PER in single utterances and 20\\\\% in 10-times longer (repeated) utterances.  Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6\\\\% level.',\n",
       "  'id': '5847',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization.  Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN.  The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data.  Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.',\n",
       "  'id': '5849',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.',\n",
       "  'id': '5850',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations, often after a single trial. While the mechanism for rapid path planning is unknown, the CA3 region in the hippocampus plays an important role, and emerging evidence suggests that place cell activity during hippocampal preplay periods may trace out future goal-directed trajectories. Here, we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment, based only on the mapped representation of the goal. We show that this representation can be implemented in a neural attractor network model, resulting in bump--like activity profiles resembling those of the CA3 region of hippocampus. Neurons tend to locally excite neurons with similar place field centers, while inhibiting other neurons with distant place field centers, such that stable bumps of activity can form at arbitrary locations in the environment. The network is initialized to represent a point in the environment, then weakly stimulated with an input corresponding to an arbitrary goal location. We show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location. Indeed, in networks with large place fields, we show that the network properties cause the bump to move smoothly from its initial location to the goal, around obstacles or walls. Our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.',\n",
       "  'id': '5856',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an ?on-the-job? setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets-- named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning.',\n",
       "  'id': '5860',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a classifier for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier end-to-end. Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.\",\n",
       "  'id': '5864',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation.  The system, called NEXT,  provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments.  The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.',\n",
       "  'id': '5868',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We consider the task of building compact deep learning pipelines suitable for deploymenton storage and power constrained mobile devices. We propose a uni-fied framework to learn a broad family of structured parameter matrices that arecharacterized by the notion of low displacement rank. Our structured transformsadmit fast function and gradient evaluation, and span a rich range of parametersharing configurations whose statistical modeling capacity can be explicitly tunedalong a continuum from structured to unstructured. Experimental results showthat these transforms can significantly accelerate inference and forward/backwardpasses during training, and offer superior accuracy-compactness-speed tradeoffsin comparison to a number of existing techniques. In keyword spotting applicationsin mobile speech recognition, our methods are much more effective thanstandard linear low-rank bottleneck layers and nearly retain the performance ofstate of the art models, while providing more than 3.5-fold compression.',\n",
       "  'id': '5869',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.',\n",
       "  'id': '5871',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models.  The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic.  For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.',\n",
       "  'id': '5881',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Existing inverse reinforcement learning (IRL) algorithms have assumed each expert?s demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts? behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model, an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert?s demonstrated trajectories. As a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL, which is, interestingly, a reduced variant of our approach.',\n",
       "  'id': '5882',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial.  Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise.  In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices.  We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework.  We show how previous continuous-dynamic samplers can be trivially reinvented in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC).  Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.',\n",
       "  'id': '5891',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal  running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH (Andoni-Indyk-Nguyen-Razenshteyn 2014) (Andoni-Razenshteyn 2015)), our algorithm is also practical, improving upon the well-studied hyperplane LSH (Charikar 2002) in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.',\n",
       "  'id': '5893',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent  approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the  number of tasks as an eigendecomposition is required in each step. Using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.',\n",
       "  'id': '5898',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We consider the following multi-component sparse PCA problem:given a set of data points, we seek to extract a small number of sparse components with \\\\emph{disjoint} supports that jointly capture the maximum possible variance.Such components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but this greedy procedure is suboptimal.We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to $1$ from the optimal.Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem.Its complexity grows as a low order polynomial in the ambient dimension of the input data, but exponentially in its rank.However, it can be effectively applied on a low-dimensional sketch of the input data.We evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.',\n",
       "  'id': '5901',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce a unifying generalization of the Lov?sz theta function, and the associated geometric embedding, for graphs with weights on both nodes and edges. We show how it can be computed exactly by semidefinite programming, and how to approximate it using SVM computations. We show how the theta function can be interpreted as a measure of diversity in graphs and use this idea, and the graph embedding in algorithms for Max-Cut, correlation clustering and document summarization, all of which are well represented as problems on weighted graphs.',\n",
       "  'id': '5902',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Bayesian networks are a popular representation of asymmetric (for example causal) relationships between random variables. Markov random fields (MRFs) are a complementary model of symmetric relationships used in computer vision, spatial modeling, and social and gene expression networks.  A chain graph model under the Lauritzen-Wermuth-Frydenberg interpretation (hereafter a chain graph model) generalizes both Bayesian networks and MRFs, and can represent asymmetric and symmetric relationships together.As in other graphical models, the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model.  One recent approach to the study of marginal graphical models is to consider a well-behaved supermodel.  Such a supermodel of marginals of Bayesian networks, defined only by conditional independences, and termed the ordinary Markov model, was studied at length in (Evans and Richardson, 2014).In this paper, we show that special mixed graphs which we call segregated graphs can be associated, via a Markov property, with supermodels of a marginal of chain graphs defined only by conditional independences.  Special features of segregated graphs imply the existence of a very natural factorization for these supermodels, and imply many existing results on the chain graph model, and ordinary Markov model carry over.  Our results suggest that segregated graphs define an analogue of the ordinary Markov model for marginals of chain graph models.',\n",
       "  'id': '5904',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The stochastic block model (SBM) has recently gathered significant attention due to new threshold phenomena. However, most developments rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in Abbe-Sandon FOCS15. In the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees. This lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees, and the model parameters are learned efficiently. For the logarithmic degree regime, this is further enhanced into a fully agnostic algorithm that achieves the CH-limit for exact recovery in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the SBM.',\n",
       "  'id': '5906',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.',\n",
       "  'id': '5920',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take `away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that has been successfully applied in practice: FW with away steps, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence under a weaker condition than strong convexity. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of the `condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.\",\n",
       "  'id': '5925',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants.  For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.',\n",
       "  'id': '5928',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.',\n",
       "  'id': '5937',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We study matrix completion problem with side information.  Side information has been considered in several matrix completion applications, and is generally shown to be useful empirically.  Recently, Xu et al. studied the effect of side information for matrix completion under a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features.  However, since in reality most given features are noisy or even weakly informative, how to develop a general model to handle general feature set, and how much the noisy features can help matrix recovery in theory, is still an important issue to investigate. In this paper, we propose a novel model that balances between features and observations simultaneously, enabling us to leverage feature information yet to be robust to feature noise.  Moreover, we study the effectof general features in theory, and show that by using our model, the sample complexity can still be lower than matrix completion as long as features are sufficiently informative. This result provides a theoretical insight of usefulness for general side information.  Finally, we consider synthetic data and two real applications - relationship prediction and semi-supervised clustering, showing that our model outperforms other methods for matrix completion with features both in theory and practice.',\n",
       "  'id': '5940',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"We present the Mind the Gap Model (MGM), an approach for interpretable feature extraction and selection.  By placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence.  It also maintains or improves performance when compared to related approaches.  We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.\",\n",
       "  'id': '5957',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags.',\n",
       "  'id': '5959',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"Subunit models provide a powerful yet parsimonious description of  neural spike responses to complex stimuli. They can be expressed by  a cascade of two linear-nonlinear (LN) stages, with the first linear  stage defined by convolution with one or more filters.  Recent  interest in such models has surged due to their biological  plausibility and accuracy for characterizing early sensory  responses. However, fitting subunit models poses a difficult  computational challenge due to the expense of evaluating the  log-likelihood and the ubiquity of local optima.  Here we address  this problem by forging a theoretical connection between  spike-triggered covariance analysis and nonlinear subunit models.  Specifically, we show that a ''convolutional'' decomposition of the  spike-triggered average (STA) and covariance (STC) provides an  asymptotically efficient estimator for the subunit model under  certain technical conditions. We also prove the identifiability of  such convolutional decomposition under mild assumptions.  Our  moment-based methods outperform highly regularized versions of the  GQM on neural data from macaque primary visual cortex, and achieves  nearly the same prediction performance as the full  maximum-likelihood estimator, yet with substantially lower cost.\",\n",
       "  'id': '5962',\n",
       "  'year': '2015'},\n",
       " {'abstract': \"We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm.On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique of deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.RFN package for GPU/CPU is available at http://www.bioinf.jku.at/software/rfn.\",\n",
       "  'id': '5963',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.',\n",
       "  'id': '5964',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Multi-output Gaussian processes provide a convenient framework for multi-task problems.  An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels.  Recently, Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework.  In this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (CSM) kernel.  This new, flexible kernel represents both the power and phase relationship between multiple observation channels.  We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model, where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel.  Results are presented for measured multi-region electrophysiological data.',\n",
       "  'id': '5966',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.',\n",
       "  'id': '5970',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs in order to identify the direct causes and effects of a target variable. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in one target variable (such as class labels). We propose a new local causal discovery algorithm, called Causal Markov Blanket (CMB), to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.',\n",
       "  'id': '5974',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'By making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services. However, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users. Two central but less explored questions are how to recommend the most desirable item \\\\emph{at the right moment}, and how to predict \\\\emph{the next returning time} of a user to a service. To address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs. We show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains $O(1 / \\\\epsilon)$ convergence rate, scales up to problems with millions of user-item pairs and thousands of millions of temporal events. Compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation questions. Finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.',\n",
       "  'id': '5979',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Both learning and inference tasks on Bayesian networks are NP-hard in general. Bounded tree-width Bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however, while inference on bounded tree-width networks is tractable, the learning problem remains NP-hard even for tree-width~2. In this paper, we propose bounded vertex cover number Bayesian networks as an alternative to bounded tree-width networks. In particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound $k$, in contrast to the general and bounded tree-width cases; on the other hand, we also show that learning problem is W[1]-hard in parameter $k$. Furthermore, we give an alternative way to learn bounded vertex cover number Bayesian networks using integer linear programming (ILP),  and show this is feasible in practice.',\n",
       "  'id': '6003',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'Within a statistical learning setting,  we propose and study an iterative regularization algorithm for least squares defined by  an incremental gradient method.   In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and  prove strong universal consistency, i.e.  almost sure convergence of the risk, as well as  sharp finite sample bounds for the iterates. Our  results are a step towards understanding the effect of multiple epochs in  stochastic gradient techniques in machine learning and rely  on  integrating  statistical and optimizationresults.',\n",
       "  'id': '6015',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results.  Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K^2 log T) without requiring such assumptions.  Our results offer the best of both worlds: O(K log T) bounds without restrictive assumptions.',\n",
       "  'id': '6023',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretic results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.',\n",
       "  'id': '6025',\n",
       "  'year': '2015'},\n",
       " {'abstract': 'How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks?  This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model?s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.',\n",
       "  'id': '6039',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"Many information systems use tags and keywords to describe and annotate content. These allow for efficient organization and categorization of items, as well as facilitate relevant search queries. As such, the selected set of tags for an item can have a considerable effect on the volume of traffic that eventually reaches an item.  In tagging systems where tags are exclusively chosen by an item's owner, who in turn is interested in maximizing traffic, a principled approach for assigning tags can prove valuable. In this paper we introduce the problem of optimal tagging, where the task is to choose a subset of tags for a new item such that the probability of browsing users reaching that item is maximized.  We formulate the problem by modeling traffic using a Markov chain, and asking how transitions in this chain should be modified to maximize traffic into a certain state of interest. The resulting optimization problem involves maximizing a certain function over subsets, under a cardinality constraint.  We show that the optimization problem is NP-hard, but has a (1-1/e)-approximation via a simple greedy algorithm due to monotonicity and submodularity. Furthermore, the structure of the problem allows for an efficient computation of the greedy step. To demonstrate the effectiveness of our method, we perform experiments on three tagging datasets, and show that the greedy algorithm outperforms other baselines.\",\n",
       "  'id': '6041',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.',\n",
       "  'id': '6042',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'A sampling-based optimization method for quadratic functions is   proposed. Our method approximately solves the following   $n$-dimensional quadratic minimization problem in constant time,   which is independent of $n$:   $z^*=\\\\min_{\\\\bv \\\\in \\\\bbR^n}\\\\bracket{\\\\bv}{A \\\\bv} +   n\\\\bracket{\\\\bv}{\\\\diag(\\\\bd)\\\\bv} + n\\\\bracket{\\\\bb}{\\\\bv}$,   where $A \\\\in \\\\bbR^{n \\\\times n}$ is a matrix and $\\\\bd,\\\\bb \\\\in \\\\bbR^n$   are vectors. Our theoretical analysis specifies the number of   samples $k(\\\\delta, \\\\epsilon)$ such that the approximated solution   $z$ satisfies $|z - z^*| = O(\\\\epsilon n^2)$ with probability   $1-\\\\delta$. The empirical performance (accuracy and runtime) is   positively confirmed by numerical experiments.',\n",
       "  'id': '6044',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima \\\\--- all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with \\\\textit{arbitrary} initialization in polynomial time.',\n",
       "  'id': '6048',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"This paper presents a dynamical system based on the Poisson-Gamma construction for sequentially observed multivariate count data.  Inherent to the model is a novel Bayesian nonparametric prior that ties and shrinks parameters in a powerful way. We develop theory about the model's infinite limit and its steady-state.  The model's inductive bias is demonstrated on a variety of real-world datasets where it is shown to learn interpretable structure and have superior predictive performance.\",\n",
       "  'id': '6083',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a ?teacher? algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.',\n",
       "  'id': '6105',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results.',\n",
       "  'id': '6109',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when the feasible set is a polytope, and the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: i) large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration ii) the worst case convergence rate depends unfavorably on the dimension In this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular, both memory and computation overheads are only linear in the dimension, and in addition, in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous works, with a linear dependence on the number of non-zeros in the optimal solution At the heart of our method, and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence that shows that our method delivers state-of-the-art performance.',\n",
       "  'id': '6115',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes. In this paper, we consider an alternate model that treats individual opinions as spins in an Ising system at dynamic equilibrium. We formalize the \\\\textit{Ising influence maximization} problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field. Under the mean-field (MF) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the MF magnetization is concave and our algorithm converges to a global optimum. We apply our algorithm on random and real-world networks, demonstrating, remarkably, that the MF optimal external fields (i.e., the external fields which maximize the MF magnetization) exhibit a phase transition from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures. We also establish a number of novel results about the structure of steady-states in the ferromagnetic MF Ising model on general graphs, which are of independent interest.',\n",
       "  'id': '6119',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: Our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.',\n",
       "  'id': '6125',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Finally, numerical evidence supports the effectiveness of our method in real-world problems.',\n",
       "  'id': '6127',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models.  It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it.  These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime.  Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs.',\n",
       "  'id': '6132',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1+eps)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms; it is satisfied even by the vanilla Hedge forecaster. Our results improve upon recent work of Syrgkanis et al. in a number of ways. We require only that players observe payoffs under other players\\' realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and show convergence under bandit feedback. Finally, we improve upon the speed of convergence by a factor of n, the number of players. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work. Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of Lykouris et al. in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved. In the bandit setting we present a new algorithm which provides a \"small loss\"-type bound with improved dependence on the number of actions in utility settings, and is both simple and efficient. This result may be of independent interest.',\n",
       "  'id': '6133',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis uses techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of arms pulled.',\n",
       "  'id': '6137',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain \"correctness\" guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.',\n",
       "  'id': '6140',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.',\n",
       "  'id': '6145',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We consider the \\\\emph{Threshold Bandit} setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \\\\emph{threshold value}. The learner selects one of $K$ actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the \\\\emph{uncensored} and \\\\emph{censored} case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically.',\n",
       "  'id': '6149',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'How does our motor system solve the problem of anticipatory control in spite of a wide spectrum of response dynamics from different musculo-skeletal systems, transport delays as well as response latencies throughout the central nervous system? To a great extent, our highly-skilled motor responses are a result of a reactive feedback system, originating in the brain-stem and spinal cord, combined with a feed-forward anticipatory system, that is adaptively fine-tuned by sensory experience and originates in the cerebellum. Based on that interaction we design the counterfactual predictive control (CFPC) architecture, an anticipatory adaptive motor control scheme in which a feed-forward module, based on the cerebellum, steers an error feedback controller with counterfactual error signals. Those are signals that trigger reactions as actual errors would, but that do not code for any current of forthcoming errors. In order to determine the optimal learning strategy, we derive a novel learning rule for the feed-forward module that involves an eligibility trace and operates at the synaptic level. In particular, our eligibility trace provides a mechanism beyond co-incidence detection in that it convolves a history of prior synaptic inputs with error signals. In the context of cerebellar physiology, this solution implies that Purkinje cell synapses should generate eligibility traces using a forward model of the system being controlled. From an engineering perspective, CFPC provides a general-purpose anticipatory control architecture equipped with a learning rule that exploits the full dynamics of the closed-loop system.',\n",
       "  'id': '6151',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.',\n",
       "  'id': '6163',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\\\bm{108}\\\\times$ and $\\\\bm{17.7}\\\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.',\n",
       "  'id': '6165',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-$?sqrt{n}$ convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e.,  when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces.',\n",
       "  'id': '6174',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We show how to estimate a model?s test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently compute gradients of the estimated error and hence perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as conditional random fields.',\n",
       "  'id': '6201',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.',\n",
       "  'id': '6202',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We analyze the learning  properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that    for  a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds  can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on  a unifying approach, encompassing both batch and stochastic gradient methods as special cases.',\n",
       "  'id': '6213',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speeded behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes.      We have conducted a large-scale psychophysics study to assess the correlation between computational models and human behavioral responses on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three state-of-the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages.       Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed the complexity of those used by human participants during rapid categorization.',\n",
       "  'id': '6218',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Inference in Markov random fields subject to consistency structure is a fundamental problem that arises in many real-life applications. In order to enforce consistency, classical approaches utilize consistency potentials or encode constraints over feasible instances. Unfortunately this comes at the price of a serious computational bottleneck. In this paper we suggest to tackle consistency by incorporating constraints on beliefs. This permits derivation of a closed-form message-passing algorithm which we refer to as the Constraints Based Convex Belief Propagation (CBCBP). Experiments show that CBCBP outperforms the standard approach while being at least an order of magnitude faster.',\n",
       "  'id': '6219',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In this paper, we address the problems of identifying linear structural equation models and discovering the constraints they imply. We first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification and constraint discovery power. Finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.',\n",
       "  'id': '6223',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced. The proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing. Moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm. Numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time.',\n",
       "  'id': '6239',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Principal components analysis~(PCA) is the optimal linear  encoder of data. Sparse linear encoders (e.g., sparse PCA) produce more interpretable features that  can promote better generalization. (\\\\rn{1}) Given a level of sparsity, what is the best approximation to PCA?  (\\\\rn{2}) Are there efficient algorithms which can achieve this optimal  combinatorial tradeoff? We answer both questions by  providing the first polynomial-time algorithms to construct \\\\emph{optimal} sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data.',\n",
       "  'id': '6252',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment-matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.',\n",
       "  'id': '6255',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.',\n",
       "  'id': '6256',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Our method aims at reasoning over natural language questions and visual images. Given a natural language question about an image, our model updates the question representation iteratively by selecting image regions relevant to the query and learns to give the correct answer. Our model contains several reasoning layers, exploiting complex visual relations in the visual question answering (VQA) task. The proposed network is end-to-end trainable through back-propagation, where its weights are initialized using pre-trained convolutional neural network (CNN) and gated recurrent unit (GRU). Our method is evaluated on challenging datasets of COCO-QA and VQA and yields state-of-the-art performance.',\n",
       "  'id': '6261',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features describing the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods, this model does not require the low-rank condition on the model parameter matrix. We prove that when the side features can span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is $O(\\\\log N)$ where $N$ is the size of the matrix. When the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an $\\\\epsilon$-recovery with $O(\\\\log N)$ sample complexity, and maintains a $\\\\O(N^{3/2})$ rate similar to classfic methods with no side information. An efficient linearized Lagrangian algorithm is developed with a strong guarantee of convergence. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets.',\n",
       "  'id': '6265',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements.  With noisy measurements we show all local minima are very close to a global optimum.  Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\\\\em  from random initialization}.',\n",
       "  'id': '6271',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to overfitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low--rank parameters.  Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that  the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a $\\\\log$ factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain--regions and cognitive neuroscience terms.',\n",
       "  'id': '6272',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.',\n",
       "  'id': '6281',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages, WebPPL and Stan, and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in WebPPL and Stan.',\n",
       "  'id': '6290',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of N items. They have recently gained prominence in several applications that rely on diverse subsets. However, their applicability to large problems is still limited due to O(N^3) complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.',\n",
       "  'id': '6296',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment.  In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.',\n",
       "  'id': '6307',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Unsupervised learning of structured predictors has been a long standing pursuit in machine learning.  Recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred.  Aside from being nonconvex, it also requires the demanding inference of normalization.  In this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally.  We further expand its applicability by resorting to a weaker form of inference---maximum a-posteriori.  The flexibility of the model is demonstrated on two structures based on total unimodularity---graph matching and linear chain.  Experimental results confirm the promise of the method.',\n",
       "  'id': '6314',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM).  In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP  is a deterministic method, which is typically fast,  empirically very successful, however in general lacking control of accuracy over loopy graphs.  In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e.,  we provide a way to compensate for BP errors via a consecutive BP-aware MCMC.  Our framework is based on the Loop Calculus (LC) approach  which allows to express the BP error  as a sum of weighted generalized loops. Although the full series is computationally intractable,  it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we, first, propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models.  Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then  design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme  for approximating the full series.  The main novelty underlying our design  is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon  the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC  outperforms both direct MCMC and bare BP schemes.',\n",
       "  'id': '6318',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.\",\n",
       "  'id': '6331',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an $\\\\ell_1$-norm constraint. By \"structured\", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning  work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models.',\n",
       "  'id': '6352',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.',\n",
       "  'id': '6353',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice.',\n",
       "  'id': '6356',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.',\n",
       "  'id': '6358',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.',\n",
       "  'id': '6368',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Learning accurate prior knowledge of natural images is of great importance for single image super-resolution (SR). Existing SR methods either learn the prior from the low/high-resolution patch pairs or estimate the prior models from the input low-resolution (LR) image. Specifically, high-frequency details are learned in the former methods. Though effective, they are heuristic and have limitations in dealing with blurred LR images; while the latter suffers from the limitations of frequency aliasing. In this paper, we propose to combine those two lines of ideas for image super-resolution. More specifically, the parametric sparse prior of the desirable high-resolution (HR) image patches are learned from both the input low-resolution (LR) image and a training image dataset. With the learned sparse priors, the sparse codes and thus the HR image patches can be accurately recovered by solving a sparse coding problem. Experimental results show that the proposed SR method outperforms existing state-of-the-art methods in terms of both subjective and objective image qualities.',\n",
       "  'id': '6378',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 82.2% to 87.8% and from 88% accuracy to 95% accuracy on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.',\n",
       "  'id': '6385',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties.  First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise.  Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells.  Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes.  These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.\",\n",
       "  'id': '6388',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free  methods in imitating complex behaviors in large, high-dimensional environments.\",\n",
       "  'id': '6391',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points $N$, namely $O(\\\\sqrt{N})$. The second algorithm illustrates how the classical mistake bound of $O(\\\\frac{1}{\\\\gamma^2})$ can be further improved to $O(\\\\frac{1}{\\\\sqrt{\\\\gamma}})$ through quantum means, where $\\\\gamma$ denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.',\n",
       "  'id': '6401',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores (r(44) = ?0.82 ? 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = ?0.19 ? 0.13, p = 0.0774) in forced fixation search; and we extend foveation to other clutter models showing stronger correlations in all cases. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. Code for building peripheral representations is  available.',\n",
       "  'id': '6404',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of  under-sampled data in k-space, and accelerating the data acquisition in MRI.  To improve the current MRI system in reconstruction accuracy and computational speed,  in this paper, we propose a novel deep architecture, dubbed ADMM-Net.  ADMM-Net is defined over a data flow graph, which is derived from the iterative  procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the  training data for CS-based reconstruction task. Experiments on MRI image  reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction  accuracies with fast computational speed.',\n",
       "  'id': '6406',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"In this paper, we develop a novel {\\\\bf ho}moto{\\\\bf p}y  {\\\\bf s}moothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and  a smooth term or  a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is $O(1/\\\\epsilon)$ without any assumption on the strong convexity. In this work, we will show that the proposed  HOPS achieved a lower iteration complexity of $\\\\tilde O(1/\\\\epsilon^{1-\\\\theta})$ with $\\\\theta\\\\in(0,1]$ capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption.  The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and $\\\\ell_1$ norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods.\",\n",
       "  'id': '6407',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'The recursive teaching dimension (RTD) of a concept class $C \\\\subseteq \\\\{0, 1\\\\}^n$, introduced by Zilles et al. [ZLHZ11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of $C$ in the recursive teaching model. In this paper, we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension.  Given a concept class $C \\\\subseteq \\\\{0, 1\\\\}^n$ with $VCD(C) = d$, we first show that $RTD(C)$ is at most $d 2^{d+1}$. This is the first upper bound for $RTD(C)$ that depends only on $VCD(C)$, independent of the size of the concept class $|C|$ and its~domain size $n$. Before our work, the best known upper bound for $RTD(C)$ is $O(d 2^d \\\\log \\\\log |C|)$, obtained by Moran et al. [MSWY15]. We remove the $\\\\log \\\\log |C|$ factor.  We also improve the lower bound on the worst-case ratio of $RTD(C)$ to $VCD(C)$. We present a family of classes $\\\\{ C_k \\\\}_{k \\\\ge 1}$ with $VCD(C_k) = 3k$ and $RTD(C_k)=5k$, which implies that the ratio of $RTD(C)$ to $VCD(C)$ in the worst case can be as large as $5/3$. Before our work, the largest ratio known was $3/2$ as obtained by Kuhlmann [Kuh99]. Since then, no finite concept class $C$ has been known to satisfy $RTD(C) > (3/2) VCD(C)$.',\n",
       "  'id': '6412',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to -- i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information.  These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on  several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.',\n",
       "  'id': '6414',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We investigate the statistical performance and computational efficiency of the  alternating minimization procedure for nonparametric tensor learning. Tensor modeling has been widely used for capturing the higher order relations between  multimodal data sources. In addition to a linear model,  a nonlinear tensor model has been received much attention recently because of its high flexibility. We consider  an alternating minimization procedure for  a general nonlinear model where the true function  consists of components in a reproducing kernel Hilbert space (RKHS). In this paper, we show that the alternating minimization method achieves linear convergence as an optimization algorithm  and that the generalization error of the resultant estimator yields the minimax optimality. We apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.',\n",
       "  'id': '6419',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.',\n",
       "  'id': '6423',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective.  Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra.  We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.',\n",
       "  'id': '6426',\n",
       "  'year': '2016'},\n",
       " {'abstract': \"We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.\",\n",
       "  'id': '6427',\n",
       "  'year': '2016'},\n",
       " {'abstract': 'Computing partition function is the most important statistical inference task arising in applications of Graphical Models (GM). Since it is computationally intractable, approximate  methods have been used in practice, where mean-field  (MF) and belief propagation (BP) are arguably the most popular and successful approaches of a variational type. In this paper, we propose two new variational schemes, coined Gauged-MF (G-MF) and Gauged-BP (G-BP), improving MF and BP, respectively. Both provide lower bounds for the partition function by utilizing the so-called gauge transformation which modifies factors of GM while keeping the partition function invariant. Moreover, we prove that both G-MF and G-BP are exact for GMs with a single loop of a special structure, even though the bare MF and BP perform badly in this case. Our extensive experiments indeed confirm that the proposed algorithms outperform and generalize MF and BP.',\n",
       "  'id': '6881',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Calcium imaging is a prominent technology in neuroscience research which allows for simultaneous recording of large numbers of neurons in awake animals. Automated extraction of neurons and their temporal activity in imaging datasets is an important step in the path to producing neuroscience results. However, nearly all imaging datasets typically contain gross contaminating sources which could be contributed by the technology used, or the underlying biological tissue. Although attempts were made to better extract neural signals in limited gross contamination scenarios, there has been no effort to address contamination in full generality through statistical estimation. In this work, we proceed in a new direction and propose to extract cells and their activity using robust estimation. We derive a minimax optimal robust loss based on a simple abstraction of calcium imaging data, and also find a simple and practical optimization routine for this loss with provably fast convergence. We use our proposed robust loss in a matrix factorization framework to extract the neurons and their temporal activity in calcium imaging datasets. We demonstrate the superiority of our robust estimation approach over existing methods on both simulated and real datasets.',\n",
       "  'id': '6883',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.',\n",
       "  'id': '6893',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate ``where'' and ``what'' processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object.    This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets: pedestrian tracking on the KTH activity recognition dataset and the more difficult KITTI object tracking dataset.\",\n",
       "  'id': '6898',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"The paper addresses the classical network tomography problem of inferring local traffic given origin-destination observations. Focussing on large complex public transportation systems, we build a scalable model that exploits input-output information to estimate the unobserved link/station loads and the users path preferences. Based on the reconstruction of the users' travel time distribution, the model is flexible enough to capture possible different path-choice strategies and correlations between users travelling on similar paths at similar times. The corresponding likelihood function is intractable for medium or large-scale networks and we propose two distinct strategies, namely the exact maximum-likelihood inference of an approximate but tractable model and the variational inference of the original intractable model. As an application of our approach, we consider the emblematic case of the London Underground network, where a tap-in/tap-out system tracks the start/exit time and location of all journeys in a day. A set of synthetic simulations and real data provided by Transport For London are used to validate and test the model on the predictions of observable and unobservable quantities.\",\n",
       "  'id': '6899',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.',\n",
       "  'id': '6908',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance.  Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program.  This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model.  The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm.  While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner.  In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model.  To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors as inputs, we show that if the network response can be described using a maximum number of $s$ non-zero weights per node, these weights can be learned from $O(s\\\\log N)$ samples.',\n",
       "  'id': '6910',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual $\\\\|T_* v_\\\\pi - v_\\\\pi\\\\|_{1,\\\\nu}$ over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.',\n",
       "  'id': '6913',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness, or add prohibitive amounts of noise. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.',\n",
       "  'id': '6915',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.',\n",
       "  'id': '6921',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images.  But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution.  To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise.  Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise.  In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption.  On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.',\n",
       "  'id': '6923',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.',\n",
       "  'id': '6932',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.',\n",
       "  'id': '6935',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Population activity measurement by calcium imaging can be combined with cellular resolution optogenetic activity perturbations to enable the mapping of neural connectivity in vivo. This requires accurate inference of perturbed and unperturbed neural activity from calcium imaging measurements, which are noisy and indirect, and can also be contaminated by photostimulation artifacts. We have developed a new fully Bayesian approach to jointly inferring spiking activity and neural connectivity from in vivo all-optical perturbation experiments. In contrast to standard approaches that perform spike inference and analysis in two separate maximum-likelihood phases, our joint model is able to propagate uncertainty in spike inference to the inference of connectivity and vice versa. We use the framework of variational autoencoders to model spiking activity using discrete latent variables, low-dimensional latent common input, and sparse spike-and-slab generalized linear coupling between neurons. Additionally, we model two properties of the optogenetic perturbation: off-target photostimulation and photostimulation transients. Using this model, we were able to fit models on 30 minutes of data in just 10 minutes. We performed an all-optical circuit mapping experiment in primary visual cortex of the awake mouse, and use our approach to predict neural connectivity between excitatory neurons in layer 2/3. Predicted connectivity is sparse and consistent with known correlations with stimulus tuning, spontaneous correlation and distance.',\n",
       "  'id': '6940',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of ?what? and ?where?.  Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space.  Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations ? a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.\",\n",
       "  'id': '6942',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sums  problems. First, we show that perhaps surprisingly, the finite sum structure, by itself, is not sufficient for obtaining a complexity bound of $\\\\tilde{\\\\cO}((n+L/\\\\mu)\\\\ln(1/\\\\epsilon))$ for $L$-smooth and $\\\\mu$-strongly convex finite sums - one must also know exactly which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sums algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' complexity bound of $\\\\tilde{\\\\cO}((n+\\\\sqrt{n L/\\\\mu})\\\\ln(1/\\\\epsilon))$, unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing $L$-smooth and non-strongly convex finite sums, the optimal complexity bound is $\\\\tilde{\\\\cO}(n+L/\\\\epsilon)$, assuming that (on average) the same update rule is used for any iteration, and $\\\\tilde{\\\\cO}(n+\\\\sqrt{nL/\\\\epsilon})$, otherwise.\",\n",
       "  'id': '6945',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The problem of selecting the best $k$-element subset from a universe is involved in many applications. While previous studies assumed a noise-free environment or a noisy monotone submodular objective function, this paper considers a more realistic and general situation where the evaluation of a subset is a noisy monotone function (not necessarily submodular), with both multiplicative and additive noises. To understand the impact of the noise, we firstly show the approximation ratio of the greedy algorithm and POSS, two powerful algorithms for noise-free subset selection, in the noisy environments. We then propose to incorporate a noise-aware strategy into POSS, resulting in the new PONSS algorithm. We prove that PONSS can achieve a better approximation ratio under some assumption such as i.i.d. noise distribution. The empirical results on influence maximization and sparse regression problems show the superior performance of PONSS.',\n",
       "  'id': '6947',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that ``all local optima are (approximately) global optima'', and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult.   In this paper, we analyze the optimization landscape of the random over-complete  tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant $\\\\epsilon > 0$, among the set of points with function values $(1+\\\\epsilon)$-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem.   Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.\",\n",
       "  'id': '6956',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they  take into account different data modalities,  such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.',\n",
       "  'id': '6957',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"Cell assemblies, originally proposed by Donald Hebb (1949), are subsets of neurons firing in a temporally coordinated way that gives rise to repeated motifs supposed to underly neural representations and information processing. Although Hebb's original proposal dates back many decades, the detection of assemblies and their role in coding is still an open and current research topic, partly because simultaneous recordings from large populations of neurons became feasible only relatively recently. Most current and easy-to-apply computational techniques focus on the identification of strictly synchronously spiking neurons. In this paper we propose a new algorithm, based on sparse convolutional coding, for detecting recurrent motifs of arbitrary structure up to a given length. Testing of our algorithm on synthetically generated datasets shows that it outperforms established methods and accurately identifies the temporal structure of embedded assemblies, even when these contain overlapping neurons or when strong background noise is present. Moreover, exploratory analysis of experimental datasets from hippocampal slices and cortical neuron cultures have provided promising results.\",\n",
       "  'id': '6958',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We establish the consistency of an algorithm of Mondrian Forests~\\\\cite{lakshminarayanan2014mondrianforests,lakshminarayanan2016mondrianuncertainty}, a randomized classification algorithm that can be implemented online.  First, we amend the original Mondrian Forest algorithm proposed in~\\\\cite{lakshminarayanan2014mondrianforests}, that considers a \\\\emph{fixed} lifetime parameter.  Indeed, the fact that this parameter is fixed actually hinders statistical consistency of the original procedure.  Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters $\\\\lambda_n$, and uses an alternative updating rule, allowing to work also in an online fashion.   Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results~\\\\cite{arlot2014purf_bias} to an \\\\emph{arbitrary dimension}.',\n",
       "  'id': '6966',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Analysis of efficiency of outcomes in game theoretic settings has been a main item of study at the intersection of economics and computer science. The notion of the price of anarchy takes a worst-case stance to efficiency analysis, considering instance independent guarantees of efficiency. We propose a data-dependent analog of the price of anarchy that refines this worst-case assuming access to samples of strategic behavior. We focus on auction settings, where the latter is non-trivial due to the private information held by participants. Our approach to bounding the efficiency from data is robust to statistical errors and mis-specification. Unlike traditional econometrics, which seek to learn the private information of players from observed behavior and then analyze properties of the outcome, we directly quantify the inefficiency without going through the private information. We apply our approach to datasets from a sponsored search auction system and find empirical results that are a significant improvement over bounds from worst-case analysis.',\n",
       "  'id': '6967',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We give the first algorithm for kernel Nystrom approximation that runs in linear time in the number of training points and is provably accurate for all kernel matrices, without dependence on regularity or incoherence conditions. The algorithm projects the kernel onto a set of s landmark points sampled by their ridge leverage scores, requiring just O(ns) kernel evaluations and O(ns^2) additional runtime. While leverage score sampling has long been known to give strong theoretical guarantees for Nystrom approximation, by employing a fast recursive sampling scheme, our algorithm is the first to make the approach scalable. Empirically we show that it finds more accurate kernel approximations in less time than popular techniques such as classic Nystrom approximation and the random Fourier features method.',\n",
       "  'id': '6973',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory.',\n",
       "  'id': '6981',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].',\n",
       "  'id': '6993',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.',\n",
       "  'id': '6996',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP$\\\\subseteq$BPP, and only an exponential-time sublinear-regret algorithm has been found. In this paper, we introduce mild assumptions to solve the problem. Under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.',\n",
       "  'id': '6998',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks.  Examples include AdaGrad, RMSProp, and Adam. We show that for simple over-parameterized problems, adaptive methods often find drastically different solutions than vanilla stochastic gradient descent (SGD).  We construct an illustrative binary classification problem where the data is linearly separable, SGD achieves zero test error, and AdaGrad and Adam attain test errors arbitrarily close to 1/2.  We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.',\n",
       "  'id': '7003',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"There has been a resurgence of interest in multiagent reinforcement learning (MARL), due partly to the recent success of deep neural networks. The simplest form of MARL is independent reinforcement learning (InRL), where each agent treats all of its experience as part of its (non stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe a meta-algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game theoretic analysis to compute meta-strategies for policy selection. The meta-algorithm generalizes previous algorithms such as InRL, iterated best response, double oracle, and fictitious play. Then, we propose a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in three partially observable settings: gridworld coordination problems, emergent language games, and poker.\",\n",
       "  'id': '7007',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'This paper considers a number of related inverse filtering problems for hidden Markov models (HMMs). In particular, given a sequence of state posteriors and the system dynamics; i) estimate the corresponding sequence of observations, ii) estimate the observation likelihoods, and iii) jointly estimate the observation likelihoods and the observation sequence. We show how to avoid a computationally expensive mixed integer linear program (MILP) by exploiting the algebraic structure of the HMM filter using simple linear algebra operations, and provide conditions for when the quantities can be uniquely reconstructed. We also propose a solution to the more general case where the posteriors are noisily observed. Finally, the proposed inverse filtering algorithms are evaluated on real-world polysomnographic data used for automatic sleep segmentation.',\n",
       "  'id': '7008',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.',\n",
       "  'id': '7012',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"We propose a generic algorithmic building block to accelerate training of  machine learning models on heterogeneous compute systems. Our scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique is built upon novel theoretical insights regarding primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on a large-scale dataset exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.\",\n",
       "  'id': '7013',\n",
       "  'year': '2017'},\n",
       " {'abstract': \"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.\",\n",
       "  'id': '7017',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.',\n",
       "  'id': '7023',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We propose a novel method to {\\\\it directly} learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems.   The proposed training objective, which we derive via principled variational methods, encourages the transition operator to \"walk back\" (prefer to revert its steps) in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution. Source Code:http://github.com/anirudh9119/walkback_nips17',\n",
       "  'id': '7026',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to optimally deal with straggling workers. The proposed strategy, named as \\\\emph{polynomial codes}, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. This is the first code that achieves the optimal utilization of redundancy for tolerating stragglers or failures in distributed matrix multiplication. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently.   Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics including computation latency and communication load.  Moreover, we extend this code to distributed convolution and show its order-wise optimality.',\n",
       "  'id': '7027',\n",
       "  'year': '2017'},\n",
       " {'abstract': 'Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through  simulations on real-world federated datasets.',\n",
       "  'id': '7029',\n",
       "  'year': '2017'},\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for entry in res['hits']['hits']:\n",
    "    abstract_list.append(entry['_source'])\n",
    "\n",
    "abstract_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RAKE score': 43.5,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'performing principal component analysis {( pca )}',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'could represent proprioceptive feedback',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'also called relevance variable',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'perspicuous online learning rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'new learning rule makes',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'different principal components',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'theoretically founded method',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'information bottleneck optimization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'convergence properties feasible',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'common benchmark tasks',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.666666666666666,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'rather complex rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'additional target signal',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 6.5,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'theoretical analysis',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 6.5,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'principal components',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 6.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'learning rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'target signal',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.666666666666666,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'rule performs',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'suitable assumptions',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'sign changes',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'sensory modalities',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'primarily linearization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'preferentially extract',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'biological interpretation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3168', 'keyword': '07b }.', 'year': '2007'},\n",
       " {'RAKE score': 3.5, 'doc_id': '3168', 'keyword': 'y_t $.', 'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'spiking neurons',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'incoming signals',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.6666666666666665,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3168', 'keyword': 'y_t', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3168', 'keyword': 'signals', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3168', 'keyword': 'neurons', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'x', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'well', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'variation', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'transparency',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'top', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'simple', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'show', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'related', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'provides', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'proposed', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'previously',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'possible', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'klampfletal',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'input', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3168',\n",
       "  'keyword': 'furthermore',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'extracted', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'ensemble', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'derived', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'cite', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'applying', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3168', 'keyword': 'addition', 'year': '2007'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'observed blood oxygen level dependent',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 22.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'biologically motivated stochastic differential model',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'functional magnetic resonance imaging',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'difficult parameter estimation problem',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'practical approaches used',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'hemodynamic activity underlying',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'effective connectivity study',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.5,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'model poses',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.5,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'differential system',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'theoretically due',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'sparse matrices',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'space complexity',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'results demonstrate',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'particle filter',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'including use',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3172', 'keyword': 'fmri ).', 'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'computationally due',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'tractability',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'time', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'task', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'tackle', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'smoother', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'signal', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'parallelisation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'nonlinearity',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'neural', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'divergence',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'discuss', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'difficulties',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'construct', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'bold', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'approach', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3172',\n",
       "  'keyword': 'application',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3172', 'keyword': 'adapt', 'year': '2007'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'remove second order correlations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.833333333333334,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'based hebbian learning rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.833333333333334,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'online learning rule',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'neural mechanism responsible',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'independent component analysis',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'exploits delayed correlations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'detecting joint variations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.583333333333334,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'rule performs ica',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.25,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'algorithms performing ica',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'temporal correlations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.25,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'solving ica',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'powerful method',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'postsynaptic neurons',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'local rate',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'higher moments',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'firing rates',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'decouple signals',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'amplitude distribution',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.25, 'doc_id': '3174', 'keyword': 'ica', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'whitening', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'understanding',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'similar', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'signal', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'require', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'present', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'preprocessing',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'pre', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'paper', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'moreover', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3174',\n",
       "  'keyword': 'interested',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'input', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'data', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3174', 'keyword': 'consider', 'year': '2007'},\n",
       " {'RAKE score': 41.1,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'simultaneous subspace selection via linear discriminant analysis',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 15.600000000000001,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'simultaneous lda subspace selection',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 15.5,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'automatic parameter estimation procedure',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 13.75,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'specific kernel gram matrix',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 11.1,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'subspace selection procedure',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 10.1,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'iterative subspace selection',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'benchmark data sets',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.75,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'specified kernel matrices',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'diskmeans using kernels',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 7.6,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'subspace selection',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 7.2,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'popular clustering algorithms',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 6.7,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'discriminative clustering framework',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.75,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'kernel matrix',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.75, 'doc_id': '3176', 'keyword': 'kernel k', 'year': '2007'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'discriminative k',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.2,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'clustering formulation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.2,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'clustering algorithms',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'theoretical study',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'recently proposed',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'provides significant',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'presented theories',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'nonlinear extension',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'new insights',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'iterative nature',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'inherent relationship',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'favorable performance',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'equivalence relationship',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'empirical results',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'convex set',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'also analyzed',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'well understood',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'also present',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.5, 'doc_id': '3176', 'keyword': 'lda', 'year': '2007'},\n",
       " {'RAKE score': 2.2,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'clustering',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.0, 'doc_id': '3176', 'keyword': 'framework', 'year': '2007'},\n",
       " {'RAKE score': 2.0, 'doc_id': '3176', 'keyword': 'diskmeans', 'year': '2007'},\n",
       " {'RAKE score': 2.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'algorithms',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3176', 'keyword': 'well', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3176', 'keyword': 'present', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3176', 'keyword': 'nature', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'shown', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'show', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'several', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'propose', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'pre', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'paper', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'means', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'learning', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'incorporated',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'however', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'experiments',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'evaluated', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'equivalent',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'due', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'connection',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'comparison',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3176',\n",
       "  'keyword': 'collection',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'based', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3176', 'keyword': 'algorithm', 'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'standard supervised learning',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'observable side information',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'sample complexity bound',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'much better ).',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'complexity term',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'time horizon',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'regret scales',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'regret incurred',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'hypothesis class',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'following properties',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'armed bandits',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3178', 'keyword': '^{ 2', 'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3178', 'keyword': '^{ 1', 'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'present epoch',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5, 'doc_id': '3178', 'keyword': '3 })$', 'year': '2007'},\n",
       " {'RAKE score': 2.0, 'doc_id': '3178', 'keyword': 'better', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3178', 'keyword': 'epoch', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3178', 'keyword': '3', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'sometimes', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'necessary', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'multi', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'knowledge', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'greedy', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3178',\n",
       "  'keyword': 'controlled',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3178', 'keyword': 'algorithm', 'year': '2007'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'derived via block coordinate descent',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.5,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'novel message passing algorithm',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'cluster based potentials',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'exact map solution',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'world problems',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'various settings',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'tunable parameters',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'tree weights',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3200', 'keyword': 'step size', 'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'previous approaches',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'map problem',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'lp relaxation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'graphical models',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'compares favorably',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'always converges',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'also describe',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'unlike max',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'new method',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.5, 'doc_id': '3200', 'keyword': 'algorithm', 'year': '2007'},\n",
       " {'RAKE score': 2.0, 'doc_id': '3200', 'keyword': 'map', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3200', 'keyword': 'method', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3200', 'keyword': 'max', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'tested', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'synthetic', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'structure', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'similar', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'require', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'real', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'proven', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'product', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'present', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'generalization',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'find', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3200', 'keyword': 'dual', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3200',\n",
       "  'keyword': 'approximating',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'factorial hmm based model',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'low level primitive model',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'higher level timing model',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'explicit timing model produces',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.333333333333334,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'coupled model also captures',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'modelled using hmm architectures',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'spike timing jitter',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'robotic control applications',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'produce good reconstructions',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'primitive timing space',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'timing code provides',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'analyse handwriting data',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'primitive activations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'done using',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'characters modelled',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'representation provides',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.333333333333334,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'also desirable',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'variance profile',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'spikes corresponding',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'scribbling style',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'compact representation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'biological movements',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'better understanding',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.75,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'shared primitives',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.75,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'primitives provide',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.75,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'motion primitives',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'movement without',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'biological movement',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.0, 'doc_id': '3204', 'keyword': 'timing', 'year': '2007'},\n",
       " {'RAKE score': 2.3333333333333335,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'also',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'handwriting',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.75,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'primitives',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'movement',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'use', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'timings', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'sub', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'show', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'shape', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'represented',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'output', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'inference', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'generating',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'gain', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3204',\n",
       "  'keyword': 'distribution',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'dataset', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'coupling', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'built', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'blocks', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'allowing', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3204', 'keyword': 'accounted', 'year': '2007'},\n",
       " {'RAKE score': 33.666666666666664,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'record excellent results using either raw',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 22.333333333333332,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'unusual among sequence labelling tasks',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'recurrent neural network trained',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 15.333333333333332,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'conventional sequence labelling algorithms',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.166666666666666,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'directly transcribing raw',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'probabilistic language model',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'obtain inputs suitable',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.666666666666666,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'line handwriting recognition',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'many pen locations',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 7.916666666666666,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'line handwriting data',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 7.333333333333333,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'sequence labelling',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.916666666666666,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'raw data',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.666666666666666,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'line database',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'recorded directly',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.25,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'processed data',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.25,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'observed data',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'well outperforming',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'underlying generator',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'system consists',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'system capable',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'benchmark hmm',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'sophisticated pre',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.0, 'doc_id': '3213', 'keyword': 'pen', 'year': '2007'},\n",
       " {'RAKE score': 1.5, 'doc_id': '3213', 'keyword': 'pre', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'unconstrained',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'spread', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'required', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'processing',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'paper', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'movement', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'letter', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'interpret', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'however', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'hmms', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'experiments',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'e', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'difficult', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'describe', 'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3213',\n",
       "  'keyword': 'consequence',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'combined', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3213', 'keyword': 'cases', 'year': '2007'},\n",
       " {'RAKE score': 31.333333333333336,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'test samples follow different input distributions',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'natural model selection procedure',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 14.166666666666666,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'training input densities ).',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 12.166666666666666,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'direct importance estimation method',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 10.833333333333334,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'test input densities',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'direct importance estimation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.166666666666666,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'input density estimates',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'since density estimation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'perform poorly especially',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'maximum likelihood estimator',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'likelihood terms need',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'high dimensional cases',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'covariate shift }),',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'covariate shift adaptation',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 8.666666666666666,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'recently developed method',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 7.666666666666666,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'first estimate training',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 5.166666666666666,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'density estimates',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'weighted according',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'tuning parameters',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'situation called',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'simulations illustrate',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'objectively optimized',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3248', 'keyword': 'key tasks', 'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'kernel width',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'hard problem',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 4.0, 'doc_id': '3248', 'keyword': 'e .,', 'year': '2007'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'accurately estimating',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'naive approach',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'approach tends',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'regaining consistency',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 3.3333333333333335,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'test',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.6666666666666665,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'training',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.6666666666666665,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'method',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.6666666666666665,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'importance',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 2.0, 'doc_id': '3248', 'keyword': 'estimate', 'year': '2007'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'approach',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.5,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'consistency',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '3248',\n",
       "  'keyword': 'usefulness',\n",
       "  'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'thus', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'require', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'ratio', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'propose', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'paper', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'one', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'lose', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'log', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'known', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'however', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'equipped', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'emph', 'year': '2007'},\n",
       " {'RAKE score': 1.0, 'doc_id': '3248', 'keyword': 'advantage', 'year': '2007'},\n",
       " {'RAKE score': 49.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'existing neuroimaging methods typically perform either discovery',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'imaging neuroscience links human behavior',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 23.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'neural correlates underlying larger sets',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 21.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'mental tasks necessitates adequate representations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'unified statistical learning problem',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'unknown neural structure',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'neural structure associated',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'interpretable neural models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'multinomial logistic regression',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'blend representation modelling',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'mental tasks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'psychological tasks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'therefore propose',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'task classification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'reference dataset',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'factored coefficients',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'brain biology',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'better generalization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'approach yields',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'testing hypotheses',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'increasing datasets',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5646', 'keyword': 'testing', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5646', 'keyword': 'datasets', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'well', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'show', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'observations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'introduced',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'however', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'ever', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'coupled', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'constrained',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5646',\n",
       "  'keyword': 'autoencoder',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'aspects', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5646', 'keyword': 'accurate', 'year': '2015'},\n",
       " {'RAKE score': 33.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': '1 ), would bring great benefits',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'two possible values',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'replacing many multiply',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'greater computational speed',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'obtain near state',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'best results obtained',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'specialized dl hardware',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 7.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'large training sets',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 7.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'deep neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 7.333333333333334,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'e ., weights',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'large models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5, 'doc_id': '5647', 'keyword': 'dl ).', 'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'deep learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'dedicated hardware',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'art results',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'achieved state',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.333333333333334,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'stored weights',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.333333333333334,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'binary weights',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'wide range',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5647', 'keyword': 'test time', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'simple accumulations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'retaining precision',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'much interest',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'invariant mnist',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'hungry components',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'gpus enabled',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'faster computation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'dropout schemes',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'digital implementation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'consumer applications',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'backward propagations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'accumulate operations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'introduce binaryconnect',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'binaryconnect acts',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'power devices',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5, 'doc_id': '5647', 'keyword': '1', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5647', 'keyword': 'training', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5647', 'keyword': 'e', 'year': '2015'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'binaryconnect',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5647', 'keyword': 'power', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'tasks', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'svhn', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'space', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'show', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'result', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'research', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'regularizer',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'progress', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'permutation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'past', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'multipliers',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'method', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'low', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'likely', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'like', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'gradients', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'g', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'future', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'forward', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'dnn', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'development',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'crucial', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'constrained',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'consists', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': 'cifar', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'breakthroughs',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5647',\n",
       "  'keyword': 'accumulated',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5647', 'keyword': '10', 'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'discrete fourier transforms provide',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'train convolutional neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'methods achieve competitive results',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 13.75,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'spectral domain also provides',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.25,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'employ spectral representations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.25,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'coefficient spectral parameterization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'significantly faster convergence',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'performs dimensionality reduction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'information per parameter',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'greatly facilitates optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'approach preserves considerably',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'propose spectral pooling',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'popular cnn configurations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.25,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'pooling output dimensionality',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'underlying model unchanged',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'representation also enables',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'frequency domain',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'convolutional filters',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'enables flexibility',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'cnn design',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.25,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'pooling strategies',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'without using',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'stochastic regularization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'significant speedup',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'randomized modification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'powerful representation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5649', 'keyword': 'new form', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'deep learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5649', 'keyword': 'cnns ).', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'approximation tasks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'efficient computation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.5, 'doc_id': '5649', 'keyword': 'results', 'year': '2015'},\n",
       " {'RAKE score': 2.25, 'doc_id': '5649', 'keyword': 'pooling', 'year': '2015'},\n",
       " {'RAKE score': 2.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'representation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5649', 'keyword': 'model', 'year': '2015'},\n",
       " {'RAKE score': 1.5,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'computation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'work', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'variety', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'truncating',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'training', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'show', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'resolution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'observe', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'number', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'max', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'leaves', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'leads', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'introduce', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'innovations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'first', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'finally', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'effectiveness',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'dropout', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'demonstrate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'convolutions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'complex', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'classification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'choice', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5649', 'keyword': 'beyond', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5649',\n",
       "  'keyword': 'advantages',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'using recurrent neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 10.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'neural networks outperform',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'significantly unsolved problem',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'promising new line',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'flexible functional form',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'computer supported education',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'allowing straightforward interpretation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'real student data',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'model student learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'complex student interactions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'human domain knowledge',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'results suggest',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'knowledge tracing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'important advantages',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'explicit encoding',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'current state',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'capture substantially',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'machine models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'art methods',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.5, 'doc_id': '5654', 'keyword': 'student', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5654', 'keyword': 'knowledge', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5654', 'keyword': 'models', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5654', 'keyword': 'art', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'structure', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'show', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'research', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'require', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'prediction',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'paper', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'interact', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'far', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'family', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'explore', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'established',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'discovery', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'curriculum',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5654',\n",
       "  'keyword': 'coursework',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5654', 'keyword': 'benefit', 'year': '2015'},\n",
       " {'RAKE score': 38.5,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'statistical model criticism using maximum mean discrepancy',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 13.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'mmd two sample tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 12.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'model criticism require',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 10.5,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'two sample tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'restricted boltzmann machines',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'gaussian process regression',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'deep belief networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'statistical model',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'therefore automatically select',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'witness function',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'typical approaches',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'possible statistics',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'measure discrepancies',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'large space',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'instead constructed',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'exploratory approach',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'discrepancy',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'analytic maximisation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'synthetic data',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'real data',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'selected statistic',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'models fail',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.5, 'doc_id': '5657', 'keyword': 'mmd', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5657', 'keyword': 'select', 'year': '2015'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'data',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5657', 'keyword': 'statistic', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5657', 'keyword': 'models', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'ways', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'used', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'trained', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'shows', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'propose', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'properties',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'procedure', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'practitioner',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'misrepresents',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'identify', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5657',\n",
       "  'keyword': 'demonstrate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'capture', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'called', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'assessed', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5657', 'keyword': 'apply', 'year': '2015'},\n",
       " {'RAKE score': 32.5,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'displaying calibrated confidence measures --- probabilities',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'users may issue many types',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 21.8,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'structured prediction presents new challenges',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'handle various subtleties pertaining',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 10.5,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'true frequency ---',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.8,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'structured prediction problems',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'obtaining high accuracy',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'g ., marginals',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'simple recalibration method',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'optical character recognition',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'predict probabilities',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.3,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'structured recalibration',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.8,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'structured setting',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.8,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'structured output',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'speech recognition',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'world datasets',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'three real',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'probability queries',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'output space',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'medical diagnosis',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'features appropriate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'facing applications',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'binary classifier',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'user', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'trains', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'range', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'provide', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'notion', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'large', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'interested',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'interest', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'important', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'extend', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'explore', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'efficacy', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5658', 'keyword': 'e', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'demonstrate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'correspond',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5658',\n",
       "  'keyword': 'calibration',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'multidimensional recurrent neural networks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 13.333333333333334,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'successfully trained using hf',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'fisher information matrix',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'connectionist temporal classification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'using hessian',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.333333333333334,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'applying hf',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'sequence labeling',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'handwriting recognition',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'em algorithm',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'convex approximation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5664', 'keyword': '15 layers', 'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'remarkable performance',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'deeper network',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5, 'doc_id': '5664', 'keyword': 'ctc poses', 'year': '2015'},\n",
       " {'RAKE score': 3.166666666666667,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'improved performance',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.3333333333333335,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'hf',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'performance',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5664', 'keyword': 'network', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5664', 'keyword': 'improved', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5664', 'keyword': 'ctc', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'utilized', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'speech', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'solution', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'shown', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'resulting', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'relationship',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'problem', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'overcome', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'objective', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'non', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'mdrnns', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'mdrnn', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'learning', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'increasing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'given', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'free', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'formulated',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'discussed', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5664',\n",
       "  'keyword': 'difficulty',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'depth', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'convexity', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5664', 'keyword': 'area', 'year': '2015'},\n",
       " {'RAKE score': 30.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'augmented variational family guarantees better approximations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'general variational inference method',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'original variational distribution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'copula variational inference',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'method uses copulas',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 7.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'preserves dependency among',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'augmented distribution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'structured approximations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'inference procedure',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'dependency among',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'currently uses',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'copulas model',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'structured approach',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'stochastic optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'reduces bias',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'many advantages',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'local optima',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'less sensitive',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'latent variables',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'helps characterize',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'distributions used',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.5, 'doc_id': '5669', 'keyword': 'inference', 'year': '2015'},\n",
       " {'RAKE score': 2.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'dependency',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'thus', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'strategy', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'scalable', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'posterior', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'mean', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'interpret', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'hyperparameters',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'generic', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5669',\n",
       "  'keyword': 'furthermore',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'field', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'families', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'develop', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'captured', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'augment', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5669', 'keyword': 'applied', 'year': '2015'},\n",
       " {'RAKE score': 24.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'complex robot ball throwing task',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'simulated planar robot tasks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'use information theoretic constraints',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.333333333333334,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'proposed method considerably outperforms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.25,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'based stochastic search approach',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 13.833333333333334,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'highly noisy objective functions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'recently also gained',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'may converge prematurely',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'inaccurate optimum introduced',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'avoid premature convergence',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.75,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'stochastic search algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'old data distribution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'modal optimization functions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'box optimization methods',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 7.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'quadratic surrogate models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.25,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'search distribution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.75,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'policy search',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'quadratic approximation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'learned models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'box optimizers',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'algorithms require',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.333333333333334,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'objective function',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'standard uni',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'scale poorly',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'problem dimension',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'operations research',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'new method',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'machine learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'learn simple',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'greedily exploit',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'general black',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'existing approaches',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5672', 'keyword': 'art black', 'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'new surrogate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.5, 'doc_id': '5672', 'keyword': 'use', 'year': '2015'},\n",
       " {'RAKE score': 2.3333333333333335,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'objective',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.3333333333333335,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'method',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5672', 'keyword': 'surrogate', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5672', 'keyword': 'modal', 'year': '2015'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'new',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'yet', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'sustain', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'state', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'quality', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'problems', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'multi', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'misled', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'maximizing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'lot', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'limited', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'introduce', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'instead', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'generality',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'exploration',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'evaluations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'ease', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'due', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'distance', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'compare', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'bound', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'attention', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'alleviate', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'algorithm', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'affected', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5672',\n",
       "  'keyword': 'additionally',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5672', 'keyword': 'able', 'year': '2015'},\n",
       " {'RAKE score': 37.833333333333336,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'first test uses smoothed empirical characteristic functions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 17.5,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'second uses distribution embeddings',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'world testing problems demonstrate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 15.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'reproducing kernel hilbert space',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.333333333333332,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'empirical characteristic functions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 10.333333333333332,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'analytic functions representing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'randomly chosen locations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'low order statistics',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'detected almost surely',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'better outright power',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.392857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'time tests based',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.25,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'time kernel',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'better power',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.392857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'time tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.25,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'time tradeoff',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.142857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'two tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.142857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'tests give',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.142857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'sample tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.142857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'new tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.142857142857142,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'based tests',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5685', 'keyword': 'smoothed', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'sample size',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'previous linear',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'performance advantage',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'nonparametric two',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'much faster',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'high dimensions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'finite number',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'expensive quadratic',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'energy distance',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'current state',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'cost linear',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'competing approaches',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'challenging real',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'artificial benchmarks',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'art quadratic',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'analyticity implies',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'retained even',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'larger class',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'distributions may',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5685', 'keyword': 'based', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5685', 'keyword': 'even', 'year': '2015'},\n",
       " {'RAKE score': 1.5,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'distributions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5685', 'keyword': 'class', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'represent', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'propose', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'observable',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'non', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'given', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'frequencies',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'experiments',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'ensemble', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'distances', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'differences',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'difference',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'consistent',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5685', 'keyword': 'cases', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5685',\n",
       "  'keyword': 'alternatives',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 25.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': '$\\\\# p $- complete problem',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'online mistake bound framework',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'ising model isomorphic',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'maximum marginal probability',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'cluster bound',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.666666666666667,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'online algorithm',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'probability distribution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'sequentially predict',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'sequential guarantee',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'prior results',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'motivates us',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'cumulative time',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'additional connectivity',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5690', 'keyword': '1 ].', 'year': '2015'},\n",
       " {'RAKE score': 3.666666666666667,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'algorithm exploits',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'vertices seen',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'unfortunately based',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'tree matching',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'general graph',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.6666666666666667,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'algorithm',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5690', 'keyword': 'vertices', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5690', 'keyword': 'tree', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5690', 'keyword': 'graph', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5690', 'keyword': 'based', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'zero', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'underpinning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'temperature',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'size', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'respect', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'quadratic', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'provide', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'predicting',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'per', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'optimal', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'limit', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'labels', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'label', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'give', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'far', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'efficient', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'develop', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'design', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'computing', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5690', 'keyword': 'classify', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'classifications',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5690',\n",
       "  'keyword': 'classification',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 31.333333333333332,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'also achieve theoretical approximation guarantees comparable',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 23.25,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'investigate two novel mixed robust',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 22.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'deep neural network objectives ),',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 21.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'world problems involving data partitioning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 21.333333333333332,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'also purely unsupervised image segmentation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 20.583333333333332,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'problems generalize purely robust instances',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 20.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'ml ), including data partitioning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 19.96666666666667,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'moreover provide new scalable algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 19.833333333333336,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'case submodular data partitioning problems',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.583333333333336,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'collectively call submodular partitioning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 15.3,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'proposing several new algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.333333333333334,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'min submodular fair allocation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 12.833333333333334,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'max submodular load balancing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 12.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'tight approximation guarantees',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.833333333333334,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'submodular multiway partition',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.833333333333334,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'submodular welfare problem',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.333333333333334,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'average case instances',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 7.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'image segmentation',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'case objectives',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.333333333333334,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'case instances',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.333333333333333,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'also average',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.25,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'data clustering',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.0, 'doc_id': '5706', 'keyword': 'slb ),', 'year': '2015'},\n",
       " {'RAKE score': 6.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'load balancing',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'world applications',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'including greedy',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'distributed ml',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.25,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'robust versions',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'namely max',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.8,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'resultant algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.8,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'relaxation algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.666666666666666,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'generally scalable',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'theory community',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5706', 'keyword': 'smp ).', 'year': '2015'},\n",
       " {'RAKE score': 4.0, 'doc_id': '5706', 'keyword': 'problems', 'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'present paper',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'many applications',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'machine learning',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'large datasets',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'existing work',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'empirically demonstrate',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'distributed optimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'additive combinations',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.5,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'large real',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 3.25, 'doc_id': '5706', 'keyword': 'robust', 'year': '2015'},\n",
       " {'RAKE score': 2.8,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.6666666666666665,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'scalable',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 2.5, 'doc_id': '5706', 'keyword': 'min', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5706', 'keyword': 'problem', 'year': '2015'},\n",
       " {'RAKE score': 2.0, 'doc_id': '5706', 'keyword': 'average', 'year': '2015'},\n",
       " {'RAKE score': 1.5, 'doc_id': '5706', 'keyword': 'real', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'swp', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'studied', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'state', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'show', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'sfa', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'scale', 'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'minorization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'minimization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'maximization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0,\n",
       "  'doc_id': '5706',\n",
       "  'keyword': 'majorization',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'gap', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'focused', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'emph', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'efficacy', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'convex', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'contrasts', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'bridge', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'art', 'year': '2015'},\n",
       " {'RAKE score': 1.0, 'doc_id': '5706', 'keyword': 'apply', 'year': '2015'},\n",
       " {'RAKE score': 19.75,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'computationally efficient differentially private estimators',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'simultaneously achieve low error',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 16.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'large public data set',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 14.75,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'exist private estimation algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.5,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'design efficient algorithms',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'unknown probability distribution',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'total variation norm',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'synthetic mixture models',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'natural distribution families',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 9.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'guaranteeing differential privacy',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'yields near sample',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 8.5,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'theoretical results show',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 6.75,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'private estimators',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 5.25,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'private counterparts',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'theoretical guarantees',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.5,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'sample size',\n",
       "  'year': '2015'},\n",
       " {'RAKE score': 4.0,\n",
       "  'doc_id': '5713',\n",
       "  'keyword': 'wide variety',\n",
       "  'year': '2015'},\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_list = []\n",
    "\n",
    "for doc in abstract_list:\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(doc['abstract'])\n",
    "    keywords = rake.get_ranked_phrases_with_scores()\n",
    "    for word in keywords:\n",
    "        keywords_list.append({\n",
    "            'doc_id': doc['id'],\n",
    "            'year': doc['year'],\n",
    "            'keyword': word[1],\n",
    "            'RAKE score': word[0]\n",
    "        })\n",
    "\n",
    "keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RAKE_abstract_keywords.csv', 'w', newline = '') as f:\n",
    "    header_present = False\n",
    "    for doc in keywords_list:\n",
    "        if not header_present:\n",
    "            w = csv.DictWriter(f, doc.keys())\n",
    "            w.writeheader()\n",
    "            header_present = True\n",
    "        w.writerow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
